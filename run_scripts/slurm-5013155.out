>>> RESTORING FROM:  /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-18_09-40-06a11tj3zz/checkpoint_650
== Status ==
Memory usage on this node: 15.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    650 |          88936.7 | 62400000 |   110.71 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m 2021-11-19 18:00:18,423	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=27065)[0m 2021-11-19 18:00:18,439	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=27065)[0m 2021-11-19 18:02:19,536	INFO trainable.py:180 -- _setup took 121.113 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=27065)[0m 2021-11-19 18:02:19,537	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=27065)[0m 2021-11-19 18:02:19,537	WARNING util.py:37 -- Install gputil for GPU system monitoring.
[2m[36m(pid=27065)[0m 2021-11-19 18:02:22,703	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=27065)[0m 2021-11-19 18:02:22,703	INFO trainable.py:423 -- Restored on 172.17.8.4 from checkpoint: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-18_09-40-06a11tj3zz/tmp0fnkpzlhrestore_from_object/checkpoint-650
[2m[36m(pid=27065)[0m 2021-11-19 18:02:22,703	INFO trainable.py:430 -- Current state after restoring: {'_iteration': 650, '_timesteps_total': 62400000, '_time_total': 88936.678129673, '_episodes_total': 62400}
== Status ==
Memory usage on this node: 20.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    650 |          88936.7 | 62400000 |   110.71 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 8.46875
    apples_agent-0_min: 0
    apples_agent-1_max: 24
    apples_agent-1_mean: 3.09375
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 27.010416666666668
    apples_agent-2_min: 12
    apples_agent-3_max: 18
    apples_agent-3_mean: 7.072916666666667
    apples_agent-3_min: 2
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.416666666666666
    apples_agent-4_min: 2
    apples_agent-5_max: 18
    apples_agent-5_mean: 3.78125
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 1.78125
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 273
    cleaning_beam_agent-1_mean: 221.9375
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 6.458333333333333
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 13.854166666666666
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 22
    cleaning_beam_agent-4_mean: 7.65625
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.0625
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-05-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 143.0
  episode_reward_mean: 114.39583333333333
  episode_reward_min: 81.0
  episodes_this_iter: 96
  episodes_total: 62496
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 22478.483
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2720389664173126
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006315164500847459
        model: {}
        policy_loss: -0.001992921344935894
        total_loss: -0.002229263074696064
        vf_explained_var: 0.003824368119239807
        vf_loss: 1.1614055633544922
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25379061698913574
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006987384986132383
        model: {}
        policy_loss: -0.0018059993162751198
        total_loss: -0.0020610149949789047
        vf_explained_var: 0.08931048214435577
        vf_loss: 0.519105076789856
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36229440569877625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009199470514431596
        model: {}
        policy_loss: -0.0018746154382824898
        total_loss: -0.001954474486410618
        vf_explained_var: 0.0428583025932312
        vf_loss: 3.7379183769226074
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5841270685195923
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009779112879186869
        model: {}
        policy_loss: -0.002091906266286969
        total_loss: -0.002793741412460804
        vf_explained_var: 0.0032059848308563232
        vf_loss: 1.3064730167388916
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4196796715259552
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005641450406983495
        model: {}
        policy_loss: -0.0015675262548029423
        total_loss: -0.002091832458972931
        vf_explained_var: 0.006801575422286987
        vf_loss: 1.0150004625320435
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5953621864318848
        entropy_coeff: 0.0017600000137463212
        kl: 0.000788696575909853
        model: {}
        policy_loss: -0.001877342350780964
        total_loss: -0.0026489882729947567
        vf_explained_var: 0.014804437756538391
        vf_loss: 1.1845076084136963
    load_time_ms: 18400.235
    num_steps_sampled: 62496000
    num_steps_trained: 62496000
    sample_time_ms: 120912.643
    update_time_ms: 4117.187
  iterations_since_restore: 1
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.819999999999997
    ram_util_percent: 12.34078431372549
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 18.0
    agent-2: 57.0
    agent-3: 33.0
    agent-4: 26.0
    agent-5: 26.0
  policy_reward_mean:
    agent-0: 16.166666666666668
    agent-1: 8.052083333333334
    agent-2: 41.302083333333336
    agent-3: 18.614583333333332
    agent-4: 14.291666666666666
    agent-5: 15.96875
  policy_reward_min:
    agent-0: 6.0
    agent-1: 0.0
    agent-2: 23.0
    agent-3: 9.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.00481076483484
    mean_inference_ms: 16.497897696899965
    mean_processing_ms: 72.9574986469575
  time_since_restore: 169.25016498565674
  time_this_iter_s: 169.25016498565674
  time_total_s: 89105.92829465866
  timestamp: 1637363118
  timesteps_since_restore: 96000
  timesteps_this_iter: 96000
  timesteps_total: 62496000
  training_iteration: 651
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    651 |          89105.9 | 62496000 |  114.396 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 6.94
    apples_agent-0_min: 1
    apples_agent-1_max: 10
    apples_agent-1_mean: 2.99
    apples_agent-1_min: 0
    apples_agent-2_max: 61
    apples_agent-2_mean: 26.99
    apples_agent-2_min: 14
    apples_agent-3_max: 29
    apples_agent-3_mean: 7.74
    apples_agent-3_min: 1
    apples_agent-4_max: 24
    apples_agent-4_mean: 9.14
    apples_agent-4_min: 2
    apples_agent-5_max: 26
    apples_agent-5_mean: 3.85
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.54
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 267
    cleaning_beam_agent-1_mean: 222.3
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 6.05
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 13.53
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 24
    cleaning_beam_agent-4_mean: 8.58
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 3.62
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-07-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 161.0
  episode_reward_mean: 111.1
  episode_reward_min: 72.0
  episodes_this_iter: 96
  episodes_total: 62592
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 17185.447
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27567580342292786
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006168466061353683
        model: {}
        policy_loss: -0.0018570375395938754
        total_loss: -0.0021824475843459368
        vf_explained_var: 0.002134263515472412
        vf_loss: 0.980938196182251
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25278809666633606
        entropy_coeff: 0.0017600000137463212
        kl: 0.000582225969992578
        model: {}
        policy_loss: -0.0014179302379488945
        total_loss: -0.0017507290467619896
        vf_explained_var: 0.07781060039997101
        vf_loss: 0.5388807654380798
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3600755035877228
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010774056427180767
        model: {}
        policy_loss: -0.0018546602223068476
        total_loss: -0.0020288913510739803
        vf_explained_var: 0.04101623594760895
        vf_loss: 3.5176217555999756
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5764847993850708
        entropy_coeff: 0.0017600000137463212
        kl: 0.00128792820032686
        model: {}
        policy_loss: -0.0016814051195979118
        total_loss: -0.0024314902257174253
        vf_explained_var: -0.002905517816543579
        vf_loss: 1.3572953939437866
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4238356947898865
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008260111790150404
        model: {}
        policy_loss: -0.0015589436516165733
        total_loss: -0.002125069033354521
        vf_explained_var: 0.013109579682350159
        vf_loss: 0.972243070602417
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5912572145462036
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017676413990557194
        model: {}
        policy_loss: -0.001894630491733551
        total_loss: -0.0026422832161188126
        vf_explained_var: 0.009244203567504883
        vf_loss: 1.161941647529602
    load_time_ms: 16528.841
    num_steps_sampled: 62592000
    num_steps_trained: 62592000
    sample_time_ms: 120260.525
    update_time_ms: 2068.311
  iterations_since_restore: 2
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.714832535885165
    ram_util_percent: 13.866507177033492
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 17.0
    agent-2: 56.0
    agent-3: 35.0
    agent-4: 25.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 14.42
    agent-1: 7.81
    agent-2: 41.33
    agent-3: 18.69
    agent-4: 13.35
    agent-5: 15.5
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 25.0
    agent-3: 9.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.10756912387499
    mean_inference_ms: 15.804528039789039
    mean_processing_ms: 73.12941809035622
  time_since_restore: 315.5350742340088
  time_this_iter_s: 146.28490924835205
  time_total_s: 89252.21320390701
  timestamp: 1637363264
  timesteps_since_restore: 192000
  timesteps_this_iter: 96000
  timesteps_total: 62592000
  training_iteration: 652
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    652 |          89252.2 | 62592000 |    111.1 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 6.2
    apples_agent-0_min: 0
    apples_agent-1_max: 40
    apples_agent-1_mean: 3.64
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 27.24
    apples_agent-2_min: 14
    apples_agent-3_max: 47
    apples_agent-3_mean: 7.17
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 8.76
    apples_agent-4_min: 1
    apples_agent-5_max: 15
    apples_agent-5_mean: 3.57
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 27
    cleaning_beam_agent-0_mean: 1.92
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 271
    cleaning_beam_agent-1_mean: 223.64
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 6.01
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 13.04
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 32
    cleaning_beam_agent-4_mean: 7.59
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.82
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-10-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 170.0
  episode_reward_mean: 111.09
  episode_reward_min: 56.0
  episodes_this_iter: 96
  episodes_total: 62688
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 15426.535
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26646119356155396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007249390473589301
        model: {}
        policy_loss: -0.0016645067371428013
        total_loss: -0.001996817532926798
        vf_explained_var: 0.01239582896232605
        vf_loss: 1.0041346549987793
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2536778450012207
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011171861551702023
        model: {}
        policy_loss: -0.0016731340438127518
        total_loss: -0.002008602023124695
        vf_explained_var: 0.07416193187236786
        vf_loss: 0.55148845911026
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36343950033187866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010667587630450726
        model: {}
        policy_loss: -0.001656165113672614
        total_loss: -0.001881229691207409
        vf_explained_var: 0.03055146336555481
        vf_loss: 3.6125259399414062
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5764113664627075
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013450197875499725
        model: {}
        policy_loss: -0.0017372171860188246
        total_loss: -0.0025426039937883615
        vf_explained_var: 0.015224158763885498
        vf_loss: 1.4184558391571045
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4243910014629364
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006223389646038413
        model: {}
        policy_loss: -0.0013780416920781136
        total_loss: -0.001988510601222515
        vf_explained_var: 0.01311466097831726
        vf_loss: 1.053416132926941
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5752962827682495
        entropy_coeff: 0.0017600000137463212
        kl: 0.001618255628272891
        model: {}
        policy_loss: -0.001794799929484725
        total_loss: -0.002621941501274705
        vf_explained_var: 0.009925246238708496
        vf_loss: 1.0446531772613525
    load_time_ms: 15867.59
    num_steps_sampled: 62688000
    num_steps_trained: 62688000
    sample_time_ms: 120425.474
    update_time_ms: 1384.423
  iterations_since_restore: 3
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.014761904761908
    ram_util_percent: 13.907142857142853
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 18.0
    agent-2: 67.0
    agent-3: 34.0
    agent-4: 30.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 14.54
    agent-1: 8.06
    agent-2: 41.45
    agent-3: 18.09
    agent-4: 14.17
    agent-5: 14.78
  policy_reward_min:
    agent-0: 3.0
    agent-1: 2.0
    agent-2: 18.0
    agent-3: 6.0
    agent-4: 4.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.256022574030553
    mean_inference_ms: 15.603544010910088
    mean_processing_ms: 73.42523067692342
  time_since_restore: 462.86975955963135
  time_this_iter_s: 147.33468532562256
  time_total_s: 89399.54788923264
  timestamp: 1637363412
  timesteps_since_restore: 288000
  timesteps_this_iter: 96000
  timesteps_total: 62688000
  training_iteration: 653
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    653 |          89399.5 | 62688000 |   111.09 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 7.25
    apples_agent-0_min: 2
    apples_agent-1_max: 24
    apples_agent-1_mean: 3.24
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 26.8
    apples_agent-2_min: 14
    apples_agent-3_max: 45
    apples_agent-3_mean: 8.47
    apples_agent-3_min: 1
    apples_agent-4_max: 25
    apples_agent-4_mean: 9.79
    apples_agent-4_min: 4
    apples_agent-5_max: 16
    apples_agent-5_mean: 3.68
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 2.05
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 259
    cleaning_beam_agent-1_mean: 217.11
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 6.18
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 11.85
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 21
    cleaning_beam_agent-4_mean: 7.77
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.93
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-12-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 165.0
  episode_reward_mean: 112.23
  episode_reward_min: 58.0
  episodes_this_iter: 96
  episodes_total: 62784
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 14534.322
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2632882595062256
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008479704847559333
        model: {}
        policy_loss: -0.0015327613800764084
        total_loss: -0.0018705204129219055
        vf_explained_var: 0.01024620234966278
        vf_loss: 1.0442962646484375
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24876518547534943
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009376477100886405
        model: {}
        policy_loss: -0.0015648119151592255
        total_loss: -0.0019274784717708826
        vf_explained_var: 0.051226601004600525
        vf_loss: 0.5172143578529358
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3582058846950531
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009365617297589779
        model: {}
        policy_loss: -0.0017433406319469213
        total_loss: -0.0019900721963495016
        vf_explained_var: 0.029917463660240173
        vf_loss: 3.602994918823242
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5752115249633789
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012945157941430807
        model: {}
        policy_loss: -0.0015603606589138508
        total_loss: -0.0024109072983264923
        vf_explained_var: 0.007479459047317505
        vf_loss: 1.2946118116378784
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42364591360092163
        entropy_coeff: 0.0017600000137463212
        kl: 0.001279086573049426
        model: {}
        policy_loss: -0.0014413350727409124
        total_loss: -0.0020517881494015455
        vf_explained_var: 0.012254506349563599
        vf_loss: 1.0318756103515625
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.573627233505249
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017625641776248813
        model: {}
        policy_loss: -0.0016972573939710855
        total_loss: -0.002545742318034172
        vf_explained_var: 0.009952321648597717
        vf_loss: 1.1703407764434814
    load_time_ms: 15507.964
    num_steps_sampled: 62784000
    num_steps_trained: 62784000
    sample_time_ms: 120461.107
    update_time_ms: 1042.813
  iterations_since_restore: 4
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.874162679425837
    ram_util_percent: 13.85645933014354
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 18.0
    agent-2: 60.0
    agent-3: 30.0
    agent-4: 26.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 14.1
    agent-1: 7.53
    agent-2: 41.31
    agent-3: 18.42
    agent-4: 14.41
    agent-5: 16.46
  policy_reward_min:
    agent-0: -34.0
    agent-1: 1.0
    agent-2: 26.0
    agent-3: 6.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.29209683691789
    mean_inference_ms: 15.505213372145255
    mean_processing_ms: 73.55858287923155
  time_since_restore: 609.849841594696
  time_this_iter_s: 146.9800820350647
  time_total_s: 89546.5279712677
  timestamp: 1637363559
  timesteps_since_restore: 384000
  timesteps_this_iter: 96000
  timesteps_total: 62784000
  training_iteration: 654
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    654 |          89546.5 | 62784000 |   112.23 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 7.49
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 2.58
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 28.12
    apples_agent-2_min: 11
    apples_agent-3_max: 22
    apples_agent-3_mean: 7.67
    apples_agent-3_min: 1
    apples_agent-4_max: 24
    apples_agent-4_mean: 9.93
    apples_agent-4_min: 4
    apples_agent-5_max: 25
    apples_agent-5_mean: 3.4
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.93
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 280
    cleaning_beam_agent-1_mean: 220.34
    cleaning_beam_agent-1_min: 171
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 5.74
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 12.51
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 23
    cleaning_beam_agent-4_mean: 7.44
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.3
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-15-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 166.0
  episode_reward_mean: 116.65
  episode_reward_min: 76.0
  episodes_this_iter: 96
  episodes_total: 62880
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 14014.595
    learner:
      agent-0:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27141129970550537
        entropy_coeff: 0.0017600000137463212
        kl: 0.000523425464052707
        model: {}
        policy_loss: -0.0015399313997477293
        total_loss: -0.0019029706018045545
        vf_explained_var: 0.008833423256874084
        vf_loss: 1.0810182094573975
      agent-1:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2537485361099243
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011365055106580257
        model: {}
        policy_loss: -0.0015847506001591682
        total_loss: -0.0019675549119710922
        vf_explained_var: 0.05451253056526184
        vf_loss: 0.4958968162536621
      agent-2:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35948723554611206
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011766917305067182
        model: {}
        policy_loss: -0.0016908086836338043
        total_loss: -0.0019290803465992212
        vf_explained_var: 0.033512622117996216
        vf_loss: 3.7971763610839844
      agent-3:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5754782557487488
        entropy_coeff: 0.0017600000137463212
        kl: 0.001705872011370957
        model: {}
        policy_loss: -0.001668448094278574
        total_loss: -0.002501334063708782
        vf_explained_var: 0.0026200711727142334
        vf_loss: 1.5863274335861206
      agent-4:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42000651359558105
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010300843277946115
        model: {}
        policy_loss: -0.0016081640496850014
        total_loss: -0.0022218809463083744
        vf_explained_var: 0.013823285698890686
        vf_loss: 1.1261909008026123
      agent-5:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5602359771728516
        entropy_coeff: 0.0017600000137463212
        kl: 0.002014239551499486
        model: {}
        policy_loss: -0.0016610865714028478
        total_loss: -0.0025016930885612965
        vf_explained_var: 0.006866201758384705
        vf_loss: 1.202315330505371
    load_time_ms: 15300.047
    num_steps_sampled: 62880000
    num_steps_trained: 62880000
    sample_time_ms: 120325.717
    update_time_ms: 838.041
  iterations_since_restore: 5
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.033492822966508
    ram_util_percent: 13.836842105263159
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 16.0
    agent-2: 60.0
    agent-3: 37.0
    agent-4: 26.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 15.04
    agent-1: 7.45
    agent-2: 42.91
    agent-3: 19.43
    agent-4: 15.15
    agent-5: 16.67
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 23.0
    agent-3: 9.0
    agent-4: 7.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.33078161812584
    mean_inference_ms: 15.435878611008858
    mean_processing_ms: 73.69821005214182
  time_since_restore: 756.1634504795074
  time_this_iter_s: 146.3136088848114
  time_total_s: 89692.84158015251
  timestamp: 1637363705
  timesteps_since_restore: 480000
  timesteps_this_iter: 96000
  timesteps_total: 62880000
  training_iteration: 655
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    655 |          89692.8 | 62880000 |   116.65 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 7.59
    apples_agent-0_min: 2
    apples_agent-1_max: 8
    apples_agent-1_mean: 2.97
    apples_agent-1_min: 0
    apples_agent-2_max: 85
    apples_agent-2_mean: 27.38
    apples_agent-2_min: 12
    apples_agent-3_max: 29
    apples_agent-3_mean: 7.96
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.04
    apples_agent-4_min: 2
    apples_agent-5_max: 17
    apples_agent-5_mean: 3.36
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 2.22
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 280
    cleaning_beam_agent-1_mean: 226.08
    cleaning_beam_agent-1_min: 166
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 5.8
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 11.92
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 24
    cleaning_beam_agent-4_mean: 8.37
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.75
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-17-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 166.0
  episode_reward_mean: 111.37
  episode_reward_min: 60.0
  episodes_this_iter: 96
  episodes_total: 62976
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 13663.79
    learner:
      agent-0:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2705669701099396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008840109221637249
        model: {}
        policy_loss: -0.0017222827300429344
        total_loss: -0.002093515358865261
        vf_explained_var: 0.010873079299926758
        vf_loss: 0.9943782091140747
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2540381848812103
        entropy_coeff: 0.0017600000137463212
        kl: 0.001230577821843326
        model: {}
        policy_loss: -0.0017544934526085854
        total_loss: -0.0021455958485603333
        vf_explained_var: 0.07026119530200958
        vf_loss: 0.4831146001815796
      agent-2:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3549067974090576
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012044557370245457
        model: {}
        policy_loss: -0.0018972965190187097
        total_loss: -0.002158685587346554
        vf_explained_var: 0.036638304591178894
        vf_loss: 3.5572166442871094
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5704140663146973
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023384448140859604
        model: {}
        policy_loss: -0.0017398730851709843
        total_loss: -0.0025992500595748425
        vf_explained_var: 0.00623774528503418
        vf_loss: 1.2993650436401367
      agent-4:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4224246144294739
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007625186117365956
        model: {}
        policy_loss: -0.001480557955801487
        total_loss: -0.0021189767867326736
        vf_explained_var: 0.019452795386314392
        vf_loss: 1.0028526782989502
      agent-5:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5575174689292908
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021112337708473206
        model: {}
        policy_loss: -0.001848212443292141
        total_loss: -0.0027049719356000423
        vf_explained_var: 0.012734517455101013
        vf_loss: 1.11275053024292
    load_time_ms: 15174.901
    num_steps_sampled: 62976000
    num_steps_trained: 62976000
    sample_time_ms: 120186.599
    update_time_ms: 701.473
  iterations_since_restore: 6
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.03557692307692
    ram_util_percent: 13.804807692307694
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 16.0
    agent-2: 62.0
    agent-3: 31.0
    agent-4: 25.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 14.41
    agent-1: 7.42
    agent-2: 41.2
    agent-3: 18.36
    agent-4: 14.32
    agent-5: 15.66
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 20.0
    agent-3: 8.0
    agent-4: 3.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.34241678318699
    mean_inference_ms: 15.386924918666557
    mean_processing_ms: 73.70346660557048
  time_since_restore: 902.2992870807648
  time_this_iter_s: 146.13583660125732
  time_total_s: 89838.97741675377
  timestamp: 1637363852
  timesteps_since_restore: 576000
  timesteps_this_iter: 96000
  timesteps_total: 62976000
  training_iteration: 656
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    656 |            89839 | 62976000 |   111.37 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 7.3
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 2.81
    apples_agent-1_min: 0
    apples_agent-2_max: 63
    apples_agent-2_mean: 26.33
    apples_agent-2_min: 12
    apples_agent-3_max: 21
    apples_agent-3_mean: 7.14
    apples_agent-3_min: 2
    apples_agent-4_max: 22
    apples_agent-4_mean: 9.33
    apples_agent-4_min: 2
    apples_agent-5_max: 22
    apples_agent-5_mean: 2.83
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 2.72
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 279
    cleaning_beam_agent-1_mean: 222.08
    cleaning_beam_agent-1_min: 166
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 5.77
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 11.39
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 23
    cleaning_beam_agent-4_mean: 8.09
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.72
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-19-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 150.0
  episode_reward_mean: 110.77
  episode_reward_min: 75.0
  episodes_this_iter: 96
  episodes_total: 63072
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 13403.442
    learner:
      agent-0:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26843827962875366
        entropy_coeff: 0.0017600000137463212
        kl: 0.000537338899448514
        model: {}
        policy_loss: -0.0014903685078024864
        total_loss: -0.0018588569946587086
        vf_explained_var: 0.006690219044685364
        vf_loss: 1.022845983505249
      agent-1:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.259574830532074
        entropy_coeff: 0.0017600000137463212
        kl: 0.001147859962657094
        model: {}
        policy_loss: -0.001748079201206565
        total_loss: -0.002158811315894127
        vf_explained_var: 0.06188075244426727
        vf_loss: 0.425329327583313
      agent-2:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35797691345214844
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011387721169739962
        model: {}
        policy_loss: -0.0017682407051324844
        total_loss: -0.00206013023853302
        vf_explained_var: 0.026146292686462402
        vf_loss: 3.3459367752075195
      agent-3:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5662060976028442
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013230342883616686
        model: {}
        policy_loss: -0.0018546483479440212
        total_loss: -0.0027146050706505775
        vf_explained_var: -0.003144562244415283
        vf_loss: 1.3243495225906372
      agent-4:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41693609952926636
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005905048456043005
        model: {}
        policy_loss: -0.0012543906923383474
        total_loss: -0.0018967573996633291
        vf_explained_var: 0.015722662210464478
        vf_loss: 0.8959864377975464
      agent-5:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5451587438583374
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012764491839334369
        model: {}
        policy_loss: -0.0015021251747384667
        total_loss: -0.002340740989893675
        vf_explained_var: 0.0027335137128829956
        vf_loss: 1.168729305267334
    load_time_ms: 15088.574
    num_steps_sampled: 63072000
    num_steps_trained: 63072000
    sample_time_ms: 120151.683
    update_time_ms: 603.718
  iterations_since_restore: 7
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.0066985645933
    ram_util_percent: 13.85311004784689
  pid: 27065
  policy_reward_max:
    agent-0: 24.0
    agent-1: 16.0
    agent-2: 58.0
    agent-3: 31.0
    agent-4: 25.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 14.92
    agent-1: 6.94
    agent-2: 40.28
    agent-3: 18.3
    agent-4: 14.01
    agent-5: 16.32
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 22.0
    agent-3: 7.0
    agent-4: 5.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.34772218578805
    mean_inference_ms: 15.365601046136364
    mean_processing_ms: 73.72480879673662
  time_since_restore: 1048.7858619689941
  time_this_iter_s: 146.48657488822937
  time_total_s: 89985.463991642
  timestamp: 1637363998
  timesteps_since_restore: 672000
  timesteps_this_iter: 96000
  timesteps_total: 63072000
  training_iteration: 657
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    657 |          89985.5 | 63072000 |   110.77 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 6.19
    apples_agent-0_min: 0
    apples_agent-1_max: 41
    apples_agent-1_mean: 3.45
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 26.48
    apples_agent-2_min: 5
    apples_agent-3_max: 26
    apples_agent-3_mean: 6.61
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.32
    apples_agent-4_min: 1
    apples_agent-5_max: 22
    apples_agent-5_mean: 3.41
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 2.21
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 277
    cleaning_beam_agent-1_mean: 220.54
    cleaning_beam_agent-1_min: 22
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 5.25
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 12.47
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 21
    cleaning_beam_agent-4_mean: 7.98
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.77
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-22-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 160.0
  episode_reward_mean: 109.51
  episode_reward_min: 13.0
  episodes_this_iter: 96
  episodes_total: 63168
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 13208.536
    learner:
      agent-0:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2731075882911682
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007983053801581264
        model: {}
        policy_loss: -0.001628512516617775
        total_loss: -0.0020013833418488503
        vf_explained_var: 0.009462311863899231
        vf_loss: 1.0655146837234497
      agent-1:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2568754255771637
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011105393059551716
        model: {}
        policy_loss: -0.001573543413542211
        total_loss: -0.0019798842258751392
        vf_explained_var: 0.07526646554470062
        vf_loss: 0.4402879774570465
      agent-2:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3619985282421112
        entropy_coeff: 0.0017600000137463212
        kl: 0.001300714910030365
        model: {}
        policy_loss: -0.0017828335985541344
        total_loss: -0.0020749198738485575
        vf_explained_var: 0.04062408208847046
        vf_loss: 3.430004358291626
      agent-3:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5634818077087402
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014000918017700315
        model: {}
        policy_loss: -0.0015970170497894287
        total_loss: -0.0024481192231178284
        vf_explained_var: 0.01156604290008545
        vf_loss: 1.3843923807144165
      agent-4:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.418375164270401
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010471028508618474
        model: {}
        policy_loss: -0.0015410177875310183
        total_loss: -0.0021725459955632687
        vf_explained_var: 0.007848888635635376
        vf_loss: 1.0317935943603516
      agent-5:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5562909841537476
        entropy_coeff: 0.0017600000137463212
        kl: 0.001237007207237184
        model: {}
        policy_loss: -0.0015313029289245605
        total_loss: -0.00239411904476583
        vf_explained_var: 0.00562894344329834
        vf_loss: 1.1432334184646606
    load_time_ms: 14961.527
    num_steps_sampled: 63168000
    num_steps_trained: 63168000
    sample_time_ms: 120177.161
    update_time_ms: 530.634
  iterations_since_restore: 8
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.099038461538463
    ram_util_percent: 13.856730769230769
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 17.0
    agent-2: 62.0
    agent-3: 31.0
    agent-4: 30.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 14.58
    agent-1: 6.94
    agent-2: 39.82
    agent-3: 17.95
    agent-4: 14.72
    agent-5: 15.5
  policy_reward_min:
    agent-0: 2.0
    agent-1: 0.0
    agent-2: 8.0
    agent-3: 0.0
    agent-4: 2.0
    agent-5: 1.0
  sampler_perf:
    mean_env_wait_ms: 28.372885535512886
    mean_inference_ms: 15.341960746087159
    mean_processing_ms: 73.76941582538406
  time_since_restore: 1195.1526765823364
  time_this_iter_s: 146.36681461334229
  time_total_s: 90131.83080625534
  timestamp: 1637364145
  timesteps_since_restore: 768000
  timesteps_this_iter: 96000
  timesteps_total: 63168000
  training_iteration: 658
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    658 |          90131.8 | 63168000 |   109.51 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 7.06
    apples_agent-0_min: 2
    apples_agent-1_max: 9
    apples_agent-1_mean: 2.89
    apples_agent-1_min: 0
    apples_agent-2_max: 40
    apples_agent-2_mean: 26.05
    apples_agent-2_min: 12
    apples_agent-3_max: 26
    apples_agent-3_mean: 7.09
    apples_agent-3_min: 2
    apples_agent-4_max: 22
    apples_agent-4_mean: 9.45
    apples_agent-4_min: 2
    apples_agent-5_max: 25
    apples_agent-5_mean: 3.35
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 2.1
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 319
    cleaning_beam_agent-1_mean: 224.03
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 5.28
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 12.23
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 20
    cleaning_beam_agent-4_mean: 8.07
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-24-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 154.0
  episode_reward_mean: 110.68
  episode_reward_min: 33.0
  episodes_this_iter: 96
  episodes_total: 63264
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 13058.405
    learner:
      agent-0:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2676430642604828
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007883678190410137
        model: {}
        policy_loss: -0.001679877983406186
        total_loss: -0.0020412844605743885
        vf_explained_var: 0.014384299516677856
        vf_loss: 1.0903186798095703
      agent-1:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.260932058095932
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010995666962116957
        model: {}
        policy_loss: -0.0016782854218035936
        total_loss: -0.002087982138618827
        vf_explained_var: 0.07662919163703918
        vf_loss: 0.48688066005706787
      agent-2:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36332494020462036
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009105692151933908
        model: {}
        policy_loss: -0.0016945134848356247
        total_loss: -0.0019565820693969727
        vf_explained_var: 0.04415729641914368
        vf_loss: 3.766728401184082
      agent-3:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5632280111312866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023825832176953554
        model: {}
        policy_loss: -0.001778684905730188
        total_loss: -0.0026380792260169983
        vf_explained_var: 0.005009651184082031
        vf_loss: 1.3002568483352661
      agent-4:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41663771867752075
        entropy_coeff: 0.0017600000137463212
        kl: 0.001079794717952609
        model: {}
        policy_loss: -0.0014546446036547422
        total_loss: -0.0020768428221344948
        vf_explained_var: 0.009128585457801819
        vf_loss: 1.1024354696273804
      agent-5:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5475064516067505
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015063744504004717
        model: {}
        policy_loss: -0.0015254507306963205
        total_loss: -0.00237762532196939
        vf_explained_var: 0.009270250797271729
        vf_loss: 1.1026266813278198
    load_time_ms: 14866.517
    num_steps_sampled: 63264000
    num_steps_trained: 63264000
    sample_time_ms: 120135.587
    update_time_ms: 473.571
  iterations_since_restore: 9
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.09375
    ram_util_percent: 13.853846153846153
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 18.0
    agent-2: 63.0
    agent-3: 33.0
    agent-4: 28.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 14.99
    agent-1: 7.04
    agent-2: 40.17
    agent-3: 17.97
    agent-4: 15.23
    agent-5: 15.28
  policy_reward_min:
    agent-0: 0.0
    agent-1: -40.0
    agent-2: 16.0
    agent-3: 5.0
    agent-4: 6.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.378755232219827
    mean_inference_ms: 15.329914561949442
    mean_processing_ms: 73.80151529408327
  time_since_restore: 1340.9997186660767
  time_this_iter_s: 145.84704208374023
  time_total_s: 90277.67784833908
  timestamp: 1637364291
  timesteps_since_restore: 864000
  timesteps_this_iter: 96000
  timesteps_total: 63264000
  training_iteration: 659
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    659 |          90277.7 | 63264000 |   110.68 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 7.21
    apples_agent-0_min: 1
    apples_agent-1_max: 23
    apples_agent-1_mean: 2.83
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 27.47
    apples_agent-2_min: 13
    apples_agent-3_max: 31
    apples_agent-3_mean: 8.24
    apples_agent-3_min: 1
    apples_agent-4_max: 32
    apples_agent-4_mean: 10.24
    apples_agent-4_min: 1
    apples_agent-5_max: 23
    apples_agent-5_mean: 3.21
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 1.73
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 268
    cleaning_beam_agent-1_mean: 224.17
    cleaning_beam_agent-1_min: 169
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 5.25
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 44
    cleaning_beam_agent-3_mean: 11.8
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 20
    cleaning_beam_agent-4_mean: 7.95
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.6
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-27-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 161.0
  episode_reward_mean: 113.54
  episode_reward_min: 61.0
  episodes_this_iter: 96
  episodes_total: 63360
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 12951.45
    learner:
      agent-0:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2737662196159363
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009362076525576413
        model: {}
        policy_loss: -0.0020086891017854214
        total_loss: -0.0023838914930820465
        vf_explained_var: 0.010584533214569092
        vf_loss: 1.0626425743103027
      agent-1:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2587295174598694
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014479588717222214
        model: {}
        policy_loss: -0.0017743902280926704
        total_loss: -0.0021818559616804123
        vf_explained_var: 0.05715388059616089
        vf_loss: 0.4733099937438965
      agent-2:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.359378457069397
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009193807491101325
        model: {}
        policy_loss: -0.0015563545748591423
        total_loss: -0.0018230248242616653
        vf_explained_var: 0.02725282311439514
        vf_loss: 3.654787063598633
      agent-3:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5621165037155151
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010790571104735136
        model: {}
        policy_loss: -0.001606294885277748
        total_loss: -0.002460937947034836
        vf_explained_var: 0.01400148868560791
        vf_loss: 1.3426274061203003
      agent-4:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4222401976585388
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006327088922262192
        model: {}
        policy_loss: -0.001339394599199295
        total_loss: -0.001974973827600479
        vf_explained_var: 0.005402594804763794
        vf_loss: 1.0732249021530151
      agent-5:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5567637085914612
        entropy_coeff: 0.0017600000137463212
        kl: 0.001231819624081254
        model: {}
        policy_loss: -0.0017418069764971733
        total_loss: -0.0025982765946537256
        vf_explained_var: 0.01633632183074951
        vf_loss: 1.229574203491211
    load_time_ms: 14804.141
    num_steps_sampled: 63360000
    num_steps_trained: 63360000
    sample_time_ms: 120151.473
    update_time_ms: 427.906
  iterations_since_restore: 10
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.931578947368426
    ram_util_percent: 13.78947368421053
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 17.0
    agent-2: 62.0
    agent-3: 35.0
    agent-4: 25.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 15.17
    agent-1: 7.23
    agent-2: 41.39
    agent-3: 18.69
    agent-4: 14.65
    agent-5: 16.41
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 24.0
    agent-3: 9.0
    agent-4: 4.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.38223168475843
    mean_inference_ms: 15.30946448718172
    mean_processing_ms: 73.78010099954774
  time_since_restore: 1487.686611175537
  time_this_iter_s: 146.68689250946045
  time_total_s: 90424.36474084854
  timestamp: 1637364438
  timesteps_since_restore: 960000
  timesteps_this_iter: 96000
  timesteps_total: 63360000
  training_iteration: 660
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    660 |          90424.4 | 63360000 |   113.54 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 7.41
    apples_agent-0_min: 1
    apples_agent-1_max: 36
    apples_agent-1_mean: 3.28
    apples_agent-1_min: 0
    apples_agent-2_max: 63
    apples_agent-2_mean: 28.17
    apples_agent-2_min: 15
    apples_agent-3_max: 35
    apples_agent-3_mean: 8.07
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.03
    apples_agent-4_min: 3
    apples_agent-5_max: 17
    apples_agent-5_mean: 2.78
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 15
    cleaning_beam_agent-0_mean: 2.1
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 267
    cleaning_beam_agent-1_mean: 218.33
    cleaning_beam_agent-1_min: 151
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 5.61
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 47
    cleaning_beam_agent-3_mean: 13.29
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 20
    cleaning_beam_agent-4_mean: 8.04
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.02
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-29-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 182.0
  episode_reward_mean: 114.47
  episode_reward_min: 57.0
  episodes_this_iter: 96
  episodes_total: 63456
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11888.142
    learner:
      agent-0:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26450207829475403
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009395722299814224
        model: {}
        policy_loss: -0.0015907632187008858
        total_loss: -0.0019412515684962273
        vf_explained_var: 0.00795312225818634
        vf_loss: 1.14854097366333
      agent-1:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25387537479400635
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009245517430827022
        model: {}
        policy_loss: -0.0016016538720577955
        total_loss: -0.0019997558556497097
        vf_explained_var: 0.07684041559696198
        vf_loss: 0.48540210723876953
      agent-2:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3650260269641876
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009595122537575662
        model: {}
        policy_loss: -0.0016170772723853588
        total_loss: -0.001881117932498455
        vf_explained_var: 0.02653239667415619
        vf_loss: 3.7821693420410156
      agent-3:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5668715834617615
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013012849958613515
        model: {}
        policy_loss: -0.0015544705092906952
        total_loss: -0.0024076122790575027
        vf_explained_var: 0.009938061237335205
        vf_loss: 1.4430267810821533
      agent-4:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4268377423286438
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012654949678108096
        model: {}
        policy_loss: -0.001546467188745737
        total_loss: -0.002200502436608076
        vf_explained_var: 0.005699440836906433
        vf_loss: 0.9695576429367065
      agent-5:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5635868906974792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016307097394019365
        model: {}
        policy_loss: -0.0015606279484927654
        total_loss: -0.002440947573632002
        vf_explained_var: 0.011382624506950378
        vf_loss: 1.1127415895462036
    load_time_ms: 14384.107
    num_steps_sampled: 63456000
    num_steps_trained: 63456000
    sample_time_ms: 120081.597
    update_time_ms: 17.649
  iterations_since_restore: 11
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.01333333333333
    ram_util_percent: 13.854761904761903
  pid: 27065
  policy_reward_max:
    agent-0: 36.0
    agent-1: 21.0
    agent-2: 81.0
    agent-3: 37.0
    agent-4: 28.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.37
    agent-1: 7.73
    agent-2: 42.43
    agent-3: 18.45
    agent-4: 14.44
    agent-5: 16.05
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 24.0
    agent-3: 8.0
    agent-4: 6.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.381854709122194
    mean_inference_ms: 15.292830356169011
    mean_processing_ms: 73.8015464472333
  time_since_restore: 1634.0224344730377
  time_this_iter_s: 146.3358232975006
  time_total_s: 90570.70056414604
  timestamp: 1637364585
  timesteps_since_restore: 1056000
  timesteps_this_iter: 96000
  timesteps_total: 63456000
  training_iteration: 661
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    661 |          90570.7 | 63456000 |   114.47 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 6.27
    apples_agent-0_min: 2
    apples_agent-1_max: 8
    apples_agent-1_mean: 2.69
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 26.09
    apples_agent-2_min: 14
    apples_agent-3_max: 28
    apples_agent-3_mean: 7.82
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 9.41
    apples_agent-4_min: 1
    apples_agent-5_max: 29
    apples_agent-5_mean: 3.02
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 2.4
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 264
    cleaning_beam_agent-1_mean: 213.62
    cleaning_beam_agent-1_min: 168
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 5.82
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 45
    cleaning_beam_agent-3_mean: 14.08
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 27
    cleaning_beam_agent-4_mean: 7.52
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.1
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-32-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 151.0
  episode_reward_mean: 111.81
  episode_reward_min: 73.0
  episodes_this_iter: 96
  episodes_total: 63552
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11889.113
    learner:
      agent-0:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25818657875061035
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008229025406762958
        model: {}
        policy_loss: -0.0015226714313030243
        total_loss: -0.0018805330619215965
        vf_explained_var: 0.016123458743095398
        vf_loss: 0.9646930694580078
      agent-1:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25179171562194824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010170484893023968
        model: {}
        policy_loss: -0.001643485389649868
        total_loss: -0.002033097669482231
        vf_explained_var: 0.07584652304649353
        vf_loss: 0.5344285368919373
      agent-2:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35686224699020386
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012712972238659859
        model: {}
        policy_loss: -0.00185780122410506
        total_loss: -0.0021420898847281933
        vf_explained_var: 0.028411805629730225
        vf_loss: 3.4366846084594727
      agent-3:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5694860816001892
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011171897640451789
        model: {}
        policy_loss: -0.0016241970006376505
        total_loss: -0.002492354018613696
        vf_explained_var: 0.009715750813484192
        vf_loss: 1.3403339385986328
      agent-4:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43539270758628845
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007175515056587756
        model: {}
        policy_loss: -0.0013333631213754416
        total_loss: -0.0020023768302053213
        vf_explained_var: 0.015419244766235352
        vf_loss: 0.9721018075942993
      agent-5:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5605321526527405
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014339755289256573
        model: {}
        policy_loss: -0.001556911040097475
        total_loss: -0.002438655123114586
        vf_explained_var: 0.016325384378433228
        vf_loss: 1.0466002225875854
    load_time_ms: 14343.77
    num_steps_sampled: 63552000
    num_steps_trained: 63552000
    sample_time_ms: 120089.52
    update_time_ms: 17.167
  iterations_since_restore: 12
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.92153110047847
    ram_util_percent: 13.770334928229667
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 57.0
    agent-3: 32.0
    agent-4: 25.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 14.25
    agent-1: 8.14
    agent-2: 40.48
    agent-3: 18.47
    agent-4: 14.42
    agent-5: 16.05
  policy_reward_min:
    agent-0: 2.0
    agent-1: 3.0
    agent-2: 26.0
    agent-3: 5.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.380896098275752
    mean_inference_ms: 15.275903649758924
    mean_processing_ms: 73.83314033788497
  time_since_restore: 1780.0064730644226
  time_this_iter_s: 145.9840385913849
  time_total_s: 90716.68460273743
  timestamp: 1637364732
  timesteps_since_restore: 1152000
  timesteps_this_iter: 96000
  timesteps_total: 63552000
  training_iteration: 662
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    662 |          90716.7 | 63552000 |   111.81 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 6.7
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 2.8
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 25.61
    apples_agent-2_min: 8
    apples_agent-3_max: 39
    apples_agent-3_mean: 8.08
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.5
    apples_agent-4_min: 1
    apples_agent-5_max: 40
    apples_agent-5_mean: 3.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 3.06
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 253
    cleaning_beam_agent-1_mean: 208.61
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 5.94
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 13.9
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 17
    cleaning_beam_agent-4_mean: 6.76
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.42
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-34-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 162.0
  episode_reward_mean: 113.06
  episode_reward_min: 52.0
  episodes_this_iter: 96
  episodes_total: 63648
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11889.779
    learner:
      agent-0:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25088441371917725
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009332039044238627
        model: {}
        policy_loss: -0.0017335671000182629
        total_loss: -0.0020744134671986103
        vf_explained_var: 0.002992495894432068
        vf_loss: 1.0066938400268555
      agent-1:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2507913410663605
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013450519181787968
        model: {}
        policy_loss: -0.0015929676592350006
        total_loss: -0.0019855471327900887
        vf_explained_var: 0.08528788387775421
        vf_loss: 0.48751458525657654
      agent-2:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35795921087265015
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014533004723489285
        model: {}
        policy_loss: -0.0020302822813391685
        total_loss: -0.0023079155944287777
        vf_explained_var: 0.05132794380187988
        vf_loss: 3.5230395793914795
      agent-3:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5668095946311951
        entropy_coeff: 0.0017600000137463212
        kl: 0.001128326985053718
        model: {}
        policy_loss: -0.00195753900334239
        total_loss: -0.0028237560763955116
        vf_explained_var: -0.0025053173303604126
        vf_loss: 1.3131686449050903
      agent-4:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4352680742740631
        entropy_coeff: 0.0017600000137463212
        kl: 0.000642461352981627
        model: {}
        policy_loss: -0.0013852582778781652
        total_loss: -0.002036878140643239
        vf_explained_var: 0.02085353434085846
        vf_loss: 1.144252896308899
      agent-5:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.552716851234436
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015187764074653387
        model: {}
        policy_loss: -0.001388826873153448
        total_loss: -0.0022398268338292837
        vf_explained_var: 0.009892508387565613
        vf_loss: 1.2171024084091187
    load_time_ms: 14286.621
    num_steps_sampled: 63648000
    num_steps_trained: 63648000
    sample_time_ms: 120049.526
    update_time_ms: 17.015
  iterations_since_restore: 13
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.085576923076925
    ram_util_percent: 13.850480769230767
  pid: 27065
  policy_reward_max:
    agent-0: 23.0
    agent-1: 19.0
    agent-2: 63.0
    agent-3: 35.0
    agent-4: 32.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 14.75
    agent-1: 7.84
    agent-2: 40.94
    agent-3: 17.9
    agent-4: 15.07
    agent-5: 16.56
  policy_reward_min:
    agent-0: 3.0
    agent-1: 2.0
    agent-2: 18.0
    agent-3: 8.0
    agent-4: 4.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.38117992446321
    mean_inference_ms: 15.27011361989044
    mean_processing_ms: 73.86464322715469
  time_since_restore: 1926.3289349079132
  time_this_iter_s: 146.3224618434906
  time_total_s: 90863.00706458092
  timestamp: 1637364878
  timesteps_since_restore: 1248000
  timesteps_this_iter: 96000
  timesteps_total: 63648000
  training_iteration: 663
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    663 |            90863 | 63648000 |   113.06 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 6.91
    apples_agent-0_min: 2
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.14
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 25.68
    apples_agent-2_min: 12
    apples_agent-3_max: 27
    apples_agent-3_mean: 6.31
    apples_agent-3_min: 1
    apples_agent-4_max: 33
    apples_agent-4_mean: 10.04
    apples_agent-4_min: 2
    apples_agent-5_max: 27
    apples_agent-5_mean: 3.33
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 2.55
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 257
    cleaning_beam_agent-1_mean: 213.07
    cleaning_beam_agent-1_min: 165
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 4.88
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 13.11
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 23
    cleaning_beam_agent-4_mean: 7.86
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.5
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-37-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 156.0
  episode_reward_mean: 110.81
  episode_reward_min: 78.0
  episodes_this_iter: 96
  episodes_total: 63744
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11893.577
    learner:
      agent-0:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.256855845451355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013949917629361153
        model: {}
        policy_loss: -0.0018242145888507366
        total_loss: -0.00216974806971848
        vf_explained_var: 0.006319001317024231
        vf_loss: 1.0650134086608887
      agent-1:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2525436282157898
        entropy_coeff: 0.0017600000137463212
        kl: 0.000917015946470201
        model: {}
        policy_loss: -0.0014649908989667892
        total_loss: -0.001851595239713788
        vf_explained_var: 0.07290895283222198
        vf_loss: 0.5785512924194336
      agent-2:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3623359799385071
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014292257837951183
        model: {}
        policy_loss: -0.0019825762137770653
        total_loss: -0.002272948157042265
        vf_explained_var: 0.035776495933532715
        vf_loss: 3.4730775356292725
      agent-3:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5716828107833862
        entropy_coeff: 0.0017600000137463212
        kl: 0.001453246222808957
        model: {}
        policy_loss: -0.0019277413375675678
        total_loss: -0.0028030076064169407
        vf_explained_var: 0.0028748661279678345
        vf_loss: 1.3086302280426025
      agent-4:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4513784646987915
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010581142269074917
        model: {}
        policy_loss: -0.0014441891107708216
        total_loss: -0.0021359133534133434
        vf_explained_var: 0.016061946749687195
        vf_loss: 1.0268076658248901
      agent-5:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5386008024215698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017748221289366484
        model: {}
        policy_loss: -0.0016401970060542226
        total_loss: -0.0024784987326711416
        vf_explained_var: 0.011318668723106384
        vf_loss: 1.0959551334381104
    load_time_ms: 14255.795
    num_steps_sampled: 63744000
    num_steps_trained: 63744000
    sample_time_ms: 119940.167
    update_time_ms: 16.859
  iterations_since_restore: 14
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.041826923076925
    ram_util_percent: 13.814423076923077
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 17.0
    agent-2: 58.0
    agent-3: 32.0
    agent-4: 32.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.03
    agent-1: 8.23
    agent-2: 39.47
    agent-3: 17.69
    agent-4: 14.86
    agent-5: 15.53
  policy_reward_min:
    agent-0: 3.0
    agent-1: 1.0
    agent-2: 22.0
    agent-3: 8.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.3735812352405
    mean_inference_ms: 15.263041791174812
    mean_processing_ms: 73.85982536379872
  time_since_restore: 2071.925459384918
  time_this_iter_s: 145.596524477005
  time_total_s: 91008.60358905792
  timestamp: 1637365024
  timesteps_since_restore: 1344000
  timesteps_this_iter: 96000
  timesteps_total: 63744000
  training_iteration: 664
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    664 |          91008.6 | 63744000 |   110.81 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 6.53
    apples_agent-0_min: 2
    apples_agent-1_max: 27
    apples_agent-1_mean: 3.34
    apples_agent-1_min: 0
    apples_agent-2_max: 55
    apples_agent-2_mean: 26.49
    apples_agent-2_min: 11
    apples_agent-3_max: 30
    apples_agent-3_mean: 7.11
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.75
    apples_agent-4_min: 1
    apples_agent-5_max: 27
    apples_agent-5_mean: 3.71
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 2.09
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 263
    cleaning_beam_agent-1_mean: 213.18
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 5.82
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 12.6
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 36
    cleaning_beam_agent-4_mean: 7.14
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.37
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-39-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 161.0
  episode_reward_mean: 111.58
  episode_reward_min: 57.0
  episodes_this_iter: 96
  episodes_total: 63840
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11889.945
    learner:
      agent-0:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2526853680610657
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007602435071021318
        model: {}
        policy_loss: -0.00161282392218709
        total_loss: -0.0019483468495309353
        vf_explained_var: 0.008624285459518433
        vf_loss: 1.091933012008667
      agent-1:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2500394284725189
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014402339002117515
        model: {}
        policy_loss: -0.0019013762939721346
        total_loss: -0.0022895783185958862
        vf_explained_var: 0.06204661726951599
        vf_loss: 0.518494725227356
      agent-2:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35484760999679565
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016627134755253792
        model: {}
        policy_loss: -0.0017447862774133682
        total_loss: -0.0019817014690488577
        vf_explained_var: 0.02714581787586212
        vf_loss: 3.8759899139404297
      agent-3:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5694977641105652
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020423901733011007
        model: {}
        policy_loss: -0.0019638054072856903
        total_loss: -0.0028223032131791115
        vf_explained_var: 0.0063173770904541016
        vf_loss: 1.4379396438598633
      agent-4:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4522676467895508
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006467541679739952
        model: {}
        policy_loss: -0.0015261839143931866
        total_loss: -0.0022155221085995436
        vf_explained_var: 0.005266249179840088
        vf_loss: 1.0664501190185547
      agent-5:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5374313592910767
        entropy_coeff: 0.0017600000137463212
        kl: 0.001150584896095097
        model: {}
        policy_loss: -0.0013633896596729755
        total_loss: -0.002195002045482397
        vf_explained_var: 0.0033096522092819214
        vf_loss: 1.142553687095642
    load_time_ms: 14228.564
    num_steps_sampled: 63840000
    num_steps_trained: 63840000
    sample_time_ms: 119929.491
    update_time_ms: 16.394
  iterations_since_restore: 15
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.083091787439617
    ram_util_percent: 13.87584541062802
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 18.0
    agent-2: 69.0
    agent-3: 32.0
    agent-4: 29.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 14.52
    agent-1: 7.65
    agent-2: 40.93
    agent-3: 18.08
    agent-4: 14.61
    agent-5: 15.79
  policy_reward_min:
    agent-0: 2.0
    agent-1: 1.0
    agent-2: 21.0
    agent-3: 5.0
    agent-4: 5.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.36680961674943
    mean_inference_ms: 15.255550167267677
    mean_processing_ms: 73.84936984469695
  time_since_restore: 2217.782974243164
  time_this_iter_s: 145.85751485824585
  time_total_s: 91154.46110391617
  timestamp: 1637365170
  timesteps_since_restore: 1440000
  timesteps_this_iter: 96000
  timesteps_total: 63840000
  training_iteration: 665
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    665 |          91154.5 | 63840000 |   111.58 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 6.56
    apples_agent-0_min: 1
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.37
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 24.89
    apples_agent-2_min: 6
    apples_agent-3_max: 32
    apples_agent-3_mean: 6.82
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 9.17
    apples_agent-4_min: 2
    apples_agent-5_max: 32
    apples_agent-5_mean: 3.03
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 2.55
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 250
    cleaning_beam_agent-1_mean: 208.17
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 5.79
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 12.34
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 29
    cleaning_beam_agent-4_mean: 7.12
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.07
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-41-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 157.0
  episode_reward_mean: 110.6
  episode_reward_min: 32.0
  episodes_this_iter: 96
  episodes_total: 63936
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11880.162
    learner:
      agent-0:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2608504295349121
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008395174518227577
        model: {}
        policy_loss: -0.0017196210101246834
        total_loss: -0.0020796465687453747
        vf_explained_var: 0.010194122791290283
        vf_loss: 0.9906511306762695
      agent-1:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24845990538597107
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009428962948732078
        model: {}
        policy_loss: -0.0016112353187054396
        total_loss: -0.0019956454634666443
        vf_explained_var: 0.0791032463312149
        vf_loss: 0.5287997722625732
      agent-2:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35663872957229614
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009606921230442822
        model: {}
        policy_loss: -0.0016922326758503914
        total_loss: -0.0019639390520751476
        vf_explained_var: 0.046598270535469055
        vf_loss: 3.559739828109741
      agent-3:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5743833780288696
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018059855792671442
        model: {}
        policy_loss: -0.0018439826089888811
        total_loss: -0.0027299760840833187
        vf_explained_var: 0.004612833261489868
        vf_loss: 1.2491284608840942
      agent-4:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44777336716651917
        entropy_coeff: 0.0017600000137463212
        kl: 0.000745354569517076
        model: {}
        policy_loss: -0.0013607894070446491
        total_loss: -0.0020416532643139362
        vf_explained_var: 0.011431053280830383
        vf_loss: 1.0721935033798218
      agent-5:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5392557382583618
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018879103008657694
        model: {}
        policy_loss: -0.0016680723056197166
        total_loss: -0.0024997973814606667
        vf_explained_var: 0.014408141374588013
        vf_loss: 1.173567295074463
    load_time_ms: 14181.026
    num_steps_sampled: 63936000
    num_steps_trained: 63936000
    sample_time_ms: 120080.524
    update_time_ms: 16.201
  iterations_since_restore: 16
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.987619047619045
    ram_util_percent: 13.87095238095238
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 63.0
    agent-3: 33.0
    agent-4: 33.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 15.08
    agent-1: 8.15
    agent-2: 39.81
    agent-3: 17.36
    agent-4: 14.46
    agent-5: 15.74
  policy_reward_min:
    agent-0: 5.0
    agent-1: 1.0
    agent-2: 14.0
    agent-3: 4.0
    agent-4: 2.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.36917465621531
    mean_inference_ms: 15.24873891369735
    mean_processing_ms: 73.85031517218326
  time_since_restore: 2364.793727874756
  time_this_iter_s: 147.0107536315918
  time_total_s: 91301.47185754776
  timestamp: 1637365317
  timesteps_since_restore: 1536000
  timesteps_this_iter: 96000
  timesteps_total: 63936000
  training_iteration: 666
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    666 |          91301.5 | 63936000 |    110.6 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 6.77
    apples_agent-0_min: 1
    apples_agent-1_max: 11
    apples_agent-1_mean: 3.06
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 26.63
    apples_agent-2_min: 14
    apples_agent-3_max: 34
    apples_agent-3_mean: 7.32
    apples_agent-3_min: 1
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.18
    apples_agent-4_min: 2
    apples_agent-5_max: 11
    apples_agent-5_mean: 3.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 2.3
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 258
    cleaning_beam_agent-1_mean: 208.94
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 5.72
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 12.09
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 21
    cleaning_beam_agent-4_mean: 6.43
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.38
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-44-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 169.0
  episode_reward_mean: 111.21
  episode_reward_min: 42.0
  episodes_this_iter: 96
  episodes_total: 64032
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11885.954
    learner:
      agent-0:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2546609044075012
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006122644990682602
        model: {}
        policy_loss: -0.0015095528215169907
        total_loss: -0.0018468350172042847
        vf_explained_var: 0.0069713592529296875
        vf_loss: 1.1091670989990234
      agent-1:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25224652886390686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010994569165632129
        model: {}
        policy_loss: -0.0018104137852787971
        total_loss: -0.0021974174305796623
        vf_explained_var: 0.061060890555381775
        vf_loss: 0.5694601535797119
      agent-2:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35776740312576294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010542167583480477
        model: {}
        policy_loss: -0.0016246659215539694
        total_loss: -0.0018881510477513075
        vf_explained_var: 0.027223721146583557
        vf_loss: 3.6618213653564453
      agent-3:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5723273158073425
        entropy_coeff: 0.0017600000137463212
        kl: 0.001290410291403532
        model: {}
        policy_loss: -0.0018002744764089584
        total_loss: -0.002666990738362074
        vf_explained_var: 0.005388736724853516
        vf_loss: 1.40579354763031
      agent-4:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4513351321220398
        entropy_coeff: 0.0017600000137463212
        kl: 0.000592068477999419
        model: {}
        policy_loss: -0.0013018178287893534
        total_loss: -0.0019850749522447586
        vf_explained_var: 0.004348099231719971
        vf_loss: 1.110945463180542
      agent-5:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5363893508911133
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015783595154061913
        model: {}
        policy_loss: -0.0015904151368886232
        total_loss: -0.0024239064659923315
        vf_explained_var: 0.011540234088897705
        vf_loss: 1.1054940223693848
    load_time_ms: 14119.856
    num_steps_sampled: 64032000
    num_steps_trained: 64032000
    sample_time_ms: 120040.308
    update_time_ms: 16.076
  iterations_since_restore: 17
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.848309178743964
    ram_util_percent: 13.86521739130435
  pid: 27065
  policy_reward_max:
    agent-0: 33.0
    agent-1: 21.0
    agent-2: 60.0
    agent-3: 33.0
    agent-4: 32.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.05
    agent-1: 8.02
    agent-2: 40.27
    agent-3: 18.31
    agent-4: 14.28
    agent-5: 15.28
  policy_reward_min:
    agent-0: 5.0
    agent-1: 0.0
    agent-2: 23.0
    agent-3: 6.0
    agent-4: 2.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.352198913335737
    mean_inference_ms: 15.23374990494375
    mean_processing_ms: 73.81074020951891
  time_since_restore: 2510.2676882743835
  time_this_iter_s: 145.47396039962769
  time_total_s: 91446.94581794739
  timestamp: 1637365463
  timesteps_since_restore: 1632000
  timesteps_this_iter: 96000
  timesteps_total: 64032000
  training_iteration: 667
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    667 |          91446.9 | 64032000 |   111.21 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 6.1
    apples_agent-0_min: 0
    apples_agent-1_max: 28
    apples_agent-1_mean: 3.46
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 25.18
    apples_agent-2_min: 5
    apples_agent-3_max: 49
    apples_agent-3_mean: 7.68
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.47
    apples_agent-4_min: 3
    apples_agent-5_max: 15
    apples_agent-5_mean: 3.43
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 2.19
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 282
    cleaning_beam_agent-1_mean: 208.01
    cleaning_beam_agent-1_min: 54
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 5.5
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 10.89
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 47
    cleaning_beam_agent-4_mean: 6.2
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 2.71
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-46-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 155.0
  episode_reward_mean: 107.76
  episode_reward_min: 29.0
  episodes_this_iter: 96
  episodes_total: 64128
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11900.022
    learner:
      agent-0:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24741438031196594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011830825824290514
        model: {}
        policy_loss: -0.001550961285829544
        total_loss: -0.001884594326838851
        vf_explained_var: 0.007755592465400696
        vf_loss: 1.0181516408920288
      agent-1:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2512689232826233
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010589209850877523
        model: {}
        policy_loss: -0.001572516281157732
        total_loss: -0.001966593787074089
        vf_explained_var: 0.07351438701152802
        vf_loss: 0.4815349578857422
      agent-2:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3519085645675659
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015502568567171693
        model: {}
        policy_loss: -0.0017960872501134872
        total_loss: -0.002050541341304779
        vf_explained_var: 0.037072181701660156
        vf_loss: 3.6490285396575928
      agent-3:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.571370542049408
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016681861598044634
        model: {}
        policy_loss: -0.0018925084732472897
        total_loss: -0.002765249228104949
        vf_explained_var: 0.007003620266914368
        vf_loss: 1.3287245035171509
      agent-4:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43960702419281006
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006090457900427282
        model: {}
        policy_loss: -0.0012330934405326843
        total_loss: -0.0019066864624619484
        vf_explained_var: 0.008761808276176453
        vf_loss: 1.0011423826217651
      agent-5:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5412299036979675
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015641649952158332
        model: {}
        policy_loss: -0.0014050239697098732
        total_loss: -0.002232937142252922
        vf_explained_var: 0.013309553265571594
        vf_loss: 1.2465022802352905
    load_time_ms: 14153.222
    num_steps_sampled: 64128000
    num_steps_trained: 64128000
    sample_time_ms: 119986.284
    update_time_ms: 15.527
  iterations_since_restore: 18
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.95933014354067
    ram_util_percent: 13.79952153110048
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 17.0
    agent-2: 61.0
    agent-3: 30.0
    agent-4: 23.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 13.98
    agent-1: 7.51
    agent-2: 38.53
    agent-3: 17.43
    agent-4: 13.74
    agent-5: 16.57
  policy_reward_min:
    agent-0: 2.0
    agent-1: 1.0
    agent-2: 8.0
    agent-3: 4.0
    agent-4: 5.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.34836172048783
    mean_inference_ms: 15.227906862318996
    mean_processing_ms: 73.81146428947676
  time_since_restore: 2656.643586397171
  time_this_iter_s: 146.37589812278748
  time_total_s: 91593.32171607018
  timestamp: 1637365609
  timesteps_since_restore: 1728000
  timesteps_this_iter: 96000
  timesteps_total: 64128000
  training_iteration: 668
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    668 |          91593.3 | 64128000 |   107.76 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 7.04
    apples_agent-0_min: 0
    apples_agent-1_max: 26
    apples_agent-1_mean: 3.16
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 24.78
    apples_agent-2_min: 1
    apples_agent-3_max: 76
    apples_agent-3_mean: 7.49
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 9.23
    apples_agent-4_min: 1
    apples_agent-5_max: 14
    apples_agent-5_mean: 2.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.95
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 267
    cleaning_beam_agent-1_mean: 202.32
    cleaning_beam_agent-1_min: 28
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 5.05
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 10.76
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 26
    cleaning_beam_agent-4_mean: 6.26
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.85
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-49-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 157.0
  episode_reward_mean: 107.14
  episode_reward_min: 14.0
  episodes_this_iter: 96
  episodes_total: 64224
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11897.622
    learner:
      agent-0:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.253623902797699
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007140266825444996
        model: {}
        policy_loss: -0.0014316365122795105
        total_loss: -0.0017750849947333336
        vf_explained_var: 0.0014549940824508667
        vf_loss: 1.0292860269546509
      agent-1:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2452637106180191
        entropy_coeff: 0.0017600000137463212
        kl: 0.001214990159496665
        model: {}
        policy_loss: -0.0014119870029389858
        total_loss: -0.0017978046089410782
        vf_explained_var: 0.08779573440551758
        vf_loss: 0.4584721624851227
      agent-2:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35273054242134094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013095596805214882
        model: {}
        policy_loss: -0.0016083563677966595
        total_loss: -0.0018623010255396366
        vf_explained_var: 0.032954633235931396
        vf_loss: 3.668612480163574
      agent-3:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5710818767547607
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012565871002152562
        model: {}
        policy_loss: -0.00163646275177598
        total_loss: -0.0025095969904214144
        vf_explained_var: 0.010150790214538574
        vf_loss: 1.319690227508545
      agent-4:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44185686111450195
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007808256195858121
        model: {}
        policy_loss: -0.0011943418066948652
        total_loss: -0.001881169038824737
        vf_explained_var: 0.008469179272651672
        vf_loss: 0.9083760976791382
      agent-5:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5389806032180786
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017760249320417643
        model: {}
        policy_loss: -0.0016814558766782284
        total_loss: -0.0025262264534831047
        vf_explained_var: 0.010655775666236877
        vf_loss: 1.0383378267288208
    load_time_ms: 14165.79
    num_steps_sampled: 64224000
    num_steps_trained: 64224000
    sample_time_ms: 119995.249
    update_time_ms: 15.674
  iterations_since_restore: 19
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.025480769230764
    ram_util_percent: 13.949519230769228
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 59.0
    agent-3: 31.0
    agent-4: 26.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 14.67
    agent-1: 7.5
    agent-2: 38.17
    agent-3: 17.52
    agent-4: 13.89
    agent-5: 15.39
  policy_reward_min:
    agent-0: 2.0
    agent-1: 0.0
    agent-2: 2.0
    agent-3: 2.0
    agent-4: 2.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.349584414060388
    mean_inference_ms: 15.225021750037023
    mean_processing_ms: 73.81502328125856
  time_since_restore: 2802.683254480362
  time_this_iter_s: 146.03966808319092
  time_total_s: 91739.36138415337
  timestamp: 1637365755
  timesteps_since_restore: 1824000
  timesteps_this_iter: 96000
  timesteps_total: 64224000
  training_iteration: 669
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    669 |          91739.4 | 64224000 |   107.14 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 6.01
    apples_agent-0_min: 0
    apples_agent-1_max: 18
    apples_agent-1_mean: 3.13
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 26.38
    apples_agent-2_min: 11
    apples_agent-3_max: 41
    apples_agent-3_mean: 8.25
    apples_agent-3_min: 1
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.12
    apples_agent-4_min: 2
    apples_agent-5_max: 32
    apples_agent-5_mean: 3.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 2.0
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 261
    cleaning_beam_agent-1_mean: 202.81
    cleaning_beam_agent-1_min: 78
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 5.23
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 45
    cleaning_beam_agent-3_mean: 11.02
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 19
    cleaning_beam_agent-4_mean: 6.38
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.76
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-51-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 154.0
  episode_reward_mean: 109.43
  episode_reward_min: 48.0
  episodes_this_iter: 96
  episodes_total: 64320
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11883.715
    learner:
      agent-0:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25216594338417053
        entropy_coeff: 0.0017600000137463212
        kl: 0.000650322122965008
        model: {}
        policy_loss: -0.0016124006360769272
        total_loss: -0.001958596520125866
        vf_explained_var: 0.005228638648986816
        vf_loss: 0.9761462807655334
      agent-1:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24445262551307678
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013552469899877906
        model: {}
        policy_loss: -0.0017931824550032616
        total_loss: -0.0021740905940532684
        vf_explained_var: 0.0852464884519577
        vf_loss: 0.49328920245170593
      agent-2:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3581588864326477
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011943972203880548
        model: {}
        policy_loss: -0.0017726505175232887
        total_loss: -0.002059216145426035
        vf_explained_var: 0.04254527390003204
        vf_loss: 3.4379515647888184
      agent-3:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5704640746116638
        entropy_coeff: 0.0017600000137463212
        kl: 0.00117157690692693
        model: {}
        policy_loss: -0.0015064277686178684
        total_loss: -0.0023779692128300667
        vf_explained_var: 0.0024626702070236206
        vf_loss: 1.3247594833374023
      agent-4:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4363192319869995
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007091506849974394
        model: {}
        policy_loss: -0.0013905190862715244
        total_loss: -0.002053688047453761
        vf_explained_var: 0.016355156898498535
        vf_loss: 1.0475144386291504
      agent-5:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5344605445861816
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010412284173071384
        model: {}
        policy_loss: -0.0015253806486725807
        total_loss: -0.0023541550617665052
        vf_explained_var: 0.009264469146728516
        vf_loss: 1.1187942028045654
    load_time_ms: 14161.617
    num_steps_sampled: 64320000
    num_steps_trained: 64320000
    sample_time_ms: 119918.786
    update_time_ms: 15.572
  iterations_since_restore: 20
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.15721153846154
    ram_util_percent: 13.953365384615381
  pid: 27065
  policy_reward_max:
    agent-0: 24.0
    agent-1: 16.0
    agent-2: 58.0
    agent-3: 29.0
    agent-4: 25.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 14.14
    agent-1: 7.56
    agent-2: 39.62
    agent-3: 17.97
    agent-4: 14.38
    agent-5: 15.76
  policy_reward_min:
    agent-0: 4.0
    agent-1: 1.0
    agent-2: 15.0
    agent-3: 7.0
    agent-4: 5.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.3457325210829
    mean_inference_ms: 15.221590439384412
    mean_processing_ms: 73.81643118181466
  time_since_restore: 2948.397839784622
  time_this_iter_s: 145.71458530426025
  time_total_s: 91885.07596945763
  timestamp: 1637365901
  timesteps_since_restore: 1920000
  timesteps_this_iter: 96000
  timesteps_total: 64320000
  training_iteration: 670
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    670 |          91885.1 | 64320000 |   109.43 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 5.99
    apples_agent-0_min: 1
    apples_agent-1_max: 20
    apples_agent-1_mean: 3.07
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 25.13
    apples_agent-2_min: 7
    apples_agent-3_max: 34
    apples_agent-3_mean: 7.97
    apples_agent-3_min: 1
    apples_agent-4_max: 18
    apples_agent-4_mean: 9.08
    apples_agent-4_min: 3
    apples_agent-5_max: 25
    apples_agent-5_mean: 3.11
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 1.83
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 245
    cleaning_beam_agent-1_mean: 200.86
    cleaning_beam_agent-1_min: 42
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 4.69
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 11.27
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 40
    cleaning_beam_agent-4_mean: 7.12
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.86
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-54-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 153.0
  episode_reward_mean: 108.02
  episode_reward_min: 34.0
  episodes_this_iter: 96
  episodes_total: 64416
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11886.743
    learner:
      agent-0:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2413197010755539
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008031611796468496
        model: {}
        policy_loss: -0.0016141561791300774
        total_loss: -0.0019362270832061768
        vf_explained_var: 0.005138203501701355
        vf_loss: 1.0265065431594849
      agent-1:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24449113011360168
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011525573208928108
        model: {}
        policy_loss: -0.0015131914988160133
        total_loss: -0.0018954863771796227
        vf_explained_var: 0.08260734379291534
        vf_loss: 0.48012036085128784
      agent-2:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3540797531604767
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014121963176876307
        model: {}
        policy_loss: -0.0018557040020823479
        total_loss: -0.002126009901985526
        vf_explained_var: 0.0345921516418457
        vf_loss: 3.528733968734741
      agent-3:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5765686631202698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017349068075418472
        model: {}
        policy_loss: -0.0018219482153654099
        total_loss: -0.0027085854671895504
        vf_explained_var: -0.0003910064697265625
        vf_loss: 1.2812345027923584
      agent-4:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45433536171913147
        entropy_coeff: 0.0017600000137463212
        kl: 0.001434185542166233
        model: {}
        policy_loss: -0.0015079602599143982
        total_loss: -0.0022084959782660007
        vf_explained_var: 0.01843021810054779
        vf_loss: 0.9909005165100098
      agent-5:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5473623871803284
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017238948494195938
        model: {}
        policy_loss: -0.0015944493934512138
        total_loss: -0.0024462630972266197
        vf_explained_var: 0.013662829995155334
        vf_loss: 1.1154173612594604
    load_time_ms: 14165.504
    num_steps_sampled: 64416000
    num_steps_trained: 64416000
    sample_time_ms: 119914.083
    update_time_ms: 15.664
  iterations_since_restore: 21
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.78047619047619
    ram_util_percent: 13.878095238095236
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 14.0
    agent-2: 60.0
    agent-3: 31.0
    agent-4: 29.0
    agent-5: 25.0
  policy_reward_mean:
    agent-0: 14.0
    agent-1: 7.37
    agent-2: 39.63
    agent-3: 17.57
    agent-4: 13.85
    agent-5: 15.6
  policy_reward_min:
    agent-0: 2.0
    agent-1: 1.0
    agent-2: 14.0
    agent-3: 3.0
    agent-4: 6.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.33793706626925
    mean_inference_ms: 15.217593455315253
    mean_processing_ms: 73.8194199751436
  time_since_restore: 3094.7575948238373
  time_this_iter_s: 146.3597550392151
  time_total_s: 92031.43572449684
  timestamp: 1637366049
  timesteps_since_restore: 2016000
  timesteps_this_iter: 96000
  timesteps_total: 64416000
  training_iteration: 671
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    671 |          92031.4 | 64416000 |   108.02 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 7.26
    apples_agent-0_min: 1
    apples_agent-1_max: 20
    apples_agent-1_mean: 3.25
    apples_agent-1_min: 0
    apples_agent-2_max: 40
    apples_agent-2_mean: 25.57
    apples_agent-2_min: 5
    apples_agent-3_max: 25
    apples_agent-3_mean: 7.05
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 9.41
    apples_agent-4_min: 3
    apples_agent-5_max: 24
    apples_agent-5_mean: 3.64
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 9
    cleaning_beam_agent-0_mean: 2.44
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 243
    cleaning_beam_agent-1_mean: 201.18
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 5.55
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 13.12
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 18
    cleaning_beam_agent-4_mean: 6.39
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.58
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-56-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 155.0
  episode_reward_mean: 108.46
  episode_reward_min: 47.0
  episodes_this_iter: 96
  episodes_total: 64512
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11886.111
    learner:
      agent-0:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2340577393770218
        entropy_coeff: 0.0017600000137463212
        kl: 0.001534297945909202
        model: {}
        policy_loss: -0.0015802488196641207
        total_loss: -0.0018946845084428787
        vf_explained_var: 0.009164854884147644
        vf_loss: 0.9750381708145142
      agent-1:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24825136363506317
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011753266444429755
        model: {}
        policy_loss: -0.001625583041459322
        total_loss: -0.002013802994042635
        vf_explained_var: 0.07523074746131897
        vf_loss: 0.48701342940330505
      agent-2:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36225223541259766
        entropy_coeff: 0.0017600000137463212
        kl: 0.001547016785480082
        model: {}
        policy_loss: -0.0017265821807086468
        total_loss: -0.0020094355568289757
        vf_explained_var: 0.03580421209335327
        vf_loss: 3.547112464904785
      agent-3:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5756486058235168
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017941469559445977
        model: {}
        policy_loss: -0.0016114427708089352
        total_loss: -0.0024893907830119133
        vf_explained_var: 0.0033325403928756714
        vf_loss: 1.35191810131073
      agent-4:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46355828642845154
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006806243327446282
        model: {}
        policy_loss: -0.0011856742203235626
        total_loss: -0.0019047418609261513
        vf_explained_var: 0.008327633142471313
        vf_loss: 0.9679560661315918
      agent-5:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5415066480636597
        entropy_coeff: 0.0017600000137463212
        kl: 0.001481904648244381
        model: {}
        policy_loss: -0.0014227039646357298
        total_loss: -0.0022695655934512615
        vf_explained_var: 0.01757623255252838
        vf_loss: 1.0618910789489746
    load_time_ms: 14154.577
    num_steps_sampled: 64512000
    num_steps_trained: 64512000
    sample_time_ms: 119870.391
    update_time_ms: 15.853
  iterations_since_restore: 22
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.011057692307695
    ram_util_percent: 13.867788461538462
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 17.0
    agent-2: 57.0
    agent-3: 33.0
    agent-4: 27.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 14.3
    agent-1: 7.72
    agent-2: 39.58
    agent-3: 17.73
    agent-4: 13.78
    agent-5: 15.35
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 18.0
    agent-3: 4.0
    agent-4: 5.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.331883268433316
    mean_inference_ms: 15.217335433387907
    mean_processing_ms: 73.81351504536718
  time_since_restore: 3240.1628289222717
  time_this_iter_s: 145.40523409843445
  time_total_s: 92176.84095859528
  timestamp: 1637366194
  timesteps_since_restore: 2112000
  timesteps_this_iter: 96000
  timesteps_total: 64512000
  training_iteration: 672
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    672 |          92176.8 | 64512000 |   108.46 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 6.47
    apples_agent-0_min: 1
    apples_agent-1_max: 20
    apples_agent-1_mean: 3.3
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 25.29
    apples_agent-2_min: 11
    apples_agent-3_max: 24
    apples_agent-3_mean: 6.8
    apples_agent-3_min: 2
    apples_agent-4_max: 37
    apples_agent-4_mean: 9.51
    apples_agent-4_min: 3
    apples_agent-5_max: 19
    apples_agent-5_mean: 3.33
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.99
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 242
    cleaning_beam_agent-1_mean: 194.78
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 5.28
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 14.4
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 27
    cleaning_beam_agent-4_mean: 7.24
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.2
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_18-59-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 108.47
  episode_reward_min: 76.0
  episodes_this_iter: 96
  episodes_total: 64608
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11882.76
    learner:
      agent-0:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24812856316566467
        entropy_coeff: 0.0017600000137463212
        kl: 0.000948198139667511
        model: {}
        policy_loss: -0.0018120752647519112
        total_loss: -0.0021480806171894073
        vf_explained_var: 0.006118223071098328
        vf_loss: 1.0070583820343018
      agent-1:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24446773529052734
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013950333232060075
        model: {}
        policy_loss: -0.0017292904667556286
        total_loss: -0.002109728753566742
        vf_explained_var: 0.05577075481414795
        vf_loss: 0.4982559084892273
      agent-2:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.360706090927124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011523778084665537
        model: {}
        policy_loss: -0.0018306486308574677
        total_loss: -0.002116262912750244
        vf_explained_var: 0.02838750183582306
        vf_loss: 3.4922800064086914
      agent-3:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.587675154209137
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013815673300996423
        model: {}
        policy_loss: -0.001769186113961041
        total_loss: -0.0026689767837524414
        vf_explained_var: 0.012615323066711426
        vf_loss: 1.3451926708221436
      agent-4:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46225011348724365
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005723393987864256
        model: {}
        policy_loss: -0.001472013769671321
        total_loss: -0.0021844529546797276
        vf_explained_var: 0.01165589690208435
        vf_loss: 1.011234998703003
      agent-5:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5515089631080627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015813700156286359
        model: {}
        policy_loss: -0.0016511810244992375
        total_loss: -0.0025097730103880167
        vf_explained_var: 0.013628974556922913
        vf_loss: 1.1206425428390503
    load_time_ms: 14192.634
    num_steps_sampled: 64608000
    num_steps_trained: 64608000
    sample_time_ms: 119785.4
    update_time_ms: 16.036
  iterations_since_restore: 23
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.114492753623182
    ram_util_percent: 13.951690821256038
  pid: 27065
  policy_reward_max:
    agent-0: 24.0
    agent-1: 16.0
    agent-2: 59.0
    agent-3: 40.0
    agent-4: 29.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 14.55
    agent-1: 7.65
    agent-2: 39.5
    agent-3: 17.24
    agent-4: 13.81
    agent-5: 15.72
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 21.0
    agent-3: 6.0
    agent-4: 5.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.3223324288588
    mean_inference_ms: 15.214566536625021
    mean_processing_ms: 73.81991844831107
  time_since_restore: 3386.0621876716614
  time_this_iter_s: 145.89935874938965
  time_total_s: 92322.74031734467
  timestamp: 1637366341
  timesteps_since_restore: 2208000
  timesteps_this_iter: 96000
  timesteps_total: 64608000
  training_iteration: 673
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    673 |          92322.7 | 64608000 |   108.47 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 6.18
    apples_agent-0_min: 0
    apples_agent-1_max: 45
    apples_agent-1_mean: 3.67
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 24.89
    apples_agent-2_min: 8
    apples_agent-3_max: 27
    apples_agent-3_mean: 6.7
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.61
    apples_agent-4_min: 0
    apples_agent-5_max: 35
    apples_agent-5_mean: 3.88
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.69
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 242
    cleaning_beam_agent-1_mean: 201.28
    cleaning_beam_agent-1_min: 75
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 5.68
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 14.35
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 27
    cleaning_beam_agent-4_mean: 6.38
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-01-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 147.0
  episode_reward_mean: 107.73
  episode_reward_min: 37.0
  episodes_this_iter: 96
  episodes_total: 64704
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11895.317
    learner:
      agent-0:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24499323964118958
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005351321306079626
        model: {}
        policy_loss: -0.0014341636560857296
        total_loss: -0.001764754531905055
        vf_explained_var: 0.008056148886680603
        vf_loss: 1.0059566497802734
      agent-1:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24441854655742645
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010323962196707726
        model: {}
        policy_loss: -0.0017000981606543064
        total_loss: -0.0020775874145329
        vf_explained_var: 0.048316314816474915
        vf_loss: 0.526847779750824
      agent-2:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3607318103313446
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012916679261252284
        model: {}
        policy_loss: -0.0018517766147851944
        total_loss: -0.002154435496777296
        vf_explained_var: 0.024743199348449707
        vf_loss: 3.3222827911376953
      agent-3:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5877635478973389
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015454570529982448
        model: {}
        policy_loss: -0.001934708096086979
        total_loss: -0.002841705922037363
        vf_explained_var: -0.0003719925880432129
        vf_loss: 1.2746827602386475
      agent-4:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45522329211235046
        entropy_coeff: 0.0017600000137463212
        kl: 0.00114634505007416
        model: {}
        policy_loss: -0.0015340629033744335
        total_loss: -0.002242414280772209
        vf_explained_var: 0.01781618595123291
        vf_loss: 0.9283949136734009
      agent-5:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5566728115081787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015577881131321192
        model: {}
        policy_loss: -0.0019805929623544216
        total_loss: -0.0028505176305770874
        vf_explained_var: 0.011231273412704468
        vf_loss: 1.0982054471969604
    load_time_ms: 14194.482
    num_steps_sampled: 64704000
    num_steps_trained: 64704000
    sample_time_ms: 119837.144
    update_time_ms: 16.107
  iterations_since_restore: 24
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.02535885167464
    ram_util_percent: 13.875598086124402
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 19.0
    agent-2: 57.0
    agent-3: 31.0
    agent-4: 23.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 14.48
    agent-1: 7.73
    agent-2: 39.04
    agent-3: 17.29
    agent-4: 13.74
    agent-5: 15.45
  policy_reward_min:
    agent-0: 5.0
    agent-1: 0.0
    agent-2: 17.0
    agent-3: 8.0
    agent-4: 2.0
    agent-5: 2.0
  sampler_perf:
    mean_env_wait_ms: 28.31584816085878
    mean_inference_ms: 15.213406021003415
    mean_processing_ms: 73.83394037505167
  time_since_restore: 3532.2974812984467
  time_this_iter_s: 146.23529362678528
  time_total_s: 92468.97561097145
  timestamp: 1637366487
  timesteps_since_restore: 2304000
  timesteps_this_iter: 96000
  timesteps_total: 64704000
  training_iteration: 674
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    674 |            92469 | 64704000 |   107.73 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 6.91
    apples_agent-0_min: 0
    apples_agent-1_max: 15
    apples_agent-1_mean: 3.34
    apples_agent-1_min: 0
    apples_agent-2_max: 68
    apples_agent-2_mean: 25.33
    apples_agent-2_min: 3
    apples_agent-3_max: 62
    apples_agent-3_mean: 6.75
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.2
    apples_agent-4_min: 0
    apples_agent-5_max: 21
    apples_agent-5_mean: 3.19
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.72
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 240
    cleaning_beam_agent-1_mean: 197.3
    cleaning_beam_agent-1_min: 18
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 6.07
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 14.78
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 22
    cleaning_beam_agent-4_mean: 6.56
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 3.05
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-03-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 144.0
  episode_reward_mean: 108.91
  episode_reward_min: 28.0
  episodes_this_iter: 96
  episodes_total: 64800
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11886.898
    learner:
      agent-0:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24901226162910461
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005326768150553107
        model: {}
        policy_loss: -0.0015648086555302143
        total_loss: -0.0018972279503941536
        vf_explained_var: 0.009955063462257385
        vf_loss: 1.0584355592727661
      agent-1:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24018606543540955
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012314493069425225
        model: {}
        policy_loss: -0.001650411169975996
        total_loss: -0.002023938111960888
        vf_explained_var: 0.0471179336309433
        vf_loss: 0.49198031425476074
      agent-2:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3609806001186371
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009613629663363099
        model: {}
        policy_loss: -0.0016687428578734398
        total_loss: -0.0019154073670506477
        vf_explained_var: 0.043014779686927795
        vf_loss: 3.886582136154175
      agent-3:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5892887115478516
        entropy_coeff: 0.0017600000137463212
        kl: 0.00225264229811728
        model: {}
        policy_loss: -0.001793903997167945
        total_loss: -0.002693459391593933
        vf_explained_var: -0.004964649677276611
        vf_loss: 1.3759340047836304
      agent-4:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44403576850891113
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008991172071546316
        model: {}
        policy_loss: -0.0012670381693169475
        total_loss: -0.0019431528635323048
        vf_explained_var: 0.012741833925247192
        vf_loss: 1.053883671760559
      agent-5:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.551925003528595
        entropy_coeff: 0.0017600000137463212
        kl: 0.001250453176908195
        model: {}
        policy_loss: -0.0016538742929697037
        total_loss: -0.0025070863775908947
        vf_explained_var: 0.012040272355079651
        vf_loss: 1.1817958354949951
    load_time_ms: 14186.918
    num_steps_sampled: 64800000
    num_steps_trained: 64800000
    sample_time_ms: 119857.045
    update_time_ms: 16.245
  iterations_since_restore: 25
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.067307692307693
    ram_util_percent: 13.88509615384615
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 16.0
    agent-2: 59.0
    agent-3: 31.0
    agent-4: 29.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 14.34
    agent-1: 7.81
    agent-2: 39.37
    agent-3: 17.85
    agent-4: 13.88
    agent-5: 15.66
  policy_reward_min:
    agent-0: 1.0
    agent-1: 0.0
    agent-2: 7.0
    agent-3: 6.0
    agent-4: 3.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.311245573939942
    mean_inference_ms: 15.212112719758544
    mean_processing_ms: 73.8458447702431
  time_since_restore: 3678.2011041641235
  time_this_iter_s: 145.90362286567688
  time_total_s: 92614.87923383713
  timestamp: 1637366633
  timesteps_since_restore: 2400000
  timesteps_this_iter: 96000
  timesteps_total: 64800000
  training_iteration: 675
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    675 |          92614.9 | 64800000 |   108.91 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 6.87
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.0
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 26.78
    apples_agent-2_min: 1
    apples_agent-3_max: 20
    apples_agent-3_mean: 7.07
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.36
    apples_agent-4_min: 0
    apples_agent-5_max: 21
    apples_agent-5_mean: 3.7
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 71
    cleaning_beam_agent-0_mean: 2.11
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 250
    cleaning_beam_agent-1_mean: 206.06
    cleaning_beam_agent-1_min: 26
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 5.14
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 38
    cleaning_beam_agent-3_mean: 15.67
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 20
    cleaning_beam_agent-4_mean: 6.85
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 2.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-06-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 158.0
  episode_reward_mean: 112.27
  episode_reward_min: 14.0
  episodes_this_iter: 96
  episodes_total: 64896
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11887.55
    learner:
      agent-0:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25615638494491577
        entropy_coeff: 0.0017600000137463212
        kl: 0.000560465850867331
        model: {}
        policy_loss: -0.0014725805958732963
        total_loss: -0.0018147829687222838
        vf_explained_var: 0.015622466802597046
        vf_loss: 1.0863227844238281
      agent-1:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24216791987419128
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012014849344268441
        model: {}
        policy_loss: -0.001649952493607998
        total_loss: -0.002029992640018463
        vf_explained_var: 0.09406371414661407
        vf_loss: 0.46175453066825867
      agent-2:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35917001962661743
        entropy_coeff: 0.0017600000137463212
        kl: 0.001266036182641983
        model: {}
        policy_loss: -0.0018456066027283669
        total_loss: -0.0021255104802548885
        vf_explained_var: 0.035581231117248535
        vf_loss: 3.522315502166748
      agent-3:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5921419262886047
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013030414702370763
        model: {}
        policy_loss: -0.0018518860451877117
        total_loss: -0.0027504514437168837
        vf_explained_var: -0.004864335060119629
        vf_loss: 1.4360456466674805
      agent-4:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.440849244594574
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008195972768589854
        model: {}
        policy_loss: -0.00142945209518075
        total_loss: -0.0021002357825636864
        vf_explained_var: 0.010282427072525024
        vf_loss: 1.0511116981506348
      agent-5:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5526741147041321
        entropy_coeff: 0.0017600000137463212
        kl: 0.002232144121080637
        model: {}
        policy_loss: -0.001573336310684681
        total_loss: -0.0024326625280082226
        vf_explained_var: 0.010861307382583618
        vf_loss: 1.1337974071502686
    load_time_ms: 14222.915
    num_steps_sampled: 64896000
    num_steps_trained: 64896000
    sample_time_ms: 119686.232
    update_time_ms: 16.067
  iterations_since_restore: 26
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.180676328502415
    ram_util_percent: 13.877777777777776
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 18.0
    agent-2: 63.0
    agent-3: 31.0
    agent-4: 31.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 14.57
    agent-1: 7.78
    agent-2: 40.79
    agent-3: 18.45
    agent-4: 14.77
    agent-5: 15.91
  policy_reward_min:
    agent-0: 1.0
    agent-1: 1.0
    agent-2: 4.0
    agent-3: 0.0
    agent-4: 3.0
    agent-5: 2.0
  sampler_perf:
    mean_env_wait_ms: 28.308367511066976
    mean_inference_ms: 15.208111834131568
    mean_processing_ms: 73.83738744090452
  time_since_restore: 3823.829679965973
  time_this_iter_s: 145.62857580184937
  time_total_s: 92760.50780963898
  timestamp: 1637366779
  timesteps_since_restore: 2496000
  timesteps_this_iter: 96000
  timesteps_total: 64896000
  training_iteration: 676
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    676 |          92760.5 | 64896000 |   112.27 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 6.59
    apples_agent-0_min: 1
    apples_agent-1_max: 37
    apples_agent-1_mean: 3.57
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 27.3
    apples_agent-2_min: 9
    apples_agent-3_max: 45
    apples_agent-3_mean: 7.58
    apples_agent-3_min: 1
    apples_agent-4_max: 26
    apples_agent-4_mean: 9.66
    apples_agent-4_min: 3
    apples_agent-5_max: 17
    apples_agent-5_mean: 3.27
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 9
    cleaning_beam_agent-0_mean: 1.25
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 264
    cleaning_beam_agent-1_mean: 213.67
    cleaning_beam_agent-1_min: 147
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 5.64
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 14.83
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 21
    cleaning_beam_agent-4_mean: 6.64
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.41
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-08-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 150.0
  episode_reward_mean: 112.32
  episode_reward_min: 65.0
  episodes_this_iter: 96
  episodes_total: 64992
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11882.169
    learner:
      agent-0:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2560100555419922
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005779548664577305
        model: {}
        policy_loss: -0.0015208302065730095
        total_loss: -0.0018709045834839344
        vf_explained_var: 0.007972180843353271
        vf_loss: 1.005020022392273
      agent-1:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2469940036535263
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007686279714107513
        model: {}
        policy_loss: -0.0015156539157032967
        total_loss: -0.0018963208422064781
        vf_explained_var: 0.07344044744968414
        vf_loss: 0.5404236912727356
      agent-2:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36667120456695557
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011095040244981647
        model: {}
        policy_loss: -0.0017806966789066792
        total_loss: -0.0020581865683197975
        vf_explained_var: 0.024317696690559387
        vf_loss: 3.678500175476074
      agent-3:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5861074328422546
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014045843854546547
        model: {}
        policy_loss: -0.0020094518549740314
        total_loss: -0.002902503591030836
        vf_explained_var: -0.002032354474067688
        vf_loss: 1.385009527206421
      agent-4:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42976099252700806
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008348539704456925
        model: {}
        policy_loss: -0.0014751108828932047
        total_loss: -0.002129869069904089
        vf_explained_var: 0.018205612897872925
        vf_loss: 1.016192078590393
      agent-5:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5550277829170227
        entropy_coeff: 0.0017600000137463212
        kl: 0.002201076364144683
        model: {}
        policy_loss: -0.0016879038885235786
        total_loss: -0.0025581982918083668
        vf_explained_var: 0.011637091636657715
        vf_loss: 1.0655105113983154
    load_time_ms: 14247.353
    num_steps_sampled: 64992000
    num_steps_trained: 64992000
    sample_time_ms: 119724.592
    update_time_ms: 16.107
  iterations_since_restore: 27
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.90095693779904
    ram_util_percent: 13.955023923444973
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 17.0
    agent-2: 62.0
    agent-3: 30.0
    agent-4: 26.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 14.5
    agent-1: 8.2
    agent-2: 41.53
    agent-3: 18.56
    agent-4: 13.95
    agent-5: 15.58
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 16.0
    agent-3: 8.0
    agent-4: 4.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.311639000679566
    mean_inference_ms: 15.205965706169486
    mean_processing_ms: 73.83476516747392
  time_since_restore: 3969.9287412166595
  time_this_iter_s: 146.09906125068665
  time_total_s: 92906.60687088966
  timestamp: 1637366925
  timesteps_since_restore: 2592000
  timesteps_this_iter: 96000
  timesteps_total: 64992000
  training_iteration: 677
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    677 |          92906.6 | 64992000 |   112.32 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 6.61
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 2.81
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 27.29
    apples_agent-2_min: 4
    apples_agent-3_max: 16
    apples_agent-3_mean: 6.67
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 9.71
    apples_agent-4_min: 3
    apples_agent-5_max: 10
    apples_agent-5_mean: 2.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.46
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 278
    cleaning_beam_agent-1_mean: 216.14
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 5.41
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 57
    cleaning_beam_agent-3_mean: 13.97
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 22
    cleaning_beam_agent-4_mean: 6.58
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.69
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-11-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 168.0
  episode_reward_mean: 114.04
  episode_reward_min: 35.0
  episodes_this_iter: 96
  episodes_total: 65088
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11875.457
    learner:
      agent-0:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2613242268562317
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008391588344238698
        model: {}
        policy_loss: -0.0016239234246313572
        total_loss: -0.0019916545134037733
        vf_explained_var: 0.00877535343170166
        vf_loss: 0.9219505786895752
      agent-1:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2491517961025238
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010297780390828848
        model: {}
        policy_loss: -0.0016306065954267979
        total_loss: -0.0020204465836286545
        vf_explained_var: 0.07088145613670349
        vf_loss: 0.486672580242157
      agent-2:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3708098828792572
        entropy_coeff: 0.0017600000137463212
        kl: 0.001393209444358945
        model: {}
        policy_loss: -0.0016613008920103312
        total_loss: -0.001921077724546194
        vf_explained_var: 0.04388947784900665
        vf_loss: 3.928473711013794
      agent-3:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5778287053108215
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015855043893679976
        model: {}
        policy_loss: -0.0019407463259994984
        total_loss: -0.002813987899571657
        vf_explained_var: -0.0013640224933624268
        vf_loss: 1.4373712539672852
      agent-4:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4402151107788086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007207758608274162
        model: {}
        policy_loss: -0.0013243160210549831
        total_loss: -0.0019961001817137003
        vf_explained_var: 0.004342913627624512
        vf_loss: 1.0299439430236816
      agent-5:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5593645572662354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021087860222905874
        model: {}
        policy_loss: -0.001811132999137044
        total_loss: -0.0026852318551391363
        vf_explained_var: 0.015058919787406921
        vf_loss: 1.103846788406372
    load_time_ms: 14215.912
    num_steps_sampled: 65088000
    num_steps_trained: 65088000
    sample_time_ms: 119689.535
    update_time_ms: 16.393
  iterations_since_restore: 28
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.056038647342998
    ram_util_percent: 13.954589371980674
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 17.0
    agent-2: 62.0
    agent-3: 32.0
    agent-4: 26.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 14.25
    agent-1: 7.62
    agent-2: 42.23
    agent-3: 18.8
    agent-4: 14.69
    agent-5: 16.45
  policy_reward_min:
    agent-0: 2.0
    agent-1: 1.0
    agent-2: 5.0
    agent-3: 5.0
    agent-4: 2.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.30932321804331
    mean_inference_ms: 15.203382535346934
    mean_processing_ms: 73.82448774108612
  time_since_restore: 4115.490966796875
  time_this_iter_s: 145.56222558021545
  time_total_s: 93052.16909646988
  timestamp: 1637367071
  timesteps_since_restore: 2688000
  timesteps_this_iter: 96000
  timesteps_total: 65088000
  training_iteration: 678
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    678 |          93052.2 | 65088000 |   114.04 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 6.21
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.06
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 26.92
    apples_agent-2_min: 12
    apples_agent-3_max: 31
    apples_agent-3_mean: 7.23
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.25
    apples_agent-4_min: 3
    apples_agent-5_max: 16
    apples_agent-5_mean: 2.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 1.8
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 249
    cleaning_beam_agent-1_mean: 210.46
    cleaning_beam_agent-1_min: 156
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 5.14
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 33
    cleaning_beam_agent-3_mean: 14.22
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 5.43
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.64
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-13-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 161.0
  episode_reward_mean: 113.57
  episode_reward_min: 63.0
  episodes_this_iter: 96
  episodes_total: 65184
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11879.898
    learner:
      agent-0:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26592153310775757
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007032750872895122
        model: {}
        policy_loss: -0.0015602062921971083
        total_loss: -0.0019303173758089542
        vf_explained_var: -0.0008356720209121704
        vf_loss: 0.9790758490562439
      agent-1:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24163848161697388
        entropy_coeff: 0.0017600000137463212
        kl: 0.001037470530718565
        model: {}
        policy_loss: -0.0013649319298565388
        total_loss: -0.0017349021509289742
        vf_explained_var: 0.06771291792392731
        vf_loss: 0.5531266927719116
      agent-2:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36210906505584717
        entropy_coeff: 0.0017600000137463212
        kl: 0.001445553731173277
        model: {}
        policy_loss: -0.0017221169546246529
        total_loss: -0.002014649100601673
        vf_explained_var: 0.01647685468196869
        vf_loss: 3.4478116035461426
      agent-3:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5713847875595093
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019357054261490703
        model: {}
        policy_loss: -0.0017110679764300585
        total_loss: -0.0025869952514767647
        vf_explained_var: -0.00592494010925293
        vf_loss: 1.2970778942108154
      agent-4:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4686865210533142
        entropy_coeff: 0.0017600000137463212
        kl: 0.001519269892014563
        model: {}
        policy_loss: -0.0015203002840280533
        total_loss: -0.002239915542304516
        vf_explained_var: 0.014875337481498718
        vf_loss: 1.0527126789093018
      agent-5:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5443419218063354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013682006392627954
        model: {}
        policy_loss: -0.001651486847549677
        total_loss: -0.002495220396667719
        vf_explained_var: 0.014299064874649048
        vf_loss: 1.1430792808532715
    load_time_ms: 14197.693
    num_steps_sampled: 65184000
    num_steps_trained: 65184000
    sample_time_ms: 119609.751
    update_time_ms: 16.098
  iterations_since_restore: 29
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.09806763285024
    ram_util_percent: 13.856521739130438
  pid: 27065
  policy_reward_max:
    agent-0: 33.0
    agent-1: 16.0
    agent-2: 63.0
    agent-3: 29.0
    agent-4: 26.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 14.42
    agent-1: 8.3
    agent-2: 40.63
    agent-3: 18.96
    agent-4: 14.89
    agent-5: 16.37
  policy_reward_min:
    agent-0: 4.0
    agent-1: 1.0
    agent-2: 21.0
    agent-3: 7.0
    agent-4: 6.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.30854339848073
    mean_inference_ms: 15.20116671806818
    mean_processing_ms: 73.81729784191226
  time_since_restore: 4260.637269258499
  time_this_iter_s: 145.14630246162415
  time_total_s: 93197.3153989315
  timestamp: 1637367216
  timesteps_since_restore: 2784000
  timesteps_this_iter: 96000
  timesteps_total: 65184000
  training_iteration: 679
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    679 |          93197.3 | 65184000 |   113.57 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 6.07
    apples_agent-0_min: 1
    apples_agent-1_max: 16
    apples_agent-1_mean: 2.99
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 26.26
    apples_agent-2_min: 13
    apples_agent-3_max: 36
    apples_agent-3_mean: 7.07
    apples_agent-3_min: 1
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.75
    apples_agent-4_min: 3
    apples_agent-5_max: 26
    apples_agent-5_mean: 3.6
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.79
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 253
    cleaning_beam_agent-1_mean: 213.49
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 5.9
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 14.82
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 22
    cleaning_beam_agent-4_mean: 5.95
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-16-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 152.0
  episode_reward_mean: 113.28
  episode_reward_min: 51.0
  episodes_this_iter: 96
  episodes_total: 65280
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11887.117
    learner:
      agent-0:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25864264369010925
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008407928980886936
        model: {}
        policy_loss: -0.0017039794474840164
        total_loss: -0.0020586480386555195
        vf_explained_var: 0.004923135042190552
        vf_loss: 1.0054726600646973
      agent-1:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24298034608364105
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010623091366142035
        model: {}
        policy_loss: -0.001527635846287012
        total_loss: -0.0019020885229110718
        vf_explained_var: 0.05078829824924469
        vf_loss: 0.5319090485572815
      agent-2:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3601430654525757
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008081074338406324
        model: {}
        policy_loss: -0.0016071870923042297
        total_loss: -0.0018890018109232187
        vf_explained_var: 0.035037070512771606
        vf_loss: 3.520400285720825
      agent-3:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5770056247711182
        entropy_coeff: 0.0017600000137463212
        kl: 0.001001768745481968
        model: {}
        policy_loss: -0.0016840922180563211
        total_loss: -0.0025700642727315426
        vf_explained_var: 0.005835339426994324
        vf_loss: 1.2955783605575562
      agent-4:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.471238374710083
        entropy_coeff: 0.0017600000137463212
        kl: 0.00076341035310179
        model: {}
        policy_loss: -0.0015656823525205255
        total_loss: -0.0023004431277513504
        vf_explained_var: 0.00908653438091278
        vf_loss: 0.9461874961853027
      agent-5:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5569117069244385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018339725211262703
        model: {}
        policy_loss: -0.0015624426305294037
        total_loss: -0.002430832013487816
        vf_explained_var: 0.010165765881538391
        vf_loss: 1.1177481412887573
    load_time_ms: 14190.469
    num_steps_sampled: 65280000
    num_steps_trained: 65280000
    sample_time_ms: 119627.826
    update_time_ms: 16.058
  iterations_since_restore: 30
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.013461538461534
    ram_util_percent: 13.952884615384612
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 18.0
    agent-2: 61.0
    agent-3: 37.0
    agent-4: 24.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 14.36
    agent-1: 7.98
    agent-2: 41.62
    agent-3: 18.7
    agent-4: 14.38
    agent-5: 16.24
  policy_reward_min:
    agent-0: 4.0
    agent-1: 0.0
    agent-2: 18.0
    agent-3: 9.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.311474378780623
    mean_inference_ms: 15.197194925951807
    mean_processing_ms: 73.81261764408677
  time_since_restore: 4406.483531713486
  time_this_iter_s: 145.84626245498657
  time_total_s: 93343.16166138649
  timestamp: 1637367362
  timesteps_since_restore: 2880000
  timesteps_this_iter: 96000
  timesteps_total: 65280000
  training_iteration: 680
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    680 |          93343.2 | 65280000 |   113.28 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 6.31
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.0
    apples_agent-1_min: 0
    apples_agent-2_max: 61
    apples_agent-2_mean: 26.87
    apples_agent-2_min: 6
    apples_agent-3_max: 42
    apples_agent-3_mean: 8.14
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.51
    apples_agent-4_min: 3
    apples_agent-5_max: 13
    apples_agent-5_mean: 2.96
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.22
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 255
    cleaning_beam_agent-1_mean: 211.43
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 5.96
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 13.42
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 25
    cleaning_beam_agent-4_mean: 5.7
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.29
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-18-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 165.0
  episode_reward_mean: 113.1
  episode_reward_min: 49.0
  episodes_this_iter: 96
  episodes_total: 65376
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11884.824
    learner:
      agent-0:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2783827781677246
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008477847441099584
        model: {}
        policy_loss: -0.0015368564054369926
        total_loss: -0.0019207438454031944
        vf_explained_var: 0.009189814329147339
        vf_loss: 1.060650110244751
      agent-1:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2435159832239151
        entropy_coeff: 0.0017600000137463212
        kl: 0.00104994373396039
        model: {}
        policy_loss: -0.0015009951312094927
        total_loss: -0.0018848092295229435
        vf_explained_var: 0.06739363074302673
        vf_loss: 0.44774767756462097
      agent-2:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3594696819782257
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014627868076786399
        model: {}
        policy_loss: -0.002012975513935089
        total_loss: -0.00228563672862947
        vf_explained_var: 0.022406652569770813
        vf_loss: 3.600041627883911
      agent-3:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5767377614974976
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019873364362865686
        model: {}
        policy_loss: -0.0018481109291315079
        total_loss: -0.0027197282761335373
        vf_explained_var: 0.0009459108114242554
        vf_loss: 1.4343957901000977
      agent-4:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46995091438293457
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009762023109942675
        model: {}
        policy_loss: -0.0015533710829913616
        total_loss: -0.0022821275051683187
        vf_explained_var: 0.009600982069969177
        vf_loss: 0.9835681915283203
      agent-5:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5633808374404907
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006752825574949384
        model: {}
        policy_loss: -0.0013381734024733305
        total_loss: -0.002212116727605462
        vf_explained_var: 0.01128271222114563
        vf_loss: 1.1760834455490112
    load_time_ms: 14177.923
    num_steps_sampled: 65376000
    num_steps_trained: 65376000
    sample_time_ms: 119612.543
    update_time_ms: 15.953
  iterations_since_restore: 31
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.906666666666666
    ram_util_percent: 13.874761904761904
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 13.0
    agent-2: 59.0
    agent-3: 36.0
    agent-4: 25.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 14.73
    agent-1: 7.75
    agent-2: 41.13
    agent-3: 19.36
    agent-4: 14.23
    agent-5: 15.9
  policy_reward_min:
    agent-0: 5.0
    agent-1: 1.0
    agent-2: 18.0
    agent-3: 6.0
    agent-4: 6.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.31154956499935
    mean_inference_ms: 15.1954167609232
    mean_processing_ms: 73.8151983961123
  time_since_restore: 4552.583861589432
  time_this_iter_s: 146.10032987594604
  time_total_s: 93489.26199126244
  timestamp: 1637367510
  timesteps_since_restore: 2976000
  timesteps_this_iter: 96000
  timesteps_total: 65376000
  training_iteration: 681
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    681 |          93489.3 | 65376000 |    113.1 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 6.46
    apples_agent-0_min: 1
    apples_agent-1_max: 13
    apples_agent-1_mean: 3.27
    apples_agent-1_min: 0
    apples_agent-2_max: 57
    apples_agent-2_mean: 26.98
    apples_agent-2_min: 6
    apples_agent-3_max: 18
    apples_agent-3_mean: 6.89
    apples_agent-3_min: 1
    apples_agent-4_max: 27
    apples_agent-4_mean: 9.83
    apples_agent-4_min: 1
    apples_agent-5_max: 14
    apples_agent-5_mean: 3.2
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.69
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 247
    cleaning_beam_agent-1_mean: 205.72
    cleaning_beam_agent-1_min: 18
    cleaning_beam_agent-2_max: 26
    cleaning_beam_agent-2_mean: 6.49
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 10.98
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 18
    cleaning_beam_agent-4_mean: 6.01
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 6
    cleaning_beam_agent-5_mean: 2.46
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-20-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 150.0
  episode_reward_mean: 111.33
  episode_reward_min: 23.0
  episodes_this_iter: 96
  episodes_total: 65472
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11888.533
    learner:
      agent-0:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2637858986854553
        entropy_coeff: 0.0017600000137463212
        kl: 0.00068474659929052
        model: {}
        policy_loss: -0.0016382113099098206
        total_loss: -0.002004795242100954
        vf_explained_var: 0.005127415060997009
        vf_loss: 0.9767741560935974
      agent-1:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23604947328567505
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017872583121061325
        model: {}
        policy_loss: -0.0016896524466574192
        total_loss: -0.0020533553324639797
        vf_explained_var: 0.07114458084106445
        vf_loss: 0.5174317359924316
      agent-2:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36821842193603516
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014183013699948788
        model: {}
        policy_loss: -0.0017257179133594036
        total_loss: -0.0020035419147461653
        vf_explained_var: 0.04317452013492584
        vf_loss: 3.702392578125
      agent-3:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5681960582733154
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018234853632748127
        model: {}
        policy_loss: -0.0015270481817424297
        total_loss: -0.0024052979424595833
        vf_explained_var: 0.0007733553647994995
        vf_loss: 1.217755675315857
      agent-4:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4686691462993622
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008226343779824674
        model: {}
        policy_loss: -0.0016783056780695915
        total_loss: -0.0024008164182305336
        vf_explained_var: 0.011859685182571411
        vf_loss: 1.0234700441360474
      agent-5:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5682576298713684
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009930479573085904
        model: {}
        policy_loss: -0.0014194983523339033
        total_loss: -0.0023142897989600897
        vf_explained_var: 0.014487490057945251
        vf_loss: 1.0534374713897705
    load_time_ms: 14173.43
    num_steps_sampled: 65472000
    num_steps_trained: 65472000
    sample_time_ms: 119624.698
    update_time_ms: 15.975
  iterations_since_restore: 32
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.05652173913044
    ram_util_percent: 13.94830917874396
  pid: 27065
  policy_reward_max:
    agent-0: 24.0
    agent-1: 20.0
    agent-2: 65.0
    agent-3: 33.0
    agent-4: 25.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 14.3
    agent-1: 8.12
    agent-2: 41.66
    agent-3: 17.93
    agent-4: 13.88
    agent-5: 15.44
  policy_reward_min:
    agent-0: 3.0
    agent-1: 2.0
    agent-2: 10.0
    agent-3: 1.0
    agent-4: 1.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.30728903259137
    mean_inference_ms: 15.194281835776453
    mean_processing_ms: 73.80965721113398
  time_since_restore: 4698.072434425354
  time_this_iter_s: 145.48857283592224
  time_total_s: 93634.75056409836
  timestamp: 1637367655
  timesteps_since_restore: 3072000
  timesteps_this_iter: 96000
  timesteps_total: 65472000
  training_iteration: 682
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    682 |          93634.8 | 65472000 |   111.33 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 5.95
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.19
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 25.54
    apples_agent-2_min: 3
    apples_agent-3_max: 40
    apples_agent-3_mean: 6.43
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.18
    apples_agent-4_min: 1
    apples_agent-5_max: 21
    apples_agent-5_mean: 3.55
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 5
    cleaning_beam_agent-0_mean: 1.39
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 245
    cleaning_beam_agent-1_mean: 195.04
    cleaning_beam_agent-1_min: 32
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 5.98
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 11.53
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 27
    cleaning_beam_agent-4_mean: 7.18
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 7
    cleaning_beam_agent-5_mean: 2.25
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-23-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 152.0
  episode_reward_mean: 106.66
  episode_reward_min: 21.0
  episodes_this_iter: 96
  episodes_total: 65568
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11886.522
    learner:
      agent-0:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27400878071784973
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006587739335373044
        model: {}
        policy_loss: -0.0015228025149554014
        total_loss: -0.0019176441710442305
        vf_explained_var: 0.011943221092224121
        vf_loss: 0.8741260170936584
      agent-1:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23172442615032196
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008729150285944343
        model: {}
        policy_loss: -0.0015053434763103724
        total_loss: -0.0018613110296428204
        vf_explained_var: 0.049266695976257324
        vf_loss: 0.51866215467453
      agent-2:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36359694600105286
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012488074135035276
        model: {}
        policy_loss: -0.001798007870092988
        total_loss: -0.0020944986026734114
        vf_explained_var: 0.008871987462043762
        vf_loss: 3.4343795776367188
      agent-3:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5676443576812744
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014055066276341677
        model: {}
        policy_loss: -0.0017100092954933643
        total_loss: -0.0025851926766335964
        vf_explained_var: -0.0020570755004882812
        vf_loss: 1.2387069463729858
      agent-4:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4710196256637573
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007961299270391464
        model: {}
        policy_loss: -0.0014932937920093536
        total_loss: -0.002230757847428322
        vf_explained_var: 0.019389748573303223
        vf_loss: 0.9153022170066833
      agent-5:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5682008266448975
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014562931610271335
        model: {}
        policy_loss: -0.001463274471461773
        total_loss: -0.002355579286813736
        vf_explained_var: 0.012761086225509644
        vf_loss: 1.0773111581802368
    load_time_ms: 14140.284
    num_steps_sampled: 65568000
    num_steps_trained: 65568000
    sample_time_ms: 119530.372
    update_time_ms: 16.063
  iterations_since_restore: 33
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.08252427184466
    ram_util_percent: 13.87621359223301
  pid: 27065
  policy_reward_max:
    agent-0: 24.0
    agent-1: 16.0
    agent-2: 57.0
    agent-3: 29.0
    agent-4: 27.0
    agent-5: 26.0
  policy_reward_mean:
    agent-0: 13.91
    agent-1: 7.95
    agent-2: 39.27
    agent-3: 17.25
    agent-4: 13.42
    agent-5: 14.86
  policy_reward_min:
    agent-0: 2.0
    agent-1: 1.0
    agent-2: 6.0
    agent-3: 5.0
    agent-4: 5.0
    agent-5: 2.0
  sampler_perf:
    mean_env_wait_ms: 28.297605470804974
    mean_inference_ms: 15.189935142566094
    mean_processing_ms: 73.79602770736767
  time_since_restore: 4842.643196821213
  time_this_iter_s: 144.57076239585876
  time_total_s: 93779.32132649422
  timestamp: 1637367800
  timesteps_since_restore: 3168000
  timesteps_this_iter: 96000
  timesteps_total: 65568000
  training_iteration: 683
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    683 |          93779.3 | 65568000 |   106.66 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 6.7
    apples_agent-0_min: 0
    apples_agent-1_max: 21
    apples_agent-1_mean: 3.05
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 25.33
    apples_agent-2_min: 13
    apples_agent-3_max: 21
    apples_agent-3_mean: 6.74
    apples_agent-3_min: 1
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.22
    apples_agent-4_min: 3
    apples_agent-5_max: 16
    apples_agent-5_mean: 3.15
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.58
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 233
    cleaning_beam_agent-1_mean: 198.93
    cleaning_beam_agent-1_min: 153
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 6.54
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 12.06
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 51
    cleaning_beam_agent-4_mean: 8.02
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.74
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-25-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 149.0
  episode_reward_mean: 109.83
  episode_reward_min: 73.0
  episodes_this_iter: 96
  episodes_total: 65664
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11868.263
    learner:
      agent-0:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26968061923980713
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010718905832618475
        model: {}
        policy_loss: -0.0017334503354504704
        total_loss: -0.0021127453073859215
        vf_explained_var: 0.007755547761917114
        vf_loss: 0.9534288048744202
      agent-1:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2333230972290039
        entropy_coeff: 0.0017600000137463212
        kl: 0.001287196297198534
        model: {}
        policy_loss: -0.0016553867608308792
        total_loss: -0.0020152749493718147
        vf_explained_var: 0.07647563517093658
        vf_loss: 0.5076103210449219
      agent-2:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36207443475723267
        entropy_coeff: 0.0017600000137463212
        kl: 0.001575556118041277
        model: {}
        policy_loss: -0.001968089025467634
        total_loss: -0.0022768646012991667
        vf_explained_var: 0.011227980256080627
        vf_loss: 3.28474497795105
      agent-3:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5634253621101379
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021455325186252594
        model: {}
        policy_loss: -0.001962108537554741
        total_loss: -0.0028249165043234825
        vf_explained_var: 0.005231961607933044
        vf_loss: 1.288150429725647
      agent-4:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46525514125823975
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006956717115826905
        model: {}
        policy_loss: -0.001244469080120325
        total_loss: -0.0019719921983778477
        vf_explained_var: 0.017811402678489685
        vf_loss: 0.9132697582244873
      agent-5:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5892553329467773
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020205655600875616
        model: {}
        policy_loss: -0.0016625936841592193
        total_loss: -0.0025941357016563416
        vf_explained_var: 0.015250563621520996
        vf_loss: 1.055480718612671
    load_time_ms: 14133.944
    num_steps_sampled: 65664000
    num_steps_trained: 65664000
    sample_time_ms: 119523.297
    update_time_ms: 15.971
  iterations_since_restore: 34
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.028846153846153
    ram_util_percent: 13.895673076923078
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 14.0
    agent-2: 57.0
    agent-3: 29.0
    agent-4: 26.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 14.7
    agent-1: 8.31
    agent-2: 39.44
    agent-3: 18.26
    agent-4: 13.79
    agent-5: 15.33
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 21.0
    agent-3: 8.0
    agent-4: 6.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.298226564916245
    mean_inference_ms: 15.188042161204898
    mean_processing_ms: 73.80124474238802
  time_since_restore: 4988.564266443253
  time_this_iter_s: 145.9210696220398
  time_total_s: 93925.24239611626
  timestamp: 1637367946
  timesteps_since_restore: 3264000
  timesteps_this_iter: 96000
  timesteps_total: 65664000
  training_iteration: 684
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    684 |          93925.2 | 65664000 |   109.83 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 6.36
    apples_agent-0_min: 1
    apples_agent-1_max: 7
    apples_agent-1_mean: 2.74
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 26.3
    apples_agent-2_min: 4
    apples_agent-3_max: 27
    apples_agent-3_mean: 7.78
    apples_agent-3_min: 1
    apples_agent-4_max: 25
    apples_agent-4_mean: 9.49
    apples_agent-4_min: 2
    apples_agent-5_max: 20
    apples_agent-5_mean: 3.53
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 9
    cleaning_beam_agent-0_mean: 1.67
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 265
    cleaning_beam_agent-1_mean: 205.86
    cleaning_beam_agent-1_min: 17
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 5.76
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 10.83
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 21
    cleaning_beam_agent-4_mean: 7.53
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.36
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-28-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 148.0
  episode_reward_mean: 112.09
  episode_reward_min: 20.0
  episodes_this_iter: 96
  episodes_total: 65760
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11880.598
    learner:
      agent-0:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26921263337135315
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007630738546140492
        model: {}
        policy_loss: -0.0016251560300588608
        total_loss: -0.001990013988688588
        vf_explained_var: 0.016787588596343994
        vf_loss: 1.0895448923110962
      agent-1:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23320017755031586
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013487611431628466
        model: {}
        policy_loss: -0.0017655680421739817
        total_loss: -0.0021290406584739685
        vf_explained_var: 0.059363678097724915
        vf_loss: 0.46961766481399536
      agent-2:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3600907623767853
        entropy_coeff: 0.0017600000137463212
        kl: 0.001237625489011407
        model: {}
        policy_loss: -0.0016692258650436997
        total_loss: -0.0019274686928838491
        vf_explained_var: 0.040061503648757935
        vf_loss: 3.755176544189453
      agent-3:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5520679950714111
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014046780997887254
        model: {}
        policy_loss: -0.0016686352901160717
        total_loss: -0.002480457304045558
        vf_explained_var: 0.007022753357887268
        vf_loss: 1.5981981754302979
      agent-4:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45393162965774536
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012152090203016996
        model: {}
        policy_loss: -0.0016416942235082388
        total_loss: -0.0023442183155566454
        vf_explained_var: 0.01512056589126587
        vf_loss: 0.96397864818573
      agent-5:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5962206125259399
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010021764319390059
        model: {}
        policy_loss: -0.001607462763786316
        total_loss: -0.0025453385896980762
        vf_explained_var: 0.01575733721256256
        vf_loss: 1.1147112846374512
    load_time_ms: 14141.682
    num_steps_sampled: 65760000
    num_steps_trained: 65760000
    sample_time_ms: 119440.068
    update_time_ms: 16.13
  iterations_since_restore: 35
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.02753623188406
    ram_util_percent: 13.956038647342993
  pid: 27065
  policy_reward_max:
    agent-0: 35.0
    agent-1: 17.0
    agent-2: 61.0
    agent-3: 30.0
    agent-4: 28.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.0
    agent-1: 7.61
    agent-2: 40.48
    agent-3: 19.58
    agent-4: 14.1
    agent-5: 15.32
  policy_reward_min:
    agent-0: 2.0
    agent-1: 0.0
    agent-2: -26.0
    agent-3: 4.0
    agent-4: 3.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.294242677915257
    mean_inference_ms: 15.184811894583406
    mean_processing_ms: 73.79458369832908
  time_since_restore: 5133.889730453491
  time_this_iter_s: 145.32546401023865
  time_total_s: 94070.5678601265
  timestamp: 1637368091
  timesteps_since_restore: 3360000
  timesteps_this_iter: 96000
  timesteps_total: 65760000
  training_iteration: 685
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    685 |          94070.6 | 65760000 |   112.09 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 6.07
    apples_agent-0_min: 0
    apples_agent-1_max: 16
    apples_agent-1_mean: 3.06
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 26.35
    apples_agent-2_min: 13
    apples_agent-3_max: 28
    apples_agent-3_mean: 6.86
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 9.64
    apples_agent-4_min: 1
    apples_agent-5_max: 15
    apples_agent-5_mean: 3.71
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 1.31
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 206.02
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 5.96
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 30
    cleaning_beam_agent-3_mean: 9.79
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 33
    cleaning_beam_agent-4_mean: 7.48
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.44
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-30-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 174.0
  episode_reward_mean: 110.26
  episode_reward_min: 65.0
  episodes_this_iter: 96
  episodes_total: 65856
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11886.281
    learner:
      agent-0:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2767360210418701
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008007442229427397
        model: {}
        policy_loss: -0.0017428371356800199
        total_loss: -0.002133673056960106
        vf_explained_var: 0.008098289370536804
        vf_loss: 0.9622032046318054
      agent-1:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23362044990062714
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013698241673409939
        model: {}
        policy_loss: -0.0016703776782378554
        total_loss: -0.002033689059317112
        vf_explained_var: 0.052856072783470154
        vf_loss: 0.47858095169067383
      agent-2:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36183398962020874
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010332460515201092
        model: {}
        policy_loss: -0.0016100583598017693
        total_loss: -0.0019024526700377464
        vf_explained_var: 0.02769210934638977
        vf_loss: 3.444348096847534
      agent-3:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5508140325546265
        entropy_coeff: 0.0017600000137463212
        kl: 0.001927038887515664
        model: {}
        policy_loss: -0.0015918407589197159
        total_loss: -0.0024182768538594246
        vf_explained_var: -0.00873073935508728
        vf_loss: 1.429969072341919
      agent-4:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4506995677947998
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006511019892059267
        model: {}
        policy_loss: -0.0014166156761348248
        total_loss: -0.002110730390995741
        vf_explained_var: -9.72747802734375e-05
        vf_loss: 0.9911614060401917
      agent-5:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6015577912330627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010830427054315805
        model: {}
        policy_loss: -0.001393991056829691
        total_loss: -0.002345829736441374
        vf_explained_var: -0.002079501748085022
        vf_loss: 1.0690155029296875
    load_time_ms: 14101.728
    num_steps_sampled: 65856000
    num_steps_trained: 65856000
    sample_time_ms: 119473.846
    update_time_ms: 16.268
  iterations_since_restore: 36
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.96826923076923
    ram_util_percent: 13.937019230769229
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 16.0
    agent-2: 71.0
    agent-3: 35.0
    agent-4: 28.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 14.39
    agent-1: 7.84
    agent-2: 40.46
    agent-3: 18.23
    agent-4: 14.24
    agent-5: 15.1
  policy_reward_min:
    agent-0: 4.0
    agent-1: 2.0
    agent-2: 23.0
    agent-3: 8.0
    agent-4: 4.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.290718422556214
    mean_inference_ms: 15.183142946684141
    mean_processing_ms: 73.79105498379158
  time_since_restore: 5279.5118951797485
  time_this_iter_s: 145.62216472625732
  time_total_s: 94216.19002485275
  timestamp: 1637368237
  timesteps_since_restore: 3456000
  timesteps_this_iter: 96000
  timesteps_total: 65856000
  training_iteration: 686
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    686 |          94216.2 | 65856000 |   110.26 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 6.51
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 2.77
    apples_agent-1_min: 0
    apples_agent-2_max: 39
    apples_agent-2_mean: 26.42
    apples_agent-2_min: 10
    apples_agent-3_max: 18
    apples_agent-3_mean: 6.84
    apples_agent-3_min: 1
    apples_agent-4_max: 24
    apples_agent-4_mean: 9.31
    apples_agent-4_min: 1
    apples_agent-5_max: 20
    apples_agent-5_mean: 3.85
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.22
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 250
    cleaning_beam_agent-1_mean: 209.51
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 5.26
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 10.08
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 30
    cleaning_beam_agent-4_mean: 7.09
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.53
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-33-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 156.0
  episode_reward_mean: 112.44
  episode_reward_min: 71.0
  episodes_this_iter: 96
  episodes_total: 65952
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11888.433
    learner:
      agent-0:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2747013568878174
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008263520430773497
        model: {}
        policy_loss: -0.001620089286006987
        total_loss: -0.0019981744699180126
        vf_explained_var: 0.003208339214324951
        vf_loss: 1.0539031028747559
      agent-1:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23447057604789734
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007570625748485327
        model: {}
        policy_loss: -0.0014645401388406754
        total_loss: -0.0018264874815940857
        vf_explained_var: 0.05048185586929321
        vf_loss: 0.5071988701820374
      agent-2:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3578573167324066
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014480113750323653
        model: {}
        policy_loss: -0.0017359987832605839
        total_loss: -0.0019934289157390594
        vf_explained_var: 0.012272477149963379
        vf_loss: 3.723989248275757
      agent-3:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5559135675430298
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014544123550876975
        model: {}
        policy_loss: -0.0016610072925686836
        total_loss: -0.002497598994523287
        vf_explained_var: 0.004434272646903992
        vf_loss: 1.4181764125823975
      agent-4:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44978809356689453
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009030987857840955
        model: {}
        policy_loss: -0.0013609640300273895
        total_loss: -0.0020552147179841995
        vf_explained_var: 0.0017438232898712158
        vf_loss: 0.9737715721130371
      agent-5:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6042797565460205
        entropy_coeff: 0.0017600000137463212
        kl: 0.002081046812236309
        model: {}
        policy_loss: -0.00172425527125597
        total_loss: -0.002677472308278084
        vf_explained_var: 0.009781122207641602
        vf_loss: 1.1031655073165894
    load_time_ms: 14087.597
    num_steps_sampled: 65952000
    num_steps_trained: 65952000
    sample_time_ms: 119433.643
    update_time_ms: 16.164
  iterations_since_restore: 37
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.073913043478257
    ram_util_percent: 13.874879227053142
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 17.0
    agent-2: 56.0
    agent-3: 38.0
    agent-4: 29.0
    agent-5: 26.0
  policy_reward_mean:
    agent-0: 15.28
    agent-1: 7.67
    agent-2: 41.6
    agent-3: 18.9
    agent-4: 13.5
    agent-5: 15.49
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 25.0
    agent-3: 10.0
    agent-4: 5.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.28853507749487
    mean_inference_ms: 15.179991038073595
    mean_processing_ms: 73.78660089374257
  time_since_restore: 5425.0817992687225
  time_this_iter_s: 145.569904088974
  time_total_s: 94361.75992894173
  timestamp: 1637368383
  timesteps_since_restore: 3552000
  timesteps_this_iter: 96000
  timesteps_total: 65952000
  training_iteration: 687
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    687 |          94361.8 | 65952000 |   112.44 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 6.92
    apples_agent-0_min: 1
    apples_agent-1_max: 22
    apples_agent-1_mean: 3.3
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 27.57
    apples_agent-2_min: 14
    apples_agent-3_max: 22
    apples_agent-3_mean: 7.25
    apples_agent-3_min: 2
    apples_agent-4_max: 24
    apples_agent-4_mean: 9.64
    apples_agent-4_min: 3
    apples_agent-5_max: 17
    apples_agent-5_mean: 3.29
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.23
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 205.82
    cleaning_beam_agent-1_min: 148
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 6.74
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 10.31
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 21
    cleaning_beam_agent-4_mean: 7.17
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 7
    cleaning_beam_agent-5_mean: 2.77
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-35-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 154.0
  episode_reward_mean: 112.11
  episode_reward_min: 56.0
  episodes_this_iter: 96
  episodes_total: 66048
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11883.716
    learner:
      agent-0:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.263958603143692
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010809313971549273
        model: {}
        policy_loss: -0.0014741502236574888
        total_loss: -0.0018418858526274562
        vf_explained_var: 0.007612541317939758
        vf_loss: 0.9683350920677185
      agent-1:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23506952822208405
        entropy_coeff: 0.0017600000137463212
        kl: 0.000998735660687089
        model: {}
        policy_loss: -0.0015699276700615883
        total_loss: -0.0019370419904589653
        vf_explained_var: 0.059891149401664734
        vf_loss: 0.46610361337661743
      agent-2:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3755631446838379
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016055882442742586
        model: {}
        policy_loss: -0.0018416992388665676
        total_loss: -0.00215236097574234
        vf_explained_var: 0.02365279197692871
        vf_loss: 3.503305196762085
      agent-3:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5531228184700012
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016000134637579322
        model: {}
        policy_loss: -0.0018725189147517085
        total_loss: -0.0026994477957487106
        vf_explained_var: 0.0010001510381698608
        vf_loss: 1.4656862020492554
      agent-4:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46749168634414673
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012924191541969776
        model: {}
        policy_loss: -0.001703241840004921
        total_loss: -0.0024314317852258682
        vf_explained_var: 0.0015422701835632324
        vf_loss: 0.9459443688392639
      agent-5:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5953239798545837
        entropy_coeff: 0.0017600000137463212
        kl: 0.002605606336146593
        model: {}
        policy_loss: -0.002103816717863083
        total_loss: -0.003039442701265216
        vf_explained_var: 0.014674842357635498
        vf_loss: 1.121429443359375
    load_time_ms: 14098.435
    num_steps_sampled: 66048000
    num_steps_trained: 66048000
    sample_time_ms: 119375.444
    update_time_ms: 16.221
  iterations_since_restore: 38
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.135265700483092
    ram_util_percent: 13.958937198067629
  pid: 27065
  policy_reward_max:
    agent-0: 23.0
    agent-1: 17.0
    agent-2: 61.0
    agent-3: 33.0
    agent-4: 26.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 14.52
    agent-1: 7.92
    agent-2: 41.71
    agent-3: 18.77
    agent-4: 14.07
    agent-5: 15.12
  policy_reward_min:
    agent-0: 4.0
    agent-1: 2.0
    agent-2: 26.0
    agent-3: 7.0
    agent-4: 5.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.286645732998874
    mean_inference_ms: 15.177543334923199
    mean_processing_ms: 73.7820638041248
  time_since_restore: 5570.125351190567
  time_this_iter_s: 145.04355192184448
  time_total_s: 94506.80348086357
  timestamp: 1637368528
  timesteps_since_restore: 3648000
  timesteps_this_iter: 96000
  timesteps_total: 66048000
  training_iteration: 688
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    688 |          94506.8 | 66048000 |   112.11 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 6.47
    apples_agent-0_min: 1
    apples_agent-1_max: 11
    apples_agent-1_mean: 3.05
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 26.58
    apples_agent-2_min: 15
    apples_agent-3_max: 36
    apples_agent-3_mean: 7.26
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 9.19
    apples_agent-4_min: 3
    apples_agent-5_max: 15
    apples_agent-5_mean: 3.2
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 9
    cleaning_beam_agent-0_mean: 1.57
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 255
    cleaning_beam_agent-1_mean: 212.68
    cleaning_beam_agent-1_min: 172
    cleaning_beam_agent-2_max: 26
    cleaning_beam_agent-2_mean: 6.77
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 31
    cleaning_beam_agent-3_mean: 9.82
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 52
    cleaning_beam_agent-4_mean: 7.04
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 7
    cleaning_beam_agent-5_mean: 2.43
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-37-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 158.0
  episode_reward_mean: 111.41
  episode_reward_min: 75.0
  episodes_this_iter: 96
  episodes_total: 66144
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11892.644
    learner:
      agent-0:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2573859989643097
        entropy_coeff: 0.0017600000137463212
        kl: 0.001111711491830647
        model: {}
        policy_loss: -0.0019243776332587004
        total_loss: -0.0022758657578378916
        vf_explained_var: 0.004459172487258911
        vf_loss: 1.0151170492172241
      agent-1:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23845142126083374
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011651332024484873
        model: {}
        policy_loss: -0.0016218882519751787
        total_loss: -0.0019951744470745325
        vf_explained_var: 0.05266211926937103
        vf_loss: 0.4638770520687103
      agent-2:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3680114150047302
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011094665387645364
        model: {}
        policy_loss: -0.0016757957637310028
        total_loss: -0.001995320664718747
        vf_explained_var: 0.0383286327123642
        vf_loss: 3.2817721366882324
      agent-3:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5522156953811646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016762706218287349
        model: {}
        policy_loss: -0.0017929929308593273
        total_loss: -0.002627917565405369
        vf_explained_var: -0.0058585405349731445
        vf_loss: 1.3697432279586792
      agent-4:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.473233163356781
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010080266511067748
        model: {}
        policy_loss: -0.0013703159056603909
        total_loss: -0.0021065836772322655
        vf_explained_var: 0.008410707116127014
        vf_loss: 0.9662123918533325
      agent-5:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5796648263931274
        entropy_coeff: 0.0017600000137463212
        kl: 0.001489832065999508
        model: {}
        policy_loss: -0.0016282021533697844
        total_loss: -0.0025250588078051805
        vf_explained_var: 0.008211776614189148
        vf_loss: 1.2335336208343506
    load_time_ms: 14117.588
    num_steps_sampled: 66144000
    num_steps_trained: 66144000
    sample_time_ms: 119451.26
    update_time_ms: 16.31
  iterations_since_restore: 39
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.885576923076925
    ram_util_percent: 13.958173076923073
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 15.0
    agent-2: 65.0
    agent-3: 34.0
    agent-4: 27.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.08
    agent-1: 7.53
    agent-2: 40.36
    agent-3: 18.6
    agent-4: 13.72
    agent-5: 16.12
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 24.0
    agent-3: 7.0
    agent-4: 4.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.286157066996253
    mean_inference_ms: 15.176009466759524
    mean_processing_ms: 73.78007150931232
  time_since_restore: 5716.336664915085
  time_this_iter_s: 146.21131372451782
  time_total_s: 94653.01479458809
  timestamp: 1637368674
  timesteps_since_restore: 3744000
  timesteps_this_iter: 96000
  timesteps_total: 66144000
  training_iteration: 689
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 25.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    689 |            94653 | 66144000 |   111.41 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 6.46
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 2.91
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 27.79
    apples_agent-2_min: 14
    apples_agent-3_max: 22
    apples_agent-3_mean: 7.17
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 9.43
    apples_agent-4_min: 1
    apples_agent-5_max: 15
    apples_agent-5_mean: 3.77
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 1.45
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 249
    cleaning_beam_agent-1_mean: 211.85
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 6.53
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 26
    cleaning_beam_agent-3_mean: 9.13
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 25
    cleaning_beam_agent-4_mean: 7.5
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.42
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-40-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 162.0
  episode_reward_mean: 116.71
  episode_reward_min: 76.0
  episodes_this_iter: 96
  episodes_total: 66240
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11891.465
    learner:
      agent-0:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2708854377269745
        entropy_coeff: 0.0017600000137463212
        kl: 0.001349442289210856
        model: {}
        policy_loss: -0.00198598625138402
        total_loss: -0.002356237033382058
        vf_explained_var: 0.007908895611763
        vf_loss: 1.0650744438171387
      agent-1:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23684322834014893
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010453840950503945
        model: {}
        policy_loss: -0.0015897847479209304
        total_loss: -0.0019581103697419167
        vf_explained_var: 0.06747056543827057
        vf_loss: 0.4851790964603424
      agent-2:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36987924575805664
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014612409286201
        model: {}
        policy_loss: -0.001904644537717104
        total_loss: -0.0021993941627442837
        vf_explained_var: 0.02951185405254364
        vf_loss: 3.562366485595703
      agent-3:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5551115274429321
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020421836525201797
        model: {}
        policy_loss: -0.0019224068382754922
        total_loss: -0.00275707570835948
        vf_explained_var: 0.0049024224281311035
        vf_loss: 1.4232726097106934
      agent-4:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4836030602455139
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014519506366923451
        model: {}
        policy_loss: -0.001523615326732397
        total_loss: -0.002277397084981203
        vf_explained_var: 0.0084601491689682
        vf_loss: 0.9736372828483582
      agent-5:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.575205385684967
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012040564324706793
        model: {}
        policy_loss: -0.0015796045772731304
        total_loss: -0.002475068438798189
        vf_explained_var: 0.018816888332366943
        vf_loss: 1.1689869165420532
    load_time_ms: 14109.079
    num_steps_sampled: 66240000
    num_steps_trained: 66240000
    sample_time_ms: 119408.54
    update_time_ms: 16.494
  iterations_since_restore: 40
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.042512077294692
    ram_util_percent: 13.876811594202898
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 18.0
    agent-2: 60.0
    agent-3: 33.0
    agent-4: 31.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 15.14
    agent-1: 7.94
    agent-2: 42.63
    agent-3: 19.74
    agent-4: 14.71
    agent-5: 16.55
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 29.0
    agent-3: 9.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.283150340638393
    mean_inference_ms: 15.17558554927636
    mean_processing_ms: 73.7660590053995
  time_since_restore: 5861.657355070114
  time_this_iter_s: 145.3206901550293
  time_total_s: 94798.33548474312
  timestamp: 1637368820
  timesteps_since_restore: 3840000
  timesteps_this_iter: 96000
  timesteps_total: 66240000
  training_iteration: 690
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    690 |          94798.3 | 66240000 |   116.71 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 6.43
    apples_agent-0_min: 1
    apples_agent-1_max: 27
    apples_agent-1_mean: 3.15
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 27.52
    apples_agent-2_min: 4
    apples_agent-3_max: 18
    apples_agent-3_mean: 7.07
    apples_agent-3_min: 1
    apples_agent-4_max: 25
    apples_agent-4_mean: 10.14
    apples_agent-4_min: 1
    apples_agent-5_max: 36
    apples_agent-5_mean: 4.14
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.26
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 260
    cleaning_beam_agent-1_mean: 215.67
    cleaning_beam_agent-1_min: 23
    cleaning_beam_agent-2_max: 26
    cleaning_beam_agent-2_mean: 6.07
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 27
    cleaning_beam_agent-3_mean: 10.45
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 19
    cleaning_beam_agent-4_mean: 6.2
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.77
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-42-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 176.0
  episode_reward_mean: 117.16
  episode_reward_min: 21.0
  episodes_this_iter: 96
  episodes_total: 66336
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11884.341
    learner:
      agent-0:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2697986960411072
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009899984579533339
        model: {}
        policy_loss: -0.001631646417081356
        total_loss: -0.001995901810005307
        vf_explained_var: 0.00884324312210083
        vf_loss: 1.1059417724609375
      agent-1:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23721492290496826
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011215839767828584
        model: {}
        policy_loss: -0.0016658154781907797
        total_loss: -0.0020277099683880806
        vf_explained_var: 0.05132108926773071
        vf_loss: 0.5560187101364136
      agent-2:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36411386728286743
        entropy_coeff: 0.0017600000137463212
        kl: 0.001154413796029985
        model: {}
        policy_loss: -0.0018375599756836891
        total_loss: -0.0020816533360630274
        vf_explained_var: 0.031687960028648376
        vf_loss: 3.9674906730651855
      agent-3:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.556367039680481
        entropy_coeff: 0.0017600000137463212
        kl: 0.001428296323865652
        model: {}
        policy_loss: -0.0014202254824340343
        total_loss: -0.002247826661914587
        vf_explained_var: 0.005288287997245789
        vf_loss: 1.5160387754440308
      agent-4:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47603780031204224
        entropy_coeff: 0.0017600000137463212
        kl: 0.001044743461534381
        model: {}
        policy_loss: -0.001706174574792385
        total_loss: -0.002431442029774189
        vf_explained_var: 0.005941018462181091
        vf_loss: 1.1256413459777832
      agent-5:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5729484558105469
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023812067229300737
        model: {}
        policy_loss: -0.0018428359180688858
        total_loss: -0.002733543049544096
        vf_explained_var: 0.01168861985206604
        vf_loss: 1.1768267154693604
    load_time_ms: 14109.443
    num_steps_sampled: 66336000
    num_steps_trained: 66336000
    sample_time_ms: 119387.575
    update_time_ms: 16.321
  iterations_since_restore: 41
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.9352380952381
    ram_util_percent: 13.944761904761902
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 21.0
    agent-2: 67.0
    agent-3: 34.0
    agent-4: 27.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 15.65
    agent-1: 8.23
    agent-2: 42.45
    agent-3: 19.68
    agent-4: 14.61
    agent-5: 16.54
  policy_reward_min:
    agent-0: 4.0
    agent-1: 0.0
    agent-2: 5.0
    agent-3: 5.0
    agent-4: 2.0
    agent-5: 2.0
  sampler_perf:
    mean_env_wait_ms: 28.282751097411715
    mean_inference_ms: 15.175009617053666
    mean_processing_ms: 73.7675841883336
  time_since_restore: 6007.475447893143
  time_this_iter_s: 145.81809282302856
  time_total_s: 94944.15357756615
  timestamp: 1637368967
  timesteps_since_restore: 3936000
  timesteps_this_iter: 96000
  timesteps_total: 66336000
  training_iteration: 691
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    691 |          94944.2 | 66336000 |   117.16 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 6.76
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.24
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 26.62
    apples_agent-2_min: 11
    apples_agent-3_max: 33
    apples_agent-3_mean: 7.56
    apples_agent-3_min: 0
    apples_agent-4_max: 28
    apples_agent-4_mean: 9.73
    apples_agent-4_min: 2
    apples_agent-5_max: 19
    apples_agent-5_mean: 3.38
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 1.03
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 214.93
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 6.09
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 9.8
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 14
    cleaning_beam_agent-4_mean: 6.02
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.78
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-45-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 170.0
  episode_reward_mean: 117.18
  episode_reward_min: 63.0
  episodes_this_iter: 96
  episodes_total: 66432
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11878.378
    learner:
      agent-0:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27655306458473206
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005545158055610955
        model: {}
        policy_loss: -0.0014670777600258589
        total_loss: -0.0018492641393095255
        vf_explained_var: -0.00041857361793518066
        vf_loss: 1.0454623699188232
      agent-1:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23746523261070251
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008266704389825463
        model: {}
        policy_loss: -0.0016010268591344357
        total_loss: -0.0019665604922920465
        vf_explained_var: 0.08001691102981567
        vf_loss: 0.524095356464386
      agent-2:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36679261922836304
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012245266698300838
        model: {}
        policy_loss: -0.001872918102890253
        total_loss: -0.002140670083463192
        vf_explained_var: 0.046597450971603394
        vf_loss: 3.7780494689941406
      agent-3:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.560774028301239
        entropy_coeff: 0.0017600000137463212
        kl: 0.001522524980828166
        model: {}
        policy_loss: -0.00172986532561481
        total_loss: -0.0025632234755903482
        vf_explained_var: 0.0057558417320251465
        vf_loss: 1.5360338687896729
      agent-4:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47345876693725586
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006765320431441069
        model: {}
        policy_loss: -0.0013363528996706009
        total_loss: -0.002063577529042959
        vf_explained_var: 0.015081793069839478
        vf_loss: 1.0606520175933838
      agent-5:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5778166651725769
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011203960748389363
        model: {}
        policy_loss: -0.0015349164605140686
        total_loss: -0.0024263309314846992
        vf_explained_var: 0.011290207505226135
        vf_loss: 1.2554304599761963
    load_time_ms: 14106.203
    num_steps_sampled: 66432000
    num_steps_trained: 66432000
    sample_time_ms: 119360.781
    update_time_ms: 16.174
  iterations_since_restore: 42
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.13333333333333
    ram_util_percent: 13.955555555555554
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 15.0
    agent-2: 59.0
    agent-3: 34.0
    agent-4: 28.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.09
    agent-1: 8.46
    agent-2: 42.58
    agent-3: 20.36
    agent-4: 14.47
    agent-5: 16.22
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 20.0
    agent-3: 8.0
    agent-4: 3.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.282654088544287
    mean_inference_ms: 15.173072026559792
    mean_processing_ms: 73.76507007743058
  time_since_restore: 6152.601786375046
  time_this_iter_s: 145.12633848190308
  time_total_s: 95089.27991604805
  timestamp: 1637369112
  timesteps_since_restore: 4032000
  timesteps_this_iter: 96000
  timesteps_total: 66432000
  training_iteration: 692
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    692 |          95089.3 | 66432000 |   117.18 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 7.31
    apples_agent-0_min: 1
    apples_agent-1_max: 10
    apples_agent-1_mean: 2.79
    apples_agent-1_min: 0
    apples_agent-2_max: 63
    apples_agent-2_mean: 27.44
    apples_agent-2_min: 15
    apples_agent-3_max: 30
    apples_agent-3_mean: 7.46
    apples_agent-3_min: 1
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.66
    apples_agent-4_min: 2
    apples_agent-5_max: 14
    apples_agent-5_mean: 3.64
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.4
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 276
    cleaning_beam_agent-1_mean: 216.84
    cleaning_beam_agent-1_min: 165
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 6.23
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 9.59
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 17
    cleaning_beam_agent-4_mean: 8.11
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.37
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-47-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 164.0
  episode_reward_mean: 114.37
  episode_reward_min: 75.0
  episodes_this_iter: 96
  episodes_total: 66528
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11880.483
    learner:
      agent-0:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27990153431892395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014798552729189396
        model: {}
        policy_loss: -0.002054721349850297
        total_loss: -0.0024415424559265375
        vf_explained_var: 0.00883910059928894
        vf_loss: 1.0580662488937378
      agent-1:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23430131375789642
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011866571148857474
        model: {}
        policy_loss: -0.0017830230062827468
        total_loss: -0.002151141408830881
        vf_explained_var: 0.0720239132642746
        vf_loss: 0.4425356984138489
      agent-2:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3611370325088501
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011049667373299599
        model: {}
        policy_loss: -0.001843967940658331
        total_loss: -0.002132980152964592
        vf_explained_var: 0.03286992013454437
        vf_loss: 3.4659080505371094
      agent-3:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5621451735496521
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015229331329464912
        model: {}
        policy_loss: -0.0019947984255850315
        total_loss: -0.0028476326260715723
        vf_explained_var: -0.0004592388868331909
        vf_loss: 1.3654201030731201
      agent-4:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4703161120414734
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010412472765892744
        model: {}
        policy_loss: -0.001637984998524189
        total_loss: -0.0023680240847170353
        vf_explained_var: 0.011831879615783691
        vf_loss: 0.9771777391433716
      agent-5:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5654858946800232
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018627617973834276
        model: {}
        policy_loss: -0.0017053568735718727
        total_loss: -0.0025866711512207985
        vf_explained_var: 0.011311262845993042
        vf_loss: 1.13942551612854
    load_time_ms: 14110.479
    num_steps_sampled: 66528000
    num_steps_trained: 66528000
    sample_time_ms: 119442.745
    update_time_ms: 16.009
  iterations_since_restore: 43
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.957692307692312
    ram_util_percent: 13.878846153846155
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 18.0
    agent-2: 63.0
    agent-3: 33.0
    agent-4: 29.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.21
    agent-1: 7.79
    agent-2: 41.77
    agent-3: 18.5
    agent-4: 14.82
    agent-5: 16.28
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 25.0
    agent-3: 7.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.282534335713695
    mean_inference_ms: 15.16903568749991
    mean_processing_ms: 73.75605539007707
  time_since_restore: 6298.056615591049
  time_this_iter_s: 145.45482921600342
  time_total_s: 95234.73474526405
  timestamp: 1637369258
  timesteps_since_restore: 4128000
  timesteps_this_iter: 96000
  timesteps_total: 66528000
  training_iteration: 693
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    693 |          95234.7 | 66528000 |   114.37 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 6.6
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.17
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 28.38
    apples_agent-2_min: 16
    apples_agent-3_max: 25
    apples_agent-3_mean: 7.64
    apples_agent-3_min: 1
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.69
    apples_agent-4_min: 3
    apples_agent-5_max: 13
    apples_agent-5_mean: 3.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.39
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 264
    cleaning_beam_agent-1_mean: 218.27
    cleaning_beam_agent-1_min: 168
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 5.83
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 10.28
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 21
    cleaning_beam_agent-4_mean: 8.59
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.61
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-50-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 118.43
  episode_reward_min: 66.0
  episodes_this_iter: 96
  episodes_total: 66624
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11884.924
    learner:
      agent-0:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2819291353225708
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007192644407041371
        model: {}
        policy_loss: -0.0016349251382052898
        total_loss: -0.002028839197009802
        vf_explained_var: 0.007774978876113892
        vf_loss: 1.0228314399719238
      agent-1:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23609432578086853
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011745997471734881
        model: {}
        policy_loss: -0.0016657453961670399
        total_loss: -0.0020302636548876762
        vf_explained_var: 0.06746576726436615
        vf_loss: 0.5100719928741455
      agent-2:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3651636242866516
        entropy_coeff: 0.0017600000137463212
        kl: 0.001108798780478537
        model: {}
        policy_loss: -0.0017329682596027851
        total_loss: -0.0019710869528353214
        vf_explained_var: 0.03528621792793274
        vf_loss: 4.045663833618164
      agent-3:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.561507523059845
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014638241846114397
        model: {}
        policy_loss: -0.001691505080088973
        total_loss: -0.002535950392484665
        vf_explained_var: 0.010771974921226501
        vf_loss: 1.438077449798584
      agent-4:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45994871854782104
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005399390356615186
        model: {}
        policy_loss: -0.0013463553041219711
        total_loss: -0.0020324504002928734
        vf_explained_var: 0.019537076354026794
        vf_loss: 1.23414146900177
      agent-5:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5711803436279297
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016991677694022655
        model: {}
        policy_loss: -0.0018752273172140121
        total_loss: -0.002752590924501419
        vf_explained_var: 0.010194465517997742
        vf_loss: 1.2791385650634766
    load_time_ms: 14103.265
    num_steps_sampled: 66624000
    num_steps_trained: 66624000
    sample_time_ms: 119376.661
    update_time_ms: 15.881
  iterations_since_restore: 44
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.02173913043478
    ram_util_percent: 13.941545893719805
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 61.0
    agent-3: 32.0
    agent-4: 33.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 14.86
    agent-1: 8.43
    agent-2: 43.65
    agent-3: 19.04
    agent-4: 15.38
    agent-5: 17.07
  policy_reward_min:
    agent-0: 3.0
    agent-1: 3.0
    agent-2: 23.0
    agent-3: 9.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.279268393187998
    mean_inference_ms: 15.166136964067118
    mean_processing_ms: 73.7486298240879
  time_since_restore: 6443.291040182114
  time_this_iter_s: 145.23442459106445
  time_total_s: 95379.96916985512
  timestamp: 1637369403
  timesteps_since_restore: 4224000
  timesteps_this_iter: 96000
  timesteps_total: 66624000
  training_iteration: 694
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    694 |            95380 | 66624000 |   118.43 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 6.43
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 2.92
    apples_agent-1_min: 0
    apples_agent-2_max: 223
    apples_agent-2_mean: 29.68
    apples_agent-2_min: 12
    apples_agent-3_max: 24
    apples_agent-3_mean: 7.1
    apples_agent-3_min: 2
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.99
    apples_agent-4_min: 1
    apples_agent-5_max: 22
    apples_agent-5_mean: 4.13
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 1.14
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 266
    cleaning_beam_agent-1_mean: 223.94
    cleaning_beam_agent-1_min: 184
    cleaning_beam_agent-2_max: 30
    cleaning_beam_agent-2_mean: 6.48
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 30
    cleaning_beam_agent-3_mean: 10.99
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 29
    cleaning_beam_agent-4_mean: 8.58
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.82
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-52-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 172.0
  episode_reward_mean: 115.54
  episode_reward_min: 79.0
  episodes_this_iter: 96
  episodes_total: 66720
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11882.559
    learner:
      agent-0:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28482431173324585
        entropy_coeff: 0.0017600000137463212
        kl: 0.001005226862616837
        model: {}
        policy_loss: -0.0015993071720004082
        total_loss: -0.00199320400133729
        vf_explained_var: 0.0019173473119735718
        vf_loss: 1.0739223957061768
      agent-1:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2368289679288864
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011755231535062194
        model: {}
        policy_loss: -0.0017628015484660864
        total_loss: -0.002125949366018176
        vf_explained_var: 0.07541647553443909
        vf_loss: 0.5367222428321838
      agent-2:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36297690868377686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011835834011435509
        model: {}
        policy_loss: -0.0016710422933101654
        total_loss: -0.001924276351928711
        vf_explained_var: 0.027419164776802063
        vf_loss: 3.856001853942871
      agent-3:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5579800605773926
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014889722224324942
        model: {}
        policy_loss: -0.0019882861524820328
        total_loss: -0.002831611782312393
        vf_explained_var: -0.0039893388748168945
        vf_loss: 1.3871920108795166
      agent-4:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44870156049728394
        entropy_coeff: 0.0017600000137463212
        kl: 0.001862981473095715
        model: {}
        policy_loss: -0.0016803499311208725
        total_loss: -0.0023793019354343414
        vf_explained_var: 0.019706979393959045
        vf_loss: 0.9076222777366638
      agent-5:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5588253736495972
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013436725130304694
        model: {}
        policy_loss: -0.0015370133332908154
        total_loss: -0.0024015018716454506
        vf_explained_var: 0.013954535126686096
        vf_loss: 1.190464973449707
    load_time_ms: 14109.227
    num_steps_sampled: 66720000
    num_steps_trained: 66720000
    sample_time_ms: 119390.821
    update_time_ms: 16.013
  iterations_since_restore: 45
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.98743961352657
    ram_util_percent: 13.882125603864736
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 17.0
    agent-2: 62.0
    agent-3: 36.0
    agent-4: 24.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 15.58
    agent-1: 8.34
    agent-2: 42.57
    agent-3: 18.76
    agent-4: 14.31
    agent-5: 15.98
  policy_reward_min:
    agent-0: 3.0
    agent-1: 1.0
    agent-2: 26.0
    agent-3: 7.0
    agent-4: 3.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.279419847015685
    mean_inference_ms: 15.163380869987003
    mean_processing_ms: 73.73671901569713
  time_since_restore: 6588.7782192230225
  time_this_iter_s: 145.4871790409088
  time_total_s: 95525.45634889603
  timestamp: 1637369549
  timesteps_since_restore: 4320000
  timesteps_this_iter: 96000
  timesteps_total: 66720000
  training_iteration: 695
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    695 |          95525.5 | 66720000 |   115.54 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 6.44
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 2.74
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 27.3
    apples_agent-2_min: 13
    apples_agent-3_max: 20
    apples_agent-3_mean: 6.92
    apples_agent-3_min: 1
    apples_agent-4_max: 42
    apples_agent-4_mean: 9.96
    apples_agent-4_min: 2
    apples_agent-5_max: 13
    apples_agent-5_mean: 3.59
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.28
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 265
    cleaning_beam_agent-1_mean: 224.71
    cleaning_beam_agent-1_min: 169
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 6.49
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 28
    cleaning_beam_agent-3_mean: 10.71
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 19
    cleaning_beam_agent-4_mean: 8.12
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.68
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-54-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 153.0
  episode_reward_mean: 115.35
  episode_reward_min: 53.0
  episodes_this_iter: 96
  episodes_total: 66816
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11879.964
    learner:
      agent-0:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26699957251548767
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010432853596284986
        model: {}
        policy_loss: -0.0018334412015974522
        total_loss: -0.002204256597906351
        vf_explained_var: 0.013104677200317383
        vf_loss: 0.9910386204719543
      agent-1:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2327083945274353
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011256090365350246
        model: {}
        policy_loss: -0.001503822742961347
        total_loss: -0.0018667130498215556
        vf_explained_var: 0.06853604316711426
        vf_loss: 0.4667424261569977
      agent-2:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3682301640510559
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010545998811721802
        model: {}
        policy_loss: -0.0014862865209579468
        total_loss: -0.0017808466218411922
        vf_explained_var: 0.027308225631713867
        vf_loss: 3.5352578163146973
      agent-3:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5577782392501831
        entropy_coeff: 0.0017600000137463212
        kl: 0.002017080318182707
        model: {}
        policy_loss: -0.001882388023659587
        total_loss: -0.0027254228480160236
        vf_explained_var: 0.0022944360971450806
        vf_loss: 1.386540174484253
      agent-4:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4425079822540283
        entropy_coeff: 0.0017600000137463212
        kl: 0.000474996049888432
        model: {}
        policy_loss: -0.0012638773769140244
        total_loss: -0.0019465144723653793
        vf_explained_var: 0.007305741310119629
        vf_loss: 0.9617732763290405
      agent-5:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5566414594650269
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019165402045473456
        model: {}
        policy_loss: -0.0014422731474041939
        total_loss: -0.002306613838300109
        vf_explained_var: 0.008663639426231384
        vf_loss: 1.1535117626190186
    load_time_ms: 14109.937
    num_steps_sampled: 66816000
    num_steps_trained: 66816000
    sample_time_ms: 119391.277
    update_time_ms: 16.199
  iterations_since_restore: 46
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.01201923076923
    ram_util_percent: 13.94759615384615
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 15.0
    agent-2: 61.0
    agent-3: 30.0
    agent-4: 27.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 15.07
    agent-1: 7.99
    agent-2: 42.51
    agent-3: 18.78
    agent-4: 14.85
    agent-5: 16.15
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 24.0
    agent-3: 8.0
    agent-4: 7.0
    agent-5: -40.0
  sampler_perf:
    mean_env_wait_ms: 28.281819224956447
    mean_inference_ms: 15.162524195048686
    mean_processing_ms: 73.73327026892254
  time_since_restore: 6734.403412103653
  time_this_iter_s: 145.6251928806305
  time_total_s: 95671.08154177666
  timestamp: 1637369695
  timesteps_since_restore: 4416000
  timesteps_this_iter: 96000
  timesteps_total: 66816000
  training_iteration: 696
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    696 |          95671.1 | 66816000 |   115.35 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 7.59
    apples_agent-0_min: 0
    apples_agent-1_max: 22
    apples_agent-1_mean: 3.2
    apples_agent-1_min: 0
    apples_agent-2_max: 54
    apples_agent-2_mean: 27.43
    apples_agent-2_min: 5
    apples_agent-3_max: 27
    apples_agent-3_mean: 7.41
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.37
    apples_agent-4_min: 1
    apples_agent-5_max: 19
    apples_agent-5_mean: 3.49
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 9
    cleaning_beam_agent-0_mean: 1.91
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 254
    cleaning_beam_agent-1_mean: 215.34
    cleaning_beam_agent-1_min: 42
    cleaning_beam_agent-2_max: 33
    cleaning_beam_agent-2_mean: 6.2
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 31
    cleaning_beam_agent-3_mean: 11.11
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 19
    cleaning_beam_agent-4_mean: 7.63
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 7
    cleaning_beam_agent-5_mean: 2.49
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-57-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 174.0
  episode_reward_mean: 115.01
  episode_reward_min: 19.0
  episodes_this_iter: 96
  episodes_total: 66912
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11897.821
    learner:
      agent-0:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2633433938026428
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006153990398161113
        model: {}
        policy_loss: -0.0014852158492431045
        total_loss: -0.0018374279607087374
        vf_explained_var: 0.010432600975036621
        vf_loss: 1.1127328872680664
      agent-1:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22806131839752197
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010106060653924942
        model: {}
        policy_loss: -0.001461226143874228
        total_loss: -0.0018146615475416183
        vf_explained_var: 0.07584510743618011
        vf_loss: 0.4795342981815338
      agent-2:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3681384027004242
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016363358590751886
        model: {}
        policy_loss: -0.0018800441175699234
        total_loss: -0.002130884677171707
        vf_explained_var: 0.03144490718841553
        vf_loss: 3.970827341079712
      agent-3:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5697599649429321
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014768012333661318
        model: {}
        policy_loss: -0.0018267808482050896
        total_loss: -0.002670721150934696
        vf_explained_var: 0.0009759962558746338
        vf_loss: 1.5883668661117554
      agent-4:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45444631576538086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013887140667065978
        model: {}
        policy_loss: -0.001565692014992237
        total_loss: -0.0022735667880624533
        vf_explained_var: 0.016962900757789612
        vf_loss: 0.9195235371589661
      agent-5:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5571100115776062
        entropy_coeff: 0.0017600000137463212
        kl: 0.000797382032033056
        model: {}
        policy_loss: -0.0013920709025114775
        total_loss: -0.0022478553000837564
        vf_explained_var: 0.016350477933883667
        vf_loss: 1.2472944259643555
    load_time_ms: 14133.77
    num_steps_sampled: 66912000
    num_steps_trained: 66912000
    sample_time_ms: 119428.893
    update_time_ms: 16.231
  iterations_since_restore: 47
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.075480769230765
    ram_util_percent: 13.88076923076923
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 15.0
    agent-2: 64.0
    agent-3: 36.0
    agent-4: 23.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 14.84
    agent-1: 7.94
    agent-2: 42.19
    agent-3: 19.36
    agent-4: 14.13
    agent-5: 16.55
  policy_reward_min:
    agent-0: 2.0
    agent-1: 1.0
    agent-2: 10.0
    agent-3: 2.0
    agent-4: 2.0
    agent-5: 1.0
  sampler_perf:
    mean_env_wait_ms: 28.28482390261159
    mean_inference_ms: 15.162449031650878
    mean_processing_ms: 73.74063443848463
  time_since_restore: 6880.788279771805
  time_this_iter_s: 146.38486766815186
  time_total_s: 95817.46640944481
  timestamp: 1637369841
  timesteps_since_restore: 4512000
  timesteps_this_iter: 96000
  timesteps_total: 66912000
  training_iteration: 697
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    697 |          95817.5 | 66912000 |   115.01 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 7.05
    apples_agent-0_min: 0
    apples_agent-1_max: 16
    apples_agent-1_mean: 2.79
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 26.53
    apples_agent-2_min: 13
    apples_agent-3_max: 27
    apples_agent-3_mean: 8.18
    apples_agent-3_min: 2
    apples_agent-4_max: 19
    apples_agent-4_mean: 8.74
    apples_agent-4_min: 2
    apples_agent-5_max: 28
    apples_agent-5_mean: 4.2
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 17
    cleaning_beam_agent-0_mean: 1.62
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 251
    cleaning_beam_agent-1_mean: 211.12
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 6.2
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 10.72
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 24
    cleaning_beam_agent-4_mean: 7.74
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 2.55
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_19-59-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 168.0
  episode_reward_mean: 111.38
  episode_reward_min: 69.0
  episodes_this_iter: 96
  episodes_total: 67008
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11901.142
    learner:
      agent-0:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2638547420501709
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006686728447675705
        model: {}
        policy_loss: -0.001687091775238514
        total_loss: -0.002050666604191065
        vf_explained_var: 0.00931704044342041
        vf_loss: 1.0081465244293213
      agent-1:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23261043429374695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010106887202709913
        model: {}
        policy_loss: -0.001675332896411419
        total_loss: -0.002035846933722496
        vf_explained_var: 0.0817258358001709
        vf_loss: 0.48880258202552795
      agent-2:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37179136276245117
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009784861467778683
        model: {}
        policy_loss: -0.0015985709615051746
        total_loss: -0.0018829982727766037
        vf_explained_var: 0.018746882677078247
        vf_loss: 3.6992502212524414
      agent-3:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5637410879135132
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015739024383947253
        model: {}
        policy_loss: -0.0016937375767156482
        total_loss: -0.0025385699700564146
        vf_explained_var: 0.0017355233430862427
        vf_loss: 1.4735000133514404
      agent-4:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46076470613479614
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014478539815172553
        model: {}
        policy_loss: -0.0015014579985290766
        total_loss: -0.002217851812019944
        vf_explained_var: 0.013965979218482971
        vf_loss: 0.9455358386039734
      agent-5:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5517763495445251
        entropy_coeff: 0.0017600000137463212
        kl: 0.000907363195437938
        model: {}
        policy_loss: -0.0012758122757077217
        total_loss: -0.002138553187251091
        vf_explained_var: 0.017384231090545654
        vf_loss: 1.0838603973388672
    load_time_ms: 14116.796
    num_steps_sampled: 67008000
    num_steps_trained: 67008000
    sample_time_ms: 119457.153
    update_time_ms: 16.048
  iterations_since_restore: 48
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.086473429951692
    ram_util_percent: 13.9487922705314
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 19.0
    agent-2: 64.0
    agent-3: 32.0
    agent-4: 27.0
    agent-5: 26.0
  policy_reward_mean:
    agent-0: 15.08
    agent-1: 7.53
    agent-2: 41.03
    agent-3: 18.55
    agent-4: 13.34
    agent-5: 15.85
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 22.0
    agent-3: 10.0
    agent-4: 5.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.284025340491898
    mean_inference_ms: 15.16039232119779
    mean_processing_ms: 73.73535524606653
  time_since_restore: 7025.96765422821
  time_this_iter_s: 145.17937445640564
  time_total_s: 95962.64578390121
  timestamp: 1637369987
  timesteps_since_restore: 4608000
  timesteps_this_iter: 96000
  timesteps_total: 67008000
  training_iteration: 698
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    698 |          95962.6 | 67008000 |   111.38 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 6.53
    apples_agent-0_min: 1
    apples_agent-1_max: 14
    apples_agent-1_mean: 3.24
    apples_agent-1_min: 0
    apples_agent-2_max: 38
    apples_agent-2_mean: 26.94
    apples_agent-2_min: 9
    apples_agent-3_max: 23
    apples_agent-3_mean: 7.36
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.46
    apples_agent-4_min: 1
    apples_agent-5_max: 28
    apples_agent-5_mean: 4.15
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 1.35
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 265
    cleaning_beam_agent-1_mean: 211.39
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 7.0
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 11.52
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 31
    cleaning_beam_agent-4_mean: 8.35
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.58
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-02-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 157.0
  episode_reward_mean: 114.46
  episode_reward_min: 41.0
  episodes_this_iter: 96
  episodes_total: 67104
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11895.031
    learner:
      agent-0:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26684510707855225
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008073454373516142
        model: {}
        policy_loss: -0.001543350052088499
        total_loss: -0.0019117780029773712
        vf_explained_var: 0.01816694438457489
        vf_loss: 1.0121904611587524
      agent-1:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.234206885099411
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010328651405870914
        model: {}
        policy_loss: -0.0017838245257735252
        total_loss: -0.002145681995898485
        vf_explained_var: 0.07740040123462677
        vf_loss: 0.5034983158111572
      agent-2:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3727642595767975
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013729252386838198
        model: {}
        policy_loss: -0.0019042117055505514
        total_loss: -0.0022085190284997225
        vf_explained_var: 0.028904318809509277
        vf_loss: 3.517573833465576
      agent-3:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5577311515808105
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013709410559386015
        model: {}
        policy_loss: -0.00187003705650568
        total_loss: -0.0026963301934301853
        vf_explained_var: 0.0007598549127578735
        vf_loss: 1.5531246662139893
      agent-4:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4589778482913971
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014154664240777493
        model: {}
        policy_loss: -0.0015627583488821983
        total_loss: -0.0022757602855563164
        vf_explained_var: 0.02324680984020233
        vf_loss: 0.9479906558990479
      agent-5:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5490468740463257
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011980317067354918
        model: {}
        policy_loss: -0.001494591124355793
        total_loss: -0.002343404106795788
        vf_explained_var: 0.01352304220199585
        vf_loss: 1.1750948429107666
    load_time_ms: 14122.503
    num_steps_sampled: 67104000
    num_steps_trained: 67104000
    sample_time_ms: 119403.751
    update_time_ms: 15.813
  iterations_since_restore: 49
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.033653846153847
    ram_util_percent: 13.957211538461536
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 16.0
    agent-2: 60.0
    agent-3: 36.0
    agent-4: 30.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 14.48
    agent-1: 7.94
    agent-2: 41.97
    agent-3: 19.78
    agent-4: 14.2
    agent-5: 16.09
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 15.0
    agent-3: 9.0
    agent-4: 6.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.283923332842363
    mean_inference_ms: 15.16000257071431
    mean_processing_ms: 73.73348716173727
  time_since_restore: 7171.661906003952
  time_this_iter_s: 145.69425177574158
  time_total_s: 96108.34003567696
  timestamp: 1637370132
  timesteps_since_restore: 4704000
  timesteps_this_iter: 96000
  timesteps_total: 67104000
  training_iteration: 699
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    699 |          96108.3 | 67104000 |   114.46 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 6.25
    apples_agent-0_min: 1
    apples_agent-1_max: 14
    apples_agent-1_mean: 2.98
    apples_agent-1_min: 0
    apples_agent-2_max: 55
    apples_agent-2_mean: 26.71
    apples_agent-2_min: 15
    apples_agent-3_max: 17
    apples_agent-3_mean: 6.76
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 9.52
    apples_agent-4_min: 3
    apples_agent-5_max: 19
    apples_agent-5_mean: 3.64
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 1.34
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 273
    cleaning_beam_agent-1_mean: 210.55
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 6.87
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 33
    cleaning_beam_agent-3_mean: 11.73
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 28
    cleaning_beam_agent-4_mean: 8.22
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 7
    cleaning_beam_agent-5_mean: 2.65
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-04-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 160.0
  episode_reward_mean: 113.51
  episode_reward_min: 72.0
  episodes_this_iter: 96
  episodes_total: 67200
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11897.025
    learner:
      agent-0:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2664737105369568
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006651357980445027
        model: {}
        policy_loss: -0.0014210171066224575
        total_loss: -0.0017879069782793522
        vf_explained_var: 0.0027638375759124756
        vf_loss: 1.0210192203521729
      agent-1:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23167550563812256
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009104788769036531
        model: {}
        policy_loss: -0.00161237851716578
        total_loss: -0.0019746602047234774
        vf_explained_var: 0.06939977407455444
        vf_loss: 0.4546617567539215
      agent-2:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3809071183204651
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016616422217339277
        model: {}
        policy_loss: -0.0020662937313318253
        total_loss: -0.002391890389844775
        vf_explained_var: 0.035932764410972595
        vf_loss: 3.448011636734009
      agent-3:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5622276663780212
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015930852387100458
        model: {}
        policy_loss: -0.0017322078347206116
        total_loss: -0.0025665066204965115
        vf_explained_var: -0.004335343837738037
        vf_loss: 1.5522081851959229
      agent-4:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4611659646034241
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005527640460059047
        model: {}
        policy_loss: -0.0013875022996217012
        total_loss: -0.0020994911901652813
        vf_explained_var: 0.016926124691963196
        vf_loss: 0.9966272711753845
      agent-5:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5405502915382385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022553582675755024
        model: {}
        policy_loss: -0.0017514709616079926
        total_loss: -0.002588690258562565
        vf_explained_var: 0.014510199427604675
        vf_loss: 1.141496181488037
    load_time_ms: 14125.908
    num_steps_sampled: 67200000
    num_steps_trained: 67200000
    sample_time_ms: 119378.227
    update_time_ms: 16.178
  iterations_since_restore: 50
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.085436893203884
    ram_util_percent: 13.880097087378642
  pid: 27065
  policy_reward_max:
    agent-0: 35.0
    agent-1: 19.0
    agent-2: 60.0
    agent-3: 37.0
    agent-4: 27.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.27
    agent-1: 7.95
    agent-2: 40.67
    agent-3: 19.48
    agent-4: 14.37
    agent-5: 15.77
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 26.0
    agent-3: 7.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.28095452799028
    mean_inference_ms: 15.159039785630267
    mean_processing_ms: 73.72440187428805
  time_since_restore: 7316.786588907242
  time_this_iter_s: 145.1246829032898
  time_total_s: 96253.46471858025
  timestamp: 1637370278
  timesteps_since_restore: 4800000
  timesteps_this_iter: 96000
  timesteps_total: 67200000
  training_iteration: 700
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    700 |          96253.5 | 67200000 |   113.51 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 6.77
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 2.97
    apples_agent-1_min: 0
    apples_agent-2_max: 55
    apples_agent-2_mean: 27.81
    apples_agent-2_min: 15
    apples_agent-3_max: 17
    apples_agent-3_mean: 7.32
    apples_agent-3_min: 1
    apples_agent-4_max: 25
    apples_agent-4_mean: 9.92
    apples_agent-4_min: 2
    apples_agent-5_max: 34
    apples_agent-5_mean: 3.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 1.51
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 273
    cleaning_beam_agent-1_mean: 213.18
    cleaning_beam_agent-1_min: 154
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 6.61
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 11.91
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 26
    cleaning_beam_agent-4_mean: 7.29
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.12
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-07-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 157.0
  episode_reward_mean: 116.05
  episode_reward_min: 76.0
  episodes_this_iter: 96
  episodes_total: 67296
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11906.128
    learner:
      agent-0:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26774564385414124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006929556839168072
        model: {}
        policy_loss: -0.0015454246895387769
        total_loss: -0.0019130457658320665
        vf_explained_var: 0.0127362459897995
        vf_loss: 1.0361336469650269
      agent-1:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23369555175304413
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011340940836817026
        model: {}
        policy_loss: -0.0016987196868285537
        total_loss: -0.0020582242868840694
        vf_explained_var: 0.0461968332529068
        vf_loss: 0.518008828163147
      agent-2:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3709277808666229
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011931095505133271
        model: {}
        policy_loss: -0.001712659141048789
        total_loss: -0.002021333435550332
        vf_explained_var: 0.02254873514175415
        vf_loss: 3.441572666168213
      agent-3:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5634064078330994
        entropy_coeff: 0.0017600000137463212
        kl: 0.001741398242302239
        model: {}
        policy_loss: -0.0018421802669763565
        total_loss: -0.0026952605694532394
        vf_explained_var: -0.0035993456840515137
        vf_loss: 1.3851369619369507
      agent-4:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4786129295825958
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015238055493682623
        model: {}
        policy_loss: -0.0015995712019503117
        total_loss: -0.00234095542691648
        vf_explained_var: 0.01497793197631836
        vf_loss: 1.0097424983978271
      agent-5:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5422494411468506
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020003044046461582
        model: {}
        policy_loss: -0.0017246450297534466
        total_loss: -0.002566554117947817
        vf_explained_var: 0.010112345218658447
        vf_loss: 1.1245278120040894
    load_time_ms: 14129.365
    num_steps_sampled: 67296000
    num_steps_trained: 67296000
    sample_time_ms: 119408.67
    update_time_ms: 16.327
  iterations_since_restore: 51
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.89668246445498
    ram_util_percent: 13.880094786729856
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 18.0
    agent-2: 60.0
    agent-3: 42.0
    agent-4: 28.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.51
    agent-1: 7.97
    agent-2: 42.6
    agent-3: 19.03
    agent-4: 14.6
    agent-5: 16.34
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 27.0
    agent-3: 9.0
    agent-4: 5.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.283515499202124
    mean_inference_ms: 15.159936335213276
    mean_processing_ms: 73.7338413761419
  time_since_restore: 7463.102921009064
  time_this_iter_s: 146.3163321018219
  time_total_s: 96399.78105068207
  timestamp: 1637370426
  timesteps_since_restore: 4896000
  timesteps_this_iter: 96000
  timesteps_total: 67296000
  training_iteration: 701
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    701 |          96399.8 | 67296000 |   116.05 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 6.77
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 2.88
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 26.69
    apples_agent-2_min: 16
    apples_agent-3_max: 21
    apples_agent-3_mean: 7.39
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.84
    apples_agent-4_min: 3
    apples_agent-5_max: 42
    apples_agent-5_mean: 3.76
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 1.98
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 274
    cleaning_beam_agent-1_mean: 217.63
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 7.15
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 13.86
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 18
    cleaning_beam_agent-4_mean: 6.29
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.89
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-09-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 153.0
  episode_reward_mean: 116.6
  episode_reward_min: 67.0
  episodes_this_iter: 96
  episodes_total: 67392
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11904.042
    learner:
      agent-0:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26727724075317383
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007751412340439856
        model: {}
        policy_loss: -0.0016592340543866158
        total_loss: -0.0020150411874055862
        vf_explained_var: 0.0015008598566055298
        vf_loss: 1.1460515260696411
      agent-1:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23595789074897766
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011716920416802168
        model: {}
        policy_loss: -0.0016747275367379189
        total_loss: -0.0020355060696601868
        vf_explained_var: 0.07291126251220703
        vf_loss: 0.5450906753540039
      agent-2:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3835175037384033
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015117161674425006
        model: {}
        policy_loss: -0.0019503559451550245
        total_loss: -0.0022518872283399105
        vf_explained_var: 0.0344487726688385
        vf_loss: 3.734596014022827
      agent-3:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5643616914749146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015398464165627956
        model: {}
        policy_loss: -0.0016553783789277077
        total_loss: -0.0024888752959668636
        vf_explained_var: 0.005103349685668945
        vf_loss: 1.5978171825408936
      agent-4:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.470589816570282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008672248222865164
        model: {}
        policy_loss: -0.0013260978739708662
        total_loss: -0.0020543369464576244
        vf_explained_var: 0.0006607621908187866
        vf_loss: 1.000006914138794
      agent-5:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5561792850494385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016911982093006372
        model: {}
        policy_loss: -0.001762446016073227
        total_loss: -0.0026136250235140324
        vf_explained_var: 0.013906896114349365
        vf_loss: 1.2769540548324585
    load_time_ms: 14124.858
    num_steps_sampled: 67392000
    num_steps_trained: 67392000
    sample_time_ms: 119454.59
    update_time_ms: 16.487
  iterations_since_restore: 52
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.091826923076926
    ram_util_percent: 13.946634615384612
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 60.0
    agent-3: 32.0
    agent-4: 26.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 15.44
    agent-1: 8.47
    agent-2: 41.31
    agent-3: 19.82
    agent-4: 14.56
    agent-5: 17.0
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 26.0
    agent-3: 6.0
    agent-4: 5.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.28289750949792
    mean_inference_ms: 15.157729498400144
    mean_processing_ms: 73.734478877596
  time_since_restore: 7608.626623392105
  time_this_iter_s: 145.52370238304138
  time_total_s: 96545.30475306511
  timestamp: 1637370571
  timesteps_since_restore: 4992000
  timesteps_this_iter: 96000
  timesteps_total: 67392000
  training_iteration: 702
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    702 |          96545.3 | 67392000 |    116.6 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 7.48
    apples_agent-0_min: 1
    apples_agent-1_max: 26
    apples_agent-1_mean: 3.54
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 28.33
    apples_agent-2_min: 13
    apples_agent-3_max: 40
    apples_agent-3_mean: 8.55
    apples_agent-3_min: 2
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.08
    apples_agent-4_min: 2
    apples_agent-5_max: 15
    apples_agent-5_mean: 3.05
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 1.46
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 260
    cleaning_beam_agent-1_mean: 217.13
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 6.51
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 12.72
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 18
    cleaning_beam_agent-4_mean: 6.73
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.86
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-11-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 119.05
  episode_reward_min: 60.0
  episodes_this_iter: 96
  episodes_total: 67488
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11902.678
    learner:
      agent-0:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26376330852508545
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008023092523217201
        model: {}
        policy_loss: -0.0018218387849628925
        total_loss: -0.0021698237396776676
        vf_explained_var: -0.0001370161771774292
        vf_loss: 1.162383794784546
      agent-1:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23624786734580994
        entropy_coeff: 0.0017600000137463212
        kl: 0.001121976412832737
        model: {}
        policy_loss: -0.001517975702881813
        total_loss: -0.0018780813552439213
        vf_explained_var: 0.07213886082172394
        vf_loss: 0.556908369064331
      agent-2:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3868752121925354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017069181194528937
        model: {}
        policy_loss: -0.0020175334066152573
        total_loss: -0.002303760964423418
        vf_explained_var: 0.0362006276845932
        vf_loss: 3.9467294216156006
      agent-3:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.572766125202179
        entropy_coeff: 0.0017600000137463212
        kl: 0.00147034483961761
        model: {}
        policy_loss: -0.0015493836253881454
        total_loss: -0.0024091387167572975
        vf_explained_var: -0.00254705548286438
        vf_loss: 1.4831461906433105
      agent-4:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4653710424900055
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006217428599484265
        model: {}
        policy_loss: -0.001169243361800909
        total_loss: -0.0018811351619660854
        vf_explained_var: 0.0005055814981460571
        vf_loss: 1.0716363191604614
      agent-5:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5569084286689758
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020303153432905674
        model: {}
        policy_loss: -0.0016646827571094036
        total_loss: -0.0025328053161501884
        vf_explained_var: 0.0010425448417663574
        vf_loss: 1.1203811168670654
    load_time_ms: 14138.673
    num_steps_sampled: 67488000
    num_steps_trained: 67488000
    sample_time_ms: 119426.041
    update_time_ms: 16.627
  iterations_since_restore: 53
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.974396135265696
    ram_util_percent: 13.8743961352657
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 18.0
    agent-2: 61.0
    agent-3: 36.0
    agent-4: 24.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 15.99
    agent-1: 8.44
    agent-2: 43.31
    agent-3: 19.99
    agent-4: 14.78
    agent-5: 16.54
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 23.0
    agent-3: 9.0
    agent-4: 5.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.28206645878926
    mean_inference_ms: 15.156134398705595
    mean_processing_ms: 73.72864191851039
  time_since_restore: 7753.9328825473785
  time_this_iter_s: 145.30625915527344
  time_total_s: 96690.61101222038
  timestamp: 1637370717
  timesteps_since_restore: 5088000
  timesteps_this_iter: 96000
  timesteps_total: 67488000
  training_iteration: 703
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    703 |          96690.6 | 67488000 |   119.05 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 6.89
    apples_agent-0_min: 1
    apples_agent-1_max: 14
    apples_agent-1_mean: 3.32
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 26.05
    apples_agent-2_min: 14
    apples_agent-3_max: 19
    apples_agent-3_mean: 7.34
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.62
    apples_agent-4_min: 2
    apples_agent-5_max: 14
    apples_agent-5_mean: 3.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.7
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 260
    cleaning_beam_agent-1_mean: 210.95
    cleaning_beam_agent-1_min: 162
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 6.83
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 12.35
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 21
    cleaning_beam_agent-4_mean: 7.03
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.08
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-14-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 159.0
  episode_reward_mean: 115.26
  episode_reward_min: 80.0
  episodes_this_iter: 96
  episodes_total: 67584
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11904.701
    learner:
      agent-0:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27059030532836914
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009209049749188125
        model: {}
        policy_loss: -0.0016407010843977332
        total_loss: -0.0020194435492157936
        vf_explained_var: 0.009381219744682312
        vf_loss: 0.9749553799629211
      agent-1:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23665332794189453
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011283988133072853
        model: {}
        policy_loss: -0.0015812953934073448
        total_loss: -0.0019449149258434772
        vf_explained_var: 0.07349584996700287
        vf_loss: 0.5289033055305481
      agent-2:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3851797580718994
        entropy_coeff: 0.0017600000137463212
        kl: 0.001186834997497499
        model: {}
        policy_loss: -0.001733164768666029
        total_loss: -0.0020545939914882183
        vf_explained_var: 0.0029242336750030518
        vf_loss: 3.564858913421631
      agent-3:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5709939002990723
        entropy_coeff: 0.0017600000137463212
        kl: 0.001786423847079277
        model: {}
        policy_loss: -0.0018369602039456367
        total_loss: -0.0026920833624899387
        vf_explained_var: 0.0025043338537216187
        vf_loss: 1.4982821941375732
      agent-4:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47281160950660706
        entropy_coeff: 0.0017600000137463212
        kl: 0.000803679577074945
        model: {}
        policy_loss: -0.0012193592265248299
        total_loss: -0.0019458145834505558
        vf_explained_var: 0.0032676905393600464
        vf_loss: 1.0569262504577637
      agent-5:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5591719150543213
        entropy_coeff: 0.0017600000137463212
        kl: 0.00212465669028461
        model: {}
        policy_loss: -0.001850631320849061
        total_loss: -0.0027214884757995605
        vf_explained_var: 0.018241018056869507
        vf_loss: 1.1328516006469727
    load_time_ms: 14145.337
    num_steps_sampled: 67584000
    num_steps_trained: 67584000
    sample_time_ms: 119477.714
    update_time_ms: 16.545
  iterations_since_restore: 54
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.97740384615385
    ram_util_percent: 13.94807692307692
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 20.0
    agent-2: 60.0
    agent-3: 35.0
    agent-4: 27.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 14.99
    agent-1: 8.12
    agent-2: 41.34
    agent-3: 19.56
    agent-4: 14.71
    agent-5: 16.54
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 27.0
    agent-3: 8.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.28293086945946
    mean_inference_ms: 15.154843882142831
    mean_processing_ms: 73.72761788030972
  time_since_restore: 7899.770125389099
  time_this_iter_s: 145.83724284172058
  time_total_s: 96836.4482550621
  timestamp: 1637370863
  timesteps_since_restore: 5184000
  timesteps_this_iter: 96000
  timesteps_total: 67584000
  training_iteration: 704
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    704 |          96836.4 | 67584000 |   115.26 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 6.76
    apples_agent-0_min: 1
    apples_agent-1_max: 11
    apples_agent-1_mean: 3.31
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 26.8
    apples_agent-2_min: 14
    apples_agent-3_max: 36
    apples_agent-3_mean: 7.32
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.3
    apples_agent-4_min: 2
    apples_agent-5_max: 15
    apples_agent-5_mean: 4.28
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 1.24
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 267
    cleaning_beam_agent-1_mean: 207.7
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 5.99
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 44
    cleaning_beam_agent-3_mean: 11.72
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 25
    cleaning_beam_agent-4_mean: 6.71
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.01
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-16-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 152.0
  episode_reward_mean: 112.72
  episode_reward_min: 75.0
  episodes_this_iter: 96
  episodes_total: 67680
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11898.689
    learner:
      agent-0:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2704266905784607
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005946361925452948
        model: {}
        policy_loss: -0.0015210453420877457
        total_loss: -0.0018989574164152145
        vf_explained_var: 0.011694744229316711
        vf_loss: 0.9803951382637024
      agent-1:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23304909467697144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008542980067431927
        model: {}
        policy_loss: -0.0015066415071487427
        total_loss: -0.0018624057993292809
        vf_explained_var: 0.05764786899089813
        vf_loss: 0.5440093278884888
      agent-2:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3836092948913574
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010883960640057921
        model: {}
        policy_loss: -0.0016960727516561747
        total_loss: -0.0020254943519830704
        vf_explained_var: 0.02370680868625641
        vf_loss: 3.4573233127593994
      agent-3:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5678006410598755
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016874853754416108
        model: {}
        policy_loss: -0.001767967827618122
        total_loss: -0.00262955529615283
        vf_explained_var: 0.0007349550724029541
        vf_loss: 1.3774088621139526
      agent-4:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46124616265296936
        entropy_coeff: 0.0017600000137463212
        kl: 0.001473723677918315
        model: {}
        policy_loss: -0.00165462214499712
        total_loss: -0.0023638922721147537
        vf_explained_var: 0.014062464237213135
        vf_loss: 1.0252435207366943
      agent-5:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5592536926269531
        entropy_coeff: 0.0017600000137463212
        kl: 0.001034517539665103
        model: {}
        policy_loss: -0.0017275558784604073
        total_loss: -0.002598845399916172
        vf_explained_var: 0.010033801198005676
        vf_loss: 1.1299883127212524
    load_time_ms: 14129.387
    num_steps_sampled: 67680000
    num_steps_trained: 67680000
    sample_time_ms: 119574.306
    update_time_ms: 16.632
  iterations_since_restore: 55
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.026442307692307
    ram_util_percent: 13.954807692307687
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 16.0
    agent-2: 60.0
    agent-3: 34.0
    agent-4: 25.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 14.44
    agent-1: 8.2
    agent-2: 40.86
    agent-3: 18.24
    agent-4: 14.78
    agent-5: 16.2
  policy_reward_min:
    agent-0: 8.0
    agent-1: 0.0
    agent-2: 21.0
    agent-3: 5.0
    agent-4: 4.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.283483391665936
    mean_inference_ms: 15.15463393646465
    mean_processing_ms: 73.73401915807925
  time_since_restore: 8046.0014481544495
  time_this_iter_s: 146.23132276535034
  time_total_s: 96982.67957782745
  timestamp: 1637371009
  timesteps_since_restore: 5280000
  timesteps_this_iter: 96000
  timesteps_total: 67680000
  training_iteration: 705
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    705 |          96982.7 | 67680000 |   112.72 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 88
    apples_agent-0_mean: 7.72
    apples_agent-0_min: 0
    apples_agent-1_max: 17
    apples_agent-1_mean: 3.09
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 27.22
    apples_agent-2_min: 10
    apples_agent-3_max: 40
    apples_agent-3_mean: 7.7
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.95
    apples_agent-4_min: 2
    apples_agent-5_max: 19
    apples_agent-5_mean: 3.82
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 1.53
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 267
    cleaning_beam_agent-1_mean: 208.74
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 25
    cleaning_beam_agent-2_mean: 5.34
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 11.92
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 26
    cleaning_beam_agent-4_mean: 7.47
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.04
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-19-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 158.0
  episode_reward_mean: 113.86
  episode_reward_min: 70.0
  episodes_this_iter: 96
  episodes_total: 67776
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11907.098
    learner:
      agent-0:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2748461663722992
        entropy_coeff: 0.0017600000137463212
        kl: 0.00067787995794788
        model: {}
        policy_loss: -0.0015829075127840042
        total_loss: -0.001969052478671074
        vf_explained_var: 0.011530742049217224
        vf_loss: 0.9758569002151489
      agent-1:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23421108722686768
        entropy_coeff: 0.0017600000137463212
        kl: 0.00107945769559592
        model: {}
        policy_loss: -0.0017518093809485435
        total_loss: -0.0021170503459870815
        vf_explained_var: 0.08823947608470917
        vf_loss: 0.4697169065475464
      agent-2:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3819446861743927
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011153888190165162
        model: {}
        policy_loss: -0.0017749438993632793
        total_loss: -0.0020813997834920883
        vf_explained_var: 0.0370650440454483
        vf_loss: 3.657644033432007
      agent-3:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5643243789672852
        entropy_coeff: 0.0017600000137463212
        kl: 0.001823928439989686
        model: {}
        policy_loss: -0.002093147486448288
        total_loss: -0.0029395143501460552
        vf_explained_var: 0.002840861678123474
        vf_loss: 1.4684420824050903
      agent-4:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4512842297554016
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013457818422466516
        model: {}
        policy_loss: -0.0014735192526131868
        total_loss: -0.002162154298275709
        vf_explained_var: 0.026119202375411987
        vf_loss: 1.0562403202056885
      agent-5:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5584436058998108
        entropy_coeff: 0.0017600000137463212
        kl: 0.00223415344953537
        model: {}
        policy_loss: -0.0019969837740063667
        total_loss: -0.002862251829355955
        vf_explained_var: 0.006495118141174316
        vf_loss: 1.1759147644042969
    load_time_ms: 14138.868
    num_steps_sampled: 67776000
    num_steps_trained: 67776000
    sample_time_ms: 119509.524
    update_time_ms: 16.505
  iterations_since_restore: 56
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.02705314009662
    ram_util_percent: 13.956521739130435
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 17.0
    agent-2: 63.0
    agent-3: 31.0
    agent-4: 25.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 14.72
    agent-1: 8.02
    agent-2: 40.94
    agent-3: 19.52
    agent-4: 14.62
    agent-5: 16.04
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 17.0
    agent-3: 9.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.28325072092057
    mean_inference_ms: 15.153388770297752
    mean_processing_ms: 73.73126236729838
  time_since_restore: 8191.146008729935
  time_this_iter_s: 145.14456057548523
  time_total_s: 97127.82413840294
  timestamp: 1637371154
  timesteps_since_restore: 5376000
  timesteps_this_iter: 96000
  timesteps_total: 67776000
  training_iteration: 706
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 25.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    706 |          97127.8 | 67776000 |   113.86 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 6.31
    apples_agent-0_min: 1
    apples_agent-1_max: 18
    apples_agent-1_mean: 3.36
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 27.16
    apples_agent-2_min: 11
    apples_agent-3_max: 27
    apples_agent-3_mean: 6.97
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 10.2
    apples_agent-4_min: 4
    apples_agent-5_max: 20
    apples_agent-5_mean: 3.62
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 1.14
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 242
    cleaning_beam_agent-1_mean: 209.05
    cleaning_beam_agent-1_min: 171
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 4.77
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 11.64
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 26
    cleaning_beam_agent-4_mean: 7.96
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.31
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-21-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 113.54
  episode_reward_min: 74.0
  episodes_this_iter: 96
  episodes_total: 67872
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11885.108
    learner:
      agent-0:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28673064708709717
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010433630086481571
        model: {}
        policy_loss: -0.0016732889926061034
        total_loss: -0.00208466500043869
        vf_explained_var: 0.006795331835746765
        vf_loss: 0.9327216148376465
      agent-1:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23422107100486755
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009366035228595138
        model: {}
        policy_loss: -0.0014212240930646658
        total_loss: -0.0017762943170964718
        vf_explained_var: 0.0713893473148346
        vf_loss: 0.5715848207473755
      agent-2:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3809744715690613
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013154976768419147
        model: {}
        policy_loss: -0.001592353917658329
        total_loss: -0.0018951883539557457
        vf_explained_var: 0.02244381606578827
        vf_loss: 3.67683744430542
      agent-3:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5576808452606201
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018138668965548277
        model: {}
        policy_loss: -0.0017647978384047747
        total_loss: -0.0026108683086931705
        vf_explained_var: 0.0014342963695526123
        vf_loss: 1.3544678688049316
      agent-4:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43993574380874634
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012400634586811066
        model: {}
        policy_loss: -0.0015010538045316935
        total_loss: -0.0021731143351644278
        vf_explained_var: 0.007494419813156128
        vf_loss: 1.0222409963607788
      agent-5:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5508825182914734
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016782899620011449
        model: {}
        policy_loss: -0.0015419470146298409
        total_loss: -0.0023922896943986416
        vf_explained_var: 0.0051101744174957275
        vf_loss: 1.1921186447143555
    load_time_ms: 14124.933
    num_steps_sampled: 67872000
    num_steps_trained: 67872000
    sample_time_ms: 119517.932
    update_time_ms: 16.977
  iterations_since_restore: 57
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.088461538461537
    ram_util_percent: 13.90048076923077
  pid: 27065
  policy_reward_max:
    agent-0: 24.0
    agent-1: 17.0
    agent-2: 70.0
    agent-3: 31.0
    agent-4: 29.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 14.43
    agent-1: 8.65
    agent-2: 41.42
    agent-3: 18.19
    agent-4: 14.25
    agent-5: 16.6
  policy_reward_min:
    agent-0: 5.0
    agent-1: 1.0
    agent-2: 22.0
    agent-3: 8.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.285593284497605
    mean_inference_ms: 15.154107781602459
    mean_processing_ms: 73.73762917516459
  time_since_restore: 8337.238025426865
  time_this_iter_s: 146.09201669692993
  time_total_s: 97273.91615509987
  timestamp: 1637371300
  timesteps_since_restore: 5472000
  timesteps_this_iter: 96000
  timesteps_total: 67872000
  training_iteration: 707
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    707 |          97273.9 | 67872000 |   113.54 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 7.05
    apples_agent-0_min: 1
    apples_agent-1_max: 11
    apples_agent-1_mean: 3.06
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 28.77
    apples_agent-2_min: 16
    apples_agent-3_max: 27
    apples_agent-3_mean: 7.16
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.36
    apples_agent-4_min: 2
    apples_agent-5_max: 20
    apples_agent-5_mean: 3.65
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 4
    cleaning_beam_agent-0_mean: 0.97
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 260
    cleaning_beam_agent-1_mean: 218.27
    cleaning_beam_agent-1_min: 164
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 4.6
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 11.44
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 23
    cleaning_beam_agent-4_mean: 7.72
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.43
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-24-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 157.0
  episode_reward_mean: 117.26
  episode_reward_min: 48.0
  episodes_this_iter: 96
  episodes_total: 67968
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11893.052
    learner:
      agent-0:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2912514805793762
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006737108342349529
        model: {}
        policy_loss: -0.0015069465152919292
        total_loss: -0.001910064136609435
        vf_explained_var: 0.005311921238899231
        vf_loss: 1.0948258638381958
      agent-1:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.239959716796875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011932626366615295
        model: {}
        policy_loss: -0.0016201591352000833
        total_loss: -0.0019942985381931067
        vf_explained_var: 0.0811566710472107
        vf_loss: 0.4818772077560425
      agent-2:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3840608596801758
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012445200700312853
        model: {}
        policy_loss: -0.001631168881431222
        total_loss: -0.001945660449564457
        vf_explained_var: 0.025978192687034607
        vf_loss: 3.614574432373047
      agent-3:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5513100028038025
        entropy_coeff: 0.0017600000137463212
        kl: 0.001334009226411581
        model: {}
        policy_loss: -0.0019574351608753204
        total_loss: -0.0028020814061164856
        vf_explained_var: -0.0033696144819259644
        vf_loss: 1.2565958499908447
      agent-4:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4316484332084656
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008209220832213759
        model: {}
        policy_loss: -0.001149764284491539
        total_loss: -0.0018059257417917252
        vf_explained_var: 0.015301153063774109
        vf_loss: 1.0354068279266357
      agent-5:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5510759949684143
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017503479029983282
        model: {}
        policy_loss: -0.0016782546881586313
        total_loss: -0.0025304940063506365
        vf_explained_var: 0.0042854249477386475
        vf_loss: 1.1765459775924683
    load_time_ms: 14130.536
    num_steps_sampled: 67968000
    num_steps_trained: 67968000
    sample_time_ms: 119604.823
    update_time_ms: 17.176
  iterations_since_restore: 58
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.122115384615384
    ram_util_percent: 13.939903846153843
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 17.0
    agent-2: 62.0
    agent-3: 29.0
    agent-4: 27.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 15.44
    agent-1: 8.29
    agent-2: 43.5
    agent-3: 18.97
    agent-4: 14.52
    agent-5: 16.54
  policy_reward_min:
    agent-0: 5.0
    agent-1: 3.0
    agent-2: 24.0
    agent-3: 6.0
    agent-4: -41.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.288745428602155
    mean_inference_ms: 15.155151802210653
    mean_processing_ms: 73.7481861597028
  time_since_restore: 8483.433766841888
  time_this_iter_s: 146.1957414150238
  time_total_s: 97420.11189651489
  timestamp: 1637371447
  timesteps_since_restore: 5568000
  timesteps_this_iter: 96000
  timesteps_total: 67968000
  training_iteration: 708
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    708 |          97420.1 | 67968000 |   117.26 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 6.66
    apples_agent-0_min: 1
    apples_agent-1_max: 11
    apples_agent-1_mean: 3.08
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 26.92
    apples_agent-2_min: 13
    apples_agent-3_max: 24
    apples_agent-3_mean: 7.7
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.34
    apples_agent-4_min: 1
    apples_agent-5_max: 32
    apples_agent-5_mean: 4.03
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.03
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 272
    cleaning_beam_agent-1_mean: 216.46
    cleaning_beam_agent-1_min: 145
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.69
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 12.28
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 18
    cleaning_beam_agent-4_mean: 7.68
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.08
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-26-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 158.0
  episode_reward_mean: 115.72
  episode_reward_min: 64.0
  episodes_this_iter: 96
  episodes_total: 68064
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11890.065
    learner:
      agent-0:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2859361171722412
        entropy_coeff: 0.0017600000137463212
        kl: 0.000751639308873564
        model: {}
        policy_loss: -0.0015678657218813896
        total_loss: -0.001971116289496422
        vf_explained_var: 0.012430742383003235
        vf_loss: 0.9999740123748779
      agent-1:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2410108894109726
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013736203545704484
        model: {}
        policy_loss: -0.0016601511742919683
        total_loss: -0.0020301826298236847
        vf_explained_var: 0.058868780732154846
        vf_loss: 0.5414429903030396
      agent-2:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3818542957305908
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012368381721898913
        model: {}
        policy_loss: -0.0017013062024489045
        total_loss: -0.002034298609942198
        vf_explained_var: 0.028449073433876038
        vf_loss: 3.3907017707824707
      agent-3:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5604808926582336
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012205501552671194
        model: {}
        policy_loss: -0.001641810522414744
        total_loss: -0.0024998071603477
        vf_explained_var: -0.002391800284385681
        vf_loss: 1.2844805717468262
      agent-4:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4396240711212158
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004924213280901313
        model: {}
        policy_loss: -0.0011852039024233818
        total_loss: -0.0018529430963099003
        vf_explained_var: 0.015510857105255127
        vf_loss: 1.0600049495697021
      agent-5:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5456356406211853
        entropy_coeff: 0.0017600000137463212
        kl: 0.001511313021183014
        model: {}
        policy_loss: -0.0017324737273156643
        total_loss: -0.0025638388469815254
        vf_explained_var: 0.014946401119232178
        vf_loss: 1.2895575761795044
    load_time_ms: 14118.519
    num_steps_sampled: 68064000
    num_steps_trained: 68064000
    sample_time_ms: 119628.883
    update_time_ms: 17.382
  iterations_since_restore: 59
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.114423076923075
    ram_util_percent: 13.959615384615379
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 16.0
    agent-2: 64.0
    agent-3: 30.0
    agent-4: 33.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 14.66
    agent-1: 8.53
    agent-2: 41.74
    agent-3: 18.42
    agent-4: 14.93
    agent-5: 17.44
  policy_reward_min:
    agent-0: 7.0
    agent-1: 0.0
    agent-2: 22.0
    agent-3: 10.0
    agent-4: 6.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.289076802431346
    mean_inference_ms: 15.154813298040272
    mean_processing_ms: 73.75511361915794
  time_since_restore: 8629.169127464294
  time_this_iter_s: 145.735360622406
  time_total_s: 97565.8472571373
  timestamp: 1637371593
  timesteps_since_restore: 5664000
  timesteps_this_iter: 96000
  timesteps_total: 68064000
  training_iteration: 709
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    709 |          97565.8 | 68064000 |   115.72 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 7.36
    apples_agent-0_min: 2
    apples_agent-1_max: 41
    apples_agent-1_mean: 3.22
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 27.91
    apples_agent-2_min: 14
    apples_agent-3_max: 22
    apples_agent-3_mean: 7.52
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.16
    apples_agent-4_min: 1
    apples_agent-5_max: 25
    apples_agent-5_mean: 4.06
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.03
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 260
    cleaning_beam_agent-1_mean: 214.74
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 4.66
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 11.63
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 26
    cleaning_beam_agent-4_mean: 8.9
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.0
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-28-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 159.0
  episode_reward_mean: 117.93
  episode_reward_min: 67.0
  episodes_this_iter: 96
  episodes_total: 68160
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11881.898
    learner:
      agent-0:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2814997434616089
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008666258654557168
        model: {}
        policy_loss: -0.0015638875775039196
        total_loss: -0.0019566696137189865
        vf_explained_var: 0.004380181431770325
        vf_loss: 1.026574730873108
      agent-1:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23894907534122467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009890375658869743
        model: {}
        policy_loss: -0.0018105413764715195
        total_loss: -0.002172127366065979
        vf_explained_var: 0.06546729803085327
        vf_loss: 0.5896732807159424
      agent-2:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37107977271080017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014536044327542186
        model: {}
        policy_loss: -0.0017858846113085747
        total_loss: -0.002097837161272764
        vf_explained_var: 0.030159279704093933
        vf_loss: 3.4114978313446045
      agent-3:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5523152947425842
        entropy_coeff: 0.0017600000137463212
        kl: 0.00173147302120924
        model: {}
        policy_loss: -0.0016177813522517681
        total_loss: -0.002446359023451805
        vf_explained_var: 0.002772599458694458
        vf_loss: 1.434949278831482
      agent-4:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4478490352630615
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009915619157254696
        model: {}
        policy_loss: -0.0014791125431656837
        total_loss: -0.002154831774532795
        vf_explained_var: 0.011058241128921509
        vf_loss: 1.1249228715896606
      agent-5:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.559415340423584
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023841168731451035
        model: {}
        policy_loss: -0.001823054626584053
        total_loss: -0.0026953471824526787
        vf_explained_var: 0.010867387056350708
        vf_loss: 1.12281334400177
    load_time_ms: 14116.856
    num_steps_sampled: 68160000
    num_steps_trained: 68160000
    sample_time_ms: 119693.567
    update_time_ms: 17.427
  iterations_since_restore: 60
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.04663461538462
    ram_util_percent: 13.956730769230765
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 18.0
    agent-2: 63.0
    agent-3: 37.0
    agent-4: 33.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.3
    agent-1: 8.48
    agent-2: 42.34
    agent-3: 19.88
    agent-4: 15.33
    agent-5: 16.6
  policy_reward_min:
    agent-0: 5.0
    agent-1: 1.0
    agent-2: 24.0
    agent-3: 11.0
    agent-4: 4.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.291293210196788
    mean_inference_ms: 15.154276230706182
    mean_processing_ms: 73.7556391386199
  time_since_restore: 8774.845116615295
  time_this_iter_s: 145.67598915100098
  time_total_s: 97711.5232462883
  timestamp: 1637371738
  timesteps_since_restore: 5760000
  timesteps_this_iter: 96000
  timesteps_total: 68160000
  training_iteration: 710
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    710 |          97711.5 | 68160000 |   117.93 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 7.03
    apples_agent-0_min: 1
    apples_agent-1_max: 19
    apples_agent-1_mean: 3.04
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 27.48
    apples_agent-2_min: 16
    apples_agent-3_max: 28
    apples_agent-3_mean: 7.5
    apples_agent-3_min: 1
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.64
    apples_agent-4_min: 3
    apples_agent-5_max: 16
    apples_agent-5_mean: 4.03
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 0.98
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 241
    cleaning_beam_agent-1_mean: 209.64
    cleaning_beam_agent-1_min: 147
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 4.63
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 38
    cleaning_beam_agent-3_mean: 11.46
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 29
    cleaning_beam_agent-4_mean: 7.94
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 3.81
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-31-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 159.0
  episode_reward_mean: 116.02
  episode_reward_min: 76.0
  episodes_this_iter: 96
  episodes_total: 68256
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11871.99
    learner:
      agent-0:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2832653522491455
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007703338633291423
        model: {}
        policy_loss: -0.0016688257455825806
        total_loss: -0.002066262997686863
        vf_explained_var: 0.010661780834197998
        vf_loss: 1.0111045837402344
      agent-1:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24131585657596588
        entropy_coeff: 0.0017600000137463212
        kl: 0.001142170513048768
        model: {}
        policy_loss: -0.001697335857897997
        total_loss: -0.0020735834259539843
        vf_explained_var: 0.07262939214706421
        vf_loss: 0.48468512296676636
      agent-2:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37301093339920044
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009265363914892077
        model: {}
        policy_loss: -0.001678941771388054
        total_loss: -0.001960387919098139
        vf_explained_var: 0.030421406030654907
        vf_loss: 3.7505314350128174
      agent-3:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5449761152267456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015142711345106363
        model: {}
        policy_loss: -0.0015952452085912228
        total_loss: -0.0024177064187824726
        vf_explained_var: 0.006466373801231384
        vf_loss: 1.3669548034667969
      agent-4:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4540310502052307
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006112153641879559
        model: {}
        policy_loss: -0.0013425700599327683
        total_loss: -0.0020411969162523746
        vf_explained_var: 0.01593339443206787
        vf_loss: 1.004676103591919
      agent-5:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5592181086540222
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012082906905561686
        model: {}
        policy_loss: -0.0016299525741487741
        total_loss: -0.002491230610758066
        vf_explained_var: 0.022351115942001343
        vf_loss: 1.2294456958770752
    load_time_ms: 14125.959
    num_steps_sampled: 68256000
    num_steps_trained: 68256000
    sample_time_ms: 119556.801
    update_time_ms: 17.463
  iterations_since_restore: 61
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.943749999999998
    ram_util_percent: 13.954326923076918
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 17.0
    agent-2: 60.0
    agent-3: 37.0
    agent-4: 29.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 15.44
    agent-1: 7.87
    agent-2: 42.21
    agent-3: 19.45
    agent-4: 14.41
    agent-5: 16.64
  policy_reward_min:
    agent-0: 8.0
    agent-1: 2.0
    agent-2: 25.0
    agent-3: 9.0
    agent-4: 4.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.289434599529923
    mean_inference_ms: 15.152329851579761
    mean_processing_ms: 73.75086178515546
  time_since_restore: 8919.741544485092
  time_this_iter_s: 144.89642786979675
  time_total_s: 97856.4196741581
  timestamp: 1637371885
  timesteps_since_restore: 5856000
  timesteps_this_iter: 96000
  timesteps_total: 68256000
  training_iteration: 711
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    711 |          97856.4 | 68256000 |   116.02 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 6.39
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 3.55
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 26.34
    apples_agent-2_min: 2
    apples_agent-3_max: 41
    apples_agent-3_mean: 7.65
    apples_agent-3_min: 1
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.31
    apples_agent-4_min: 1
    apples_agent-5_max: 12
    apples_agent-5_mean: 3.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.21
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 265
    cleaning_beam_agent-1_mean: 215.05
    cleaning_beam_agent-1_min: 35
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 4.19
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 12.03
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 25
    cleaning_beam_agent-4_mean: 8.22
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-33-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 157.0
  episode_reward_mean: 115.57
  episode_reward_min: 15.0
  episodes_this_iter: 96
  episodes_total: 68352
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11874.346
    learner:
      agent-0:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28622475266456604
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016392864054068923
        model: {}
        policy_loss: -0.0018966497154906392
        total_loss: -0.0022951290011405945
        vf_explained_var: 0.010681316256523132
        vf_loss: 1.0527398586273193
      agent-1:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23857343196868896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009289003210142255
        model: {}
        policy_loss: -0.0013712947256863117
        total_loss: -0.0017359228804707527
        vf_explained_var: 0.08933013677597046
        vf_loss: 0.5525966882705688
      agent-2:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37468421459198
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014520420227199793
        model: {}
        policy_loss: -0.0019282225985080004
        total_loss: -0.0022233843337744474
        vf_explained_var: 0.02413494884967804
        vf_loss: 3.6428043842315674
      agent-3:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5469527244567871
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013352942187339067
        model: {}
        policy_loss: -0.0016396855935454369
        total_loss: -0.002437773160636425
        vf_explained_var: 0.006340935826301575
        vf_loss: 1.6455078125
      agent-4:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46107709407806396
        entropy_coeff: 0.0017600000137463212
        kl: 0.000764987722504884
        model: {}
        policy_loss: -0.0013278397964313626
        total_loss: -0.0020464526023715734
        vf_explained_var: 0.014654964208602905
        vf_loss: 0.9287908673286438
      agent-5:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5750988721847534
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015902541344985366
        model: {}
        policy_loss: -0.0019251876510679722
        total_loss: -0.002827364718541503
        vf_explained_var: 0.013649582862854004
        vf_loss: 1.1000263690948486
    load_time_ms: 14141.702
    num_steps_sampled: 68352000
    num_steps_trained: 68352000
    sample_time_ms: 119507.699
    update_time_ms: 17.307
  iterations_since_restore: 62
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.041545893719803
    ram_util_percent: 13.95024154589372
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 18.0
    agent-2: 59.0
    agent-3: 37.0
    agent-4: 24.0
    agent-5: 26.0
  policy_reward_mean:
    agent-0: 15.0
    agent-1: 8.45
    agent-2: 41.57
    agent-3: 19.84
    agent-4: 14.45
    agent-5: 16.26
  policy_reward_min:
    agent-0: 3.0
    agent-1: 0.0
    agent-2: 5.0
    agent-3: 2.0
    agent-4: 4.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 28.28839259838162
    mean_inference_ms: 15.150917469863371
    mean_processing_ms: 73.74341912525465
  time_since_restore: 9064.95087313652
  time_this_iter_s: 145.20932865142822
  time_total_s: 98001.62900280952
  timestamp: 1637372030
  timesteps_since_restore: 5952000
  timesteps_this_iter: 96000
  timesteps_total: 68352000
  training_iteration: 712
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    712 |          98001.6 | 68352000 |   115.57 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 6.43
    apples_agent-0_min: 0
    apples_agent-1_max: 32
    apples_agent-1_mean: 2.94
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 27.98
    apples_agent-2_min: 13
    apples_agent-3_max: 29
    apples_agent-3_mean: 7.93
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.98
    apples_agent-4_min: 2
    apples_agent-5_max: 23
    apples_agent-5_mean: 3.88
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 1.22
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 289
    cleaning_beam_agent-1_mean: 218.53
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 5.21
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 28
    cleaning_beam_agent-3_mean: 10.08
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 23
    cleaning_beam_agent-4_mean: 8.05
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 4.47
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-36-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 152.0
  episode_reward_mean: 118.21
  episode_reward_min: 79.0
  episodes_this_iter: 96
  episodes_total: 68448
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11881.81
    learner:
      agent-0:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2880125045776367
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007872436544857919
        model: {}
        policy_loss: -0.0016948897391557693
        total_loss: -0.0020941710099577904
        vf_explained_var: 0.019671157002449036
        vf_loss: 1.0762343406677246
      agent-1:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2407601773738861
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009315679781138897
        model: {}
        policy_loss: -0.001454701879993081
        total_loss: -0.001827622763812542
        vf_explained_var: 0.06491780281066895
        vf_loss: 0.5081789493560791
      agent-2:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37752729654312134
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009062972967512906
        model: {}
        policy_loss: -0.0016296524554491043
        total_loss: -0.0019403910264372826
        vf_explained_var: 0.029285743832588196
        vf_loss: 3.537062644958496
      agent-3:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5419289469718933
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010741959558799863
        model: {}
        policy_loss: -0.0016781657468527555
        total_loss: -0.00249002268537879
        vf_explained_var: 0.0023418068885803223
        vf_loss: 1.4193663597106934
      agent-4:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45812809467315674
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006547895609401166
        model: {}
        policy_loss: -0.001299203373491764
        total_loss: -0.0020027686841785908
        vf_explained_var: 0.013043686747550964
        vf_loss: 1.027436375617981
      agent-5:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.569911539554596
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015052997041493654
        model: {}
        policy_loss: -0.001578533323481679
        total_loss: -0.002456060843542218
        vf_explained_var: 0.0095510333776474
        vf_loss: 1.2551805973052979
    load_time_ms: 14148.713
    num_steps_sampled: 68448000
    num_steps_trained: 68448000
    sample_time_ms: 119614.888
    update_time_ms: 17.295
  iterations_since_restore: 63
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.02727272727273
    ram_util_percent: 13.955980861244015
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 16.0
    agent-2: 64.0
    agent-3: 34.0
    agent-4: 24.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.56
    agent-1: 7.86
    agent-2: 42.59
    agent-3: 19.87
    agent-4: 14.64
    agent-5: 17.69
  policy_reward_min:
    agent-0: 5.0
    agent-1: 0.0
    agent-2: 22.0
    agent-3: 12.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.290080646888722
    mean_inference_ms: 15.151610582941375
    mean_processing_ms: 73.74821325170849
  time_since_restore: 9211.532335281372
  time_this_iter_s: 146.58146214485168
  time_total_s: 98148.21046495438
  timestamp: 1637372177
  timesteps_since_restore: 6048000
  timesteps_this_iter: 96000
  timesteps_total: 68448000
  training_iteration: 713
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    713 |          98148.2 | 68448000 |   118.21 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 6.83
    apples_agent-0_min: 2
    apples_agent-1_max: 8
    apples_agent-1_mean: 2.89
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 27.73
    apples_agent-2_min: 4
    apples_agent-3_max: 38
    apples_agent-3_mean: 7.85
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 9.82
    apples_agent-4_min: 0
    apples_agent-5_max: 23
    apples_agent-5_mean: 3.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 1.01
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 271
    cleaning_beam_agent-1_mean: 225.25
    cleaning_beam_agent-1_min: 22
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.52
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 11.48
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 23
    cleaning_beam_agent-4_mean: 8.89
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 4.45
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-38-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 162.0
  episode_reward_mean: 116.48
  episode_reward_min: 10.0
  episodes_this_iter: 96
  episodes_total: 68544
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11874.967
    learner:
      agent-0:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3081050515174866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010047582909464836
        model: {}
        policy_loss: -0.0017070446629077196
        total_loss: -0.002144095953553915
        vf_explained_var: 0.011956125497817993
        vf_loss: 1.0521657466888428
      agent-1:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24353940784931183
        entropy_coeff: 0.0017600000137463212
        kl: 0.001513842260465026
        model: {}
        policy_loss: -0.002002669032663107
        total_loss: -0.0023720257449895144
        vf_explained_var: 0.0654037594795227
        vf_loss: 0.5927010774612427
      agent-2:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38041454553604126
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010616984218358994
        model: {}
        policy_loss: -0.0018189074471592903
        total_loss: -0.0021028555929660797
        vf_explained_var: 0.03644531965255737
        vf_loss: 3.8558096885681152
      agent-3:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.543715238571167
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008975057862699032
        model: {}
        policy_loss: -0.0014694733545184135
        total_loss: -0.002287687035277486
        vf_explained_var: 0.007436618208885193
        vf_loss: 1.3872766494750977
      agent-4:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4558241069316864
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006849811761640012
        model: {}
        policy_loss: -0.001200815662741661
        total_loss: -0.001897687092423439
        vf_explained_var: 0.016538262367248535
        vf_loss: 1.0538052320480347
      agent-5:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5783708691596985
        entropy_coeff: 0.0017600000137463212
        kl: 0.001899017603136599
        model: {}
        policy_loss: -0.0017167949117720127
        total_loss: -0.0026288991793990135
        vf_explained_var: 0.01738610863685608
        vf_loss: 1.0582947731018066
    load_time_ms: 14151.516
    num_steps_sampled: 68544000
    num_steps_trained: 68544000
    sample_time_ms: 119648.404
    update_time_ms: 17.67
  iterations_since_restore: 64
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.99665071770335
    ram_util_percent: 13.964114832535882
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 21.0
    agent-2: 59.0
    agent-3: 31.0
    agent-4: 31.0
    agent-5: 26.0
  policy_reward_mean:
    agent-0: 14.83
    agent-1: 8.42
    agent-2: 42.97
    agent-3: 19.8
    agent-4: 14.76
    agent-5: 15.7
  policy_reward_min:
    agent-0: 2.0
    agent-1: 1.0
    agent-2: 6.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 1.0
  sampler_perf:
    mean_env_wait_ms: 28.291868493158482
    mean_inference_ms: 15.151139099776854
    mean_processing_ms: 73.75106650744817
  time_since_restore: 9357.658613681793
  time_this_iter_s: 146.12627840042114
  time_total_s: 98294.3367433548
  timestamp: 1637372323
  timesteps_since_restore: 6144000
  timesteps_this_iter: 96000
  timesteps_total: 68544000
  training_iteration: 714
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    714 |          98294.3 | 68544000 |   116.48 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 6.98
    apples_agent-0_min: 1
    apples_agent-1_max: 34
    apples_agent-1_mean: 3.23
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 28.15
    apples_agent-2_min: 11
    apples_agent-3_max: 32
    apples_agent-3_mean: 8.29
    apples_agent-3_min: 2
    apples_agent-4_max: 51
    apples_agent-4_mean: 10.42
    apples_agent-4_min: 1
    apples_agent-5_max: 24
    apples_agent-5_mean: 4.63
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 0.93
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 272
    cleaning_beam_agent-1_mean: 223.38
    cleaning_beam_agent-1_min: 157
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 4.3
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 11.37
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 32
    cleaning_beam_agent-4_mean: 8.37
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 3.75
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-41-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 164.0
  episode_reward_mean: 119.35
  episode_reward_min: 74.0
  episodes_this_iter: 96
  episodes_total: 68640
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11866.643
    learner:
      agent-0:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30815112590789795
        entropy_coeff: 0.0017600000137463212
        kl: 0.001038566348142922
        model: {}
        policy_loss: -0.0018293325556442142
        total_loss: -0.0022695674560964108
        vf_explained_var: 0.009121149778366089
        vf_loss: 1.0211081504821777
      agent-1:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24342532455921173
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008568349294364452
        model: {}
        policy_loss: -0.0014217216521501541
        total_loss: -0.0018001552671194077
        vf_explained_var: 0.07489019632339478
        vf_loss: 0.49993523955345154
      agent-2:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37443962693214417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009079528972506523
        model: {}
        policy_loss: -0.001548777800053358
        total_loss: -0.0018291054293513298
        vf_explained_var: 0.011575400829315186
        vf_loss: 3.7868475914001465
      agent-3:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5424423813819885
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011715287109836936
        model: {}
        policy_loss: -0.0016292673535645008
        total_loss: -0.0024431815836578608
        vf_explained_var: 0.012094229459762573
        vf_loss: 1.4078648090362549
      agent-4:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45128169655799866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007694375817663968
        model: {}
        policy_loss: -0.0013988064602017403
        total_loss: -0.0020948071032762527
        vf_explained_var: 0.015682324767112732
        vf_loss: 0.9825308322906494
      agent-5:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5789799690246582
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011033740593120456
        model: {}
        policy_loss: -0.0015719151124358177
        total_loss: -0.002469003899022937
        vf_explained_var: 0.00740930438041687
        vf_loss: 1.219132661819458
    load_time_ms: 14148.314
    num_steps_sampled: 68640000
    num_steps_trained: 68640000
    sample_time_ms: 119612.309
    update_time_ms: 17.242
  iterations_since_restore: 65
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.059420289855073
    ram_util_percent: 13.9487922705314
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 16.0
    agent-2: 69.0
    agent-3: 33.0
    agent-4: 27.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.86
    agent-1: 8.13
    agent-2: 43.52
    agent-3: 20.66
    agent-4: 14.81
    agent-5: 16.37
  policy_reward_min:
    agent-0: 5.0
    agent-1: 0.0
    agent-2: 20.0
    agent-3: 10.0
    agent-4: 5.0
    agent-5: -29.0
  sampler_perf:
    mean_env_wait_ms: 28.29204694839917
    mean_inference_ms: 15.149951577807407
    mean_processing_ms: 73.7500356072198
  time_since_restore: 9503.370457649231
  time_this_iter_s: 145.71184396743774
  time_total_s: 98440.04858732224
  timestamp: 1637372469
  timesteps_since_restore: 6240000
  timesteps_this_iter: 96000
  timesteps_total: 68640000
  training_iteration: 715
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    715 |            98440 | 68640000 |   119.35 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 6.14
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 2.76
    apples_agent-1_min: 0
    apples_agent-2_max: 54
    apples_agent-2_mean: 27.12
    apples_agent-2_min: 1
    apples_agent-3_max: 31
    apples_agent-3_mean: 7.64
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.9
    apples_agent-4_min: 2
    apples_agent-5_max: 22
    apples_agent-5_mean: 4.35
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 0.85
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 255
    cleaning_beam_agent-1_mean: 210.33
    cleaning_beam_agent-1_min: 36
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 5.42
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 11.91
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 24
    cleaning_beam_agent-4_mean: 8.76
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.77
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-43-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 157.0
  episode_reward_mean: 113.97
  episode_reward_min: 10.0
  episodes_this_iter: 96
  episodes_total: 68736
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11858.055
    learner:
      agent-0:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32074639201164246
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008141405414789915
        model: {}
        policy_loss: -0.0017306562513113022
        total_loss: -0.002200573682785034
        vf_explained_var: 0.005316436290740967
        vf_loss: 0.9459807872772217
      agent-1:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23583248257637024
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011698059970512986
        model: {}
        policy_loss: -0.0017426896374672651
        total_loss: -0.0021099296864122152
        vf_explained_var: 0.07568606734275818
        vf_loss: 0.47826555371284485
      agent-2:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3831619322299957
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010341638699173927
        model: {}
        policy_loss: -0.001679187873378396
        total_loss: -0.001989326672628522
        vf_explained_var: 0.030022025108337402
        vf_loss: 3.642260789871216
      agent-3:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5495497584342957
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018018253613263369
        model: {}
        policy_loss: -0.0016746902838349342
        total_loss: -0.0024964017793536186
        vf_explained_var: -0.0061547160148620605
        vf_loss: 1.454953908920288
      agent-4:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4497675895690918
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014735395088791847
        model: {}
        policy_loss: -0.0017149272607639432
        total_loss: -0.002414853312075138
        vf_explained_var: 0.009424075484275818
        vf_loss: 0.9166303873062134
      agent-5:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5813700556755066
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017255423590540886
        model: {}
        policy_loss: -0.0017636233242228627
        total_loss: -0.00266568036749959
        vf_explained_var: 0.01603098213672638
        vf_loss: 1.2115486860275269
    load_time_ms: 14159.594
    num_steps_sampled: 68736000
    num_steps_trained: 68736000
    sample_time_ms: 119760.761
    update_time_ms: 17.204
  iterations_since_restore: 66
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.910952380952388
    ram_util_percent: 13.946666666666664
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 18.0
    agent-2: 73.0
    agent-3: 32.0
    agent-4: 28.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 14.51
    agent-1: 7.91
    agent-2: 41.05
    agent-3: 19.63
    agent-4: 14.21
    agent-5: 16.66
  policy_reward_min:
    agent-0: 2.0
    agent-1: 0.0
    agent-2: 3.0
    agent-3: 1.0
    agent-4: 4.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 28.293262021443887
    mean_inference_ms: 15.150425882572902
    mean_processing_ms: 73.75151467098323
  time_since_restore: 9650.064725637436
  time_this_iter_s: 146.69426798820496
  time_total_s: 98586.74285531044
  timestamp: 1637372616
  timesteps_since_restore: 6336000
  timesteps_this_iter: 96000
  timesteps_total: 68736000
  training_iteration: 716
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    716 |          98586.7 | 68736000 |   113.97 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 6.47
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 2.94
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 28.46
    apples_agent-2_min: 15
    apples_agent-3_max: 24
    apples_agent-3_mean: 7.24
    apples_agent-3_min: 2
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.3
    apples_agent-4_min: 2
    apples_agent-5_max: 25
    apples_agent-5_mean: 3.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 0.95
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 296
    cleaning_beam_agent-1_mean: 220.04
    cleaning_beam_agent-1_min: 147
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 5.06
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 26
    cleaning_beam_agent-3_mean: 11.41
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 25
    cleaning_beam_agent-4_mean: 7.4
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 4.26
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-46-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 159.0
  episode_reward_mean: 119.36
  episode_reward_min: 80.0
  episodes_this_iter: 96
  episodes_total: 68832
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11862.982
    learner:
      agent-0:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3033721446990967
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015807710587978363
        model: {}
        policy_loss: -0.0020005281548947096
        total_loss: -0.00242467038333416
        vf_explained_var: 0.01985757052898407
        vf_loss: 1.097947597503662
      agent-1:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24037812650203705
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008245441131293774
        model: {}
        policy_loss: -0.0014569712802767754
        total_loss: -0.0018312027677893639
        vf_explained_var: 0.06994563341140747
        vf_loss: 0.48837220668792725
      agent-2:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38401252031326294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013337101554498076
        model: {}
        policy_loss: -0.0017843206878751516
        total_loss: -0.002084751147776842
        vf_explained_var: 0.03269076347351074
        vf_loss: 3.7543320655822754
      agent-3:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5414109230041504
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012063269969075918
        model: {}
        policy_loss: -0.0015625772066414356
        total_loss: -0.002378336153924465
        vf_explained_var: 0.006731241941452026
        vf_loss: 1.3712447881698608
      agent-4:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4590296745300293
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011266929795965552
        model: {}
        policy_loss: -0.0013888003304600716
        total_loss: -0.002093297429382801
        vf_explained_var: 0.015402480959892273
        vf_loss: 1.033952236175537
      agent-5:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5803676247596741
        entropy_coeff: 0.0017600000137463212
        kl: 0.001608067424967885
        model: {}
        policy_loss: -0.0017909668385982513
        total_loss: -0.0026984233409166336
        vf_explained_var: 0.00875827670097351
        vf_loss: 1.1398695707321167
    load_time_ms: 14152.289
    num_steps_sampled: 68832000
    num_steps_trained: 68832000
    sample_time_ms: 119708.765
    update_time_ms: 16.787
  iterations_since_restore: 67
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.11980676328503
    ram_util_percent: 13.953623188405798
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 15.0
    agent-2: 59.0
    agent-3: 33.0
    agent-4: 28.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 16.1
    agent-1: 8.07
    agent-2: 43.19
    agent-3: 19.89
    agent-4: 15.0
    agent-5: 17.11
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 23.0
    agent-3: 11.0
    agent-4: 7.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.29422665645603
    mean_inference_ms: 15.150991224077561
    mean_processing_ms: 73.7535986631802
  time_since_restore: 9795.576600313187
  time_this_iter_s: 145.51187467575073
  time_total_s: 98732.25472998619
  timestamp: 1637372762
  timesteps_since_restore: 6432000
  timesteps_this_iter: 96000
  timesteps_total: 68832000
  training_iteration: 717
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 25.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    717 |          98732.3 | 68832000 |   119.36 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 7.09
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 3.36
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 28.34
    apples_agent-2_min: 15
    apples_agent-3_max: 30
    apples_agent-3_mean: 7.97
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.04
    apples_agent-4_min: 1
    apples_agent-5_max: 24
    apples_agent-5_mean: 3.78
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 0.85
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 265
    cleaning_beam_agent-1_mean: 215.63
    cleaning_beam_agent-1_min: 159
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 5.18
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 25
    cleaning_beam_agent-3_mean: 9.71
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 30
    cleaning_beam_agent-4_mean: 8.32
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.28
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-48-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 157.0
  episode_reward_mean: 117.83
  episode_reward_min: 70.0
  episodes_this_iter: 96
  episodes_total: 68928
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11851.936
    learner:
      agent-0:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2985687255859375
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008511061314493418
        model: {}
        policy_loss: -0.0017440358642488718
        total_loss: -0.002170006511732936
        vf_explained_var: 0.012070253491401672
        vf_loss: 0.9950927495956421
      agent-1:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24226635694503784
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011530658230185509
        model: {}
        policy_loss: -0.001710975542664528
        total_loss: -0.002073085866868496
        vf_explained_var: 0.08166149258613586
        vf_loss: 0.6428297162055969
      agent-2:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37138622999191284
        entropy_coeff: 0.0017600000137463212
        kl: 0.001180265098810196
        model: {}
        policy_loss: -0.001944082323461771
        total_loss: -0.002224105643108487
        vf_explained_var: 0.04136921465396881
        vf_loss: 3.7361340522766113
      agent-3:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5355027318000793
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011926762526854873
        model: {}
        policy_loss: -0.0016599325463175774
        total_loss: -0.0024571921676397324
        vf_explained_var: 0.00022044777870178223
        vf_loss: 1.4522000551223755
      agent-4:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.457505464553833
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008331463905051351
        model: {}
        policy_loss: -0.0015153042040765285
        total_loss: -0.0022178692743182182
        vf_explained_var: 0.011813148856163025
        vf_loss: 1.0264561176300049
      agent-5:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5712332129478455
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016854216810315847
        model: {}
        policy_loss: -0.0017033359035849571
        total_loss: -0.0025889892131090164
        vf_explained_var: 0.009109973907470703
        vf_loss: 1.1972097158432007
    load_time_ms: 14162.083
    num_steps_sampled: 68928000
    num_steps_trained: 68928000
    sample_time_ms: 119850.491
    update_time_ms: 16.72
  iterations_since_restore: 68
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.985238095238092
    ram_util_percent: 13.908571428571426
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 21.0
    agent-2: 58.0
    agent-3: 32.0
    agent-4: 25.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.37
    agent-1: 8.82
    agent-2: 42.92
    agent-3: 20.29
    agent-4: 13.94
    agent-5: 16.49
  policy_reward_min:
    agent-0: 4.0
    agent-1: 0.0
    agent-2: 26.0
    agent-3: 8.0
    agent-4: 3.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.296473639951873
    mean_inference_ms: 15.152767141004404
    mean_processing_ms: 73.76149812469741
  time_since_restore: 9943.235983133316
  time_this_iter_s: 147.6593828201294
  time_total_s: 98879.91411280632
  timestamp: 1637372909
  timesteps_since_restore: 6528000
  timesteps_this_iter: 96000
  timesteps_total: 68928000
  training_iteration: 718
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 25.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    718 |          98879.9 | 68928000 |   117.83 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 6.69
    apples_agent-0_min: 1
    apples_agent-1_max: 21
    apples_agent-1_mean: 3.27
    apples_agent-1_min: 0
    apples_agent-2_max: 40
    apples_agent-2_mean: 28.27
    apples_agent-2_min: 13
    apples_agent-3_max: 20
    apples_agent-3_mean: 7.88
    apples_agent-3_min: 2
    apples_agent-4_max: 18
    apples_agent-4_mean: 9.82
    apples_agent-4_min: 2
    apples_agent-5_max: 22
    apples_agent-5_mean: 3.79
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.07
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 260
    cleaning_beam_agent-1_mean: 217.59
    cleaning_beam_agent-1_min: 175
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 4.41
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 21
    cleaning_beam_agent-3_mean: 9.12
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 26
    cleaning_beam_agent-4_mean: 7.76
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.92
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-50-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 116.57
  episode_reward_min: 7.0
  episodes_this_iter: 96
  episodes_total: 69024
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11851.852
    learner:
      agent-0:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28498294949531555
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005533343646675348
        model: {}
        policy_loss: -0.0011448115110397339
        total_loss: -0.0013983729295432568
        vf_explained_var: 0.006448239088058472
        vf_loss: 2.480105400085449
      agent-1:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24004557728767395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010290578939020634
        model: {}
        policy_loss: -0.0013887642417103052
        total_loss: -0.0017610721988603473
        vf_explained_var: 0.061957404017448425
        vf_loss: 0.5016886591911316
      agent-2:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.368377685546875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010567432036623359
        model: {}
        policy_loss: -0.001607334241271019
        total_loss: -0.001771335955709219
        vf_explained_var: 0.0004925578832626343
        vf_loss: 4.843423366546631
      agent-3:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5332452654838562
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012598696630448103
        model: {}
        policy_loss: -0.0017146454192698002
        total_loss: -0.002488324884325266
        vf_explained_var: 0.007919549942016602
        vf_loss: 1.6483447551727295
      agent-4:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4502982199192047
        entropy_coeff: 0.0017600000137463212
        kl: 0.001419461565092206
        model: {}
        policy_loss: -0.001670396188274026
        total_loss: -0.002369876019656658
        vf_explained_var: 0.015666276216506958
        vf_loss: 0.9304357767105103
      agent-5:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5647790431976318
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016672485508024693
        model: {}
        policy_loss: -0.0016561610391363502
        total_loss: -0.0025414302945137024
        vf_explained_var: 0.013393819332122803
        vf_loss: 1.087419033050537
    load_time_ms: 14159.387
    num_steps_sampled: 69024000
    num_steps_trained: 69024000
    sample_time_ms: 119908.082
    update_time_ms: 16.743
  iterations_since_restore: 69
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.933971291866026
    ram_util_percent: 13.89234449760765
  pid: 27065
  policy_reward_max:
    agent-0: 24.0
    agent-1: 16.0
    agent-2: 66.0
    agent-3: 36.0
    agent-4: 26.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 14.26
    agent-1: 8.01
    agent-2: 42.95
    agent-3: 20.01
    agent-4: 14.62
    agent-5: 16.72
  policy_reward_min:
    agent-0: -34.0
    agent-1: 1.0
    agent-2: -9.0
    agent-3: -25.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.29964292201703
    mean_inference_ms: 15.153348738248429
    mean_processing_ms: 73.76683061067342
  time_since_restore: 10089.482110738754
  time_this_iter_s: 146.24612760543823
  time_total_s: 99026.16024041176
  timestamp: 1637373056
  timesteps_since_restore: 6624000
  timesteps_this_iter: 96000
  timesteps_total: 69024000
  training_iteration: 719
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    719 |          99026.2 | 69024000 |   116.57 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 6.78
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.0
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 27.54
    apples_agent-2_min: 6
    apples_agent-3_max: 28
    apples_agent-3_mean: 7.82
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.21
    apples_agent-4_min: 1
    apples_agent-5_max: 16
    apples_agent-5_mean: 4.13
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.02
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 292
    cleaning_beam_agent-1_mean: 210.79
    cleaning_beam_agent-1_min: 42
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 5.23
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 23
    cleaning_beam_agent-3_mean: 9.67
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 22
    cleaning_beam_agent-4_mean: 8.79
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.38
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-53-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 160.0
  episode_reward_mean: 115.28
  episode_reward_min: 25.0
  episodes_this_iter: 96
  episodes_total: 69120
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11855.311
    learner:
      agent-0:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2678256034851074
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006606036331504583
        model: {}
        policy_loss: -0.001583367120474577
        total_loss: -0.00195365771651268
        vf_explained_var: 0.008503049612045288
        vf_loss: 1.0108263492584229
      agent-1:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23659685254096985
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011628816137090325
        model: {}
        policy_loss: -0.0016796593554317951
        total_loss: -0.002043168293312192
        vf_explained_var: 0.08088555932044983
        vf_loss: 0.5290117859840393
      agent-2:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3696085214614868
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006515256827697158
        model: {}
        policy_loss: -0.001495181117206812
        total_loss: -0.0017687603831291199
        vf_explained_var: 0.032872870564460754
        vf_loss: 3.7693405151367188
      agent-3:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5254595279693604
        entropy_coeff: 0.0017600000137463212
        kl: 0.001262948615476489
        model: {}
        policy_loss: -0.0016315476968884468
        total_loss: -0.0023987763561308384
        vf_explained_var: 0.004076778888702393
        vf_loss: 1.5758018493652344
      agent-4:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45489993691444397
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011356279719620943
        model: {}
        policy_loss: -0.0014309663092717528
        total_loss: -0.002134102862328291
        vf_explained_var: 0.006046518683433533
        vf_loss: 0.974859356880188
      agent-5:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5578908324241638
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018298561917617917
        model: {}
        policy_loss: -0.0017407627310603857
        total_loss: -0.002607827540487051
        vf_explained_var: 0.0018630623817443848
        vf_loss: 1.148197054862976
    load_time_ms: 14182.949
    num_steps_sampled: 69120000
    num_steps_trained: 69120000
    sample_time_ms: 119999.208
    update_time_ms: 15.98
  iterations_since_restore: 70
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.9622009569378
    ram_util_percent: 12.3688995215311
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 23.0
    agent-2: 73.0
    agent-3: 31.0
    agent-4: 25.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 14.82
    agent-1: 8.05
    agent-2: 42.1
    agent-3: 20.04
    agent-4: 13.9
    agent-5: 16.37
  policy_reward_min:
    agent-0: 1.0
    agent-1: 0.0
    agent-2: 11.0
    agent-3: 4.0
    agent-4: 2.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.298871773001192
    mean_inference_ms: 15.153501439807105
    mean_processing_ms: 73.77006496132775
  time_since_restore: 10236.326251029968
  time_this_iter_s: 146.844140291214
  time_total_s: 99173.00438070297
  timestamp: 1637373203
  timesteps_since_restore: 6720000
  timesteps_this_iter: 96000
  timesteps_total: 69120000
  training_iteration: 720
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    720 |            99173 | 69120000 |   115.28 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 6.89
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 2.82
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 27.19
    apples_agent-2_min: 9
    apples_agent-3_max: 20
    apples_agent-3_mean: 7.04
    apples_agent-3_min: 0
    apples_agent-4_max: 26
    apples_agent-4_mean: 9.96
    apples_agent-4_min: 2
    apples_agent-5_max: 19
    apples_agent-5_mean: 3.82
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 0.94
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 207.83
    cleaning_beam_agent-1_min: 150
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 5.22
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 44
    cleaning_beam_agent-3_mean: 11.1
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 21
    cleaning_beam_agent-4_mean: 7.75
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.1
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-55-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 171.0
  episode_reward_mean: 114.94
  episode_reward_min: 74.0
  episodes_this_iter: 96
  episodes_total: 69216
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11852.402
    learner:
      agent-0:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2697919011116028
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011360883945599198
        model: {}
        policy_loss: -0.0017238473519682884
        total_loss: -0.002096235752105713
        vf_explained_var: 0.010104313492774963
        vf_loss: 1.0244559049606323
      agent-1:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23567019402980804
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009436460677534342
        model: {}
        policy_loss: -0.001267788466066122
        total_loss: -0.0016346951015293598
        vf_explained_var: 0.06866496801376343
        vf_loss: 0.4787227511405945
      agent-2:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38421541452407837
        entropy_coeff: 0.0017600000137463212
        kl: 0.001734318328090012
        model: {}
        policy_loss: -0.0018738945946097374
        total_loss: -0.0022040926851332188
        vf_explained_var: 0.039347171783447266
        vf_loss: 3.4601988792419434
      agent-3:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5250124335289001
        entropy_coeff: 0.0017600000137463212
        kl: 0.001569506712257862
        model: {}
        policy_loss: -0.0017669962253421545
        total_loss: -0.0025374586693942547
        vf_explained_var: 0.0067314207553863525
        vf_loss: 1.5355899333953857
      agent-4:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4651954174041748
        entropy_coeff: 0.0017600000137463212
        kl: 0.001645794720388949
        model: {}
        policy_loss: -0.001610560342669487
        total_loss: -0.00233476422727108
        vf_explained_var: 0.014336585998535156
        vf_loss: 0.9454340934753418
      agent-5:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5575465559959412
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008125823806039989
        model: {}
        policy_loss: -0.0015410184860229492
        total_loss: -0.0024070164654403925
        vf_explained_var: 0.006439030170440674
        vf_loss: 1.1528310775756836
    load_time_ms: 14164.957
    num_steps_sampled: 69216000
    num_steps_trained: 69216000
    sample_time_ms: 120043.907
    update_time_ms: 16.028
  iterations_since_restore: 71
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.78995215311005
    ram_util_percent: 12.363636363636363
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 18.0
    agent-2: 61.0
    agent-3: 42.0
    agent-4: 25.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 15.42
    agent-1: 7.71
    agent-2: 41.35
    agent-3: 19.88
    agent-4: 14.02
    agent-5: 16.56
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 23.0
    agent-3: 8.0
    agent-4: 3.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.297648669054393
    mean_inference_ms: 15.150830620932394
    mean_processing_ms: 73.76818715178712
  time_since_restore: 10381.441254377365
  time_this_iter_s: 145.11500334739685
  time_total_s: 99318.11938405037
  timestamp: 1637373350
  timesteps_since_restore: 6816000
  timesteps_this_iter: 96000
  timesteps_total: 69216000
  training_iteration: 721
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    721 |          99318.1 | 69216000 |   114.94 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 6.31
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 2.93
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 26.67
    apples_agent-2_min: 4
    apples_agent-3_max: 48
    apples_agent-3_mean: 7.9
    apples_agent-3_min: 1
    apples_agent-4_max: 18
    apples_agent-4_mean: 9.76
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 3.56
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 1.24
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 250
    cleaning_beam_agent-1_mean: 205.86
    cleaning_beam_agent-1_min: 17
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 5.34
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 26
    cleaning_beam_agent-3_mean: 9.25
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 21
    cleaning_beam_agent-4_mean: 6.8
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.9
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_20-58-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 171.0
  episode_reward_mean: 110.97
  episode_reward_min: 14.0
  episodes_this_iter: 96
  episodes_total: 69312
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11842.711
    learner:
      agent-0:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26405197381973267
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006329642492346466
        model: {}
        policy_loss: -0.0014874991029500961
        total_loss: -0.0018550627864897251
        vf_explained_var: 0.022400647401809692
        vf_loss: 0.9716614484786987
      agent-1:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23084640502929688
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011377569753676653
        model: {}
        policy_loss: -0.0014510351466014981
        total_loss: -0.0018095687264576554
        vf_explained_var: 0.09722508490085602
        vf_loss: 0.4775564670562744
      agent-2:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39296188950538635
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008946220041252673
        model: {}
        policy_loss: -0.0017692223191261292
        total_loss: -0.002101575955748558
        vf_explained_var: 0.04316812753677368
        vf_loss: 3.5925822257995605
      agent-3:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5185998678207397
        entropy_coeff: 0.0017600000137463212
        kl: 0.001127342926338315
        model: {}
        policy_loss: -0.0016195597127079964
        total_loss: -0.0023787294048815966
        vf_explained_var: 0.01852506399154663
        vf_loss: 1.535636067390442
      agent-4:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4644475281238556
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007628482417203486
        model: {}
        policy_loss: -0.0015251794829964638
        total_loss: -0.002240062691271305
        vf_explained_var: 0.016049310564994812
        vf_loss: 1.0254377126693726
      agent-5:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5588266849517822
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018881898140534759
        model: {}
        policy_loss: -0.0018597501330077648
        total_loss: -0.002729356987401843
        vf_explained_var: 0.011203452944755554
        vf_loss: 1.1392760276794434
    load_time_ms: 14153.642
    num_steps_sampled: 69312000
    num_steps_trained: 69312000
    sample_time_ms: 119988.883
    update_time_ms: 15.978
  iterations_since_restore: 72
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.996116504854374
    ram_util_percent: 12.360679611650482
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 15.0
    agent-2: 55.0
    agent-3: 42.0
    agent-4: 23.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 14.07
    agent-1: 7.64
    agent-2: 39.76
    agent-3: 19.75
    agent-4: 14.19
    agent-5: 15.56
  policy_reward_min:
    agent-0: 1.0
    agent-1: 1.0
    agent-2: 6.0
    agent-3: 1.0
    agent-4: 0.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.294959005528344
    mean_inference_ms: 15.14869658305725
    mean_processing_ms: 73.76221403855052
  time_since_restore: 10525.89044380188
  time_this_iter_s: 144.44918942451477
  time_total_s: 99462.56857347488
  timestamp: 1637373494
  timesteps_since_restore: 6912000
  timesteps_this_iter: 96000
  timesteps_total: 69312000
  training_iteration: 722
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    722 |          99462.6 | 69312000 |   110.97 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 6.64
    apples_agent-0_min: 1
    apples_agent-1_max: 36
    apples_agent-1_mean: 3.09
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 26.97
    apples_agent-2_min: 9
    apples_agent-3_max: 23
    apples_agent-3_mean: 7.32
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 9.65
    apples_agent-4_min: 1
    apples_agent-5_max: 17
    apples_agent-5_mean: 3.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 1.06
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 241
    cleaning_beam_agent-1_mean: 203.45
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 5.09
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 26
    cleaning_beam_agent-3_mean: 8.51
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 22
    cleaning_beam_agent-4_mean: 7.67
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-00-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 157.0
  episode_reward_mean: 113.79
  episode_reward_min: 46.0
  episodes_this_iter: 96
  episodes_total: 69408
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11831.135
    learner:
      agent-0:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2566691040992737
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010177724761888385
        model: {}
        policy_loss: -0.0018309848383069038
        total_loss: -0.002181564224883914
        vf_explained_var: 0.011987686157226562
        vf_loss: 1.0116028785705566
      agent-1:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23140724003314972
        entropy_coeff: 0.0017600000137463212
        kl: 0.001088803866878152
        model: {}
        policy_loss: -0.001610859064385295
        total_loss: -0.0019640978425741196
        vf_explained_var: 0.07427971065044403
        vf_loss: 0.5403775572776794
      agent-2:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39038118720054626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016277467366307974
        model: {}
        policy_loss: -0.0020996946841478348
        total_loss: -0.002433896530419588
        vf_explained_var: 0.029790058732032776
        vf_loss: 3.528693199157715
      agent-3:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5226101875305176
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017449709121137857
        model: {}
        policy_loss: -0.0017100758850574493
        total_loss: -0.002488349564373493
        vf_explained_var: 0.00788956880569458
        vf_loss: 1.415233850479126
      agent-4:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4499322175979614
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007492051227018237
        model: {}
        policy_loss: -0.001436358317732811
        total_loss: -0.001996993785724044
        vf_explained_var: 0.007772579789161682
        vf_loss: 2.312455654144287
      agent-5:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5539132356643677
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018097821157425642
        model: {}
        policy_loss: -0.00172160635702312
        total_loss: -0.0025678526144474745
        vf_explained_var: 0.011956751346588135
        vf_loss: 1.2864062786102295
    load_time_ms: 14158.494
    num_steps_sampled: 69408000
    num_steps_trained: 69408000
    sample_time_ms: 119850.88
    update_time_ms: 15.993
  iterations_since_restore: 73
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.164563106796116
    ram_util_percent: 12.383980582524273
  pid: 27065
  policy_reward_max:
    agent-0: 24.0
    agent-1: 18.0
    agent-2: 57.0
    agent-3: 30.0
    agent-4: 22.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 14.8
    agent-1: 8.25
    agent-2: 40.74
    agent-3: 19.73
    agent-4: 13.53
    agent-5: 16.74
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 14.0
    agent-3: 5.0
    agent-4: -41.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.2913323131132
    mean_inference_ms: 15.147364602526036
    mean_processing_ms: 73.75858207848961
  time_since_restore: 10670.911376953125
  time_this_iter_s: 145.02093315124512
  time_total_s: 99607.58950662613
  timestamp: 1637373639
  timesteps_since_restore: 7008000
  timesteps_this_iter: 96000
  timesteps_total: 69408000
  training_iteration: 723
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    723 |          99607.6 | 69408000 |   113.79 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 6.03
    apples_agent-0_min: 1
    apples_agent-1_max: 15
    apples_agent-1_mean: 3.52
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 27.29
    apples_agent-2_min: 11
    apples_agent-3_max: 23
    apples_agent-3_mean: 7.29
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.66
    apples_agent-4_min: 3
    apples_agent-5_max: 13
    apples_agent-5_mean: 3.7
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 5
    cleaning_beam_agent-0_mean: 0.98
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 239
    cleaning_beam_agent-1_mean: 204.4
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.66
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 26
    cleaning_beam_agent-3_mean: 8.46
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 19
    cleaning_beam_agent-4_mean: 7.33
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.0
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-03-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 115.03
  episode_reward_min: 69.0
  episodes_this_iter: 96
  episodes_total: 69504
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11830.782
    learner:
      agent-0:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2590641379356384
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006600295891985297
        model: {}
        policy_loss: -0.00160687614697963
        total_loss: -0.0019602179527282715
        vf_explained_var: 0.018593087792396545
        vf_loss: 1.0261096954345703
      agent-1:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22942286729812622
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008975625969469547
        model: {}
        policy_loss: -0.0014017641078680754
        total_loss: -0.0017496512737125158
        vf_explained_var: 0.07294684648513794
        vf_loss: 0.5589500665664673
      agent-2:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38157427310943604
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010231778724119067
        model: {}
        policy_loss: -0.0017040316015481949
        total_loss: -0.0020287837833166122
        vf_explained_var: 0.042788729071617126
        vf_loss: 3.468177318572998
      agent-3:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5296432375907898
        entropy_coeff: 0.0017600000137463212
        kl: 0.001828657346777618
        model: {}
        policy_loss: -0.0018836776725947857
        total_loss: -0.0026772492565214634
        vf_explained_var: 0.005704134702682495
        vf_loss: 1.3859965801239014
      agent-4:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44035929441452026
        entropy_coeff: 0.0017600000137463212
        kl: 0.00067599379690364
        model: {}
        policy_loss: -0.00120815250556916
        total_loss: -0.0018835553200915456
        vf_explained_var: 0.016443073749542236
        vf_loss: 0.9962959289550781
      agent-5:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5546146631240845
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021645259112119675
        model: {}
        policy_loss: -0.0018958388827741146
        total_loss: -0.002760302508249879
        vf_explained_var: 0.01238243281841278
        vf_loss: 1.1165688037872314
    load_time_ms: 14164.001
    num_steps_sampled: 69504000
    num_steps_trained: 69504000
    sample_time_ms: 119652.426
    update_time_ms: 15.71
  iterations_since_restore: 74
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.87621359223301
    ram_util_percent: 12.44514563106796
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 18.0
    agent-2: 62.0
    agent-3: 31.0
    agent-4: 26.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 14.6
    agent-1: 8.6
    agent-2: 41.69
    agent-3: 19.57
    agent-4: 14.03
    agent-5: 16.54
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 24.0
    agent-3: 9.0
    agent-4: 6.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.285785947306035
    mean_inference_ms: 15.14551558265252
    mean_processing_ms: 73.74469825861955
  time_since_restore: 10815.101284265518
  time_this_iter_s: 144.1899073123932
  time_total_s: 99751.77941393852
  timestamp: 1637373784
  timesteps_since_restore: 7104000
  timesteps_this_iter: 96000
  timesteps_total: 69504000
  training_iteration: 724
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    724 |          99751.8 | 69504000 |   115.03 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 5.91
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.0
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 26.44
    apples_agent-2_min: 12
    apples_agent-3_max: 23
    apples_agent-3_mean: 7.61
    apples_agent-3_min: 2
    apples_agent-4_max: 19
    apples_agent-4_mean: 10.02
    apples_agent-4_min: 3
    apples_agent-5_max: 17
    apples_agent-5_mean: 3.6
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 1.02
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 256
    cleaning_beam_agent-1_mean: 204.79
    cleaning_beam_agent-1_min: 153
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 4.77
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 8.52
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 20
    cleaning_beam_agent-4_mean: 7.66
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.37
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-05-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 177.0
  episode_reward_mean: 113.1
  episode_reward_min: 72.0
  episodes_this_iter: 96
  episodes_total: 69600
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11839.138
    learner:
      agent-0:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2571769952774048
        entropy_coeff: 0.0017600000137463212
        kl: 0.000721819931641221
        model: {}
        policy_loss: -0.001480677630752325
        total_loss: -0.0018319319933652878
        vf_explained_var: 0.015359371900558472
        vf_loss: 1.013748288154602
      agent-1:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23147247731685638
        entropy_coeff: 0.0017600000137463212
        kl: 0.001056811772286892
        model: {}
        policy_loss: -0.0014397352933883667
        total_loss: -0.0017999280244112015
        vf_explained_var: 0.0891936868429184
        vf_loss: 0.4719949960708618
      agent-2:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38074374198913574
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011284461943432689
        model: {}
        policy_loss: -0.001776824239641428
        total_loss: -0.0020900098606944084
        vf_explained_var: 0.018718227744102478
        vf_loss: 3.5692152976989746
      agent-3:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5128803253173828
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020764926448464394
        model: {}
        policy_loss: -0.0019418513402342796
        total_loss: -0.002708408748731017
        vf_explained_var: 0.011500880122184753
        vf_loss: 1.361136794090271
      agent-4:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44272181391716003
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006580072222277522
        model: {}
        policy_loss: -0.0013061035424470901
        total_loss: -0.0019849231466650963
        vf_explained_var: 0.0172044038772583
        vf_loss: 1.0037052631378174
      agent-5:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5574824213981628
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018851750064641237
        model: {}
        policy_loss: -0.001588144339621067
        total_loss: -0.0024554822593927383
        vf_explained_var: 0.011179149150848389
        vf_loss: 1.1382778882980347
    load_time_ms: 14151.479
    num_steps_sampled: 69600000
    num_steps_trained: 69600000
    sample_time_ms: 119635.465
    update_time_ms: 15.691
  iterations_since_restore: 75
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.909178743961355
    ram_util_percent: 12.36521739130435
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 16.0
    agent-2: 60.0
    agent-3: 35.0
    agent-4: 26.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 14.27
    agent-1: 8.13
    agent-2: 40.93
    agent-3: 19.02
    agent-4: 14.83
    agent-5: 15.92
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 19.0
    agent-3: 7.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.283910850956463
    mean_inference_ms: 15.145148576410667
    mean_processing_ms: 73.73703930033949
  time_since_restore: 10960.602964878082
  time_this_iter_s: 145.5016806125641
  time_total_s: 99897.28109455109
  timestamp: 1637373929
  timesteps_since_restore: 7200000
  timesteps_this_iter: 96000
  timesteps_total: 69600000
  training_iteration: 725
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    725 |          99897.3 | 69600000 |    113.1 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 6.1
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 2.96
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 26.58
    apples_agent-2_min: 15
    apples_agent-3_max: 22
    apples_agent-3_mean: 7.17
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.26
    apples_agent-4_min: 3
    apples_agent-5_max: 55
    apples_agent-5_mean: 3.41
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 1.36
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 246
    cleaning_beam_agent-1_mean: 198.56
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 4.52
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 22
    cleaning_beam_agent-3_mean: 8.3
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 20
    cleaning_beam_agent-4_mean: 8.63
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 4.07
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-07-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 151.0
  episode_reward_mean: 112.38
  episode_reward_min: 64.0
  episodes_this_iter: 96
  episodes_total: 69696
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11833.427
    learner:
      agent-0:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2538576126098633
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006864450988359749
        model: {}
        policy_loss: -0.0017331312410533428
        total_loss: -0.0020862314850091934
        vf_explained_var: 0.010938137769699097
        vf_loss: 0.9368729591369629
      agent-1:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2274140566587448
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011082094861194491
        model: {}
        policy_loss: -0.0017463855911046267
        total_loss: -0.0020955526269972324
        vf_explained_var: 0.0762982964515686
        vf_loss: 0.5108257532119751
      agent-2:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3859769105911255
        entropy_coeff: 0.0017600000137463212
        kl: 0.001286279410123825
        model: {}
        policy_loss: -0.0018987366929650307
        total_loss: -0.00222815852612257
        vf_explained_var: 0.03366898000240326
        vf_loss: 3.498960256576538
      agent-3:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5208203196525574
        entropy_coeff: 0.0017600000137463212
        kl: 0.001439148560166359
        model: {}
        policy_loss: -0.0015654545277357101
        total_loss: -0.0023490963503718376
        vf_explained_var: 0.00038442015647888184
        vf_loss: 1.330033540725708
      agent-4:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4431130290031433
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006121098413132131
        model: {}
        policy_loss: -0.0013955857139080763
        total_loss: -0.0020861562807112932
        vf_explained_var: 0.015218302607536316
        vf_loss: 0.8930919766426086
      agent-5:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5520773530006409
        entropy_coeff: 0.0017600000137463212
        kl: 0.00121830974239856
        model: {}
        policy_loss: -0.0016255147056654096
        total_loss: -0.002481054048985243
        vf_explained_var: 0.007655709981918335
        vf_loss: 1.1611884832382202
    load_time_ms: 14137.332
    num_steps_sampled: 69696000
    num_steps_trained: 69696000
    sample_time_ms: 119494.092
    update_time_ms: 15.73
  iterations_since_restore: 76
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.91352657004831
    ram_util_percent: 12.366183574879228
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 17.0
    agent-2: 60.0
    agent-3: 31.0
    agent-4: 26.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 14.44
    agent-1: 8.0
    agent-2: 40.79
    agent-3: 19.13
    agent-4: 13.89
    agent-5: 16.13
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 23.0
    agent-3: 6.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.281286386864085
    mean_inference_ms: 15.14454773529655
    mean_processing_ms: 73.73197916439445
  time_since_restore: 11105.735525608063
  time_this_iter_s: 145.13256072998047
  time_total_s: 100042.41365528107
  timestamp: 1637374075
  timesteps_since_restore: 7296000
  timesteps_this_iter: 96000
  timesteps_total: 69696000
  training_iteration: 726
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    726 |           100042 | 69696000 |   112.38 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 6.37
    apples_agent-0_min: 1
    apples_agent-1_max: 11
    apples_agent-1_mean: 2.99
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 27.28
    apples_agent-2_min: 17
    apples_agent-3_max: 24
    apples_agent-3_mean: 6.97
    apples_agent-3_min: 2
    apples_agent-4_max: 17
    apples_agent-4_mean: 9.57
    apples_agent-4_min: 2
    apples_agent-5_max: 23
    apples_agent-5_mean: 3.46
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 4
    cleaning_beam_agent-0_mean: 1.08
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 246
    cleaning_beam_agent-1_mean: 200.63
    cleaning_beam_agent-1_min: 150
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 4.49
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 9.06
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 22
    cleaning_beam_agent-4_mean: 9.18
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.55
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-10-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 146.0
  episode_reward_mean: 114.75
  episode_reward_min: 76.0
  episodes_this_iter: 96
  episodes_total: 69792
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11822.086
    learner:
      agent-0:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2596142888069153
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009965029312297702
        model: {}
        policy_loss: -0.0017020190134644508
        total_loss: -0.002063259482383728
        vf_explained_var: 0.010646462440490723
        vf_loss: 0.9568109512329102
      agent-1:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22834284603595734
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012767467414960265
        model: {}
        policy_loss: -0.001586283091455698
        total_loss: -0.0019333534874022007
        vf_explained_var: 0.06523281335830688
        vf_loss: 0.5481408834457397
      agent-2:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3864516317844391
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013734225649386644
        model: {}
        policy_loss: -0.001718130661174655
        total_loss: -0.0020625069737434387
        vf_explained_var: 0.008267775177955627
        vf_loss: 3.3577823638916016
      agent-3:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5261887907981873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014500670367851853
        model: {}
        policy_loss: -0.0018233596347272396
        total_loss: -0.0026182588189840317
        vf_explained_var: 0.00993908941745758
        vf_loss: 1.3119419813156128
      agent-4:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4467165470123291
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010114260949194431
        model: {}
        policy_loss: -0.0014871004968881607
        total_loss: -0.0021727094426751137
        vf_explained_var: 0.0040101706981658936
        vf_loss: 1.0061322450637817
      agent-5:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5377618074417114
        entropy_coeff: 0.0017600000137463212
        kl: 0.002247418975457549
        model: {}
        policy_loss: -0.0016152140451595187
        total_loss: -0.0024551833048462868
        vf_explained_var: 0.00199301540851593
        vf_loss: 1.0649542808532715
    load_time_ms: 14135.059
    num_steps_sampled: 69792000
    num_steps_trained: 69792000
    sample_time_ms: 119462.72
    update_time_ms: 15.853
  iterations_since_restore: 77
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.989371980676324
    ram_util_percent: 12.362801932367152
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 19.0
    agent-2: 64.0
    agent-3: 33.0
    agent-4: 27.0
    agent-5: 26.0
  policy_reward_mean:
    agent-0: 14.82
    agent-1: 8.23
    agent-2: 41.97
    agent-3: 19.41
    agent-4: 14.28
    agent-5: 16.04
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 26.0
    agent-3: 9.0
    agent-4: 5.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.280055516444836
    mean_inference_ms: 15.14309508937329
    mean_processing_ms: 73.72966617612664
  time_since_restore: 11250.79608798027
  time_this_iter_s: 145.06056237220764
  time_total_s: 100187.47421765327
  timestamp: 1637374220
  timesteps_since_restore: 7392000
  timesteps_this_iter: 96000
  timesteps_total: 69792000
  training_iteration: 727
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    727 |           100187 | 69792000 |   114.75 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 87
    apples_agent-0_mean: 7.15
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 2.78
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 25.52
    apples_agent-2_min: 11
    apples_agent-3_max: 24
    apples_agent-3_mean: 6.66
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 8.93
    apples_agent-4_min: 2
    apples_agent-5_max: 19
    apples_agent-5_mean: 3.15
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 1.1
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 231
    cleaning_beam_agent-1_mean: 194.06
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.76
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 7.92
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 19
    cleaning_beam_agent-4_mean: 7.86
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.88
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-12-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 165.0
  episode_reward_mean: 109.05
  episode_reward_min: 44.0
  episodes_this_iter: 96
  episodes_total: 69888
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11821.806
    learner:
      agent-0:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2697969675064087
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008707960951142013
        model: {}
        policy_loss: -0.001693587750196457
        total_loss: -0.0020539667457342148
        vf_explained_var: 0.009574681520462036
        vf_loss: 1.1446261405944824
      agent-1:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22774159908294678
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013440591283142567
        model: {}
        policy_loss: -0.0015440937131643295
        total_loss: -0.0018945615738630295
        vf_explained_var: 0.06575918197631836
        vf_loss: 0.5035508871078491
      agent-2:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39056792855262756
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012564740609377623
        model: {}
        policy_loss: -0.0020620536524802446
        total_loss: -0.0022560369689017534
        vf_explained_var: 0.02770264446735382
        vf_loss: 4.934174060821533
      agent-3:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5175172090530396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010802983306348324
        model: {}
        policy_loss: -0.0015787221491336823
        total_loss: -0.0023505431599915028
        vf_explained_var: 0.0007758289575576782
        vf_loss: 1.3900927305221558
      agent-4:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43750426173210144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011562479194253683
        model: {}
        policy_loss: -0.0016080632340162992
        total_loss: -0.0022764650639146566
        vf_explained_var: 0.008991733193397522
        vf_loss: 1.0160549879074097
      agent-5:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5320709943771362
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016004787757992744
        model: {}
        policy_loss: -0.0015139116439968348
        total_loss: -0.002343929372727871
        vf_explained_var: 0.010686859488487244
        vf_loss: 1.0642739534378052
    load_time_ms: 14134.285
    num_steps_sampled: 69888000
    num_steps_trained: 69888000
    sample_time_ms: 119296.791
    update_time_ms: 15.768
  iterations_since_restore: 78
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.916346153846156
    ram_util_percent: 12.364903846153844
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 60.0
    agent-3: 32.0
    agent-4: 28.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 14.98
    agent-1: 7.95
    agent-2: 39.39
    agent-3: 18.14
    agent-4: 12.97
    agent-5: 15.62
  policy_reward_min:
    agent-0: 1.0
    agent-1: 1.0
    agent-2: -6.0
    agent-3: 6.0
    agent-4: 2.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.27819118089352
    mean_inference_ms: 15.142421432344607
    mean_processing_ms: 73.72622177377139
  time_since_restore: 11396.720437049866
  time_this_iter_s: 145.92434906959534
  time_total_s: 100333.39856672287
  timestamp: 1637374366
  timesteps_since_restore: 7488000
  timesteps_this_iter: 96000
  timesteps_total: 69888000
  training_iteration: 728
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    728 |           100333 | 69888000 |   109.05 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 6.56
    apples_agent-0_min: 0
    apples_agent-1_max: 39
    apples_agent-1_mean: 3.51
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 27.18
    apples_agent-2_min: 14
    apples_agent-3_max: 24
    apples_agent-3_mean: 7.25
    apples_agent-3_min: 2
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.48
    apples_agent-4_min: 3
    apples_agent-5_max: 20
    apples_agent-5_mean: 3.28
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 5
    cleaning_beam_agent-0_mean: 0.9
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 250
    cleaning_beam_agent-1_mean: 204.11
    cleaning_beam_agent-1_min: 147
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 4.56
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 8.51
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 18
    cleaning_beam_agent-4_mean: 8.27
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.34
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-15-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 159.0
  episode_reward_mean: 113.31
  episode_reward_min: 74.0
  episodes_this_iter: 96
  episodes_total: 69984
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11813.985
    learner:
      agent-0:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2698611915111542
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007532197632826865
        model: {}
        policy_loss: -0.001758757047355175
        total_loss: -0.0021334062330424786
        vf_explained_var: 0.007823750376701355
        vf_loss: 1.0030579566955566
      agent-1:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23112395405769348
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011370835127308965
        model: {}
        policy_loss: -0.001558068674057722
        total_loss: -0.0019146313425153494
        vf_explained_var: 0.06416118144989014
        vf_loss: 0.502157986164093
      agent-2:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38001805543899536
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010953049641102552
        model: {}
        policy_loss: -0.001743135740980506
        total_loss: -0.002071359660476446
        vf_explained_var: 0.0047971755266189575
        vf_loss: 3.4060583114624023
      agent-3:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5168392062187195
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014336295425891876
        model: {}
        policy_loss: -0.001416921615600586
        total_loss: -0.0021906718611717224
        vf_explained_var: 0.0032509565353393555
        vf_loss: 1.358879804611206
      agent-4:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4326523542404175
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014880106318742037
        model: {}
        policy_loss: -0.0015599005855619907
        total_loss: -0.002223857445642352
        vf_explained_var: 0.015120327472686768
        vf_loss: 0.9751402735710144
      agent-5:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5301758050918579
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015385597944259644
        model: {}
        policy_loss: -0.0017523402348160744
        total_loss: -0.0025762002915143967
        vf_explained_var: 0.004859417676925659
        vf_loss: 1.092468023300171
    load_time_ms: 14126.051
    num_steps_sampled: 69984000
    num_steps_trained: 69984000
    sample_time_ms: 119270.695
    update_time_ms: 15.739
  iterations_since_restore: 79
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.979326923076922
    ram_util_percent: 12.369230769230768
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 14.0
    agent-2: 64.0
    agent-3: 31.0
    agent-4: 28.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 15.06
    agent-1: 8.0
    agent-2: 41.4
    agent-3: 19.18
    agent-4: 14.42
    agent-5: 15.25
  policy_reward_min:
    agent-0: 3.0
    agent-1: 1.0
    agent-2: 22.0
    agent-3: 10.0
    agent-4: 7.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.27834461883075
    mean_inference_ms: 15.142963118208463
    mean_processing_ms: 73.72775077885187
  time_since_restore: 11542.603569984436
  time_this_iter_s: 145.8831329345703
  time_total_s: 100479.28169965744
  timestamp: 1637374512
  timesteps_since_restore: 7584000
  timesteps_this_iter: 96000
  timesteps_total: 69984000
  training_iteration: 729
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    729 |           100479 | 69984000 |   113.31 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 6.69
    apples_agent-0_min: 0
    apples_agent-1_max: 17
    apples_agent-1_mean: 3.34
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 26.59
    apples_agent-2_min: 4
    apples_agent-3_max: 20
    apples_agent-3_mean: 7.77
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 9.43
    apples_agent-4_min: 1
    apples_agent-5_max: 26
    apples_agent-5_mean: 3.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 5
    cleaning_beam_agent-0_mean: 0.96
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 312
    cleaning_beam_agent-1_mean: 205.57
    cleaning_beam_agent-1_min: 21
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 4.08
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 8.7
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 27
    cleaning_beam_agent-4_mean: 7.47
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.04
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-17-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 158.0
  episode_reward_mean: 112.37
  episode_reward_min: 17.0
  episodes_this_iter: 96
  episodes_total: 70080
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11810.262
    learner:
      agent-0:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2671228051185608
        entropy_coeff: 0.0017600000137463212
        kl: 0.000755361863411963
        model: {}
        policy_loss: -0.0017524610739201307
        total_loss: -0.0021211327984929085
        vf_explained_var: 0.011588841676712036
        vf_loss: 1.014658808708191
      agent-1:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22774913907051086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008927081362344325
        model: {}
        policy_loss: -0.0014147041365504265
        total_loss: -0.0017668665386736393
        vf_explained_var: 0.08326937258243561
        vf_loss: 0.4867672920227051
      agent-2:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3843860328197479
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011440736707299948
        model: {}
        policy_loss: -0.002041904255747795
        total_loss: -0.00238767359405756
        vf_explained_var: 0.019469767808914185
        vf_loss: 3.307511329650879
      agent-3:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5228617787361145
        entropy_coeff: 0.0017600000137463212
        kl: 0.001078333705663681
        model: {}
        policy_loss: -0.0013458163011819124
        total_loss: -0.002120095770806074
        vf_explained_var: -0.0005714595317840576
        vf_loss: 1.4595730304718018
      agent-4:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43196189403533936
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009360424010083079
        model: {}
        policy_loss: -0.0014289463870227337
        total_loss: -0.0020943181589245796
        vf_explained_var: 0.013487175107002258
        vf_loss: 0.9488152265548706
      agent-5:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5202521085739136
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017937691882252693
        model: {}
        policy_loss: -0.0015664547681808472
        total_loss: -0.002371128648519516
        vf_explained_var: 0.005578964948654175
        vf_loss: 1.109660267829895
    load_time_ms: 14099.258
    num_steps_sampled: 70080000
    num_steps_trained: 70080000
    sample_time_ms: 119108.066
    update_time_ms: 15.965
  iterations_since_restore: 80
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.973429951690818
    ram_util_percent: 12.367632850241547
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 18.0
    agent-2: 57.0
    agent-3: 45.0
    agent-4: 24.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 14.87
    agent-1: 7.85
    agent-2: 39.73
    agent-3: 19.9
    agent-4: 13.87
    agent-5: 16.15
  policy_reward_min:
    agent-0: 1.0
    agent-1: 1.0
    agent-2: 8.0
    agent-3: 1.0
    agent-4: 1.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.276277630086383
    mean_inference_ms: 15.142426798302058
    mean_processing_ms: 73.72426956346793
  time_since_restore: 11687.527330160141
  time_this_iter_s: 144.92376017570496
  time_total_s: 100624.20545983315
  timestamp: 1637374657
  timesteps_since_restore: 7680000
  timesteps_this_iter: 96000
  timesteps_total: 70080000
  training_iteration: 730
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    730 |           100624 | 70080000 |   112.37 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 6.92
    apples_agent-0_min: 1
    apples_agent-1_max: 18
    apples_agent-1_mean: 2.92
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 26.62
    apples_agent-2_min: 14
    apples_agent-3_max: 31
    apples_agent-3_mean: 7.71
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.66
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 3.33
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 18
    cleaning_beam_agent-0_mean: 1.3
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 268
    cleaning_beam_agent-1_mean: 207.76
    cleaning_beam_agent-1_min: 150
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 4.34
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 31
    cleaning_beam_agent-3_mean: 8.66
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 23
    cleaning_beam_agent-4_mean: 8.39
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.3
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-20-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 157.0
  episode_reward_mean: 115.79
  episode_reward_min: 74.0
  episodes_this_iter: 96
  episodes_total: 70176
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11825.101
    learner:
      agent-0:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2754417657852173
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010015829466283321
        model: {}
        policy_loss: -0.001623022835701704
        total_loss: -0.0020060583483427763
        vf_explained_var: 0.011225774884223938
        vf_loss: 1.01746666431427
      agent-1:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23128271102905273
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006674788892269135
        model: {}
        policy_loss: -0.001346495933830738
        total_loss: -0.0017037168145179749
        vf_explained_var: 0.0861339271068573
        vf_loss: 0.49835705757141113
      agent-2:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3801767826080322
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009940647287294269
        model: {}
        policy_loss: -0.001593621214851737
        total_loss: -0.0019278910476714373
        vf_explained_var: 0.019893914461135864
        vf_loss: 3.3484208583831787
      agent-3:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5226827263832092
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016829302767291665
        model: {}
        policy_loss: -0.0017270902171730995
        total_loss: -0.0025070318952202797
        vf_explained_var: 0.007294028997421265
        vf_loss: 1.3997750282287598
      agent-4:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43226873874664307
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005301532801240683
        model: {}
        policy_loss: -0.0011784853413701057
        total_loss: -0.001837835879996419
        vf_explained_var: 0.017986997961997986
        vf_loss: 1.014442801475525
      agent-5:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5240465998649597
        entropy_coeff: 0.0017600000137463212
        kl: 0.001504767220467329
        model: {}
        policy_loss: -0.0016035647131502628
        total_loss: -0.002405371982604265
        vf_explained_var: 0.004069849848747253
        vf_loss: 1.205128788948059
    load_time_ms: 14109.257
    num_steps_sampled: 70176000
    num_steps_trained: 70176000
    sample_time_ms: 119170.681
    update_time_ms: 15.94
  iterations_since_restore: 81
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.793333333333337
    ram_util_percent: 12.367619047619046
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 18.0
    agent-2: 61.0
    agent-3: 32.0
    agent-4: 26.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 15.14
    agent-1: 8.58
    agent-2: 41.66
    agent-3: 19.72
    agent-4: 14.03
    agent-5: 16.66
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 26.0
    agent-3: 9.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.276421521941426
    mean_inference_ms: 15.14194215426437
    mean_processing_ms: 73.72505972303159
  time_since_restore: 11833.521399974823
  time_this_iter_s: 145.994069814682
  time_total_s: 100770.19952964783
  timestamp: 1637374805
  timesteps_since_restore: 7776000
  timesteps_this_iter: 96000
  timesteps_total: 70176000
  training_iteration: 731
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    731 |           100770 | 70176000 |   115.79 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 6.5
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 2.89
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 27.43
    apples_agent-2_min: 10
    apples_agent-3_max: 33
    apples_agent-3_mean: 7.68
    apples_agent-3_min: 1
    apples_agent-4_max: 29
    apples_agent-4_mean: 9.7
    apples_agent-4_min: 2
    apples_agent-5_max: 34
    apples_agent-5_mean: 3.68
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 1.15
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 248
    cleaning_beam_agent-1_mean: 205.63
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 3.77
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 9.35
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 22
    cleaning_beam_agent-4_mean: 8.63
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.03
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-22-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 158.0
  episode_reward_mean: 113.87
  episode_reward_min: 72.0
  episodes_this_iter: 96
  episodes_total: 70272
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11832.239
    learner:
      agent-0:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28341731429100037
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006368614267557859
        model: {}
        policy_loss: -0.0015488023636862636
        total_loss: -0.0019462776836007833
        vf_explained_var: 0.014011994004249573
        vf_loss: 1.0133836269378662
      agent-1:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2281213104724884
        entropy_coeff: 0.0017600000137463212
        kl: 0.001152637181803584
        model: {}
        policy_loss: -0.001575558097101748
        total_loss: -0.001929759280756116
        vf_explained_var: 0.07396848499774933
        vf_loss: 0.4729386866092682
      agent-2:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3673580288887024
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010706416796892881
        model: {}
        policy_loss: -0.0017213881947100163
        total_loss: -0.001996799372136593
        vf_explained_var: 0.02710634469985962
        vf_loss: 3.711399555206299
      agent-3:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5216415524482727
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015620053745806217
        model: {}
        policy_loss: -0.0016322522424161434
        total_loss: -0.0024052569642663
        vf_explained_var: -0.009792715311050415
        vf_loss: 1.4508442878723145
      agent-4:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4370405375957489
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009246277622878551
        model: {}
        policy_loss: -0.0015950016677379608
        total_loss: -0.0022592274472117424
        vf_explained_var: 0.010512247681617737
        vf_loss: 1.049666404724121
      agent-5:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5296362042427063
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010124766267836094
        model: {}
        policy_loss: -0.0014515649527311325
        total_loss: -0.0022603385150432587
        vf_explained_var: 0.009141460061073303
        vf_loss: 1.2338570356369019
    load_time_ms: 14107.265
    num_steps_sampled: 70272000
    num_steps_trained: 70272000
    sample_time_ms: 119191.07
    update_time_ms: 16.051
  iterations_since_restore: 82
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.038349514563105
    ram_util_percent: 12.377669902912622
  pid: 27065
  policy_reward_max:
    agent-0: 24.0
    agent-1: 16.0
    agent-2: 59.0
    agent-3: 32.0
    agent-4: 28.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 14.45
    agent-1: 8.05
    agent-2: 41.82
    agent-3: 18.88
    agent-4: 14.37
    agent-5: 16.3
  policy_reward_min:
    agent-0: 5.0
    agent-1: 3.0
    agent-2: 25.0
    agent-3: 7.0
    agent-4: 3.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.274065623101286
    mean_inference_ms: 15.141724391809687
    mean_processing_ms: 73.72164687488807
  time_since_restore: 11978.232847213745
  time_this_iter_s: 144.71144723892212
  time_total_s: 100914.91097688675
  timestamp: 1637374949
  timesteps_since_restore: 7872000
  timesteps_this_iter: 96000
  timesteps_total: 70272000
  training_iteration: 732
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    732 |           100915 | 70272000 |   113.87 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 6.31
    apples_agent-0_min: 2
    apples_agent-1_max: 17
    apples_agent-1_mean: 3.27
    apples_agent-1_min: 0
    apples_agent-2_max: 40
    apples_agent-2_mean: 26.8
    apples_agent-2_min: 10
    apples_agent-3_max: 16
    apples_agent-3_mean: 6.99
    apples_agent-3_min: 2
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.77
    apples_agent-4_min: 3
    apples_agent-5_max: 33
    apples_agent-5_mean: 3.76
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 0.94
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 256
    cleaning_beam_agent-1_mean: 213.88
    cleaning_beam_agent-1_min: 143
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 3.89
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 33
    cleaning_beam_agent-3_mean: 8.64
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 29
    cleaning_beam_agent-4_mean: 8.92
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.2
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-24-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 164.0
  episode_reward_mean: 115.16
  episode_reward_min: 63.0
  episodes_this_iter: 96
  episodes_total: 70368
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11826.885
    learner:
      agent-0:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2882327735424042
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009074825793504715
        model: {}
        policy_loss: -0.001864102203398943
        total_loss: -0.0022695057559758425
        vf_explained_var: 0.010855615139007568
        vf_loss: 1.0188755989074707
      agent-1:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2347756028175354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007366346544586122
        model: {}
        policy_loss: -0.0014536045491695404
        total_loss: -0.0018179547041654587
        vf_explained_var: 0.060702621936798096
        vf_loss: 0.48856157064437866
      agent-2:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37605321407318115
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014405323890969157
        model: {}
        policy_loss: -0.0020567262545228004
        total_loss: -0.0023583201691508293
        vf_explained_var: 0.012795642018318176
        vf_loss: 3.6025943756103516
      agent-3:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5201537013053894
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015816125087440014
        model: {}
        policy_loss: -0.0017946106381714344
        total_loss: -0.0025566797703504562
        vf_explained_var: -0.003314390778541565
        vf_loss: 1.5340054035186768
      agent-4:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4396641254425049
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009501079330220819
        model: {}
        policy_loss: -0.0014723326312378049
        total_loss: -0.0021471267100423574
        vf_explained_var: 0.014814674854278564
        vf_loss: 0.9901240468025208
      agent-5:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5282405018806458
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012040986912325025
        model: {}
        policy_loss: -0.0015958789736032486
        total_loss: -0.0024103568866848946
        vf_explained_var: 0.007455602288246155
        vf_loss: 1.1522406339645386
    load_time_ms: 14084.915
    num_steps_sampled: 70368000
    num_steps_trained: 70368000
    sample_time_ms: 119213.063
    update_time_ms: 15.914
  iterations_since_restore: 83
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.965700483091787
    ram_util_percent: 12.368115942028988
  pid: 27065
  policy_reward_max:
    agent-0: 23.0
    agent-1: 15.0
    agent-2: 59.0
    agent-3: 37.0
    agent-4: 25.0
    agent-5: 38.0
  policy_reward_mean:
    agent-0: 15.07
    agent-1: 7.98
    agent-2: 42.61
    agent-3: 19.21
    agent-4: 14.34
    agent-5: 15.95
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 24.0
    agent-3: 8.0
    agent-4: 5.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.272379303320758
    mean_inference_ms: 15.141951464928002
    mean_processing_ms: 73.71880127750983
  time_since_restore: 12123.266869306564
  time_this_iter_s: 145.0340220928192
  time_total_s: 101059.94499897957
  timestamp: 1637375095
  timesteps_since_restore: 7968000
  timesteps_this_iter: 96000
  timesteps_total: 70368000
  training_iteration: 733
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    733 |           101060 | 70368000 |   115.16 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 6.38
    apples_agent-0_min: 0
    apples_agent-1_max: 24
    apples_agent-1_mean: 3.57
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 26.83
    apples_agent-2_min: 12
    apples_agent-3_max: 47
    apples_agent-3_mean: 7.47
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.05
    apples_agent-4_min: 4
    apples_agent-5_max: 24
    apples_agent-5_mean: 3.61
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 9
    cleaning_beam_agent-0_mean: 0.95
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 266
    cleaning_beam_agent-1_mean: 215.08
    cleaning_beam_agent-1_min: 172
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 4.15
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 8.82
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 35
    cleaning_beam_agent-4_mean: 7.73
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.03
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-27-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 172.0
  episode_reward_mean: 117.87
  episode_reward_min: 74.0
  episodes_this_iter: 96
  episodes_total: 70464
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11851.171
    learner:
      agent-0:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2858387231826782
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006232818122953176
        model: {}
        policy_loss: -0.0015714731998741627
        total_loss: -0.001968442928045988
        vf_explained_var: 0.006308943033218384
        vf_loss: 1.0610917806625366
      agent-1:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23467281460762024
        entropy_coeff: 0.0017600000137463212
        kl: 0.001200952217914164
        model: {}
        policy_loss: -0.0016972392331808805
        total_loss: -0.0020543294958770275
        vf_explained_var: 0.07072976231575012
        vf_loss: 0.5593407154083252
      agent-2:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3701857924461365
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011547149624675512
        model: {}
        policy_loss: -0.0015319492667913437
        total_loss: -0.0017292331904172897
        vf_explained_var: 0.03178277611732483
        vf_loss: 4.542415618896484
      agent-3:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5214396119117737
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018257482443004847
        model: {}
        policy_loss: -0.0015716558555141091
        total_loss: -0.0023308326490223408
        vf_explained_var: -0.003991305828094482
        vf_loss: 1.5856060981750488
      agent-4:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4442136585712433
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007569250301457942
        model: {}
        policy_loss: -0.0013535938924178481
        total_loss: -0.0020372774451971054
        vf_explained_var: 0.013336539268493652
        vf_loss: 0.9813290238380432
      agent-5:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5322155952453613
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018692048033699393
        model: {}
        policy_loss: -0.0016259532421827316
        total_loss: -0.0024474048987030983
        vf_explained_var: 0.0040847063064575195
        vf_loss: 1.152458906173706
    load_time_ms: 14082.928
    num_steps_sampled: 70464000
    num_steps_trained: 70464000
    sample_time_ms: 119219.828
    update_time_ms: 16.03
  iterations_since_restore: 84
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.88543689320388
    ram_util_percent: 12.420873786407766
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 19.0
    agent-2: 60.0
    agent-3: 35.0
    agent-4: 28.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.4
    agent-1: 8.44
    agent-2: 41.68
    agent-3: 20.33
    agent-4: 14.93
    agent-5: 17.09
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: -2.0
    agent-3: 7.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.27003304620626
    mean_inference_ms: 15.140284346583696
    mean_processing_ms: 73.70711470567063
  time_since_restore: 12267.755019664764
  time_this_iter_s: 144.48815035820007
  time_total_s: 101204.43314933777
  timestamp: 1637375239
  timesteps_since_restore: 8064000
  timesteps_this_iter: 96000
  timesteps_total: 70464000
  training_iteration: 734
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    734 |           101204 | 70464000 |   117.87 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 6.73
    apples_agent-0_min: 2
    apples_agent-1_max: 17
    apples_agent-1_mean: 3.01
    apples_agent-1_min: 0
    apples_agent-2_max: 77
    apples_agent-2_mean: 28.38
    apples_agent-2_min: 13
    apples_agent-3_max: 51
    apples_agent-3_mean: 7.24
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.94
    apples_agent-4_min: 3
    apples_agent-5_max: 35
    apples_agent-5_mean: 4.5
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 1.1
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 264
    cleaning_beam_agent-1_mean: 211.23
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 4.03
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 8.15
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 19
    cleaning_beam_agent-4_mean: 8.0
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.39
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-29-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 166.0
  episode_reward_mean: 116.59
  episode_reward_min: 67.0
  episodes_this_iter: 96
  episodes_total: 70560
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11856.685
    learner:
      agent-0:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29828080534935
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011796741746366024
        model: {}
        policy_loss: -0.0019057244062423706
        total_loss: -0.002324312925338745
        vf_explained_var: 0.0034825652837753296
        vf_loss: 1.0638675689697266
      agent-1:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2347528636455536
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013103107921779156
        model: {}
        policy_loss: -0.0017081995029002428
        total_loss: -0.0020695277489721775
        vf_explained_var: 0.07809184491634369
        vf_loss: 0.5183429718017578
      agent-2:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36243122816085815
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011438769288361073
        model: {}
        policy_loss: -0.0015470933867618442
        total_loss: -0.0017993034562096
        vf_explained_var: 0.02914068102836609
        vf_loss: 3.856694221496582
      agent-3:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5126945972442627
        entropy_coeff: 0.0017600000137463212
        kl: 0.001219478202983737
        model: {}
        policy_loss: -0.0015554757555946708
        total_loss: -0.0023227885831147432
        vf_explained_var: 0.0011588186025619507
        vf_loss: 1.3502593040466309
      agent-4:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44883543252944946
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006687010172754526
        model: {}
        policy_loss: -0.001310386462137103
        total_loss: -0.0020010375883430243
        vf_explained_var: 0.014259815216064453
        vf_loss: 0.9930112361907959
      agent-5:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5354576706886292
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017693259287625551
        model: {}
        policy_loss: -0.001735967816784978
        total_loss: -0.0025625189300626516
        vf_explained_var: 0.010039791464805603
        vf_loss: 1.1585228443145752
    load_time_ms: 14093.559
    num_steps_sampled: 70560000
    num_steps_trained: 70560000
    sample_time_ms: 119100.956
    update_time_ms: 16.217
  iterations_since_restore: 85
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.014077669902914
    ram_util_percent: 12.366019417475728
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 16.0
    agent-2: 62.0
    agent-3: 32.0
    agent-4: 32.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 15.57
    agent-1: 8.13
    agent-2: 42.7
    agent-3: 19.28
    agent-4: 14.42
    agent-5: 16.49
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 23.0
    agent-3: 7.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.266654965989513
    mean_inference_ms: 15.13891154869424
    mean_processing_ms: 73.7028940367044
  time_since_restore: 12412.269714355469
  time_this_iter_s: 144.51469469070435
  time_total_s: 101348.94784402847
  timestamp: 1637375384
  timesteps_since_restore: 8160000
  timesteps_this_iter: 96000
  timesteps_total: 70560000
  training_iteration: 735
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    735 |           101349 | 70560000 |   116.59 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 5.84
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 2.76
    apples_agent-1_min: 0
    apples_agent-2_max: 60
    apples_agent-2_mean: 26.83
    apples_agent-2_min: 11
    apples_agent-3_max: 22
    apples_agent-3_mean: 7.29
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 8.79
    apples_agent-4_min: 1
    apples_agent-5_max: 22
    apples_agent-5_mean: 3.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 9
    cleaning_beam_agent-0_mean: 1.05
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 256
    cleaning_beam_agent-1_mean: 207.6
    cleaning_beam_agent-1_min: 85
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 4.59
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 26
    cleaning_beam_agent-3_mean: 8.19
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 16
    cleaning_beam_agent-4_mean: 7.88
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.94
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-32-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 182.0
  episode_reward_mean: 110.49
  episode_reward_min: 20.0
  episodes_this_iter: 96
  episodes_total: 70656
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11861.453
    learner:
      agent-0:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2938045263290405
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006157620809972286
        model: {}
        policy_loss: -0.0008928012102842331
        total_loss: -0.001168261282145977
        vf_explained_var: 0.005678579211235046
        vf_loss: 2.4163522720336914
      agent-1:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23311875760555267
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017400385113433003
        model: {}
        policy_loss: -0.0018588700331747532
        total_loss: -0.0022241182159632444
        vf_explained_var: 0.07411998510360718
        vf_loss: 0.45043182373046875
      agent-2:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3687412142753601
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011718119494616985
        model: {}
        policy_loss: -0.0017682034522294998
        total_loss: -0.002093592192977667
        vf_explained_var: 0.05039258301258087
        vf_loss: 3.2359421253204346
      agent-3:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5147906541824341
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015537143917754292
        model: {}
        policy_loss: -0.0015723002143204212
        total_loss: -0.002334420569241047
        vf_explained_var: 0.003186836838722229
        vf_loss: 1.4390554428100586
      agent-4:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44601914286613464
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007317160489037633
        model: {}
        policy_loss: -0.0013539702631533146
        total_loss: -0.00204433873295784
        vf_explained_var: 0.006266772747039795
        vf_loss: 0.946243405342102
      agent-5:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5334441065788269
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009466160554438829
        model: {}
        policy_loss: -0.0014532883651554585
        total_loss: -0.0022775521501898766
        vf_explained_var: 0.008690372109413147
        vf_loss: 1.1459784507751465
    load_time_ms: 14086.126
    num_steps_sampled: 70656000
    num_steps_trained: 70656000
    sample_time_ms: 118934.033
    update_time_ms: 16.329
  iterations_since_restore: 86
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.106372549019607
    ram_util_percent: 12.387254901960784
  pid: 27065
  policy_reward_max:
    agent-0: 24.0
    agent-1: 18.0
    agent-2: 60.0
    agent-3: 35.0
    agent-4: 28.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 13.95
    agent-1: 7.47
    agent-2: 39.97
    agent-3: 19.04
    agent-4: 13.9
    agent-5: 16.16
  policy_reward_min:
    agent-0: -45.0
    agent-1: 1.0
    agent-2: 20.0
    agent-3: 6.0
    agent-4: 4.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.260969680996688
    mean_inference_ms: 15.13597593187156
    mean_processing_ms: 73.6874363999782
  time_since_restore: 12555.616715192795
  time_this_iter_s: 143.34700083732605
  time_total_s: 101492.2948448658
  timestamp: 1637375527
  timesteps_since_restore: 8256000
  timesteps_this_iter: 96000
  timesteps_total: 70656000
  training_iteration: 736
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    736 |           101492 | 70656000 |   110.49 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 6.9
    apples_agent-0_min: 1
    apples_agent-1_max: 30
    apples_agent-1_mean: 3.93
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 26.5
    apples_agent-2_min: 10
    apples_agent-3_max: 43
    apples_agent-3_mean: 7.35
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.22
    apples_agent-4_min: 1
    apples_agent-5_max: 46
    apples_agent-5_mean: 3.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 1.03
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 249
    cleaning_beam_agent-1_mean: 209.53
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 3.72
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 22
    cleaning_beam_agent-3_mean: 7.66
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 18
    cleaning_beam_agent-4_mean: 7.38
    cleaning_beam_agent-4_min: 2
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.81
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-34-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 177.0
  episode_reward_mean: 113.6
  episode_reward_min: 52.0
  episodes_this_iter: 96
  episodes_total: 70752
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11860.56
    learner:
      agent-0:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2943233549594879
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008243300835601985
        model: {}
        policy_loss: -0.001784113235771656
        total_loss: -0.0022041890770196915
        vf_explained_var: 0.014618411660194397
        vf_loss: 0.9793294668197632
      agent-1:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2292853593826294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011805330868810415
        model: {}
        policy_loss: -0.0015297392383217812
        total_loss: -0.0018876530230045319
        vf_explained_var: 0.08233779668807983
        vf_loss: 0.4562655985355377
      agent-2:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35860690474510193
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008035608916543424
        model: {}
        policy_loss: -0.0014277882874011993
        total_loss: -0.0016895157750695944
        vf_explained_var: 0.04440571367740631
        vf_loss: 3.694228172302246
      agent-3:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5120545029640198
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011959008406847715
        model: {}
        policy_loss: -0.001550314947962761
        total_loss: -0.002306030597537756
        vf_explained_var: 0.010278359055519104
        vf_loss: 1.4550248384475708
      agent-4:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4440821409225464
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005316345486789942
        model: {}
        policy_loss: -0.001201670616865158
        total_loss: -0.0018879473209381104
        vf_explained_var: 0.01621021330356598
        vf_loss: 0.9530767202377319
      agent-5:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5184263586997986
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013050311245024204
        model: {}
        policy_loss: -0.0015415882226079702
        total_loss: -0.002339922823011875
        vf_explained_var: 0.007969602942466736
        vf_loss: 1.1409482955932617
    load_time_ms: 14127.3
    num_steps_sampled: 70752000
    num_steps_trained: 70752000
    sample_time_ms: 118850.44
    update_time_ms: 16.121
  iterations_since_restore: 87
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.845893719806764
    ram_util_percent: 12.367149758454106
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 15.0
    agent-2: 65.0
    agent-3: 36.0
    agent-4: 29.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 14.77
    agent-1: 8.17
    agent-2: 41.49
    agent-3: 20.02
    agent-4: 13.51
    agent-5: 15.64
  policy_reward_min:
    agent-0: 4.0
    agent-1: 2.0
    agent-2: 17.0
    agent-3: 2.0
    agent-4: -40.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.257985273599488
    mean_inference_ms: 15.13444466301881
    mean_processing_ms: 73.67784076871666
  time_since_restore: 12700.353258132935
  time_this_iter_s: 144.73654294013977
  time_total_s: 101637.03138780594
  timestamp: 1637375672
  timesteps_since_restore: 8352000
  timesteps_this_iter: 96000
  timesteps_total: 70752000
  training_iteration: 737
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    737 |           101637 | 70752000 |    113.6 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 7.24
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 2.99
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 26.64
    apples_agent-2_min: 13
    apples_agent-3_max: 15
    apples_agent-3_mean: 6.79
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 9.59
    apples_agent-4_min: 1
    apples_agent-5_max: 46
    apples_agent-5_mean: 3.82
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 4
    cleaning_beam_agent-0_mean: 0.89
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 256
    cleaning_beam_agent-1_mean: 213.11
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 4.12
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 33
    cleaning_beam_agent-3_mean: 8.76
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 16
    cleaning_beam_agent-4_mean: 6.84
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-36-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 158.0
  episode_reward_mean: 115.76
  episode_reward_min: 87.0
  episodes_this_iter: 96
  episodes_total: 70848
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11878.073
    learner:
      agent-0:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2894778251647949
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006943722255527973
        model: {}
        policy_loss: -0.0015994635177776217
        total_loss: -0.0020064841955900192
        vf_explained_var: 0.008981361985206604
        vf_loss: 1.0246213674545288
      agent-1:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23478400707244873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009679094655439258
        model: {}
        policy_loss: -0.0016200344543904066
        total_loss: -0.0019809294026345015
        vf_explained_var: 0.06755457818508148
        vf_loss: 0.5232443809509277
      agent-2:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35836437344551086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007080668001435697
        model: {}
        policy_loss: -0.001558369374834001
        total_loss: -0.001855646027252078
        vf_explained_var: 0.024211928248405457
        vf_loss: 3.3344380855560303
      agent-3:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5068081617355347
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022390694357454777
        model: {}
        policy_loss: -0.001773654017597437
        total_loss: -0.002529717516154051
        vf_explained_var: 0.006092265248298645
        vf_loss: 1.3591917753219604
      agent-4:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4365098476409912
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011204468319192529
        model: {}
        policy_loss: -0.001524556428194046
        total_loss: -0.0021975126583129168
        vf_explained_var: 0.007813334465026855
        vf_loss: 0.9529885649681091
      agent-5:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5224094986915588
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013014639262109995
        model: {}
        policy_loss: -0.0014886020217090845
        total_loss: -0.0023028787691146135
        vf_explained_var: 0.010666653513908386
        vf_loss: 1.0516077280044556
    load_time_ms: 14106.186
    num_steps_sampled: 70848000
    num_steps_trained: 70848000
    sample_time_ms: 118776.195
    update_time_ms: 16.0
  iterations_since_restore: 88
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.853398058252427
    ram_util_percent: 12.372815533980583
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 16.0
    agent-2: 59.0
    agent-3: 33.0
    agent-4: 24.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 15.54
    agent-1: 8.38
    agent-2: 41.72
    agent-3: 19.98
    agent-4: 14.06
    agent-5: 16.08
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 26.0
    agent-3: 11.0
    agent-4: 4.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.254512161190377
    mean_inference_ms: 15.133088812618148
    mean_processing_ms: 73.66585362136092
  time_since_restore: 12845.497967720032
  time_this_iter_s: 145.14470958709717
  time_total_s: 101782.17609739304
  timestamp: 1637375817
  timesteps_since_restore: 8448000
  timesteps_this_iter: 96000
  timesteps_total: 70848000
  training_iteration: 738
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    738 |           101782 | 70848000 |   115.76 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 6.59
    apples_agent-0_min: 1
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.04
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 27.05
    apples_agent-2_min: 11
    apples_agent-3_max: 39
    apples_agent-3_mean: 7.85
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.17
    apples_agent-4_min: 2
    apples_agent-5_max: 20
    apples_agent-5_mean: 3.52
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.03
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 253
    cleaning_beam_agent-1_mean: 213.7
    cleaning_beam_agent-1_min: 165
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.19
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 7.7
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 22
    cleaning_beam_agent-4_mean: 6.89
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 4.1
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-39-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 157.0
  episode_reward_mean: 118.18
  episode_reward_min: 71.0
  episodes_this_iter: 96
  episodes_total: 70944
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11885.709
    learner:
      agent-0:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2847856879234314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007827599765732884
        model: {}
        policy_loss: -0.0016586980782449245
        total_loss: -0.0020639142021536827
        vf_explained_var: -2.3096799850463867e-05
        vf_loss: 0.9600598216056824
      agent-1:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23509156703948975
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015113857807591558
        model: {}
        policy_loss: -0.0018414733931422234
        total_loss: -0.0022045811638236046
        vf_explained_var: 0.06223912537097931
        vf_loss: 0.5064985752105713
      agent-2:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35914599895477295
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012754639610648155
        model: {}
        policy_loss: -0.0017745010554790497
        total_loss: -0.0020705154165625572
        vf_explained_var: 0.030384168028831482
        vf_loss: 3.360826015472412
      agent-3:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49814972281455994
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014483202248811722
        model: {}
        policy_loss: -0.0015123416669666767
        total_loss: -0.0022272630594670773
        vf_explained_var: 0.003007560968399048
        vf_loss: 1.6182031631469727
      agent-4:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42830097675323486
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009982078336179256
        model: {}
        policy_loss: -0.0013287123292684555
        total_loss: -0.001974235288798809
        vf_explained_var: 0.017590627074241638
        vf_loss: 1.0828664302825928
      agent-5:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.52116858959198
        entropy_coeff: 0.0017600000137463212
        kl: 0.001804305356927216
        model: {}
        policy_loss: -0.0015149079263210297
        total_loss: -0.002320962492376566
        vf_explained_var: 0.005529940128326416
        vf_loss: 1.112032413482666
    load_time_ms: 14130.848
    num_steps_sampled: 70944000
    num_steps_trained: 70944000
    sample_time_ms: 118708.489
    update_time_ms: 16.182
  iterations_since_restore: 89
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.853846153846156
    ram_util_percent: 12.440384615384612
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 61.0
    agent-3: 33.0
    agent-4: 29.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 14.86
    agent-1: 8.51
    agent-2: 42.24
    agent-3: 20.63
    agent-4: 14.91
    agent-5: 17.03
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 19.0
    agent-3: 9.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.253646519149985
    mean_inference_ms: 15.13244103017514
    mean_processing_ms: 73.65885371698258
  time_since_restore: 12991.084419488907
  time_this_iter_s: 145.58645176887512
  time_total_s: 101927.76254916191
  timestamp: 1637375963
  timesteps_since_restore: 8544000
  timesteps_this_iter: 96000
  timesteps_total: 70944000
  training_iteration: 739
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    739 |           101928 | 70944000 |   118.18 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 6.51
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 3.55
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 26.3
    apples_agent-2_min: 6
    apples_agent-3_max: 28
    apples_agent-3_mean: 7.75
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.14
    apples_agent-4_min: 3
    apples_agent-5_max: 25
    apples_agent-5_mean: 3.7
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 0.93
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 275
    cleaning_beam_agent-1_mean: 213.13
    cleaning_beam_agent-1_min: 61
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 3.84
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 25
    cleaning_beam_agent-3_mean: 7.95
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 21
    cleaning_beam_agent-4_mean: 6.9
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 4.12
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-41-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 164.0
  episode_reward_mean: 115.65
  episode_reward_min: 31.0
  episodes_this_iter: 96
  episodes_total: 71040
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11882.143
    learner:
      agent-0:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27966171503067017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006699827499687672
        model: {}
        policy_loss: -0.0016105149406939745
        total_loss: -0.002005749847739935
        vf_explained_var: 0.01048821210861206
        vf_loss: 0.9697008728981018
      agent-1:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22841408848762512
        entropy_coeff: 0.0017600000137463212
        kl: 0.000840106513351202
        model: {}
        policy_loss: -0.0015008817426860332
        total_loss: -0.0018501179292798042
        vf_explained_var: 0.0789334774017334
        vf_loss: 0.5277062058448792
      agent-2:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.355732262134552
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012431127252057195
        model: {}
        policy_loss: -0.0016567446291446686
        total_loss: -0.0019298107363283634
        vf_explained_var: 0.022848621010780334
        vf_loss: 3.530214548110962
      agent-3:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.501880407333374
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022023532073944807
        model: {}
        policy_loss: -0.0018432457000017166
        total_loss: -0.002584120724350214
        vf_explained_var: 0.006616190075874329
        vf_loss: 1.4243288040161133
      agent-4:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43173396587371826
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005309146363288164
        model: {}
        policy_loss: -0.001187599147669971
        total_loss: -0.0018532720860093832
        vf_explained_var: 0.0204763263463974
        vf_loss: 0.9417808055877686
      agent-5:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5152003169059753
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013215262442827225
        model: {}
        policy_loss: -0.0016612051986157894
        total_loss: -0.0024448230396956205
        vf_explained_var: 0.009995982050895691
        vf_loss: 1.2313642501831055
    load_time_ms: 14132.622
    num_steps_sampled: 71040000
    num_steps_trained: 71040000
    sample_time_ms: 118721.531
    update_time_ms: 16.317
  iterations_since_restore: 90
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.942995169082124
    ram_util_percent: 12.294202898550727
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 17.0
    agent-2: 56.0
    agent-3: 33.0
    agent-4: 29.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 14.67
    agent-1: 8.55
    agent-2: 41.44
    agent-3: 19.58
    agent-4: 14.71
    agent-5: 16.7
  policy_reward_min:
    agent-0: 4.0
    agent-1: 1.0
    agent-2: 11.0
    agent-3: 4.0
    agent-4: 2.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.252045489405017
    mean_inference_ms: 15.13143888935686
    mean_processing_ms: 73.65295363676623
  time_since_restore: 13136.115052938461
  time_this_iter_s: 145.03063344955444
  time_total_s: 102072.79318261147
  timestamp: 1637376108
  timesteps_since_restore: 8640000
  timesteps_this_iter: 96000
  timesteps_total: 71040000
  training_iteration: 740
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    740 |           102073 | 71040000 |   115.65 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 6.26
    apples_agent-0_min: 0
    apples_agent-1_max: 49
    apples_agent-1_mean: 3.07
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 27.18
    apples_agent-2_min: 3
    apples_agent-3_max: 19
    apples_agent-3_mean: 7.69
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 8.62
    apples_agent-4_min: 1
    apples_agent-5_max: 16
    apples_agent-5_mean: 3.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 19
    cleaning_beam_agent-0_mean: 1.28
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 279
    cleaning_beam_agent-1_mean: 218.13
    cleaning_beam_agent-1_min: 34
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 4.21
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 8.48
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 17
    cleaning_beam_agent-4_mean: 6.26
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.12
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-44-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 154.0
  episode_reward_mean: 113.41
  episode_reward_min: 20.0
  episodes_this_iter: 96
  episodes_total: 71136
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11894.186
    learner:
      agent-0:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2859402298927307
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009188114781863987
        model: {}
        policy_loss: -0.0017045247368514538
        total_loss: -0.002105217194184661
        vf_explained_var: 0.009769141674041748
        vf_loss: 1.025646686553955
      agent-1:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2316025197505951
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012882588198408484
        model: {}
        policy_loss: -0.0015366224106401205
        total_loss: -0.0018960785819217563
        vf_explained_var: 0.0635024756193161
        vf_loss: 0.4816434383392334
      agent-2:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3547578752040863
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012041318695992231
        model: {}
        policy_loss: -0.001828051172196865
        total_loss: -0.00207582488656044
        vf_explained_var: 0.019071951508522034
        vf_loss: 3.7660083770751953
      agent-3:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5034716725349426
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020946012809872627
        model: {}
        policy_loss: -0.001823042519390583
        total_loss: -0.0025585032999515533
        vf_explained_var: 0.0027509182691574097
        vf_loss: 1.5065220594406128
      agent-4:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4150981307029724
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016122794477269053
        model: {}
        policy_loss: -0.0014568506740033627
        total_loss: -0.0020962245762348175
        vf_explained_var: 0.017751693725585938
        vf_loss: 0.912007749080658
      agent-5:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5181467533111572
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012719340156763792
        model: {}
        policy_loss: -0.0012974499259144068
        total_loss: -0.0020952923223376274
        vf_explained_var: 0.012697473168373108
        vf_loss: 1.1409826278686523
    load_time_ms: 14136.872
    num_steps_sampled: 71136000
    num_steps_trained: 71136000
    sample_time_ms: 118691.128
    update_time_ms: 16.106
  iterations_since_restore: 91
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.723333333333333
    ram_util_percent: 12.368571428571427
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 17.0
    agent-2: 63.0
    agent-3: 33.0
    agent-4: 28.0
    agent-5: 26.0
  policy_reward_mean:
    agent-0: 14.57
    agent-1: 7.54
    agent-2: 42.07
    agent-3: 19.66
    agent-4: 13.58
    agent-5: 15.99
  policy_reward_min:
    agent-0: 3.0
    agent-1: 1.0
    agent-2: 6.0
    agent-3: 5.0
    agent-4: 1.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.250249461496225
    mean_inference_ms: 15.130562071103135
    mean_processing_ms: 73.64850993330965
  time_since_restore: 13281.970276594162
  time_this_iter_s: 145.85522365570068
  time_total_s: 102218.64840626717
  timestamp: 1637376256
  timesteps_since_restore: 8736000
  timesteps_this_iter: 96000
  timesteps_total: 71136000
  training_iteration: 741
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    741 |           102219 | 71136000 |   113.41 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 5.94
    apples_agent-0_min: 0
    apples_agent-1_max: 21
    apples_agent-1_mean: 3.04
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 27.4
    apples_agent-2_min: 15
    apples_agent-3_max: 27
    apples_agent-3_mean: 7.8
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.51
    apples_agent-4_min: 2
    apples_agent-5_max: 17
    apples_agent-5_mean: 3.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 5
    cleaning_beam_agent-0_mean: 0.84
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 262
    cleaning_beam_agent-1_mean: 216.41
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.95
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 10.53
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 21
    cleaning_beam_agent-4_mean: 5.43
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.14
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-46-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 156.0
  episode_reward_mean: 115.26
  episode_reward_min: 68.0
  episodes_this_iter: 96
  episodes_total: 71232
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11879.091
    learner:
      agent-0:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2821747064590454
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011696582660079002
        model: {}
        policy_loss: -0.0017765173688530922
        total_loss: -0.0021790405735373497
        vf_explained_var: 0.01138046383857727
        vf_loss: 0.9410422444343567
      agent-1:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23439137637615204
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017376221949234605
        model: {}
        policy_loss: -0.0016832181718200445
        total_loss: -0.002045708242803812
        vf_explained_var: 0.0734216719865799
        vf_loss: 0.5003665685653687
      agent-2:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3495819568634033
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011487812735140324
        model: {}
        policy_loss: -0.001583566190674901
        total_loss: -0.001829873537644744
        vf_explained_var: 0.02486233413219452
        vf_loss: 3.6895968914031982
      agent-3:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5060281753540039
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014919640962034464
        model: {}
        policy_loss: -0.0015638803597539663
        total_loss: -0.002303896239027381
        vf_explained_var: 0.0019548386335372925
        vf_loss: 1.505942702293396
      agent-4:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4076840877532959
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008102311985567212
        model: {}
        policy_loss: -0.001167644513770938
        total_loss: -0.001789686270058155
        vf_explained_var: 0.012013956904411316
        vf_loss: 0.9548125863075256
      agent-5:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5344545841217041
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020675300620496273
        model: {}
        policy_loss: -0.001411935780197382
        total_loss: -0.0022396896965801716
        vf_explained_var: 0.01566872000694275
        vf_loss: 1.1288820505142212
    load_time_ms: 14132.023
    num_steps_sampled: 71232000
    num_steps_trained: 71232000
    sample_time_ms: 118638.345
    update_time_ms: 16.151
  iterations_since_restore: 92
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.871707317073174
    ram_util_percent: 12.44341463414634
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 20.0
    agent-2: 61.0
    agent-3: 35.0
    agent-4: 29.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 14.52
    agent-1: 7.95
    agent-2: 42.19
    agent-3: 19.99
    agent-4: 13.88
    agent-5: 16.73
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 26.0
    agent-3: 7.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.247672605211505
    mean_inference_ms: 15.127572768793957
    mean_processing_ms: 73.63559657544043
  time_since_restore: 13425.949169874191
  time_this_iter_s: 143.9788932800293
  time_total_s: 102362.6272995472
  timestamp: 1637376400
  timesteps_since_restore: 8832000
  timesteps_this_iter: 96000
  timesteps_total: 71232000
  training_iteration: 742
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    742 |           102363 | 71232000 |   115.26 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 6.3
    apples_agent-0_min: 1
    apples_agent-1_max: 43
    apples_agent-1_mean: 3.32
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 26.75
    apples_agent-2_min: 12
    apples_agent-3_max: 30
    apples_agent-3_mean: 7.59
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.31
    apples_agent-4_min: 3
    apples_agent-5_max: 23
    apples_agent-5_mean: 3.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.13
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 264
    cleaning_beam_agent-1_mean: 209.67
    cleaning_beam_agent-1_min: 164
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 4.78
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 8.77
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 5.71
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.38
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-49-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 152.0
  episode_reward_mean: 115.09
  episode_reward_min: 79.0
  episodes_this_iter: 96
  episodes_total: 71328
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11887.578
    learner:
      agent-0:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28177595138549805
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010517316404730082
        model: {}
        policy_loss: -0.0017439140938222408
        total_loss: -0.0021354975178837776
        vf_explained_var: 0.006866171956062317
        vf_loss: 1.043426513671875
      agent-1:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2318415343761444
        entropy_coeff: 0.0017600000137463212
        kl: 0.001410878961905837
        model: {}
        policy_loss: -0.0019343844614923
        total_loss: -0.0022884428035467863
        vf_explained_var: 0.07201981544494629
        vf_loss: 0.5398258566856384
      agent-2:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35204997658729553
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013009767280891538
        model: {}
        policy_loss: -0.001739107072353363
        total_loss: -0.0019956103060394526
        vf_explained_var: 0.010224387049674988
        vf_loss: 3.631047010421753
      agent-3:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5011204481124878
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011405330151319504
        model: {}
        policy_loss: -0.0016293656080961227
        total_loss: -0.0023595085367560387
        vf_explained_var: -0.002987980842590332
        vf_loss: 1.5183314085006714
      agent-4:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40451177954673767
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012783536221832037
        model: {}
        policy_loss: -0.0014358535408973694
        total_loss: -0.002045832574367523
        vf_explained_var: 0.012961328029632568
        vf_loss: 1.0195857286453247
      agent-5:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5330654382705688
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015660506905987859
        model: {}
        policy_loss: -0.001749485731124878
        total_loss: -0.0025770263746380806
        vf_explained_var: 0.012496128678321838
        vf_loss: 1.106543779373169
    load_time_ms: 14133.685
    num_steps_sampled: 71328000
    num_steps_trained: 71328000
    sample_time_ms: 118553.841
    update_time_ms: 16.493
  iterations_since_restore: 93
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.900970873786406
    ram_util_percent: 12.371844660194176
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 20.0
    agent-2: 67.0
    agent-3: 32.0
    agent-4: 26.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.1
    agent-1: 8.33
    agent-2: 41.88
    agent-3: 19.94
    agent-4: 13.8
    agent-5: 16.04
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 25.0
    agent-3: 7.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.24596752680218
    mean_inference_ms: 15.125734378684966
    mean_processing_ms: 73.62821038865688
  time_since_restore: 13570.229797124863
  time_this_iter_s: 144.2806272506714
  time_total_s: 102506.90792679787
  timestamp: 1637376545
  timesteps_since_restore: 8928000
  timesteps_this_iter: 96000
  timesteps_total: 71328000
  training_iteration: 743
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    743 |           102507 | 71328000 |   115.09 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 6.03
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 3.41
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 28.44
    apples_agent-2_min: 16
    apples_agent-3_max: 53
    apples_agent-3_mean: 9.15
    apples_agent-3_min: 3
    apples_agent-4_max: 26
    apples_agent-4_mean: 9.96
    apples_agent-4_min: 3
    apples_agent-5_max: 30
    apples_agent-5_mean: 4.04
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.08
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 262
    cleaning_beam_agent-1_mean: 206.94
    cleaning_beam_agent-1_min: 158
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.34
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 9.34
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 15
    cleaning_beam_agent-4_mean: 5.2
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.18
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-51-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 120.16
  episode_reward_min: 67.0
  episodes_this_iter: 96
  episodes_total: 71424
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11868.832
    learner:
      agent-0:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2766055464744568
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011879595695063472
        model: {}
        policy_loss: -0.0016383975744247437
        total_loss: -0.0020369980484247208
        vf_explained_var: 0.012342974543571472
        vf_loss: 0.8822852373123169
      agent-1:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23007932305335999
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009499526349827647
        model: {}
        policy_loss: -0.0014446123968809843
        total_loss: -0.00179701903834939
        vf_explained_var: 0.07043176889419556
        vf_loss: 0.5253586173057556
      agent-2:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35639986395835876
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014377334155142307
        model: {}
        policy_loss: -0.0017986167222261429
        total_loss: -0.002072787843644619
        vf_explained_var: 0.031114712357521057
        vf_loss: 3.53092622756958
      agent-3:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4990963637828827
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007854716386646032
        model: {}
        policy_loss: -0.0013151848688721657
        total_loss: -0.0020212638191878796
        vf_explained_var: 0.00946776568889618
        vf_loss: 1.7233376502990723
      agent-4:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3966619670391083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009146045194938779
        model: {}
        policy_loss: -0.001146386843174696
        total_loss: -0.001742611639201641
        vf_explained_var: 0.022358417510986328
        vf_loss: 1.0190004110336304
      agent-5:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5257181525230408
        entropy_coeff: 0.0017600000137463212
        kl: 0.001499013975262642
        model: {}
        policy_loss: -0.001629670150578022
        total_loss: -0.0024347417056560516
        vf_explained_var: 0.004868552088737488
        vf_loss: 1.2019184827804565
    load_time_ms: 14131.612
    num_steps_sampled: 71424000
    num_steps_trained: 71424000
    sample_time_ms: 118664.276
    update_time_ms: 16.363
  iterations_since_restore: 94
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.09033816425121
    ram_util_percent: 12.281159420289855
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 17.0
    agent-2: 58.0
    agent-3: 40.0
    agent-4: 26.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 14.69
    agent-1: 8.53
    agent-2: 43.48
    agent-3: 21.73
    agent-4: 14.75
    agent-5: 16.98
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 22.0
    agent-3: 12.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.244741766470124
    mean_inference_ms: 15.126383015595033
    mean_processing_ms: 73.63054755355219
  time_since_restore: 13715.61088681221
  time_this_iter_s: 145.3810896873474
  time_total_s: 102652.28901648521
  timestamp: 1637376690
  timesteps_since_restore: 9024000
  timesteps_this_iter: 96000
  timesteps_total: 71424000
  training_iteration: 744
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    744 |           102652 | 71424000 |   120.16 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 6.56
    apples_agent-0_min: 1
    apples_agent-1_max: 19
    apples_agent-1_mean: 3.53
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 25.98
    apples_agent-2_min: 15
    apples_agent-3_max: 53
    apples_agent-3_mean: 7.97
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 8.61
    apples_agent-4_min: 2
    apples_agent-5_max: 43
    apples_agent-5_mean: 3.54
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.38
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 254
    cleaning_beam_agent-1_mean: 202.81
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 5.25
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 7.84
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 19
    cleaning_beam_agent-4_mean: 5.84
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.03
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-53-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 154.0
  episode_reward_mean: 110.52
  episode_reward_min: 12.0
  episodes_this_iter: 96
  episodes_total: 71520
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11862.931
    learner:
      agent-0:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2724204957485199
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006957866135053337
        model: {}
        policy_loss: -0.001471424475312233
        total_loss: -0.0018515032716095448
        vf_explained_var: 0.00461578369140625
        vf_loss: 0.9938275814056396
      agent-1:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22377708554267883
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014722396153956652
        model: {}
        policy_loss: -0.0015990264946594834
        total_loss: -0.001935923588462174
        vf_explained_var: 0.07189039885997772
        vf_loss: 0.5695005059242249
      agent-2:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36598867177963257
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015034571988508105
        model: {}
        policy_loss: -0.0017637703567743301
        total_loss: -0.002085339277982712
        vf_explained_var: 0.04138161242008209
        vf_loss: 3.2257003784179688
      agent-3:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49367910623550415
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012777585070580244
        model: {}
        policy_loss: -0.0015705178957432508
        total_loss: -0.0023018273059278727
        vf_explained_var: 0.0015985071659088135
        vf_loss: 1.3756663799285889
      agent-4:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.401863694190979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008545750170014799
        model: {}
        policy_loss: -0.0014393776655197144
        total_loss: -0.002052632160484791
        vf_explained_var: 0.017769470810890198
        vf_loss: 0.9402498006820679
      agent-5:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5239953398704529
        entropy_coeff: 0.0017600000137463212
        kl: 0.001257447642274201
        model: {}
        policy_loss: -0.0014857479836791754
        total_loss: -0.0022890055552124977
        vf_explained_var: 0.006226822733879089
        vf_loss: 1.189761996269226
    load_time_ms: 14137.136
    num_steps_sampled: 71520000
    num_steps_trained: 71520000
    sample_time_ms: 118629.94
    update_time_ms: 16.517
  iterations_since_restore: 95
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.947087378640777
    ram_util_percent: 12.299029126213592
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 17.0
    agent-2: 54.0
    agent-3: 29.0
    agent-4: 26.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 13.93
    agent-1: 8.71
    agent-2: 40.31
    agent-3: 19.21
    agent-4: 12.61
    agent-5: 15.75
  policy_reward_min:
    agent-0: 4.0
    agent-1: 3.0
    agent-2: -14.0
    agent-3: 7.0
    agent-4: -41.0
    agent-5: -35.0
  sampler_perf:
    mean_env_wait_ms: 28.241598154276197
    mean_inference_ms: 15.125514058060983
    mean_processing_ms: 73.62445130535885
  time_since_restore: 13859.777838468552
  time_this_iter_s: 144.16695165634155
  time_total_s: 102796.45596814156
  timestamp: 1637376834
  timesteps_since_restore: 9120000
  timesteps_this_iter: 96000
  timesteps_total: 71520000
  training_iteration: 745
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    745 |           102796 | 71520000 |   110.52 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 6.64
    apples_agent-0_min: 1
    apples_agent-1_max: 19
    apples_agent-1_mean: 3.32
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 26.86
    apples_agent-2_min: 12
    apples_agent-3_max: 24
    apples_agent-3_mean: 7.73
    apples_agent-3_min: 1
    apples_agent-4_max: 24
    apples_agent-4_mean: 9.53
    apples_agent-4_min: 0
    apples_agent-5_max: 28
    apples_agent-5_mean: 3.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 1.32
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 237
    cleaning_beam_agent-1_mean: 198.63
    cleaning_beam_agent-1_min: 87
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 5.59
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 9.59
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 25
    cleaning_beam_agent-4_mean: 6.28
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.72
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-56-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 184.0
  episode_reward_mean: 114.61
  episode_reward_min: 66.0
  episodes_this_iter: 96
  episodes_total: 71616
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11860.357
    learner:
      agent-0:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26093730330467224
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009730601450428367
        model: {}
        policy_loss: -0.0016920510679483414
        total_loss: -0.0020334357395768166
        vf_explained_var: 0.005368933081626892
        vf_loss: 1.1786606311798096
      agent-1:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22457624971866608
        entropy_coeff: 0.0017600000137463212
        kl: 0.001009330153465271
        model: {}
        policy_loss: -0.0014770771376788616
        total_loss: -0.0018213887233287096
        vf_explained_var: 0.08656327426433563
        vf_loss: 0.5094363689422607
      agent-2:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36658114194869995
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015307427383959293
        model: {}
        policy_loss: -0.0016680953558534384
        total_loss: -0.001943955896422267
        vf_explained_var: 0.03373372554779053
        vf_loss: 3.6932058334350586
      agent-3:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.501351535320282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010181511752307415
        model: {}
        policy_loss: -0.0017567730974406004
        total_loss: -0.0024899051059037447
        vf_explained_var: 0.00627918541431427
        vf_loss: 1.492495059967041
      agent-4:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41036468744277954
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010849906830117106
        model: {}
        policy_loss: -0.001294983085244894
        total_loss: -0.0019097216427326202
        vf_explained_var: 0.015310168266296387
        vf_loss: 1.0750601291656494
      agent-5:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5165117383003235
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017058454686775804
        model: {}
        policy_loss: -0.0014777074102312326
        total_loss: -0.002266249619424343
        vf_explained_var: 0.014135405421257019
        vf_loss: 1.2051749229431152
    load_time_ms: 14143.327
    num_steps_sampled: 71616000
    num_steps_trained: 71616000
    sample_time_ms: 118736.575
    update_time_ms: 16.423
  iterations_since_restore: 96
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.916019417475727
    ram_util_percent: 12.283495145631067
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 17.0
    agent-2: 61.0
    agent-3: 48.0
    agent-4: 28.0
    agent-5: 26.0
  policy_reward_mean:
    agent-0: 14.8
    agent-1: 8.24
    agent-2: 41.36
    agent-3: 19.84
    agent-4: 14.2
    agent-5: 16.17
  policy_reward_min:
    agent-0: 4.0
    agent-1: 2.0
    agent-2: 17.0
    agent-3: 8.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.238564417346737
    mean_inference_ms: 15.124752772036313
    mean_processing_ms: 73.61599102539716
  time_since_restore: 14004.231013298035
  time_this_iter_s: 144.45317482948303
  time_total_s: 102940.90914297104
  timestamp: 1637376979
  timesteps_since_restore: 9216000
  timesteps_this_iter: 96000
  timesteps_total: 71616000
  training_iteration: 746
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    746 |           102941 | 71616000 |   114.61 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 7.11
    apples_agent-0_min: 0
    apples_agent-1_max: 27
    apples_agent-1_mean: 3.06
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 26.65
    apples_agent-2_min: 5
    apples_agent-3_max: 46
    apples_agent-3_mean: 7.55
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 8.93
    apples_agent-4_min: 2
    apples_agent-5_max: 13
    apples_agent-5_mean: 3.41
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.54
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 271
    cleaning_beam_agent-1_mean: 200.83
    cleaning_beam_agent-1_min: 17
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 4.99
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 8.89
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 15
    cleaning_beam_agent-4_mean: 5.6
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.51
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_21-58-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 165.0
  episode_reward_mean: 112.28
  episode_reward_min: 17.0
  episodes_this_iter: 96
  episodes_total: 71712
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11875.979
    learner:
      agent-0:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2619744539260864
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007300035795196891
        model: {}
        policy_loss: -0.0015916889533400536
        total_loss: -0.0019437754526734352
        vf_explained_var: 0.006645664572715759
        vf_loss: 1.0899025201797485
      agent-1:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22175365686416626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009161416674032807
        model: {}
        policy_loss: -0.0015338999219238758
        total_loss: -0.0018706151749938726
        vf_explained_var: 0.08161376416683197
        vf_loss: 0.5357243418693542
      agent-2:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3674754500389099
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011500255204737186
        model: {}
        policy_loss: -0.0017746496014297009
        total_loss: -0.0020412267185747623
        vf_explained_var: 0.025249779224395752
        vf_loss: 3.801802635192871
      agent-3:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4938229024410248
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016117235645651817
        model: {}
        policy_loss: -0.001531894551590085
        total_loss: -0.002260410925373435
        vf_explained_var: 0.004085853695869446
        vf_loss: 1.406123399734497
      agent-4:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4140653610229492
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009092649561353028
        model: {}
        policy_loss: -0.0013822984183207154
        total_loss: -0.002017148770391941
        vf_explained_var: 0.006264775991439819
        vf_loss: 0.9390311241149902
      agent-5:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5128779411315918
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010791898239403963
        model: {}
        policy_loss: -0.0015423865988850594
        total_loss: -0.002333482261747122
        vf_explained_var: 0.010059267282485962
        vf_loss: 1.1156902313232422
    load_time_ms: 14115.977
    num_steps_sampled: 71712000
    num_steps_trained: 71712000
    sample_time_ms: 118806.089
    update_time_ms: 16.484
  iterations_since_restore: 97
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.890821256038645
    ram_util_percent: 12.300000000000002
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 20.0
    agent-2: 59.0
    agent-3: 34.0
    agent-4: 24.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 14.85
    agent-1: 8.04
    agent-2: 40.72
    agent-3: 19.54
    agent-4: 13.72
    agent-5: 15.41
  policy_reward_min:
    agent-0: 2.0
    agent-1: 0.0
    agent-2: 9.0
    agent-3: 2.0
    agent-4: 1.0
    agent-5: 1.0
  sampler_perf:
    mean_env_wait_ms: 28.23679489496322
    mean_inference_ms: 15.123421918020801
    mean_processing_ms: 73.60994559619883
  time_since_restore: 14149.430711746216
  time_this_iter_s: 145.19969844818115
  time_total_s: 103086.10884141922
  timestamp: 1637377124
  timesteps_since_restore: 9312000
  timesteps_this_iter: 96000
  timesteps_total: 71712000
  training_iteration: 747
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    747 |           103086 | 71712000 |   112.28 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 6.47
    apples_agent-0_min: 1
    apples_agent-1_max: 12
    apples_agent-1_mean: 3.31
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 27.04
    apples_agent-2_min: 13
    apples_agent-3_max: 23
    apples_agent-3_mean: 7.94
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.95
    apples_agent-4_min: 2
    apples_agent-5_max: 14
    apples_agent-5_mean: 3.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.31
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 257
    cleaning_beam_agent-1_mean: 205.05
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.64
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 28
    cleaning_beam_agent-3_mean: 8.96
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 14
    cleaning_beam_agent-4_mean: 5.53
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.69
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-01-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 165.0
  episode_reward_mean: 116.79
  episode_reward_min: 60.0
  episodes_this_iter: 96
  episodes_total: 71808
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11856.559
    learner:
      agent-0:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26520851254463196
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010165948187932372
        model: {}
        policy_loss: -0.0015025460161268711
        total_loss: -0.0018566744402050972
        vf_explained_var: 0.008437111973762512
        vf_loss: 1.1263761520385742
      agent-1:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2244683802127838
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010952051961794496
        model: {}
        policy_loss: -0.001862203236669302
        total_loss: -0.002205067314207554
        vf_explained_var: 0.07354816794395447
        vf_loss: 0.5220097899436951
      agent-2:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3671862483024597
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008176563424058259
        model: {}
        policy_loss: -0.001607644371688366
        total_loss: -0.0019275806844234467
        vf_explained_var: 0.05571015179157257
        vf_loss: 3.263134479522705
      agent-3:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4994833469390869
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009165362571366131
        model: {}
        policy_loss: -0.0013597283978015184
        total_loss: -0.0020860224030911922
        vf_explained_var: -0.005458027124404907
        vf_loss: 1.5279945135116577
      agent-4:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4200550317764282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006402561557479203
        model: {}
        policy_loss: -0.001137545332312584
        total_loss: -0.0017810820136219263
        vf_explained_var: 0.02217581868171692
        vf_loss: 0.9576212763786316
      agent-5:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.515998125076294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009358114330098033
        model: {}
        policy_loss: -0.0012758728116750717
        total_loss: -0.002075771801173687
        vf_explained_var: 0.004036396741867065
        vf_loss: 1.0825904607772827
    load_time_ms: 14128.808
    num_steps_sampled: 71808000
    num_steps_trained: 71808000
    sample_time_ms: 118754.951
    update_time_ms: 16.622
  iterations_since_restore: 98
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.00097087378641
    ram_util_percent: 12.464563106796115
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 19.0
    agent-2: 55.0
    agent-3: 36.0
    agent-4: 24.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.07
    agent-1: 8.78
    agent-2: 41.3
    agent-3: 20.52
    agent-4: 14.84
    agent-5: 16.28
  policy_reward_min:
    agent-0: 8.0
    agent-1: 2.0
    agent-2: 23.0
    agent-3: 9.0
    agent-4: 7.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.23222641836769
    mean_inference_ms: 15.121206229021013
    mean_processing_ms: 73.59387605693732
  time_since_restore: 14293.995857715607
  time_this_iter_s: 144.56514596939087
  time_total_s: 103230.67398738861
  timestamp: 1637377269
  timesteps_since_restore: 9408000
  timesteps_this_iter: 96000
  timesteps_total: 71808000
  training_iteration: 748
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    748 |           103231 | 71808000 |   116.79 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 6.71
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 2.86
    apples_agent-1_min: 0
    apples_agent-2_max: 165
    apples_agent-2_mean: 28.43
    apples_agent-2_min: 13
    apples_agent-3_max: 18
    apples_agent-3_mean: 7.43
    apples_agent-3_min: 2
    apples_agent-4_max: 28
    apples_agent-4_mean: 9.87
    apples_agent-4_min: 3
    apples_agent-5_max: 18
    apples_agent-5_mean: 3.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 1.82
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 257
    cleaning_beam_agent-1_mean: 210.81
    cleaning_beam_agent-1_min: 144
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 4.35
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 9.69
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 21
    cleaning_beam_agent-4_mean: 5.44
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.13
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-03-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 163.0
  episode_reward_mean: 116.19
  episode_reward_min: 59.0
  episodes_this_iter: 96
  episodes_total: 71904
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11849.753
    learner:
      agent-0:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2657008171081543
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007244482403621078
        model: {}
        policy_loss: -0.0016796074341982603
        total_loss: -0.002051345072686672
        vf_explained_var: 0.008688956499099731
        vf_loss: 0.9589284658432007
      agent-1:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22553174197673798
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011490349425002933
        model: {}
        policy_loss: -0.0016675232909619808
        total_loss: -0.002016176702454686
        vf_explained_var: 0.07161913812160492
        vf_loss: 0.4828500747680664
      agent-2:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3655836284160614
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014878434594720602
        model: {}
        policy_loss: -0.0017443294636905193
        total_loss: -0.002038234379142523
        vf_explained_var: 0.025796860456466675
        vf_loss: 3.495213270187378
      agent-3:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5051990747451782
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016391323879361153
        model: {}
        policy_loss: -0.0013888920657336712
        total_loss: -0.0021235658787190914
        vf_explained_var: 0.0009723305702209473
        vf_loss: 1.54474937915802
      agent-4:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4078390598297119
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007990172598510981
        model: {}
        policy_loss: -0.001272361958399415
        total_loss: -0.0018799894023686647
        vf_explained_var: 0.01403859257698059
        vf_loss: 1.1016952991485596
      agent-5:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5009528398513794
        entropy_coeff: 0.0017600000137463212
        kl: 0.00196325546130538
        model: {}
        policy_loss: -0.0015754476189613342
        total_loss: -0.0023446097038686275
        vf_explained_var: 0.011336177587509155
        vf_loss: 1.1251696348190308
    load_time_ms: 14109.857
    num_steps_sampled: 71904000
    num_steps_trained: 71904000
    sample_time_ms: 118713.532
    update_time_ms: 16.406
  iterations_since_restore: 99
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.87815533980583
    ram_util_percent: 12.369417475728158
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 16.0
    agent-2: 60.0
    agent-3: 35.0
    agent-4: 26.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 14.51
    agent-1: 8.17
    agent-2: 41.91
    agent-3: 20.2
    agent-4: 15.09
    agent-5: 16.31
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 21.0
    agent-3: -33.0
    agent-4: 3.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.230944730573604
    mean_inference_ms: 15.119974076285986
    mean_processing_ms: 73.58861369357402
  time_since_restore: 14438.830208539963
  time_this_iter_s: 144.83435082435608
  time_total_s: 103375.50833821297
  timestamp: 1637377414
  timesteps_since_restore: 9504000
  timesteps_this_iter: 96000
  timesteps_total: 71904000
  training_iteration: 749
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    749 |           103376 | 71904000 |   116.19 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 6.87
    apples_agent-0_min: 1
    apples_agent-1_max: 26
    apples_agent-1_mean: 2.96
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 28.11
    apples_agent-2_min: 6
    apples_agent-3_max: 38
    apples_agent-3_mean: 8.82
    apples_agent-3_min: 2
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.37
    apples_agent-4_min: 3
    apples_agent-5_max: 10
    apples_agent-5_mean: 3.4
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 1.49
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 250
    cleaning_beam_agent-1_mean: 211.82
    cleaning_beam_agent-1_min: 30
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 4.38
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 9.33
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 18
    cleaning_beam_agent-4_mean: 5.63
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.84
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-05-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 168.0
  episode_reward_mean: 120.09
  episode_reward_min: 33.0
  episodes_this_iter: 96
  episodes_total: 72000
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11874.683
    learner:
      agent-0:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26410990953445435
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009546307846903801
        model: {}
        policy_loss: -0.0017727045342326164
        total_loss: -0.0021362234838306904
        vf_explained_var: 0.007777571678161621
        vf_loss: 1.0131739377975464
      agent-1:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22444459795951843
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007511992589570582
        model: {}
        policy_loss: -0.0013730144128203392
        total_loss: -0.0017182979499921203
        vf_explained_var: 0.08449685573577881
        vf_loss: 0.4974029064178467
      agent-2:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36402320861816406
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013517611660063267
        model: {}
        policy_loss: -0.0018574106507003307
        total_loss: -0.0021262653172016144
        vf_explained_var: 0.019786924123764038
        vf_loss: 3.718257427215576
      agent-3:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49896785616874695
        entropy_coeff: 0.0017600000137463212
        kl: 0.001337530673481524
        model: {}
        policy_loss: -0.0013474221341311932
        total_loss: -0.0020677438005805016
        vf_explained_var: 0.010324850678443909
        vf_loss: 1.5786197185516357
      agent-4:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42168688774108887
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009874380193650723
        model: {}
        policy_loss: -0.0013354849070310593
        total_loss: -0.001981437671929598
        vf_explained_var: 0.015931352972984314
        vf_loss: 0.9621667861938477
      agent-5:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5032208561897278
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011156295659020543
        model: {}
        policy_loss: -0.0015017634723335505
        total_loss: -0.002260714303702116
        vf_explained_var: 0.013637170195579529
        vf_loss: 1.2671722173690796
    load_time_ms: 14122.162
    num_steps_sampled: 72000000
    num_steps_trained: 72000000
    sample_time_ms: 118624.043
    update_time_ms: 16.196
  iterations_since_restore: 100
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.945145631067966
    ram_util_percent: 12.383980582524272
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 16.0
    agent-2: 67.0
    agent-3: 35.0
    agent-4: 26.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.59
    agent-1: 8.3
    agent-2: 43.36
    agent-3: 21.11
    agent-4: 14.34
    agent-5: 17.39
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 9.0
    agent-3: 6.0
    agent-4: 1.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.229900846370597
    mean_inference_ms: 15.119514364558558
    mean_processing_ms: 73.58288593701911
  time_since_restore: 14583.339401483536
  time_this_iter_s: 144.509192943573
  time_total_s: 103520.01753115654
  timestamp: 1637377559
  timesteps_since_restore: 9600000
  timesteps_this_iter: 96000
  timesteps_total: 72000000
  training_iteration: 750
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    750 |           103520 | 72000000 |   120.09 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 6.64
    apples_agent-0_min: 2
    apples_agent-1_max: 37
    apples_agent-1_mean: 3.09
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 26.69
    apples_agent-2_min: 11
    apples_agent-3_max: 18
    apples_agent-3_mean: 7.12
    apples_agent-3_min: 1
    apples_agent-4_max: 29
    apples_agent-4_mean: 10.1
    apples_agent-4_min: 2
    apples_agent-5_max: 21
    apples_agent-5_mean: 3.54
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.48
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 245
    cleaning_beam_agent-1_mean: 209.62
    cleaning_beam_agent-1_min: 165
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 4.33
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 9.28
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 18
    cleaning_beam_agent-4_mean: 5.34
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.83
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-08-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 153.0
  episode_reward_mean: 116.98
  episode_reward_min: 71.0
  episodes_this_iter: 96
  episodes_total: 72096
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11871.292
    learner:
      agent-0:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27119770646095276
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009142198832705617
        model: {}
        policy_loss: -0.001523772720247507
        total_loss: -0.0018973019905388355
        vf_explained_var: 0.0068795084953308105
        vf_loss: 1.037768006324768
      agent-1:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22525833547115326
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009017230477184057
        model: {}
        policy_loss: -0.0016205718275159597
        total_loss: -0.0019675716757774353
        vf_explained_var: 0.07497940957546234
        vf_loss: 0.4945157766342163
      agent-2:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36994096636772156
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011043124832212925
        model: {}
        policy_loss: -0.001777120865881443
        total_loss: -0.0020886901766061783
        vf_explained_var: 0.0352759063243866
        vf_loss: 3.395237922668457
      agent-3:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5048967003822327
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017706395592540503
        model: {}
        policy_loss: -0.001583766657859087
        total_loss: -0.002321272622793913
        vf_explained_var: -0.0039035379886627197
        vf_loss: 1.5111348628997803
      agent-4:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.429573655128479
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007245277520269156
        model: {}
        policy_loss: -0.00138102809432894
        total_loss: -0.002040216000750661
        vf_explained_var: 0.02085842192173004
        vf_loss: 0.9686051607131958
      agent-5:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5061907172203064
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020473324693739414
        model: {}
        policy_loss: -0.0015184031799435616
        total_loss: -0.0023007281124591827
        vf_explained_var: 0.01272563636302948
        vf_loss: 1.0857222080230713
    load_time_ms: 14119.491
    num_steps_sampled: 72096000
    num_steps_trained: 72096000
    sample_time_ms: 118568.417
    update_time_ms: 16.349
  iterations_since_restore: 101
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.713397129186603
    ram_util_percent: 12.369377990430621
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 18.0
    agent-2: 65.0
    agent-3: 34.0
    agent-4: 27.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 15.2
    agent-1: 7.95
    agent-2: 42.33
    agent-3: 20.96
    agent-4: 14.45
    agent-5: 16.09
  policy_reward_min:
    agent-0: 7.0
    agent-1: 3.0
    agent-2: 25.0
    agent-3: 10.0
    agent-4: 7.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.228518423382948
    mean_inference_ms: 15.118675616991764
    mean_processing_ms: 73.57828835189054
  time_since_restore: 14728.533401727676
  time_this_iter_s: 145.19400024414062
  time_total_s: 103665.21153140068
  timestamp: 1637377706
  timesteps_since_restore: 9696000
  timesteps_this_iter: 96000
  timesteps_total: 72096000
  training_iteration: 751
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    751 |           103665 | 72096000 |   116.98 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 6.35
    apples_agent-0_min: 0
    apples_agent-1_max: 29
    apples_agent-1_mean: 3.75
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 27.34
    apples_agent-2_min: 11
    apples_agent-3_max: 30
    apples_agent-3_mean: 7.8
    apples_agent-3_min: 1
    apples_agent-4_max: 18
    apples_agent-4_mean: 9.46
    apples_agent-4_min: 2
    apples_agent-5_max: 15
    apples_agent-5_mean: 3.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 9
    cleaning_beam_agent-0_mean: 1.95
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 258
    cleaning_beam_agent-1_mean: 216.93
    cleaning_beam_agent-1_min: 170
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 4.12
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 28
    cleaning_beam_agent-3_mean: 10.44
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 34
    cleaning_beam_agent-4_mean: 5.31
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.2
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-10-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 156.0
  episode_reward_mean: 117.14
  episode_reward_min: 68.0
  episodes_this_iter: 96
  episodes_total: 72192
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11897.838
    learner:
      agent-0:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27354696393013
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009693347383290529
        model: {}
        policy_loss: -0.0018566865473985672
        total_loss: -0.0022413989063352346
        vf_explained_var: 0.0060554444789886475
        vf_loss: 0.9672853946685791
      agent-1:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22693033516407013
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009351064218208194
        model: {}
        policy_loss: -0.001528952969238162
        total_loss: -0.0018810343462973833
        vf_explained_var: 0.07919304072856903
        vf_loss: 0.4731643795967102
      agent-2:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36772266030311584
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013640422839671373
        model: {}
        policy_loss: -0.0017730137333273888
        total_loss: -0.002040955936536193
        vf_explained_var: 0.04253651201725006
        vf_loss: 3.7925214767456055
      agent-3:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5134564638137817
        entropy_coeff: 0.0017600000137463212
        kl: 0.001328919199295342
        model: {}
        policy_loss: -0.001467994530685246
        total_loss: -0.0022209512535482645
        vf_explained_var: 0.013508126139640808
        vf_loss: 1.507261037826538
      agent-4:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42946168780326843
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005262086051516235
        model: {}
        policy_loss: -0.0013834331184625626
        total_loss: -0.0020423978567123413
        vf_explained_var: 0.010798558592796326
        vf_loss: 0.9688608050346375
      agent-5:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5087442398071289
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009195271995849907
        model: {}
        policy_loss: -0.0014128059847280383
        total_loss: -0.0021899505518376827
        vf_explained_var: 0.010100960731506348
        vf_loss: 1.1824289560317993
    load_time_ms: 14133.639
    num_steps_sampled: 72192000
    num_steps_trained: 72192000
    sample_time_ms: 118600.306
    update_time_ms: 16.237
  iterations_since_restore: 102
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.92330097087379
    ram_util_percent: 12.29708737864078
  pid: 27065
  policy_reward_max:
    agent-0: 24.0
    agent-1: 17.0
    agent-2: 69.0
    agent-3: 36.0
    agent-4: 27.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 15.2
    agent-1: 8.22
    agent-2: 42.66
    agent-3: 20.45
    agent-4: 13.94
    agent-5: 16.67
  policy_reward_min:
    agent-0: 3.0
    agent-1: 2.0
    agent-2: 22.0
    agent-3: 10.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.22693619186444
    mean_inference_ms: 15.117055389229735
    mean_processing_ms: 73.57282658695588
  time_since_restore: 14873.239243030548
  time_this_iter_s: 144.7058413028717
  time_total_s: 103809.91737270355
  timestamp: 1637377851
  timesteps_since_restore: 9792000
  timesteps_this_iter: 96000
  timesteps_total: 72192000
  training_iteration: 752
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    752 |           103810 | 72192000 |   117.14 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 6.93
    apples_agent-0_min: 1
    apples_agent-1_max: 17
    apples_agent-1_mean: 3.59
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 27.0
    apples_agent-2_min: 10
    apples_agent-3_max: 27
    apples_agent-3_mean: 7.36
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 9.76
    apples_agent-4_min: 4
    apples_agent-5_max: 13
    apples_agent-5_mean: 3.72
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 9
    cleaning_beam_agent-0_mean: 1.87
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 266
    cleaning_beam_agent-1_mean: 213.05
    cleaning_beam_agent-1_min: 65
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 4.28
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 31
    cleaning_beam_agent-3_mean: 9.7
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 4.45
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.87
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-13-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 152.0
  episode_reward_mean: 115.61
  episode_reward_min: 54.0
  episodes_this_iter: 96
  episodes_total: 72288
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11897.205
    learner:
      agent-0:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2693701386451721
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009710064041428268
        model: {}
        policy_loss: -0.0016532656736671925
        total_loss: -0.0020379815250635147
        vf_explained_var: 0.0014225244522094727
        vf_loss: 0.8937658667564392
      agent-1:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22233478724956512
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014958494575694203
        model: {}
        policy_loss: -0.0017922064289450645
        total_loss: -0.002130771055817604
        vf_explained_var: 0.08614377677440643
        vf_loss: 0.5274496078491211
      agent-2:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36546146869659424
        entropy_coeff: 0.0017600000137463212
        kl: 0.001370893558487296
        model: {}
        policy_loss: -0.0017441631061956286
        total_loss: -0.0020240258891135454
        vf_explained_var: 0.03093498945236206
        vf_loss: 3.633510112762451
      agent-3:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4985036551952362
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016270780470222235
        model: {}
        policy_loss: -0.0017957361415028572
        total_loss: -0.0025218366645276546
        vf_explained_var: 0.0016668736934661865
        vf_loss: 1.5126526355743408
      agent-4:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4359855651855469
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010894467122852802
        model: {}
        policy_loss: -0.001490411814302206
        total_loss: -0.0021690507419407368
        vf_explained_var: 0.006436049938201904
        vf_loss: 0.8869689106941223
      agent-5:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5162079334259033
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017349444096907973
        model: {}
        policy_loss: -0.0015370259061455727
        total_loss: -0.0023311711847782135
        vf_explained_var: 0.011817142367362976
        vf_loss: 1.1438171863555908
    load_time_ms: 14121.343
    num_steps_sampled: 72288000
    num_steps_trained: 72288000
    sample_time_ms: 118750.206
    update_time_ms: 16.714
  iterations_since_restore: 103
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.877884615384616
    ram_util_percent: 12.369711538461537
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 20.0
    agent-2: 57.0
    agent-3: 35.0
    agent-4: 23.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 14.28
    agent-1: 8.64
    agent-2: 42.43
    agent-3: 19.83
    agent-4: 13.9
    agent-5: 16.53
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 13.0
    agent-3: 10.0
    agent-4: 7.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.226268433265727
    mean_inference_ms: 15.116519011465067
    mean_processing_ms: 73.57061139373313
  time_since_restore: 15018.841263532639
  time_this_iter_s: 145.60202050209045
  time_total_s: 103955.51939320564
  timestamp: 1637377996
  timesteps_since_restore: 9888000
  timesteps_this_iter: 96000
  timesteps_total: 72288000
  training_iteration: 753
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    753 |           103956 | 72288000 |   115.61 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 6.46
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 3.5
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 27.88
    apples_agent-2_min: 8
    apples_agent-3_max: 50
    apples_agent-3_mean: 8.39
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 9.98
    apples_agent-4_min: 3
    apples_agent-5_max: 25
    apples_agent-5_mean: 3.78
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 19
    cleaning_beam_agent-0_mean: 2.34
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 267
    cleaning_beam_agent-1_mean: 217.04
    cleaning_beam_agent-1_min: 54
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.61
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 9.85
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 4.16
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.87
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-15-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 164.0
  episode_reward_mean: 118.84
  episode_reward_min: 41.0
  episodes_this_iter: 96
  episodes_total: 72384
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11887.907
    learner:
      agent-0:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26565736532211304
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009204923990182579
        model: {}
        policy_loss: -0.0017342697829008102
        total_loss: -0.0021027149632573128
        vf_explained_var: 0.0035204440355300903
        vf_loss: 0.9911062121391296
      agent-1:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22544945776462555
        entropy_coeff: 0.0017600000137463212
        kl: 0.001009942963719368
        model: {}
        policy_loss: -0.0016458705067634583
        total_loss: -0.0019931457936763763
        vf_explained_var: 0.09014363586902618
        vf_loss: 0.4951329529285431
      agent-2:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3671989142894745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012909178622066975
        model: {}
        policy_loss: -0.0018227146938443184
        total_loss: -0.0021093878895044327
        vf_explained_var: 0.02272963523864746
        vf_loss: 3.5959320068359375
      agent-3:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5007716417312622
        entropy_coeff: 0.0017600000137463212
        kl: 0.001838516560383141
        model: {}
        policy_loss: -0.001638328772969544
        total_loss: -0.002369275316596031
        vf_explained_var: 0.013297900557518005
        vf_loss: 1.504124402999878
      agent-4:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4559609293937683
        entropy_coeff: 0.0017600000137463212
        kl: 0.001179783488623798
        model: {}
        policy_loss: -0.0013575683115050197
        total_loss: -0.0020556072704494
        vf_explained_var: 0.015606909990310669
        vf_loss: 1.0445183515548706
      agent-5:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5132346153259277
        entropy_coeff: 0.0017600000137463212
        kl: 0.001411524834111333
        model: {}
        policy_loss: -0.0015319883823394775
        total_loss: -0.002318319398909807
        vf_explained_var: 0.006601452827453613
        vf_loss: 1.1696178913116455
    load_time_ms: 14127.998
    num_steps_sampled: 72384000
    num_steps_trained: 72384000
    sample_time_ms: 118749.458
    update_time_ms: 16.775
  iterations_since_restore: 104
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.814975845410626
    ram_util_percent: 12.29323671497585
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 16.0
    agent-2: 69.0
    agent-3: 33.0
    agent-4: 26.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 14.96
    agent-1: 8.17
    agent-2: 43.42
    agent-3: 20.8
    agent-4: 14.97
    agent-5: 16.52
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 12.0
    agent-3: 5.0
    agent-4: 7.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.22574098140919
    mean_inference_ms: 15.11477635413393
    mean_processing_ms: 73.56398462740034
  time_since_restore: 15164.231243133545
  time_this_iter_s: 145.38997960090637
  time_total_s: 104100.90937280655
  timestamp: 1637378142
  timesteps_since_restore: 9984000
  timesteps_this_iter: 96000
  timesteps_total: 72384000
  training_iteration: 754
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    754 |           104101 | 72384000 |   118.84 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 6.37
    apples_agent-0_min: 2
    apples_agent-1_max: 16
    apples_agent-1_mean: 3.39
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 27.95
    apples_agent-2_min: 16
    apples_agent-3_max: 33
    apples_agent-3_mean: 8.19
    apples_agent-3_min: 1
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.59
    apples_agent-4_min: 1
    apples_agent-5_max: 75
    apples_agent-5_mean: 4.29
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 2.31
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 262
    cleaning_beam_agent-1_mean: 213.77
    cleaning_beam_agent-1_min: 151
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.64
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 9.55
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 4.28
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.85
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-18-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 158.0
  episode_reward_mean: 116.25
  episode_reward_min: 70.0
  episodes_this_iter: 96
  episodes_total: 72480
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11908.84
    learner:
      agent-0:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.274136483669281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010299033019691706
        model: {}
        policy_loss: -0.0016767224296927452
        total_loss: -0.0020611314103007317
        vf_explained_var: 0.0043444037437438965
        vf_loss: 0.9807202816009521
      agent-1:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2235899120569229
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005885393475182354
        model: {}
        policy_loss: -0.0012301872484385967
        total_loss: -0.0015745828859508038
        vf_explained_var: 0.08169418573379517
        vf_loss: 0.4912366271018982
      agent-2:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36733096837997437
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015227235853672028
        model: {}
        policy_loss: -0.0019723258446902037
        total_loss: -0.002266111085191369
        vf_explained_var: 0.023984551429748535
        vf_loss: 3.527153253555298
      agent-3:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.497749388217926
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016344455070793629
        model: {}
        policy_loss: -0.0015966936480253935
        total_loss: -0.0023156930692493916
        vf_explained_var: 0.018929436802864075
        vf_loss: 1.5703816413879395
      agent-4:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4639061689376831
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006520049064420164
        model: {}
        policy_loss: -0.0012105293571949005
        total_loss: -0.0019264998845756054
        vf_explained_var: 0.018666446208953857
        vf_loss: 1.0050674676895142
      agent-5:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5135975480079651
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011734155705198646
        model: {}
        policy_loss: -0.0015498170396313071
        total_loss: -0.0023387055844068527
        vf_explained_var: 0.014660656452178955
        vf_loss: 1.1504400968551636
    load_time_ms: 14119.38
    num_steps_sampled: 72480000
    num_steps_trained: 72480000
    sample_time_ms: 118810.697
    update_time_ms: 17.26
  iterations_since_restore: 105
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.86699029126213
    ram_util_percent: 12.367475728155341
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 17.0
    agent-2: 62.0
    agent-3: 36.0
    agent-4: 26.0
    agent-5: 25.0
  policy_reward_mean:
    agent-0: 14.26
    agent-1: 8.04
    agent-2: 42.88
    agent-3: 20.57
    agent-4: 13.84
    agent-5: 16.66
  policy_reward_min:
    agent-0: -36.0
    agent-1: 2.0
    agent-2: 20.0
    agent-3: 9.0
    agent-4: 4.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.224542719083452
    mean_inference_ms: 15.112712879864919
    mean_processing_ms: 73.55639732910464
  time_since_restore: 15309.10202050209
  time_this_iter_s: 144.87077736854553
  time_total_s: 104245.7801501751
  timestamp: 1637378287
  timesteps_since_restore: 10080000
  timesteps_this_iter: 96000
  timesteps_total: 72480000
  training_iteration: 755
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    755 |           104246 | 72480000 |   116.25 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 6.74
    apples_agent-0_min: 1
    apples_agent-1_max: 10
    apples_agent-1_mean: 2.96
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 27.72
    apples_agent-2_min: 6
    apples_agent-3_max: 41
    apples_agent-3_mean: 7.85
    apples_agent-3_min: 1
    apples_agent-4_max: 17
    apples_agent-4_mean: 9.15
    apples_agent-4_min: 1
    apples_agent-5_max: 19
    apples_agent-5_mean: 4.16
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 1.71
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 257
    cleaning_beam_agent-1_mean: 217.04
    cleaning_beam_agent-1_min: 67
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.09
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 38
    cleaning_beam_agent-3_mean: 9.95
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 4.14
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.62
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-20-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 172.0
  episode_reward_mean: 118.77
  episode_reward_min: 28.0
  episodes_this_iter: 96
  episodes_total: 72576
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11923.088
    learner:
      agent-0:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2740984261035919
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011093287030234933
        model: {}
        policy_loss: -0.0017599528655409813
        total_loss: -0.002133844420313835
        vf_explained_var: 0.0050756484270095825
        vf_loss: 1.0852638483047485
      agent-1:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22457772493362427
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011295343283563852
        model: {}
        policy_loss: -0.0015795810613781214
        total_loss: -0.0019242370035499334
        vf_explained_var: 0.08251164853572845
        vf_loss: 0.5060117840766907
      agent-2:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3609203100204468
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010454603470861912
        model: {}
        policy_loss: -0.0014973256038501859
        total_loss: -0.0017550173215568066
        vf_explained_var: 0.029575839638710022
        vf_loss: 3.7752768993377686
      agent-3:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4991763234138489
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012351039331406355
        model: {}
        policy_loss: -0.0012885434553027153
        total_loss: -0.0020034248009324074
        vf_explained_var: 0.014928638935089111
        vf_loss: 1.6367145776748657
      agent-4:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4523007273674011
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005460471147671342
        model: {}
        policy_loss: -0.0012957248836755753
        total_loss: -0.001997510902583599
        vf_explained_var: 0.015635669231414795
        vf_loss: 0.9426300525665283
      agent-5:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5256234407424927
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010756447445601225
        model: {}
        policy_loss: -0.0013928664848208427
        total_loss: -0.002188456244766712
        vf_explained_var: 0.010774195194244385
        vf_loss: 1.2951093912124634
    load_time_ms: 14118.002
    num_steps_sampled: 72576000
    num_steps_trained: 72576000
    sample_time_ms: 118814.784
    update_time_ms: 17.182
  iterations_since_restore: 106
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.884951456310677
    ram_util_percent: 12.373300970873787
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 17.0
    agent-2: 59.0
    agent-3: 35.0
    agent-4: 24.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.83
    agent-1: 8.12
    agent-2: 42.54
    agent-3: 21.14
    agent-4: 13.77
    agent-5: 17.37
  policy_reward_min:
    agent-0: 4.0
    agent-1: 2.0
    agent-2: 10.0
    agent-3: 3.0
    agent-4: 2.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.222833089356815
    mean_inference_ms: 15.11098449743418
    mean_processing_ms: 73.55022717481067
  time_since_restore: 15453.779196023941
  time_this_iter_s: 144.6771755218506
  time_total_s: 104390.45732569695
  timestamp: 1637378432
  timesteps_since_restore: 10176000
  timesteps_this_iter: 96000
  timesteps_total: 72576000
  training_iteration: 756
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    756 |           104390 | 72576000 |   118.77 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 7.51
    apples_agent-0_min: 1
    apples_agent-1_max: 11
    apples_agent-1_mean: 3.63
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 28.96
    apples_agent-2_min: 14
    apples_agent-3_max: 21
    apples_agent-3_mean: 8.23
    apples_agent-3_min: 1
    apples_agent-4_max: 32
    apples_agent-4_mean: 10.82
    apples_agent-4_min: 2
    apples_agent-5_max: 21
    apples_agent-5_mean: 4.13
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 2.09
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 267
    cleaning_beam_agent-1_mean: 219.06
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 3.48
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 23
    cleaning_beam_agent-3_mean: 10.63
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 4.34
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.22
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-22-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 180.0
  episode_reward_mean: 123.38
  episode_reward_min: 79.0
  episodes_this_iter: 96
  episodes_total: 72672
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11920.481
    learner:
      agent-0:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28099820017814636
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007924275705590844
        model: {}
        policy_loss: -0.0018080431036651134
        total_loss: -0.002193392487242818
        vf_explained_var: 0.008472457528114319
        vf_loss: 1.09206223487854
      agent-1:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22619785368442535
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010258544934913516
        model: {}
        policy_loss: -0.0015259110368788242
        total_loss: -0.0018688910640776157
        vf_explained_var: 0.09056909382343292
        vf_loss: 0.5512760281562805
      agent-2:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3580680787563324
        entropy_coeff: 0.0017600000137463212
        kl: 0.001302251941524446
        model: {}
        policy_loss: -0.001745348097756505
        total_loss: -0.002002772642299533
        vf_explained_var: 0.040832653641700745
        vf_loss: 3.7277517318725586
      agent-3:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5083543658256531
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017661777092143893
        model: {}
        policy_loss: -0.0015463539166375995
        total_loss: -0.002269609598442912
        vf_explained_var: -0.003368943929672241
        vf_loss: 1.7144708633422852
      agent-4:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45467761158943176
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007699100533500314
        model: {}
        policy_loss: -0.0014376211911439896
        total_loss: -0.002139265649020672
        vf_explained_var: 0.008530929684638977
        vf_loss: 0.9858602285385132
      agent-5:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5223422050476074
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014219637960195541
        model: {}
        policy_loss: -0.0016150223091244698
        total_loss: -0.0024007176980376244
        vf_explained_var: 0.010579153895378113
        vf_loss: 1.3362557888031006
    load_time_ms: 14107.148
    num_steps_sampled: 72672000
    num_steps_trained: 72672000
    sample_time_ms: 118832.768
    update_time_ms: 17.335
  iterations_since_restore: 107
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.900000000000002
    ram_util_percent: 12.4487922705314
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 20.0
    agent-2: 66.0
    agent-3: 34.0
    agent-4: 28.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 16.13
    agent-1: 8.78
    agent-2: 44.03
    agent-3: 21.68
    agent-4: 15.07
    agent-5: 17.69
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 27.0
    agent-3: 10.0
    agent-4: 4.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.222491556093964
    mean_inference_ms: 15.109821989926724
    mean_processing_ms: 73.54747365914763
  time_since_restore: 15599.025762319565
  time_this_iter_s: 145.24656629562378
  time_total_s: 104535.70389199257
  timestamp: 1637378577
  timesteps_since_restore: 10272000
  timesteps_this_iter: 96000
  timesteps_total: 72672000
  training_iteration: 757
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    757 |           104536 | 72672000 |   123.38 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 6.43
    apples_agent-0_min: 1
    apples_agent-1_max: 12
    apples_agent-1_mean: 3.17
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 28.43
    apples_agent-2_min: 13
    apples_agent-3_max: 29
    apples_agent-3_mean: 7.74
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.82
    apples_agent-4_min: 3
    apples_agent-5_max: 21
    apples_agent-5_mean: 3.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 2.65
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 278
    cleaning_beam_agent-1_mean: 225.67
    cleaning_beam_agent-1_min: 166
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 3.79
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 53
    cleaning_beam_agent-3_mean: 11.51
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 4.16
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.04
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-25-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 178.0
  episode_reward_mean: 118.69
  episode_reward_min: 76.0
  episodes_this_iter: 96
  episodes_total: 72768
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11927.813
    learner:
      agent-0:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2814079225063324
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007225837325677276
        model: {}
        policy_loss: -0.0017361727077513933
        total_loss: -0.0021308227442204952
        vf_explained_var: 0.013322815299034119
        vf_loss: 1.0063036680221558
      agent-1:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2299751341342926
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010383051121607423
        model: {}
        policy_loss: -0.0015003383159637451
        total_loss: -0.0018561622127890587
        vf_explained_var: 0.08590252697467804
        vf_loss: 0.4893176257610321
      agent-2:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3591676354408264
        entropy_coeff: 0.0017600000137463212
        kl: 0.002040762919932604
        model: {}
        policy_loss: -0.00195549288764596
        total_loss: -0.0022075639571994543
        vf_explained_var: 0.023720338940620422
        vf_loss: 3.8006200790405273
      agent-3:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5076747536659241
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016088886186480522
        model: {}
        policy_loss: -0.0016015401342883706
        total_loss: -0.002343330532312393
        vf_explained_var: 0.004661962389945984
        vf_loss: 1.5171830654144287
      agent-4:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45288732647895813
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006387403118424118
        model: {}
        policy_loss: -0.0013677971437573433
        total_loss: -0.0020683156326413155
        vf_explained_var: 0.00958356261253357
        vf_loss: 0.9656358361244202
      agent-5:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5318219065666199
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019130355212837458
        model: {}
        policy_loss: -0.0017211753875017166
        total_loss: -0.0025438955053687096
        vf_explained_var: 0.014797687530517578
        vf_loss: 1.1328814029693604
    load_time_ms: 14107.53
    num_steps_sampled: 72768000
    num_steps_trained: 72768000
    sample_time_ms: 118837.001
    update_time_ms: 17.183
  iterations_since_restore: 108
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.926570048309177
    ram_util_percent: 12.454589371980676
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 17.0
    agent-2: 66.0
    agent-3: 34.0
    agent-4: 24.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 15.21
    agent-1: 8.05
    agent-2: 42.73
    agent-3: 21.12
    agent-4: 14.4
    agent-5: 17.18
  policy_reward_min:
    agent-0: 2.0
    agent-1: 1.0
    agent-2: 21.0
    agent-3: 7.0
    agent-4: 6.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.222797410386814
    mean_inference_ms: 15.108396317077357
    mean_processing_ms: 73.54266088668955
  time_since_restore: 15743.709098100662
  time_this_iter_s: 144.6833357810974
  time_total_s: 104680.38722777367
  timestamp: 1637378722
  timesteps_since_restore: 10368000
  timesteps_this_iter: 96000
  timesteps_total: 72768000
  training_iteration: 758
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    758 |           104680 | 72768000 |   118.69 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 6.92
    apples_agent-0_min: 1
    apples_agent-1_max: 19
    apples_agent-1_mean: 3.36
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 28.37
    apples_agent-2_min: 17
    apples_agent-3_max: 21
    apples_agent-3_mean: 7.57
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.95
    apples_agent-4_min: 5
    apples_agent-5_max: 14
    apples_agent-5_mean: 3.12
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 2.78
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 276
    cleaning_beam_agent-1_mean: 224.94
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 3.85
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 11.95
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 15
    cleaning_beam_agent-4_mean: 4.61
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.55
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-27-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 168.0
  episode_reward_mean: 122.56
  episode_reward_min: 70.0
  episodes_this_iter: 96
  episodes_total: 72864
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11925.365
    learner:
      agent-0:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26496797800064087
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012307004071772099
        model: {}
        policy_loss: -0.001839041244238615
        total_loss: -0.0021946216002106667
        vf_explained_var: 0.002417042851448059
        vf_loss: 1.1076198816299438
      agent-1:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.23128348588943481
        entropy_coeff: 0.0017600000137463212
        kl: 0.001010035164654255
        model: {}
        policy_loss: -0.0014437814243137836
        total_loss: -0.0017956183291971684
        vf_explained_var: 0.06697443127632141
        vf_loss: 0.5522401332855225
      agent-2:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3554898500442505
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013917462201789021
        model: {}
        policy_loss: -0.0015343357808887959
        total_loss: -0.001778425881639123
        vf_explained_var: 0.02575811743736267
        vf_loss: 3.8157198429107666
      agent-3:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5048698782920837
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011792631121352315
        model: {}
        policy_loss: -0.0014683706685900688
        total_loss: -0.0021928558126091957
        vf_explained_var: 0.003294140100479126
        vf_loss: 1.6408761739730835
      agent-4:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4458622336387634
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012006203178316355
        model: {}
        policy_loss: -0.0014100997941568494
        total_loss: -0.0020836330950260162
        vf_explained_var: 0.006408721208572388
        vf_loss: 1.1118289232254028
      agent-5:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5585726499557495
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016054910374805331
        model: {}
        policy_loss: -0.0017505567520856857
        total_loss: -0.002608758397400379
        vf_explained_var: 0.002380385994911194
        vf_loss: 1.2488129138946533
    load_time_ms: 14105.619
    num_steps_sampled: 72864000
    num_steps_trained: 72864000
    sample_time_ms: 118853.855
    update_time_ms: 17.141
  iterations_since_restore: 109
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.06456310679611
    ram_util_percent: 12.337378640776699
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 17.0
    agent-2: 71.0
    agent-3: 41.0
    agent-4: 29.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 15.7
    agent-1: 8.53
    agent-2: 43.97
    agent-3: 21.65
    agent-4: 15.4
    agent-5: 17.31
  policy_reward_min:
    agent-0: 7.0
    agent-1: 3.0
    agent-2: 29.0
    agent-3: 10.0
    agent-4: 3.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.223341738613033
    mean_inference_ms: 15.107616050633533
    mean_processing_ms: 73.54015356568543
  time_since_restore: 15888.670667171478
  time_this_iter_s: 144.96156907081604
  time_total_s: 104825.34879684448
  timestamp: 1637378867
  timesteps_since_restore: 10464000
  timesteps_this_iter: 96000
  timesteps_total: 72864000
  training_iteration: 759
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    759 |           104825 | 72864000 |   122.56 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 6.98
    apples_agent-0_min: 1
    apples_agent-1_max: 13
    apples_agent-1_mean: 3.22
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 28.38
    apples_agent-2_min: 15
    apples_agent-3_max: 21
    apples_agent-3_mean: 8.33
    apples_agent-3_min: 2
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.33
    apples_agent-4_min: 3
    apples_agent-5_max: 23
    apples_agent-5_mean: 3.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 9
    cleaning_beam_agent-0_mean: 3.07
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 285
    cleaning_beam_agent-1_mean: 228.62
    cleaning_beam_agent-1_min: 166
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 3.81
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 13.16
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 37
    cleaning_beam_agent-4_mean: 4.66
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 4.01
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-30-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 165.0
  episode_reward_mean: 120.94
  episode_reward_min: 79.0
  episodes_this_iter: 96
  episodes_total: 72960
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11933.693
    learner:
      agent-0:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25715529918670654
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014074610080569983
        model: {}
        policy_loss: -0.0017661292804405093
        total_loss: -0.0021098474971950054
        vf_explained_var: 0.005792975425720215
        vf_loss: 1.0887351036071777
      agent-1:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22860953211784363
        entropy_coeff: 0.0017600000137463212
        kl: 0.000938541954383254
        model: {}
        policy_loss: -0.0015022072475403547
        total_loss: -0.0018502227030694485
        vf_explained_var: 0.07084715366363525
        vf_loss: 0.5433614253997803
      agent-2:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35522761940956116
        entropy_coeff: 0.0017600000137463212
        kl: 0.001743472646921873
        model: {}
        policy_loss: -0.001991827739402652
        total_loss: -0.0022499789483845234
        vf_explained_var: 0.04099118709564209
        vf_loss: 3.670513391494751
      agent-3:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.508817732334137
        entropy_coeff: 0.0017600000137463212
        kl: 0.002149922540411353
        model: {}
        policy_loss: -0.0017478326335549355
        total_loss: -0.0024673431180417538
        vf_explained_var: 0.0015585720539093018
        vf_loss: 1.7600610256195068
      agent-4:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43362531065940857
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007618861272931099
        model: {}
        policy_loss: -0.001376572297886014
        total_loss: -0.0020378984045237303
        vf_explained_var: 0.014921098947525024
        vf_loss: 1.0185908079147339
      agent-5:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5827841758728027
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016150427982211113
        model: {}
        policy_loss: -0.0016911393031477928
        total_loss: -0.00260283425450325
        vf_explained_var: 0.014353886246681213
        vf_loss: 1.1400315761566162
    load_time_ms: 14089.194
    num_steps_sampled: 72960000
    num_steps_trained: 72960000
    sample_time_ms: 118840.452
    update_time_ms: 17.375
  iterations_since_restore: 110
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.107766990291257
    ram_util_percent: 12.468932038834948
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 19.0
    agent-2: 61.0
    agent-3: 34.0
    agent-4: 25.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 15.73
    agent-1: 8.35
    agent-2: 43.83
    agent-3: 22.29
    agent-4: 14.23
    agent-5: 16.51
  policy_reward_min:
    agent-0: 6.0
    agent-1: 0.0
    agent-2: 25.0
    agent-3: 13.0
    agent-4: 6.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.22318946028188
    mean_inference_ms: 15.10708210829057
    mean_processing_ms: 73.53346294693222
  time_since_restore: 16032.969913721085
  time_this_iter_s: 144.29924654960632
  time_total_s: 104969.64804339409
  timestamp: 1637379011
  timesteps_since_restore: 10560000
  timesteps_this_iter: 96000
  timesteps_total: 72960000
  training_iteration: 760
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    760 |           104970 | 72960000 |   120.94 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 6.37
    apples_agent-0_min: 0
    apples_agent-1_max: 38
    apples_agent-1_mean: 3.37
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 27.43
    apples_agent-2_min: 13
    apples_agent-3_max: 25
    apples_agent-3_mean: 8.07
    apples_agent-3_min: 2
    apples_agent-4_max: 24
    apples_agent-4_mean: 9.63
    apples_agent-4_min: 2
    apples_agent-5_max: 25
    apples_agent-5_mean: 3.95
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 33
    cleaning_beam_agent-0_mean: 3.45
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 405
    cleaning_beam_agent-1_mean: 224.48
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 3.57
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 12.53
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.87
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.65
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-32-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 165.0
  episode_reward_mean: 118.14
  episode_reward_min: 59.0
  episodes_this_iter: 96
  episodes_total: 73056
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11909.95
    learner:
      agent-0:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2505572438240051
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007212719647213817
        model: {}
        policy_loss: -0.0014298760797828436
        total_loss: -0.0017527970485389233
        vf_explained_var: 0.005627542734146118
        vf_loss: 1.1805624961853027
      agent-1:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2257855236530304
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008296488085761666
        model: {}
        policy_loss: -0.0016106581315398216
        total_loss: -0.001950628124177456
        vf_explained_var: 0.09133568406105042
        vf_loss: 0.5741181373596191
      agent-2:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3587527275085449
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010974106844514608
        model: {}
        policy_loss: -0.0016155516495928168
        total_loss: -0.001858653617091477
        vf_explained_var: 0.03279802203178406
        vf_loss: 3.883035659790039
      agent-3:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5081959962844849
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025795921683311462
        model: {}
        policy_loss: -0.0017660188023000956
        total_loss: -0.002476423978805542
        vf_explained_var: -0.0017934739589691162
        vf_loss: 1.840179443359375
      agent-4:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43457040190696716
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008503460558131337
        model: {}
        policy_loss: -0.001298501156270504
        total_loss: -0.0019697730895131826
        vf_explained_var: 0.01569807529449463
        vf_loss: 0.9357097148895264
      agent-5:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5748245716094971
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010128443827852607
        model: {}
        policy_loss: -0.0016077267937362194
        total_loss: -0.0025001191534101963
        vf_explained_var: 0.01598919928073883
        vf_loss: 1.1930181980133057
    load_time_ms: 14081.88
    num_steps_sampled: 73056000
    num_steps_trained: 73056000
    sample_time_ms: 118851.625
    update_time_ms: 17.46
  iterations_since_restore: 111
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.77224880382775
    ram_util_percent: 12.366985645933013
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 17.0
    agent-2: 56.0
    agent-3: 40.0
    agent-4: 29.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.05
    agent-1: 8.67
    agent-2: 42.07
    agent-3: 21.81
    agent-4: 13.8
    agent-5: 16.74
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 18.0
    agent-3: 8.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.222978956599473
    mean_inference_ms: 15.106218544212979
    mean_processing_ms: 73.53168679387402
  time_since_restore: 16178.026542425156
  time_this_iter_s: 145.05662870407104
  time_total_s: 105114.70467209816
  timestamp: 1637379158
  timesteps_since_restore: 10656000
  timesteps_this_iter: 96000
  timesteps_total: 73056000
  training_iteration: 761
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    761 |           105115 | 73056000 |   118.14 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 7.03
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.18
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 28.28
    apples_agent-2_min: 3
    apples_agent-3_max: 32
    apples_agent-3_mean: 8.62
    apples_agent-3_min: 2
    apples_agent-4_max: 24
    apples_agent-4_mean: 9.6
    apples_agent-4_min: 2
    apples_agent-5_max: 13
    apples_agent-5_mean: 3.86
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 23
    cleaning_beam_agent-0_mean: 4.17
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 270
    cleaning_beam_agent-1_mean: 220.76
    cleaning_beam_agent-1_min: 75
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 4.31
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 47
    cleaning_beam_agent-3_mean: 14.01
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.83
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.3
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-35-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 170.0
  episode_reward_mean: 119.63
  episode_reward_min: 36.0
  episodes_this_iter: 96
  episodes_total: 73152
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11900.706
    learner:
      agent-0:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2557726204395294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007608704036101699
        model: {}
        policy_loss: -0.0016458844766020775
        total_loss: -0.001988696865737438
        vf_explained_var: 0.008770838379859924
        vf_loss: 1.0734646320343018
      agent-1:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2263689935207367
        entropy_coeff: 0.0017600000137463212
        kl: 0.001138343010097742
        model: {}
        policy_loss: -0.001625381177291274
        total_loss: -0.001967960735782981
        vf_explained_var: 0.07763521373271942
        vf_loss: 0.5582805871963501
      agent-2:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3672589063644409
        entropy_coeff: 0.0017600000137463212
        kl: 0.001647610915824771
        model: {}
        policy_loss: -0.001956147840246558
        total_loss: -0.0021859542466700077
        vf_explained_var: 0.02003704011440277
        vf_loss: 4.165690898895264
      agent-3:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5133858919143677
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012853541411459446
        model: {}
        policy_loss: -0.0016711344942450523
        total_loss: -0.002407004125416279
        vf_explained_var: 0.00825183093547821
        vf_loss: 1.6768749952316284
      agent-4:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4291496276855469
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011529026087373495
        model: {}
        policy_loss: -0.0014861332019791007
        total_loss: -0.0021283025853335857
        vf_explained_var: 0.011874377727508545
        vf_loss: 1.1313360929489136
      agent-5:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5655967593193054
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014658172149211168
        model: {}
        policy_loss: -0.0017709506209939718
        total_loss: -0.0026374845765531063
        vf_explained_var: 0.020271271467208862
        vf_loss: 1.2891581058502197
    load_time_ms: 14067.304
    num_steps_sampled: 73152000
    num_steps_trained: 73152000
    sample_time_ms: 118785.093
    update_time_ms: 17.464
  iterations_since_restore: 112
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.992682926829268
    ram_util_percent: 12.451707317073168
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 20.0
    agent-2: 63.0
    agent-3: 34.0
    agent-4: 27.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 15.45
    agent-1: 8.88
    agent-2: 42.61
    agent-3: 20.77
    agent-4: 15.03
    agent-5: 16.89
  policy_reward_min:
    agent-0: 2.0
    agent-1: 1.0
    agent-2: 3.0
    agent-3: 8.0
    agent-4: 5.0
    agent-5: 2.0
  sampler_perf:
    mean_env_wait_ms: 28.221160887605564
    mean_inference_ms: 15.104621897172862
    mean_processing_ms: 73.52568261659567
  time_since_restore: 16321.820469379425
  time_this_iter_s: 143.7939269542694
  time_total_s: 105258.49859905243
  timestamp: 1637379302
  timesteps_since_restore: 10752000
  timesteps_this_iter: 96000
  timesteps_total: 73152000
  training_iteration: 762
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    762 |           105258 | 73152000 |   119.63 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 7.31
    apples_agent-0_min: 1
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.15
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 27.53
    apples_agent-2_min: 14
    apples_agent-3_max: 19
    apples_agent-3_mean: 6.8
    apples_agent-3_min: 1
    apples_agent-4_max: 25
    apples_agent-4_mean: 9.9
    apples_agent-4_min: 2
    apples_agent-5_max: 35
    apples_agent-5_mean: 4.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 4.32
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 255
    cleaning_beam_agent-1_mean: 217.96
    cleaning_beam_agent-1_min: 172
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 3.7
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 12.13
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 3.66
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-37-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 173.0
  episode_reward_mean: 118.29
  episode_reward_min: 82.0
  episodes_this_iter: 96
  episodes_total: 73248
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11903.045
    learner:
      agent-0:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2594361901283264
        entropy_coeff: 0.0017600000137463212
        kl: 0.000637503806501627
        model: {}
        policy_loss: -0.001650240970775485
        total_loss: -0.0020031300373375416
        vf_explained_var: 0.005451679229736328
        vf_loss: 1.0371952056884766
      agent-1:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22270074486732483
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010940285865217447
        model: {}
        policy_loss: -0.0014583044685423374
        total_loss: -0.0018031410872936249
        vf_explained_var: 0.087429940700531
        vf_loss: 0.47118496894836426
      agent-2:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36255601048469543
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013494211016222835
        model: {}
        policy_loss: -0.0016146968118846416
        total_loss: -0.0018989322707057
        vf_explained_var: 0.01930759847164154
        vf_loss: 3.5386459827423096
      agent-3:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5061333775520325
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011473802151158452
        model: {}
        policy_loss: -0.001731622964143753
        total_loss: -0.002476681023836136
        vf_explained_var: 0.0048447102308273315
        vf_loss: 1.4573838710784912
      agent-4:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42223554849624634
        entropy_coeff: 0.0017600000137463212
        kl: 0.001151597243733704
        model: {}
        policy_loss: -0.0012706287670880556
        total_loss: -0.0019221006659790874
        vf_explained_var: 0.01471756398677826
        vf_loss: 0.9165939688682556
      agent-5:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5557765364646912
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017582511063665152
        model: {}
        policy_loss: -0.0018018543487414718
        total_loss: -0.0026555254589766264
        vf_explained_var: 0.01920562982559204
        vf_loss: 1.2449668645858765
    load_time_ms: 14098.395
    num_steps_sampled: 73248000
    num_steps_trained: 73248000
    sample_time_ms: 118698.965
    update_time_ms: 16.524
  iterations_since_restore: 113
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.846634615384616
    ram_util_percent: 12.452403846153844
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 18.0
    agent-2: 65.0
    agent-3: 35.0
    agent-4: 24.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 15.64
    agent-1: 8.3
    agent-2: 42.8
    agent-3: 20.68
    agent-4: 14.12
    agent-5: 16.75
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 30.0
    agent-3: 8.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.22051702804201
    mean_inference_ms: 15.104287981243777
    mean_processing_ms: 73.52188059111158
  time_since_restore: 16466.986456632614
  time_this_iter_s: 145.1659872531891
  time_total_s: 105403.66458630562
  timestamp: 1637379448
  timesteps_since_restore: 10848000
  timesteps_this_iter: 96000
  timesteps_total: 73248000
  training_iteration: 763
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    763 |           105404 | 73248000 |   118.29 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 6.7
    apples_agent-0_min: 1
    apples_agent-1_max: 27
    apples_agent-1_mean: 3.31
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 28.29
    apples_agent-2_min: 9
    apples_agent-3_max: 32
    apples_agent-3_mean: 8.38
    apples_agent-3_min: 1
    apples_agent-4_max: 17
    apples_agent-4_mean: 9.82
    apples_agent-4_min: 3
    apples_agent-5_max: 25
    apples_agent-5_mean: 4.1
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 19
    cleaning_beam_agent-0_mean: 4.13
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 282
    cleaning_beam_agent-1_mean: 217.29
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 3.58
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 12.54
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 18
    cleaning_beam_agent-4_mean: 3.78
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.81
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-39-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 176.0
  episode_reward_mean: 122.01
  episode_reward_min: 49.0
  episodes_this_iter: 96
  episodes_total: 73344
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11908.838
    learner:
      agent-0:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2579284906387329
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008571532089263201
        model: {}
        policy_loss: -0.0015791358891874552
        total_loss: -0.0019216891378164291
        vf_explained_var: 0.006464436650276184
        vf_loss: 1.114028811454773
      agent-1:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2238045483827591
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012631779536604881
        model: {}
        policy_loss: -0.0014333585277199745
        total_loss: -0.0017752265557646751
        vf_explained_var: 0.07265838980674744
        vf_loss: 0.5202723145484924
      agent-2:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3597034811973572
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010997825302183628
        model: {}
        policy_loss: -0.0014751728158444166
        total_loss: -0.0017013601027429104
        vf_explained_var: 0.04123967885971069
        vf_loss: 4.068936347961426
      agent-3:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5055810213088989
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020034662447869778
        model: {}
        policy_loss: -0.0017620392609387636
        total_loss: -0.002495847176760435
        vf_explained_var: 0.00047107040882110596
        vf_loss: 1.5601332187652588
      agent-4:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4286056458950043
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014626423362642527
        model: {}
        policy_loss: -0.001525983214378357
        total_loss: -0.002187632955610752
        vf_explained_var: 0.01758497953414917
        vf_loss: 0.9269500374794006
      agent-5:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.540540337562561
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008269937243312597
        model: {}
        policy_loss: -0.0014641834422945976
        total_loss: -0.0022958610206842422
        vf_explained_var: -0.0014082938432693481
        vf_loss: 1.19675612449646
    load_time_ms: 14080.613
    num_steps_sampled: 73344000
    num_steps_trained: 73344000
    sample_time_ms: 118620.988
    update_time_ms: 16.809
  iterations_since_restore: 114
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.01268292682927
    ram_util_percent: 12.450731707317072
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 22.0
    agent-2: 70.0
    agent-3: 36.0
    agent-4: 28.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 15.7
    agent-1: 8.37
    agent-2: 44.25
    agent-3: 21.77
    agent-4: 14.55
    agent-5: 17.37
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 12.0
    agent-3: 6.0
    agent-4: 4.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.219385994118806
    mean_inference_ms: 15.103336103946432
    mean_processing_ms: 73.5167781528716
  time_since_restore: 16611.43168234825
  time_this_iter_s: 144.4452257156372
  time_total_s: 105548.10981202126
  timestamp: 1637379592
  timesteps_since_restore: 10944000
  timesteps_this_iter: 96000
  timesteps_total: 73344000
  training_iteration: 764
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    764 |           105548 | 73344000 |   122.01 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 6.22
    apples_agent-0_min: 0
    apples_agent-1_max: 46
    apples_agent-1_mean: 3.51
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 27.85
    apples_agent-2_min: 13
    apples_agent-3_max: 28
    apples_agent-3_mean: 7.78
    apples_agent-3_min: 1
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.84
    apples_agent-4_min: 1
    apples_agent-5_max: 27
    apples_agent-5_mean: 3.98
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 4.81
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 257
    cleaning_beam_agent-1_mean: 209.19
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 3.25
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 44
    cleaning_beam_agent-3_mean: 12.83
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.26
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.42
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-42-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 162.0
  episode_reward_mean: 120.6
  episode_reward_min: 71.0
  episodes_this_iter: 96
  episodes_total: 73440
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11884.663
    learner:
      agent-0:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25327470898628235
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007360863382928073
        model: {}
        policy_loss: -0.0015449589118361473
        total_loss: -0.0018900269642472267
        vf_explained_var: -0.0029941946268081665
        vf_loss: 1.006952166557312
      agent-1:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22088097035884857
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010534074390307069
        model: {}
        policy_loss: -0.0014135485980659723
        total_loss: -0.001746877795085311
        vf_explained_var: 0.07459850609302521
        vf_loss: 0.5542261600494385
      agent-2:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35614001750946045
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008889405289664865
        model: {}
        policy_loss: -0.0014218059368431568
        total_loss: -0.0016951416619122028
        vf_explained_var: 0.031145408749580383
        vf_loss: 3.534712553024292
      agent-3:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5171960592269897
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013064949307590723
        model: {}
        policy_loss: -0.0014343864750117064
        total_loss: -0.0021756459027528763
        vf_explained_var: 0.006011709570884705
        vf_loss: 1.6900725364685059
      agent-4:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43500474095344543
        entropy_coeff: 0.0017600000137463212
        kl: 0.00039125699549913406
        model: {}
        policy_loss: -0.0010657724924385548
        total_loss: -0.0017303233034908772
        vf_explained_var: 0.012763381004333496
        vf_loss: 1.0105772018432617
      agent-5:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5569559335708618
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018643512157723308
        model: {}
        policy_loss: -0.0019066715613007545
        total_loss: -0.002763913944363594
        vf_explained_var: 0.015190750360488892
        vf_loss: 1.2300255298614502
    load_time_ms: 14082.852
    num_steps_sampled: 73440000
    num_steps_trained: 73440000
    sample_time_ms: 118612.272
    update_time_ms: 15.809
  iterations_since_restore: 115
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.935922330097085
    ram_util_percent: 12.447572815533979
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 16.0
    agent-2: 60.0
    agent-3: 35.0
    agent-4: 25.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.06
    agent-1: 8.85
    agent-2: 43.53
    agent-3: 21.44
    agent-4: 14.38
    agent-5: 17.34
  policy_reward_min:
    agent-0: 2.0
    agent-1: 3.0
    agent-2: 24.0
    agent-3: 9.0
    agent-4: 3.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.218047916156415
    mean_inference_ms: 15.102422890501614
    mean_processing_ms: 73.51206201478875
  time_since_restore: 16755.98645067215
  time_this_iter_s: 144.55476832389832
  time_total_s: 105692.66458034515
  timestamp: 1637379737
  timesteps_since_restore: 11040000
  timesteps_this_iter: 96000
  timesteps_total: 73440000
  training_iteration: 765
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    765 |           105693 | 73440000 |    120.6 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 6.74
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.37
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 28.14
    apples_agent-2_min: 9
    apples_agent-3_max: 26
    apples_agent-3_mean: 7.62
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.32
    apples_agent-4_min: 3
    apples_agent-5_max: 12
    apples_agent-5_mean: 3.7
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 23
    cleaning_beam_agent-0_mean: 5.08
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 249
    cleaning_beam_agent-1_mean: 208.78
    cleaning_beam_agent-1_min: 159
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 3.5
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 11.75
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 3.74
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.74
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-44-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 165.0
  episode_reward_mean: 119.94
  episode_reward_min: 73.0
  episodes_this_iter: 96
  episodes_total: 73536
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11858.219
    learner:
      agent-0:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2515708804130554
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006116250297054648
        model: {}
        policy_loss: -0.001506282016634941
        total_loss: -0.0018511144444346428
        vf_explained_var: 0.00439240038394928
        vf_loss: 0.9793215394020081
      agent-1:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2202039659023285
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007719421410001814
        model: {}
        policy_loss: -0.0014310264959931374
        total_loss: -0.0017612103838473558
        vf_explained_var: 0.0877414345741272
        vf_loss: 0.5737530589103699
      agent-2:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36048269271850586
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012473632814362645
        model: {}
        policy_loss: -0.0019566272385418415
        total_loss: -0.0022281352430582047
        vf_explained_var: 0.025263667106628418
        vf_loss: 3.629406213760376
      agent-3:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5156353116035461
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012443187879398465
        model: {}
        policy_loss: -0.001380912377499044
        total_loss: -0.002129822038114071
        vf_explained_var: 0.006994679570198059
        vf_loss: 1.5860844850540161
      agent-4:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4248138964176178
        entropy_coeff: 0.0017600000137463212
        kl: 0.000837801257148385
        model: {}
        policy_loss: -0.0014348176773637533
        total_loss: -0.0020800777710974216
        vf_explained_var: 0.011471837759017944
        vf_loss: 1.0241276025772095
      agent-5:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.552209734916687
        entropy_coeff: 0.0017600000137463212
        kl: 0.001509334659203887
        model: {}
        policy_loss: -0.001831304281949997
        total_loss: -0.0026886719278991222
        vf_explained_var: 0.007414519786834717
        vf_loss: 1.1452008485794067
    load_time_ms: 14092.606
    num_steps_sampled: 73536000
    num_steps_trained: 73536000
    sample_time_ms: 118695.827
    update_time_ms: 15.844
  iterations_since_restore: 116
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.856730769230772
    ram_util_percent: 12.404807692307692
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 62.0
    agent-3: 36.0
    agent-4: 31.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 15.28
    agent-1: 8.98
    agent-2: 43.8
    agent-3: 20.82
    agent-4: 14.9
    agent-5: 16.16
  policy_reward_min:
    agent-0: 4.0
    agent-1: 3.0
    agent-2: 23.0
    agent-3: 9.0
    agent-4: 5.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.21748717815613
    mean_inference_ms: 15.102372223558312
    mean_processing_ms: 73.50951668273484
  time_since_restore: 16901.331009864807
  time_this_iter_s: 145.34455919265747
  time_total_s: 105838.00913953781
  timestamp: 1637379882
  timesteps_since_restore: 11136000
  timesteps_this_iter: 96000
  timesteps_total: 73536000
  training_iteration: 766
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    766 |           105838 | 73536000 |   119.94 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 7.54
    apples_agent-0_min: 1
    apples_agent-1_max: 16
    apples_agent-1_mean: 3.35
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 27.65
    apples_agent-2_min: 12
    apples_agent-3_max: 46
    apples_agent-3_mean: 8.19
    apples_agent-3_min: 2
    apples_agent-4_max: 30
    apples_agent-4_mean: 10.33
    apples_agent-4_min: 2
    apples_agent-5_max: 22
    apples_agent-5_mean: 4.23
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 19
    cleaning_beam_agent-0_mean: 5.55
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 271
    cleaning_beam_agent-1_mean: 209.56
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 3.87
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 12.28
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.36
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 3.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-47-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 166.0
  episode_reward_mean: 119.23
  episode_reward_min: 68.0
  episodes_this_iter: 96
  episodes_total: 73632
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11845.265
    learner:
      agent-0:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24527516961097717
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007424411596730351
        model: {}
        policy_loss: -0.001680002547800541
        total_loss: -0.0020064665004611015
        vf_explained_var: 0.00795377790927887
        vf_loss: 1.0522056818008423
      agent-1:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2180200219154358
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014411761658266187
        model: {}
        policy_loss: -0.0016950713470578194
        total_loss: -0.0020182114094495773
        vf_explained_var: 0.07351309061050415
        vf_loss: 0.6057322025299072
      agent-2:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3634195029735565
        entropy_coeff: 0.0017600000137463212
        kl: 0.001204092288389802
        model: {}
        policy_loss: -0.0018970912788063288
        total_loss: -0.002165853278711438
        vf_explained_var: 0.023893043398857117
        vf_loss: 3.708578109741211
      agent-3:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5179621577262878
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009491958189755678
        model: {}
        policy_loss: -0.0015865461900830269
        total_loss: -0.0023283855989575386
        vf_explained_var: 0.007364973425865173
        vf_loss: 1.6977494955062866
      agent-4:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4221702218055725
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010387173388153315
        model: {}
        policy_loss: -0.0014547230675816536
        total_loss: -0.0021019745618104935
        vf_explained_var: 0.011495858430862427
        vf_loss: 0.9577157497406006
      agent-5:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5547851324081421
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019488416146486998
        model: {}
        policy_loss: -0.0017343346262350678
        total_loss: -0.0025922073982656
        vf_explained_var: 0.007736623287200928
        vf_loss: 1.1854760646820068
    load_time_ms: 14094.105
    num_steps_sampled: 73632000
    num_steps_trained: 73632000
    sample_time_ms: 118688.96
    update_time_ms: 15.898
  iterations_since_restore: 117
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.030243902439018
    ram_util_percent: 12.345853658536583
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 19.0
    agent-2: 66.0
    agent-3: 40.0
    agent-4: 26.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 14.69
    agent-1: 8.57
    agent-2: 42.59
    agent-3: 21.5
    agent-4: 15.15
    agent-5: 16.73
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 22.0
    agent-3: 7.0
    agent-4: 6.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.217751272393606
    mean_inference_ms: 15.101669643316592
    mean_processing_ms: 73.5083669029371
  time_since_restore: 17046.402888059616
  time_this_iter_s: 145.07187819480896
  time_total_s: 105983.08101773262
  timestamp: 1637380027
  timesteps_since_restore: 11232000
  timesteps_this_iter: 96000
  timesteps_total: 73632000
  training_iteration: 767
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    767 |           105983 | 73632000 |   119.23 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 6.66
    apples_agent-0_min: 1
    apples_agent-1_max: 29
    apples_agent-1_mean: 3.5
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 27.39
    apples_agent-2_min: 4
    apples_agent-3_max: 32
    apples_agent-3_mean: 7.72
    apples_agent-3_min: 1
    apples_agent-4_max: 18
    apples_agent-4_mean: 9.44
    apples_agent-4_min: 0
    apples_agent-5_max: 24
    apples_agent-5_mean: 4.29
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 23
    cleaning_beam_agent-0_mean: 5.09
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 258
    cleaning_beam_agent-1_mean: 208.82
    cleaning_beam_agent-1_min: 17
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 3.89
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 12.83
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 15
    cleaning_beam_agent-4_mean: 4.02
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.97
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-49-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 159.0
  episode_reward_mean: 115.15
  episode_reward_min: 14.0
  episodes_this_iter: 96
  episodes_total: 73728
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11828.893
    learner:
      agent-0:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24926570057868958
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007279752753674984
        model: {}
        policy_loss: -0.001572594977915287
        total_loss: -0.001910070888698101
        vf_explained_var: 0.013518467545509338
        vf_loss: 1.0122969150543213
      agent-1:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21940261125564575
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012833982473239303
        model: {}
        policy_loss: -0.001645051408559084
        total_loss: -0.001977291889488697
        vf_explained_var: 0.08592744171619415
        vf_loss: 0.5390931367874146
      agent-2:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36220604181289673
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008683287887834013
        model: {}
        policy_loss: -0.0016165879787877202
        total_loss: -0.0018898883135989308
        vf_explained_var: 0.027164384722709656
        vf_loss: 3.641852617263794
      agent-3:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5108284950256348
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012222541263327003
        model: {}
        policy_loss: -0.001469755545258522
        total_loss: -0.0022110603749752045
        vf_explained_var: -0.0008270442485809326
        vf_loss: 1.57755446434021
      agent-4:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43733304738998413
        entropy_coeff: 0.0017600000137463212
        kl: 0.000796697975602001
        model: {}
        policy_loss: -0.0012899315916001797
        total_loss: -0.0019639653619378805
        vf_explained_var: -0.0002881288528442383
        vf_loss: 0.9566764831542969
      agent-5:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5748072862625122
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025046593509614468
        model: {}
        policy_loss: -0.0017005307599902153
        total_loss: -0.002591766184195876
        vf_explained_var: 0.0181158185005188
        vf_loss: 1.2042310237884521
    load_time_ms: 14093.991
    num_steps_sampled: 73728000
    num_steps_trained: 73728000
    sample_time_ms: 118790.279
    update_time_ms: 16.0
  iterations_since_restore: 118
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.8125
    ram_util_percent: 12.327884615384615
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 19.0
    agent-2: 63.0
    agent-3: 38.0
    agent-4: 28.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 14.37
    agent-1: 8.63
    agent-2: 42.33
    agent-3: 20.23
    agent-4: 13.61
    agent-5: 15.98
  policy_reward_min:
    agent-0: 2.0
    agent-1: 0.0
    agent-2: 8.0
    agent-3: 3.0
    agent-4: 0.0
    agent-5: 1.0
  sampler_perf:
    mean_env_wait_ms: 28.217372573840585
    mean_inference_ms: 15.101637813384995
    mean_processing_ms: 73.50448910627209
  time_since_restore: 17191.99798464775
  time_this_iter_s: 145.59509658813477
  time_total_s: 106128.67611432076
  timestamp: 1637380173
  timesteps_since_restore: 11328000
  timesteps_this_iter: 96000
  timesteps_total: 73728000
  training_iteration: 768
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    768 |           106129 | 73728000 |   115.15 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 7.19
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 4.29
    apples_agent-1_min: 0
    apples_agent-2_max: 54
    apples_agent-2_mean: 27.59
    apples_agent-2_min: 12
    apples_agent-3_max: 30
    apples_agent-3_mean: 8.1
    apples_agent-3_min: 1
    apples_agent-4_max: 18
    apples_agent-4_mean: 9.34
    apples_agent-4_min: 3
    apples_agent-5_max: 22
    apples_agent-5_mean: 3.69
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 5.16
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 251
    cleaning_beam_agent-1_mean: 209.74
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 4.42
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 10.79
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.6
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.05
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-51-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 118.63
  episode_reward_min: 46.0
  episodes_this_iter: 96
  episodes_total: 73824
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11821.836
    learner:
      agent-0:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25057464838027954
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005943404394201934
        model: {}
        policy_loss: -0.001568034291267395
        total_loss: -0.001905985176563263
        vf_explained_var: 0.010067462921142578
        vf_loss: 1.0305832624435425
      agent-1:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2196507751941681
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007356061250902712
        model: {}
        policy_loss: -0.0013449527323246002
        total_loss: -0.001681523397564888
        vf_explained_var: 0.09277927875518799
        vf_loss: 0.5001655220985413
      agent-2:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37140706181526184
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012561274925246835
        model: {}
        policy_loss: -0.0018888004124164581
        total_loss: -0.0021775136701762676
        vf_explained_var: 0.03902842104434967
        vf_loss: 3.6496241092681885
      agent-3:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5146445631980896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016864357749000192
        model: {}
        policy_loss: -0.0016564326360821724
        total_loss: -0.0023966440930962563
        vf_explained_var: 0.009746238589286804
        vf_loss: 1.655627965927124
      agent-4:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4486420452594757
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012745282147079706
        model: {}
        policy_loss: -0.0015838941326364875
        total_loss: -0.002270421013236046
        vf_explained_var: 0.004296541213989258
        vf_loss: 1.0308209657669067
      agent-5:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5779883861541748
        entropy_coeff: 0.0017600000137463212
        kl: 0.001575598493218422
        model: {}
        policy_loss: -0.001687405863776803
        total_loss: -0.0025815123226493597
        vf_explained_var: 0.0151786208152771
        vf_loss: 1.2315096855163574
    load_time_ms: 14087.659
    num_steps_sampled: 73824000
    num_steps_trained: 73824000
    sample_time_ms: 118885.15
    update_time_ms: 15.975
  iterations_since_restore: 119
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.93768115942029
    ram_util_percent: 12.389855072463767
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 18.0
    agent-2: 71.0
    agent-3: 35.0
    agent-4: 26.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.09
    agent-1: 8.6
    agent-2: 43.18
    agent-3: 20.96
    agent-4: 14.22
    agent-5: 16.58
  policy_reward_min:
    agent-0: 4.0
    agent-1: 1.0
    agent-2: 20.0
    agent-3: 8.0
    agent-4: 4.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.21694947156418
    mean_inference_ms: 15.102108507886902
    mean_processing_ms: 73.50289405498032
  time_since_restore: 17337.738165140152
  time_this_iter_s: 145.74018049240112
  time_total_s: 106274.41629481316
  timestamp: 1637380319
  timesteps_since_restore: 11424000
  timesteps_this_iter: 96000
  timesteps_total: 73824000
  training_iteration: 769
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    769 |           106274 | 73824000 |   118.63 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 6.73
    apples_agent-0_min: 1
    apples_agent-1_max: 24
    apples_agent-1_mean: 3.36
    apples_agent-1_min: 0
    apples_agent-2_max: 66
    apples_agent-2_mean: 27.96
    apples_agent-2_min: 11
    apples_agent-3_max: 26
    apples_agent-3_mean: 7.69
    apples_agent-3_min: 1
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.38
    apples_agent-4_min: 1
    apples_agent-5_max: 24
    apples_agent-5_mean: 3.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 34
    cleaning_beam_agent-0_mean: 4.62
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 268
    cleaning_beam_agent-1_mean: 214.47
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 3.25
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 26
    cleaning_beam_agent-3_mean: 10.46
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.28
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.47
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-54-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 177.0
  episode_reward_mean: 117.05
  episode_reward_min: 69.0
  episodes_this_iter: 96
  episodes_total: 73920
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11786.841
    learner:
      agent-0:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25345301628112793
        entropy_coeff: 0.0017600000137463212
        kl: 0.000728094601072371
        model: {}
        policy_loss: -0.0015289527364075184
        total_loss: -0.001867895945906639
        vf_explained_var: 0.013516008853912354
        vf_loss: 1.0713589191436768
      agent-1:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22159603238105774
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010266683530062437
        model: {}
        policy_loss: -0.0015475029358640313
        total_loss: -0.0018794257193803787
        vf_explained_var: 0.07305346429347992
        vf_loss: 0.5808688998222351
      agent-2:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37389570474624634
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012586030643433332
        model: {}
        policy_loss: -0.0018950365483760834
        total_loss: -0.0020659526344388723
        vf_explained_var: 0.025703847408294678
        vf_loss: 4.8714094161987305
      agent-3:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.508411169052124
        entropy_coeff: 0.0017600000137463212
        kl: 0.001714762533083558
        model: {}
        policy_loss: -0.0014909999445080757
        total_loss: -0.002218961948528886
        vf_explained_var: 0.004254743456840515
        vf_loss: 1.6684041023254395
      agent-4:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45389246940612793
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007791824173182249
        model: {}
        policy_loss: -0.001231896923854947
        total_loss: -0.001943729119375348
        vf_explained_var: 0.010433152318000793
        vf_loss: 0.8702175617218018
      agent-5:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5720065832138062
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014173017116263509
        model: {}
        policy_loss: -0.0019189601298421621
        total_loss: -0.002814974170178175
        vf_explained_var: 0.010756164789199829
        vf_loss: 1.1071799993515015
    load_time_ms: 14084.452
    num_steps_sampled: 73920000
    num_steps_trained: 73920000
    sample_time_ms: 118959.187
    update_time_ms: 16.004
  iterations_since_restore: 120
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.933333333333337
    ram_util_percent: 12.360869565217392
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 18.0
    agent-2: 67.0
    agent-3: 34.0
    agent-4: 26.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 14.82
    agent-1: 8.79
    agent-2: 42.25
    agent-3: 21.28
    agent-4: 13.77
    agent-5: 16.14
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: -15.0
    agent-3: 8.0
    agent-4: 6.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.217349224992912
    mean_inference_ms: 15.101510510927637
    mean_processing_ms: 73.50185913132219
  time_since_restore: 17482.43337917328
  time_this_iter_s: 144.69521403312683
  time_total_s: 106419.11150884628
  timestamp: 1637380464
  timesteps_since_restore: 11520000
  timesteps_this_iter: 96000
  timesteps_total: 73920000
  training_iteration: 770
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    770 |           106419 | 73920000 |   117.05 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 6.8
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 3.16
    apples_agent-1_min: 0
    apples_agent-2_max: 55
    apples_agent-2_mean: 27.4
    apples_agent-2_min: 14
    apples_agent-3_max: 51
    apples_agent-3_mean: 8.48
    apples_agent-3_min: 1
    apples_agent-4_max: 31
    apples_agent-4_mean: 9.98
    apples_agent-4_min: 1
    apples_agent-5_max: 24
    apples_agent-5_mean: 3.45
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 3.88
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 260
    cleaning_beam_agent-1_mean: 207.93
    cleaning_beam_agent-1_min: 74
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 3.52
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 10.43
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 15
    cleaning_beam_agent-4_mean: 3.41
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 5.26
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-56-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 165.0
  episode_reward_mean: 118.13
  episode_reward_min: 51.0
  episodes_this_iter: 96
  episodes_total: 74016
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11801.067
    learner:
      agent-0:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24667996168136597
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011466541327536106
        model: {}
        policy_loss: -0.0018230481073260307
        total_loss: -0.002150784246623516
        vf_explained_var: 0.012055158615112305
        vf_loss: 1.0642361640930176
      agent-1:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2210099846124649
        entropy_coeff: 0.0017600000137463212
        kl: 0.001007735962048173
        model: {}
        policy_loss: -0.0015175030566751957
        total_loss: -0.0018547347281128168
        vf_explained_var: 0.0883241593837738
        vf_loss: 0.5174790620803833
      agent-2:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37906575202941895
        entropy_coeff: 0.0017600000137463212
        kl: 0.001269889180548489
        model: {}
        policy_loss: -0.0018476895056664944
        total_loss: -0.0021622469648718834
        vf_explained_var: 0.03796111047267914
        vf_loss: 3.525984287261963
      agent-3:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5134257078170776
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013485790695995092
        model: {}
        policy_loss: -0.0016024978831410408
        total_loss: -0.0023454567417502403
        vf_explained_var: -0.001340329647064209
        vf_loss: 1.6067116260528564
      agent-4:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44379425048828125
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008078361861407757
        model: {}
        policy_loss: -0.0013058241456747055
        total_loss: -0.0019803098402917385
        vf_explained_var: 0.007750466465950012
        vf_loss: 1.0659348964691162
      agent-5:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.568133533000946
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015041397418826818
        model: {}
        policy_loss: -0.0016175750643014908
        total_loss: -0.0024957163259387016
        vf_explained_var: 0.018474727869033813
        vf_loss: 1.2177362442016602
    load_time_ms: 14079.049
    num_steps_sampled: 74016000
    num_steps_trained: 74016000
    sample_time_ms: 118927.08
    update_time_ms: 16.189
  iterations_since_restore: 121
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.78421052631579
    ram_util_percent: 12.44114832535885
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 18.0
    agent-2: 62.0
    agent-3: 36.0
    agent-4: 29.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 15.18
    agent-1: 8.52
    agent-2: 42.03
    agent-3: 21.28
    agent-4: 14.29
    agent-5: 16.83
  policy_reward_min:
    agent-0: 4.0
    agent-1: 1.0
    agent-2: 15.0
    agent-3: 8.0
    agent-4: 5.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.21652387130668
    mean_inference_ms: 15.10142555717733
    mean_processing_ms: 73.49984558146103
  time_since_restore: 17627.201476097107
  time_this_iter_s: 144.76809692382812
  time_total_s: 106563.87960577011
  timestamp: 1637380611
  timesteps_since_restore: 11616000
  timesteps_this_iter: 96000
  timesteps_total: 74016000
  training_iteration: 771
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    771 |           106564 | 74016000 |   118.13 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 6.73
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 3.38
    apples_agent-1_min: 0
    apples_agent-2_max: 60
    apples_agent-2_mean: 28.54
    apples_agent-2_min: 8
    apples_agent-3_max: 21
    apples_agent-3_mean: 8.27
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.56
    apples_agent-4_min: 1
    apples_agent-5_max: 31
    apples_agent-5_mean: 4.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 15
    cleaning_beam_agent-0_mean: 5.09
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 261
    cleaning_beam_agent-1_mean: 207.09
    cleaning_beam_agent-1_min: 56
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 3.51
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 22
    cleaning_beam_agent-3_mean: 9.38
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.06
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 4.57
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_22-59-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 179.0
  episode_reward_mean: 118.86
  episode_reward_min: 40.0
  episodes_this_iter: 96
  episodes_total: 74112
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11811.146
    learner:
      agent-0:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24955937266349792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008411772432737052
        model: {}
        policy_loss: -0.0016446443041786551
        total_loss: -0.001974406186491251
        vf_explained_var: 0.008602112531661987
        vf_loss: 1.094623327255249
      agent-1:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21880179643630981
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008258557645604014
        model: {}
        policy_loss: -0.0013395424466580153
        total_loss: -0.0016655770596116781
        vf_explained_var: 0.08800457417964935
        vf_loss: 0.5905440449714661
      agent-2:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3800550103187561
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008811756270006299
        model: {}
        policy_loss: -0.0016018017195165157
        total_loss: -0.0018685194663703442
        vf_explained_var: 0.04464966058731079
        vf_loss: 4.021814823150635
      agent-3:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5073201656341553
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012952182441949844
        model: {}
        policy_loss: -0.0014934525825083256
        total_loss: -0.0022229307796806097
        vf_explained_var: 0.006489396095275879
        vf_loss: 1.634079933166504
      agent-4:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4336312413215637
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007297838455997407
        model: {}
        policy_loss: -0.0013875279109925032
        total_loss: -0.0020528396125882864
        vf_explained_var: 0.010601431131362915
        vf_loss: 0.9788233041763306
      agent-5:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.562231719493866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017946000443771482
        model: {}
        policy_loss: -0.0015762667171657085
        total_loss: -0.0024488794151693583
        vf_explained_var: 0.017667070031166077
        vf_loss: 1.169184923171997
    load_time_ms: 14105.037
    num_steps_sampled: 74112000
    num_steps_trained: 74112000
    sample_time_ms: 119010.985
    update_time_ms: 16.109
  iterations_since_restore: 122
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.968932038834954
    ram_util_percent: 12.376699029126215
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 21.0
    agent-2: 69.0
    agent-3: 34.0
    agent-4: 29.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 15.13
    agent-1: 8.87
    agent-2: 43.28
    agent-3: 21.01
    agent-4: 14.15
    agent-5: 16.42
  policy_reward_min:
    agent-0: 3.0
    agent-1: 2.0
    agent-2: 15.0
    agent-3: 3.0
    agent-4: 5.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.2157503374342
    mean_inference_ms: 15.100959001600529
    mean_processing_ms: 73.49773924566811
  time_since_restore: 17772.27673482895
  time_this_iter_s: 145.07525873184204
  time_total_s: 106708.95486450195
  timestamp: 1637380756
  timesteps_since_restore: 11712000
  timesteps_this_iter: 96000
  timesteps_total: 74112000
  training_iteration: 772
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    772 |           106709 | 74112000 |   118.86 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 7.51
    apples_agent-0_min: 0
    apples_agent-1_max: 15
    apples_agent-1_mean: 3.28
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 28.03
    apples_agent-2_min: 4
    apples_agent-3_max: 31
    apples_agent-3_mean: 7.92
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 9.25
    apples_agent-4_min: 0
    apples_agent-5_max: 17
    apples_agent-5_mean: 3.77
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 21
    cleaning_beam_agent-0_mean: 5.22
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 261
    cleaning_beam_agent-1_mean: 207.79
    cleaning_beam_agent-1_min: 15
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 3.14
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 26
    cleaning_beam_agent-3_mean: 10.04
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.3
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 5.1
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-01-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 169.0
  episode_reward_mean: 117.95
  episode_reward_min: 19.0
  episodes_this_iter: 96
  episodes_total: 74208
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11811.401
    learner:
      agent-0:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.24697965383529663
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008505582227371633
        model: {}
        policy_loss: -0.0016056904569268227
        total_loss: -0.0019330848008394241
        vf_explained_var: 0.004110902547836304
        vf_loss: 1.072907567024231
      agent-1:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2200509011745453
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009524893248453736
        model: {}
        policy_loss: -0.0014624344184994698
        total_loss: -0.0017941538244485855
        vf_explained_var: 0.09173737466335297
        vf_loss: 0.5556851625442505
      agent-2:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3761807084083557
        entropy_coeff: 0.0017600000137463212
        kl: 0.000961747020483017
        model: {}
        policy_loss: -0.001637808047235012
        total_loss: -0.0019247885793447495
        vf_explained_var: 0.03520764410495758
        vf_loss: 3.7509899139404297
      agent-3:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4933159351348877
        entropy_coeff: 0.0017600000137463212
        kl: 0.001953647006303072
        model: {}
        policy_loss: -0.0016061088535934687
        total_loss: -0.00231528514996171
        vf_explained_var: 0.013296440243721008
        vf_loss: 1.590591311454773
      agent-4:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4395541548728943
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007089027785696089
        model: {}
        policy_loss: -0.0010806485079228878
        total_loss: -0.0017626872286200523
        vf_explained_var: 0.00882294774055481
        vf_loss: 0.9157453775405884
      agent-5:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.559948742389679
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014829332940280437
        model: {}
        policy_loss: -0.0018732869066298008
        total_loss: -0.0027217441238462925
        vf_explained_var: 0.014618992805480957
        vf_loss: 1.370553731918335
    load_time_ms: 14077.826
    num_steps_sampled: 74208000
    num_steps_trained: 74208000
    sample_time_ms: 118997.56
    update_time_ms: 16.16
  iterations_since_restore: 123
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.08164251207729
    ram_util_percent: 12.458937198067632
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 18.0
    agent-2: 60.0
    agent-3: 35.0
    agent-4: 24.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 15.43
    agent-1: 8.79
    agent-2: 42.05
    agent-3: 21.01
    agent-4: 13.97
    agent-5: 16.7
  policy_reward_min:
    agent-0: 0.0
    agent-1: 3.0
    agent-2: 6.0
    agent-3: 4.0
    agent-4: 2.0
    agent-5: 2.0
  sampler_perf:
    mean_env_wait_ms: 28.215029043855143
    mean_inference_ms: 15.100399787084081
    mean_processing_ms: 73.49641749649915
  time_since_restore: 17916.93132162094
  time_this_iter_s: 144.6545867919922
  time_total_s: 106853.60945129395
  timestamp: 1637380901
  timesteps_since_restore: 11808000
  timesteps_this_iter: 96000
  timesteps_total: 74208000
  training_iteration: 773
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    773 |           106854 | 74208000 |   117.95 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 6.25
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.16
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 28.51
    apples_agent-2_min: 16
    apples_agent-3_max: 31
    apples_agent-3_mean: 8.2
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 9.5
    apples_agent-4_min: 3
    apples_agent-5_max: 15
    apples_agent-5_mean: 4.08
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 18
    cleaning_beam_agent-0_mean: 5.98
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 256
    cleaning_beam_agent-1_mean: 204.91
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 3.29
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 8.91
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.29
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 5.77
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-04-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 173.0
  episode_reward_mean: 119.38
  episode_reward_min: 94.0
  episodes_this_iter: 96
  episodes_total: 74304
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11812.454
    learner:
      agent-0:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25127437710762024
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010958757484331727
        model: {}
        policy_loss: -0.0017952469643205404
        total_loss: -0.0021280667278915644
        vf_explained_var: 0.010369762778282166
        vf_loss: 1.0942440032958984
      agent-1:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2192504107952118
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011582777369767427
        model: {}
        policy_loss: -0.0015398189425468445
        total_loss: -0.0018684477545320988
        vf_explained_var: 0.10137392580509186
        vf_loss: 0.5725119709968567
      agent-2:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3843844532966614
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013244518777355552
        model: {}
        policy_loss: -0.0018674209713935852
        total_loss: -0.002165592275559902
        vf_explained_var: 0.04366333782672882
        vf_loss: 3.783426284790039
      agent-3:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49675166606903076
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010672222124412656
        model: {}
        policy_loss: -0.0015046558110043406
        total_loss: -0.0022141821682453156
        vf_explained_var: 0.005764409899711609
        vf_loss: 1.6475735902786255
      agent-4:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4442991018295288
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005112470826134086
        model: {}
        policy_loss: -0.0013267956674098969
        total_loss: -0.0020103103015571833
        vf_explained_var: 0.015416190028190613
        vf_loss: 0.9844961166381836
      agent-5:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5701442360877991
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011208740761503577
        model: {}
        policy_loss: -0.0016747742192819715
        total_loss: -0.002559137297794223
        vf_explained_var: 0.018171921372413635
        vf_loss: 1.1908882856369019
    load_time_ms: 14087.516
    num_steps_sampled: 74304000
    num_steps_trained: 74304000
    sample_time_ms: 118986.419
    update_time_ms: 15.73
  iterations_since_restore: 124
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.073658536585366
    ram_util_percent: 12.367804878048782
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 66.0
    agent-3: 39.0
    agent-4: 26.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 15.27
    agent-1: 9.0
    agent-2: 43.15
    agent-3: 21.04
    agent-4: 14.06
    agent-5: 16.86
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 24.0
    agent-3: 9.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.21465108223942
    mean_inference_ms: 15.100093443970252
    mean_processing_ms: 73.4945239087156
  time_since_restore: 18061.37764954567
  time_this_iter_s: 144.4463279247284
  time_total_s: 106998.05577921867
  timestamp: 1637381045
  timesteps_since_restore: 11904000
  timesteps_this_iter: 96000
  timesteps_total: 74304000
  training_iteration: 774
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    774 |           106998 | 74304000 |   119.38 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 7.19
    apples_agent-0_min: 1
    apples_agent-1_max: 15
    apples_agent-1_mean: 3.54
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 29.65
    apples_agent-2_min: 14
    apples_agent-3_max: 27
    apples_agent-3_mean: 8.43
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 9.8
    apples_agent-4_min: 3
    apples_agent-5_max: 17
    apples_agent-5_mean: 4.22
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 27
    cleaning_beam_agent-0_mean: 4.94
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 254
    cleaning_beam_agent-1_mean: 208.26
    cleaning_beam_agent-1_min: 151
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 3.46
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 28
    cleaning_beam_agent-3_mean: 9.33
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.91
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 5.03
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-06-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 179.0
  episode_reward_mean: 122.55
  episode_reward_min: 79.0
  episodes_this_iter: 96
  episodes_total: 74400
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11814.119
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2513194978237152
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005010072491131723
        model: {}
        policy_loss: -0.0009742945549078286
        total_loss: -0.001176692545413971
        vf_explained_var: 0.010819733142852783
        vf_loss: 2.399230480194092
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2174610197544098
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015528679359704256
        model: {}
        policy_loss: -0.0016618482768535614
        total_loss: -0.001988996285945177
        vf_explained_var: 0.0686827003955841
        vf_loss: 0.555854320526123
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38737696409225464
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015462730079889297
        model: {}
        policy_loss: -0.001997470622882247
        total_loss: -0.0022978459019213915
        vf_explained_var: 0.010349571704864502
        vf_loss: 3.8140971660614014
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49739253520965576
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017892478499561548
        model: {}
        policy_loss: -0.0016182223334908485
        total_loss: -0.0023146960884332657
        vf_explained_var: 0.01448892056941986
        vf_loss: 1.7893939018249512
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44900575280189514
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011350084096193314
        model: {}
        policy_loss: -0.0015785004943609238
        total_loss: -0.0022598858922719955
        vf_explained_var: 0.006132975220680237
        vf_loss: 1.0886597633361816
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5650461912155151
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015880765859037638
        model: {}
        policy_loss: -0.0019180625677108765
        total_loss: -0.0027941400185227394
        vf_explained_var: 0.013829678297042847
        vf_loss: 1.1840291023254395
    load_time_ms: 14099.165
    num_steps_sampled: 74400000
    num_steps_trained: 74400000
    sample_time_ms: 119028.244
    update_time_ms: 16.295
  iterations_since_restore: 125
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.955072463768115
    ram_util_percent: 12.44830917874396
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 17.0
    agent-2: 65.0
    agent-3: 38.0
    agent-4: 29.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 15.25
    agent-1: 8.86
    agent-2: 43.96
    agent-3: 22.27
    agent-4: 15.26
    agent-5: 16.95
  policy_reward_min:
    agent-0: -38.0
    agent-1: 2.0
    agent-2: 20.0
    agent-3: 9.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.21384438102179
    mean_inference_ms: 15.099776190121496
    mean_processing_ms: 73.4943424322494
  time_since_restore: 18206.52477836609
  time_this_iter_s: 145.1471288204193
  time_total_s: 107143.2029080391
  timestamp: 1637381190
  timesteps_since_restore: 12000000
  timesteps_this_iter: 96000
  timesteps_total: 74400000
  training_iteration: 775
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    775 |           107143 | 74400000 |   122.55 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 7.16
    apples_agent-0_min: 1
    apples_agent-1_max: 18
    apples_agent-1_mean: 3.17
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 30.58
    apples_agent-2_min: 18
    apples_agent-3_max: 55
    apples_agent-3_mean: 8.39
    apples_agent-3_min: 1
    apples_agent-4_max: 27
    apples_agent-4_mean: 10.49
    apples_agent-4_min: 3
    apples_agent-5_max: 26
    apples_agent-5_mean: 3.96
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 15
    cleaning_beam_agent-0_mean: 4.18
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 260
    cleaning_beam_agent-1_mean: 216.03
    cleaning_beam_agent-1_min: 148
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 3.45
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 10.87
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.01
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.43
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-08-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 173.0
  episode_reward_mean: 124.32
  episode_reward_min: 75.0
  episodes_this_iter: 96
  episodes_total: 74496
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11823.99
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2537100911140442
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008643966866657138
        model: {}
        policy_loss: -0.0017312243580818176
        total_loss: -0.002057329285889864
        vf_explained_var: 0.003408476710319519
        vf_loss: 1.2042782306671143
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22128063440322876
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008387788548134267
        model: {}
        policy_loss: -0.0015342470724135637
        total_loss: -0.0018687954870983958
        vf_explained_var: 0.0834868848323822
        vf_loss: 0.5490572452545166
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3908824324607849
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014748603571206331
        model: {}
        policy_loss: -0.0017171415966004133
        total_loss: -0.00201316736638546
        vf_explained_var: 0.011552870273590088
        vf_loss: 3.919290065765381
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49918532371520996
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021897819824516773
        model: {}
        policy_loss: -0.0015197284519672394
        total_loss: -0.002231850288808346
        vf_explained_var: 0.010624110698699951
        vf_loss: 1.664475917816162
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44177111983299255
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006111299735493958
        model: {}
        policy_loss: -0.001283168327063322
        total_loss: -0.0019569555297493935
        vf_explained_var: 0.012802332639694214
        vf_loss: 1.0372779369354248
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5733309984207153
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016388786025345325
        model: {}
        policy_loss: -0.0017604855820536613
        total_loss: -0.0026576987002044916
        vf_explained_var: 0.002479061484336853
        vf_loss: 1.118509292602539
    load_time_ms: 14089.39
    num_steps_sampled: 74496000
    num_steps_trained: 74496000
    sample_time_ms: 119010.023
    update_time_ms: 16.074
  iterations_since_restore: 126
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.93768115942029
    ram_util_percent: 12.4487922705314
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 19.0
    agent-2: 68.0
    agent-3: 39.0
    agent-4: 27.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 16.3
    agent-1: 8.92
    agent-2: 45.06
    agent-3: 21.64
    agent-4: 15.57
    agent-5: 16.83
  policy_reward_min:
    agent-0: 5.0
    agent-1: 3.0
    agent-2: 29.0
    agent-3: 11.0
    agent-4: 7.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.21369016081894
    mean_inference_ms: 15.09910621102788
    mean_processing_ms: 73.49330523351138
  time_since_restore: 18351.62783718109
  time_this_iter_s: 145.10305881500244
  time_total_s: 107288.3059668541
  timestamp: 1637381336
  timesteps_since_restore: 12096000
  timesteps_this_iter: 96000
  timesteps_total: 74496000
  training_iteration: 776
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    776 |           107288 | 74496000 |   124.32 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 6.32
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 2.83
    apples_agent-1_min: 0
    apples_agent-2_max: 40
    apples_agent-2_mean: 27.47
    apples_agent-2_min: 8
    apples_agent-3_max: 21
    apples_agent-3_mean: 7.87
    apples_agent-3_min: 0
    apples_agent-4_max: 27
    apples_agent-4_mean: 10.18
    apples_agent-4_min: 2
    apples_agent-5_max: 23
    apples_agent-5_mean: 3.28
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 18
    cleaning_beam_agent-0_mean: 4.29
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 257
    cleaning_beam_agent-1_mean: 209.02
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 4.24
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 20
    cleaning_beam_agent-3_mean: 8.38
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 2.46
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.73
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-11-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 160.0
  episode_reward_mean: 118.13
  episode_reward_min: 47.0
  episodes_this_iter: 96
  episodes_total: 74592
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11840.033
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2536091208457947
        entropy_coeff: 0.0017600000137463212
        kl: 0.000673945527523756
        model: {}
        policy_loss: -0.0015837093815207481
        total_loss: -0.0019312268123030663
        vf_explained_var: 0.0055741071701049805
        vf_loss: 0.988350510597229
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.215079665184021
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008396775228902698
        model: {}
        policy_loss: -0.0014183828607201576
        total_loss: -0.0017482833936810493
        vf_explained_var: 0.10352140665054321
        vf_loss: 0.48640045523643494
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39457565546035767
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014484080020338297
        model: {}
        policy_loss: -0.0016601872630417347
        total_loss: -0.0019987886771559715
        vf_explained_var: 0.004450410604476929
        vf_loss: 3.558511972427368
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4984736740589142
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016248461324721575
        model: {}
        policy_loss: -0.0014947685413062572
        total_loss: -0.0022192378528416157
        vf_explained_var: 0.016117185354232788
        vf_loss: 1.5284165143966675
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43732717633247375
        entropy_coeff: 0.0017600000137463212
        kl: 0.000892309588380158
        model: {}
        policy_loss: -0.0013552401214838028
        total_loss: -0.002019686158746481
        vf_explained_var: 0.014382123947143555
        vf_loss: 1.0524874925613403
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5808568000793457
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016160683007910848
        model: {}
        policy_loss: -0.00166692107450217
        total_loss: -0.002580723725259304
        vf_explained_var: 0.022255361080169678
        vf_loss: 1.0850578546524048
    load_time_ms: 14083.574
    num_steps_sampled: 74592000
    num_steps_trained: 74592000
    sample_time_ms: 119028.408
    update_time_ms: 16.418
  iterations_since_restore: 127
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.916346153846156
    ram_util_percent: 12.372115384615382
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 16.0
    agent-2: 65.0
    agent-3: 36.0
    agent-4: 27.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 14.6
    agent-1: 8.42
    agent-2: 42.28
    agent-3: 21.67
    agent-4: 15.11
    agent-5: 16.05
  policy_reward_min:
    agent-0: 6.0
    agent-1: 0.0
    agent-2: 15.0
    agent-3: 6.0
    agent-4: 7.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.213691468852204
    mean_inference_ms: 15.099075427697631
    mean_processing_ms: 73.49273225668479
  time_since_restore: 18497.05293917656
  time_this_iter_s: 145.42510199546814
  time_total_s: 107433.73106884956
  timestamp: 1637381481
  timesteps_since_restore: 12192000
  timesteps_this_iter: 96000
  timesteps_total: 74592000
  training_iteration: 777
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    777 |           107434 | 74592000 |   118.13 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 7.3
    apples_agent-0_min: 1
    apples_agent-1_max: 15
    apples_agent-1_mean: 2.87
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 28.99
    apples_agent-2_min: 13
    apples_agent-3_max: 21
    apples_agent-3_mean: 7.49
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.05
    apples_agent-4_min: 2
    apples_agent-5_max: 15
    apples_agent-5_mean: 3.57
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 4.3
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 251
    cleaning_beam_agent-1_mean: 207.9
    cleaning_beam_agent-1_min: 69
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 3.16
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 31
    cleaning_beam_agent-3_mean: 8.78
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.6
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 5.09
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-13-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 181.0
  episode_reward_mean: 121.08
  episode_reward_min: 51.0
  episodes_this_iter: 96
  episodes_total: 74688
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11845.097
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25539934635162354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008428427972830832
        model: {}
        policy_loss: -0.0014466047286987305
        total_loss: -0.0017932336777448654
        vf_explained_var: 0.0029414743185043335
        vf_loss: 1.028735876083374
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21120908856391907
        entropy_coeff: 0.0017600000137463212
        kl: 0.001150336116552353
        model: {}
        policy_loss: -0.0016458098543807864
        total_loss: -0.0019691430497914553
        vf_explained_var: 0.10071699321269989
        vf_loss: 0.4839547276496887
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3909861743450165
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014127130853012204
        model: {}
        policy_loss: -0.0018389718607068062
        total_loss: -0.002132362686097622
        vf_explained_var: 0.023073628544807434
        vf_loss: 3.947479724884033
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49455538392066956
        entropy_coeff: 0.0017600000137463212
        kl: 0.001223389757797122
        model: {}
        policy_loss: -0.001447406248189509
        total_loss: -0.0021430673077702522
        vf_explained_var: -0.00305330753326416
        vf_loss: 1.747552752494812
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42965301871299744
        entropy_coeff: 0.0017600000137463212
        kl: 0.001048891106620431
        model: {}
        policy_loss: -0.00138325453735888
        total_loss: -0.002040753373876214
        vf_explained_var: 0.005768820643424988
        vf_loss: 0.9869242310523987
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5734433531761169
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007651112391613424
        model: {}
        policy_loss: -0.001569061423651874
        total_loss: -0.002459051553159952
        vf_explained_var: 0.014460563659667969
        vf_loss: 1.192688226699829
    load_time_ms: 14070.653
    num_steps_sampled: 74688000
    num_steps_trained: 74688000
    sample_time_ms: 118916.981
    update_time_ms: 16.483
  iterations_since_restore: 128
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.041463414634148
    ram_util_percent: 12.367804878048778
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 60.0
    agent-3: 34.0
    agent-4: 23.0
    agent-5: 39.0
  policy_reward_mean:
    agent-0: 14.96
    agent-1: 8.35
    agent-2: 43.81
    agent-3: 22.14
    agent-4: 14.75
    agent-5: 17.07
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 21.0
    agent-3: 6.0
    agent-4: 7.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.213188985027045
    mean_inference_ms: 15.098343544269646
    mean_processing_ms: 73.49004347479135
  time_since_restore: 18641.395476341248
  time_this_iter_s: 144.3425371646881
  time_total_s: 107578.07360601425
  timestamp: 1637381626
  timesteps_since_restore: 12288000
  timesteps_this_iter: 96000
  timesteps_total: 74688000
  training_iteration: 778
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    778 |           107578 | 74688000 |   121.08 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 6.32
    apples_agent-0_min: 1
    apples_agent-1_max: 25
    apples_agent-1_mean: 3.38
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 27.51
    apples_agent-2_min: 13
    apples_agent-3_max: 54
    apples_agent-3_mean: 9.14
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 9.98
    apples_agent-4_min: 1
    apples_agent-5_max: 25
    apples_agent-5_mean: 4.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 23
    cleaning_beam_agent-0_mean: 4.99
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 239
    cleaning_beam_agent-1_mean: 205.75
    cleaning_beam_agent-1_min: 64
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 3.61
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 10.01
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.73
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 4.38
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-16-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 153.0
  episode_reward_mean: 118.28
  episode_reward_min: 57.0
  episodes_this_iter: 96
  episodes_total: 74784
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11853.173
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25370094180107117
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007242259453050792
        model: {}
        policy_loss: -0.0015328876907005906
        total_loss: -0.0018774591153487563
        vf_explained_var: 0.005552962422370911
        vf_loss: 1.0194451808929443
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.213629350066185
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015134119894355536
        model: {}
        policy_loss: -0.001677953521721065
        total_loss: -0.0020023640245199203
        vf_explained_var: 0.07839047908782959
        vf_loss: 0.5157475471496582
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39777857065200806
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009176533785648644
        model: {}
        policy_loss: -0.001747518079355359
        total_loss: -0.002082674764096737
        vf_explained_var: 0.009827837347984314
        vf_loss: 3.649338722229004
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49075761437416077
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011038765078410506
        model: {}
        policy_loss: -0.001292241271585226
        total_loss: -0.0020004548132419586
        vf_explained_var: 0.0012333840131759644
        vf_loss: 1.5551875829696655
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4309813976287842
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010156858479604125
        model: {}
        policy_loss: -0.0014040672685950994
        total_loss: -0.0020621332805603743
        vf_explained_var: 0.008552759885787964
        vf_loss: 1.0045963525772095
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5740177631378174
        entropy_coeff: 0.0017600000137463212
        kl: 0.001451177871786058
        model: {}
        policy_loss: -0.0016291881911456585
        total_loss: -0.002519295085221529
        vf_explained_var: 0.010663822293281555
        vf_loss: 1.2016345262527466
    load_time_ms: 14096.528
    num_steps_sampled: 74784000
    num_steps_trained: 74784000
    sample_time_ms: 118827.877
    update_time_ms: 16.521
  iterations_since_restore: 129
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.80338164251208
    ram_util_percent: 12.445893719806762
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 16.0
    agent-2: 62.0
    agent-3: 39.0
    agent-4: 25.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 15.18
    agent-1: 8.24
    agent-2: 41.75
    agent-3: 21.65
    agent-4: 14.85
    agent-5: 16.61
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 20.0
    agent-3: 8.0
    agent-4: 5.0
    agent-5: -33.0
  sampler_perf:
    mean_env_wait_ms: 28.21168295257587
    mean_inference_ms: 15.097431729918688
    mean_processing_ms: 73.48473363026585
  time_since_restore: 18786.63675260544
  time_this_iter_s: 145.24127626419067
  time_total_s: 107723.31488227844
  timestamp: 1637381771
  timesteps_since_restore: 12384000
  timesteps_this_iter: 96000
  timesteps_total: 74784000
  training_iteration: 779
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    779 |           107723 | 74784000 |   118.28 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 6.8
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 2.96
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 28.27
    apples_agent-2_min: 19
    apples_agent-3_max: 24
    apples_agent-3_mean: 8.22
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.37
    apples_agent-4_min: 2
    apples_agent-5_max: 15
    apples_agent-5_mean: 3.77
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 17
    cleaning_beam_agent-0_mean: 4.84
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 249
    cleaning_beam_agent-1_mean: 210.06
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 3.78
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 8.58
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 2.82
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 4.72
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-18-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 164.0
  episode_reward_mean: 121.12
  episode_reward_min: 77.0
  episodes_this_iter: 96
  episodes_total: 74880
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11854.333
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2514756917953491
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006516514113172889
        model: {}
        policy_loss: -0.0014923046110197902
        total_loss: -0.001823308295570314
        vf_explained_var: 0.0052054524421691895
        vf_loss: 1.1159557104110718
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21783363819122314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008792992448434234
        model: {}
        policy_loss: -0.0013469012919813395
        total_loss: -0.0016828528605401516
        vf_explained_var: 0.10059930384159088
        vf_loss: 0.4743562340736389
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3887373208999634
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012176698073744774
        model: {}
        policy_loss: -0.0017854338511824608
        total_loss: -0.002112193964421749
        vf_explained_var: 0.01763804256916046
        vf_loss: 3.574190139770508
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48205751180648804
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008795588510110974
        model: {}
        policy_loss: -0.001324022188782692
        total_loss: -0.002006819471716881
        vf_explained_var: -0.006865113973617554
        vf_loss: 1.6562480926513672
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4271696209907532
        entropy_coeff: 0.0017600000137463212
        kl: 0.00045650862739421427
        model: {}
        policy_loss: -0.0012278369395062327
        total_loss: -0.001880762865766883
        vf_explained_var: 0.004087105393409729
        vf_loss: 0.9888975620269775
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5912275910377502
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015042796730995178
        model: {}
        policy_loss: -0.0016931642312556505
        total_loss: -0.0026245564222335815
        vf_explained_var: 0.0027036666870117188
        vf_loss: 1.0917115211486816
    load_time_ms: 14089.911
    num_steps_sampled: 74880000
    num_steps_trained: 74880000
    sample_time_ms: 118761.74
    update_time_ms: 16.453
  iterations_since_restore: 130
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.970243902439023
    ram_util_percent: 12.441951219512193
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 18.0
    agent-2: 59.0
    agent-3: 32.0
    agent-4: 26.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 16.11
    agent-1: 8.84
    agent-2: 43.14
    agent-3: 22.49
    agent-4: 14.45
    agent-5: 16.09
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 30.0
    agent-3: 9.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.21065752508863
    mean_inference_ms: 15.096540801856722
    mean_processing_ms: 73.4809171489953
  time_since_restore: 18930.577699184418
  time_this_iter_s: 143.9409465789795
  time_total_s: 107867.25582885742
  timestamp: 1637381915
  timesteps_since_restore: 12480000
  timesteps_this_iter: 96000
  timesteps_total: 74880000
  training_iteration: 780
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    780 |           107867 | 74880000 |   121.12 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 6.37
    apples_agent-0_min: 2
    apples_agent-1_max: 12
    apples_agent-1_mean: 2.79
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 28.58
    apples_agent-2_min: 13
    apples_agent-3_max: 27
    apples_agent-3_mean: 8.77
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 9.92
    apples_agent-4_min: 2
    apples_agent-5_max: 14
    apples_agent-5_mean: 3.73
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 18
    cleaning_beam_agent-0_mean: 5.05
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 258
    cleaning_beam_agent-1_mean: 215.32
    cleaning_beam_agent-1_min: 180
    cleaning_beam_agent-2_max: 8
    cleaning_beam_agent-2_mean: 3.34
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 8.6
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 2.58
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.3
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-21-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 171.0
  episode_reward_mean: 120.64
  episode_reward_min: 67.0
  episodes_this_iter: 96
  episodes_total: 74976
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11854.078
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2444053590297699
        entropy_coeff: 0.0017600000137463212
        kl: 0.001046471414156258
        model: {}
        policy_loss: -0.0015995553694665432
        total_loss: -0.0019266237504780293
        vf_explained_var: 0.006307139992713928
        vf_loss: 1.030876636505127
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22191502153873444
        entropy_coeff: 0.0017600000137463212
        kl: 0.001071518287062645
        model: {}
        policy_loss: -0.0014230813831090927
        total_loss: -0.0017652339302003384
        vf_explained_var: 0.06693823635578156
        vf_loss: 0.484194815158844
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3814581036567688
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012701519299298525
        model: {}
        policy_loss: -0.0016338321147486567
        total_loss: -0.0019307777984067798
        vf_explained_var: 0.02922770380973816
        vf_loss: 3.744224786758423
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48492011427879333
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013394145062193274
        model: {}
        policy_loss: -0.0014783423393964767
        total_loss: -0.0021779099479317665
        vf_explained_var: 0.0005810558795928955
        vf_loss: 1.538905382156372
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4348125755786896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009011263609863818
        model: {}
        policy_loss: -0.0014306632801890373
        total_loss: -0.002089562825858593
        vf_explained_var: 0.010629802942276001
        vf_loss: 1.063741683959961
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5782299637794495
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016294388333335519
        model: {}
        policy_loss: -0.0016352944076061249
        total_loss: -0.0025258539244532585
        vf_explained_var: 0.019840553402900696
        vf_loss: 1.2712424993515015
    load_time_ms: 14100.497
    num_steps_sampled: 74976000
    num_steps_trained: 74976000
    sample_time_ms: 118775.263
    update_time_ms: 16.135
  iterations_since_restore: 131
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.731904761904765
    ram_util_percent: 12.387142857142855
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 15.0
    agent-2: 61.0
    agent-3: 32.0
    agent-4: 27.0
    agent-5: 26.0
  policy_reward_mean:
    agent-0: 15.55
    agent-1: 8.13
    agent-2: 43.86
    agent-3: 21.32
    agent-4: 14.46
    agent-5: 17.32
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 21.0
    agent-3: 5.0
    agent-4: -34.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.21068096194605
    mean_inference_ms: 15.09654669227117
    mean_processing_ms: 73.4788742663025
  time_since_restore: 19075.619965076447
  time_this_iter_s: 145.0422658920288
  time_total_s: 108012.29809474945
  timestamp: 1637382062
  timesteps_since_restore: 12576000
  timesteps_this_iter: 96000
  timesteps_total: 74976000
  training_iteration: 781
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    781 |           108012 | 74976000 |   120.64 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 7.28
    apples_agent-0_min: 1
    apples_agent-1_max: 11
    apples_agent-1_mean: 3.12
    apples_agent-1_min: 0
    apples_agent-2_max: 40
    apples_agent-2_mean: 28.91
    apples_agent-2_min: 17
    apples_agent-3_max: 28
    apples_agent-3_mean: 7.95
    apples_agent-3_min: 2
    apples_agent-4_max: 22
    apples_agent-4_mean: 9.96
    apples_agent-4_min: 2
    apples_agent-5_max: 16
    apples_agent-5_mean: 3.23
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 25
    cleaning_beam_agent-0_mean: 5.69
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 241
    cleaning_beam_agent-1_mean: 207.38
    cleaning_beam_agent-1_min: 171
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 3.21
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 27
    cleaning_beam_agent-3_mean: 9.12
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 27
    cleaning_beam_agent-4_mean: 2.86
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.92
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-23-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 173.0
  episode_reward_mean: 121.37
  episode_reward_min: 75.0
  episodes_this_iter: 96
  episodes_total: 75072
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11892.267
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2483898550271988
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010134548647329211
        model: {}
        policy_loss: -0.0016372358659282327
        total_loss: -0.0019673805218189955
        vf_explained_var: 0.007534310221672058
        vf_loss: 1.0702060461044312
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21195802092552185
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010429162066429853
        model: {}
        policy_loss: -0.0015167860547080636
        total_loss: -0.0018373914062976837
        vf_explained_var: 0.07076843082904816
        vf_loss: 0.5244143605232239
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37683233618736267
        entropy_coeff: 0.0017600000137463212
        kl: 0.001434109639376402
        model: {}
        policy_loss: -0.0014716173755005002
        total_loss: -0.001760703744366765
        vf_explained_var: 0.013582959771156311
        vf_loss: 3.741396903991699
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4895852506160736
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016398978186771274
        model: {}
        policy_loss: -0.001824534498155117
        total_loss: -0.0025381739251315594
        vf_explained_var: -0.007050991058349609
        vf_loss: 1.4802985191345215
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43993014097213745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010598563821986318
        model: {}
        policy_loss: -0.001571817323565483
        total_loss: -0.002248789183795452
        vf_explained_var: 0.01092231273651123
        vf_loss: 0.9730495810508728
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5883045792579651
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008998425328172743
        model: {}
        policy_loss: -0.0015864884480834007
        total_loss: -0.0025179423391819
        vf_explained_var: 0.019336313009262085
        vf_loss: 1.0396084785461426
    load_time_ms: 14087.443
    num_steps_sampled: 75072000
    num_steps_trained: 75072000
    sample_time_ms: 118845.903
    update_time_ms: 16.188
  iterations_since_restore: 132
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.96057692307692
    ram_util_percent: 12.381249999999998
  pid: 27065
  policy_reward_max:
    agent-0: 35.0
    agent-1: 16.0
    agent-2: 60.0
    agent-3: 34.0
    agent-4: 26.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 16.27
    agent-1: 8.93
    agent-2: 44.08
    agent-3: 20.94
    agent-4: 14.62
    agent-5: 16.53
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 28.0
    agent-3: 13.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.210663738660575
    mean_inference_ms: 15.096663668399144
    mean_processing_ms: 73.48050563234989
  time_since_restore: 19221.5768597126
  time_this_iter_s: 145.95689463615417
  time_total_s: 108158.2549893856
  timestamp: 1637382208
  timesteps_since_restore: 12672000
  timesteps_this_iter: 96000
  timesteps_total: 75072000
  training_iteration: 782
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    782 |           108158 | 75072000 |   121.37 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 6.6
    apples_agent-0_min: 1
    apples_agent-1_max: 13
    apples_agent-1_mean: 2.98
    apples_agent-1_min: 0
    apples_agent-2_max: 61
    apples_agent-2_mean: 28.21
    apples_agent-2_min: 6
    apples_agent-3_max: 42
    apples_agent-3_mean: 8.35
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 8.85
    apples_agent-4_min: 0
    apples_agent-5_max: 28
    apples_agent-5_mean: 3.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 23
    cleaning_beam_agent-0_mean: 4.96
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 237
    cleaning_beam_agent-1_mean: 203.22
    cleaning_beam_agent-1_min: 34
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 3.12
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 8.72
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.1
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 4.55
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-25-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 155.0
  episode_reward_mean: 117.57
  episode_reward_min: 27.0
  episodes_this_iter: 96
  episodes_total: 75168
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11887.448
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2492147833108902
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009548036614432931
        model: {}
        policy_loss: -0.001627627294510603
        total_loss: -0.0019682366400957108
        vf_explained_var: 0.0077804625034332275
        vf_loss: 0.9800885319709778
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21270163357257843
        entropy_coeff: 0.0017600000137463212
        kl: 0.000824226881377399
        model: {}
        policy_loss: -0.001429922878742218
        total_loss: -0.0017524219583719969
        vf_explained_var: 0.07405494153499603
        vf_loss: 0.5185745358467102
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3741239607334137
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015391067136079073
        model: {}
        policy_loss: -0.002086317865177989
        total_loss: -0.0024091878440231085
        vf_explained_var: 0.006450623273849487
        vf_loss: 3.355870246887207
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.486543208360672
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019073178991675377
        model: {}
        policy_loss: -0.0017576338723301888
        total_loss: -0.0024580643512308598
        vf_explained_var: -0.0034265220165252686
        vf_loss: 1.558880090713501
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4182623624801636
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015675474423915148
        model: {}
        policy_loss: -0.0015577119775116444
        total_loss: -0.002202242147177458
        vf_explained_var: 0.015088662505149841
        vf_loss: 0.9161105751991272
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5909385085105896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013249348849058151
        model: {}
        policy_loss: -0.001626770943403244
        total_loss: -0.0025587594136595726
        vf_explained_var: 0.026457160711288452
        vf_loss: 1.0806772708892822
    load_time_ms: 14089.182
    num_steps_sampled: 75168000
    num_steps_trained: 75168000
    sample_time_ms: 118841.756
    update_time_ms: 16.084
  iterations_since_restore: 133
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.97815533980583
    ram_util_percent: 12.3747572815534
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 18.0
    agent-2: 59.0
    agent-3: 46.0
    agent-4: 26.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 15.03
    agent-1: 8.4
    agent-2: 43.06
    agent-3: 21.72
    agent-4: 13.6
    agent-5: 15.76
  policy_reward_min:
    agent-0: 4.0
    agent-1: 2.0
    agent-2: 7.0
    agent-3: 5.0
    agent-4: 2.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.209745572680035
    mean_inference_ms: 15.096001259794063
    mean_processing_ms: 73.47733752293959
  time_since_restore: 19366.16100859642
  time_this_iter_s: 144.58414888381958
  time_total_s: 108302.83913826942
  timestamp: 1637382353
  timesteps_since_restore: 12768000
  timesteps_this_iter: 96000
  timesteps_total: 75168000
  training_iteration: 783
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    783 |           108303 | 75168000 |   117.57 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 6.44
    apples_agent-0_min: 1
    apples_agent-1_max: 13
    apples_agent-1_mean: 3.25
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 28.27
    apples_agent-2_min: 16
    apples_agent-3_max: 69
    apples_agent-3_mean: 9.55
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 9.94
    apples_agent-4_min: 3
    apples_agent-5_max: 16
    apples_agent-5_mean: 3.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 28
    cleaning_beam_agent-0_mean: 5.45
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 261
    cleaning_beam_agent-1_mean: 208.77
    cleaning_beam_agent-1_min: 161
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 3.49
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 22
    cleaning_beam_agent-3_mean: 8.25
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.06
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.99
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-28-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 175.0
  episode_reward_mean: 120.62
  episode_reward_min: 83.0
  episodes_this_iter: 96
  episodes_total: 75264
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11874.57
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2541237473487854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005427864380180836
        model: {}
        policy_loss: -0.0016236724331974983
        total_loss: -0.0019629397429525852
        vf_explained_var: 0.006907358765602112
        vf_loss: 1.0799177885055542
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21510741114616394
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011112176580354571
        model: {}
        policy_loss: -0.0015241773799061775
        total_loss: -0.0018510343506932259
        vf_explained_var: 0.06988650560379028
        vf_loss: 0.5172886848449707
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3750420808792114
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010706032626330853
        model: {}
        policy_loss: -0.0015587974339723587
        total_loss: -0.0018561137840151787
        vf_explained_var: 0.027737990021705627
        vf_loss: 3.627533197402954
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.481243371963501
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015915402909740806
        model: {}
        policy_loss: -0.001420509535819292
        total_loss: -0.002112119924277067
        vf_explained_var: 0.004408881068229675
        vf_loss: 1.5537679195404053
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40427452325820923
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008767381077632308
        model: {}
        policy_loss: -0.001074114814400673
        total_loss: -0.0016790283843874931
        vf_explained_var: 0.010612308979034424
        vf_loss: 1.066098690032959
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5746947526931763
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013778435532003641
        model: {}
        policy_loss: -0.0016279318369925022
        total_loss: -0.002521784510463476
        vf_explained_var: 0.01886516809463501
        vf_loss: 1.1761184930801392
    load_time_ms: 14087.851
    num_steps_sampled: 75264000
    num_steps_trained: 75264000
    sample_time_ms: 118882.277
    update_time_ms: 16.369
  iterations_since_restore: 134
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.998058252427185
    ram_util_percent: 12.450485436893205
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 18.0
    agent-2: 66.0
    agent-3: 33.0
    agent-4: 27.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.34
    agent-1: 8.73
    agent-2: 43.01
    agent-3: 21.94
    agent-4: 15.03
    agent-5: 16.57
  policy_reward_min:
    agent-0: 4.0
    agent-1: 2.0
    agent-2: 28.0
    agent-3: 12.0
    agent-4: 7.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.209457841481157
    mean_inference_ms: 15.09532347958988
    mean_processing_ms: 73.47576253557327
  time_since_restore: 19510.94251561165
  time_this_iter_s: 144.78150701522827
  time_total_s: 108447.62064528465
  timestamp: 1637382498
  timesteps_since_restore: 12864000
  timesteps_this_iter: 96000
  timesteps_total: 75264000
  training_iteration: 784
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    784 |           108448 | 75264000 |   120.62 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 8.28
    apples_agent-0_min: 0
    apples_agent-1_max: 47
    apples_agent-1_mean: 3.5
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 28.52
    apples_agent-2_min: 15
    apples_agent-3_max: 46
    apples_agent-3_mean: 9.45
    apples_agent-3_min: 2
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.1
    apples_agent-4_min: 1
    apples_agent-5_max: 35
    apples_agent-5_mean: 4.39
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 5.08
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 259
    cleaning_beam_agent-1_mean: 213.57
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 2.96
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 25
    cleaning_beam_agent-3_mean: 8.45
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.17
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.04
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-30-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 172.0
  episode_reward_mean: 124.34
  episode_reward_min: 86.0
  episodes_this_iter: 96
  episodes_total: 75360
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11875.681
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26331672072410583
        entropy_coeff: 0.0017600000137463212
        kl: 0.001342866220511496
        model: {}
        policy_loss: -0.0017683207988739014
        total_loss: -0.0021223919466137886
        vf_explained_var: 0.006116732954978943
        vf_loss: 1.0936763286590576
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2141890823841095
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008932322962209582
        model: {}
        policy_loss: -0.0015325052663683891
        total_loss: -0.0018556143622845411
        vf_explained_var: 0.06925533711910248
        vf_loss: 0.538671612739563
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36708956956863403
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015552896074950695
        model: {}
        policy_loss: -0.0020359610207378864
        total_loss: -0.002305507892742753
        vf_explained_var: 0.022275790572166443
        vf_loss: 3.7653098106384277
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4860285520553589
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016005703946575522
        model: {}
        policy_loss: -0.0017268283991143107
        total_loss: -0.0024117836728692055
        vf_explained_var: -0.0012030303478240967
        vf_loss: 1.704547643661499
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39395302534103394
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006565453950315714
        model: {}
        policy_loss: -0.0012349393218755722
        total_loss: -0.0018198387697339058
        vf_explained_var: 0.013188406825065613
        vf_loss: 1.0845670700073242
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5577002763748169
        entropy_coeff: 0.0017600000137463212
        kl: 0.001390294637531042
        model: {}
        policy_loss: -0.001493370858952403
        total_loss: -0.0023580300621688366
        vf_explained_var: 0.011693045496940613
        vf_loss: 1.1689338684082031
    load_time_ms: 14085.929
    num_steps_sampled: 75360000
    num_steps_trained: 75360000
    sample_time_ms: 118882.411
    update_time_ms: 16.094
  iterations_since_restore: 135
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.07487922705314
    ram_util_percent: 12.389855072463767
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 19.0
    agent-2: 65.0
    agent-3: 38.0
    agent-4: 28.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 15.82
    agent-1: 8.62
    agent-2: 44.76
    agent-3: 22.44
    agent-4: 15.97
    agent-5: 16.73
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 26.0
    agent-3: 11.0
    agent-4: 4.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.20970856518923
    mean_inference_ms: 15.094843390993056
    mean_processing_ms: 73.47397910126499
  time_since_restore: 19656.042040109634
  time_this_iter_s: 145.09952449798584
  time_total_s: 108592.72016978264
  timestamp: 1637382643
  timesteps_since_restore: 12960000
  timesteps_this_iter: 96000
  timesteps_total: 75360000
  training_iteration: 785
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    785 |           108593 | 75360000 |   124.34 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 7.18
    apples_agent-0_min: 1
    apples_agent-1_max: 10
    apples_agent-1_mean: 2.87
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 29.63
    apples_agent-2_min: 15
    apples_agent-3_max: 60
    apples_agent-3_mean: 8.0
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.71
    apples_agent-4_min: 4
    apples_agent-5_max: 44
    apples_agent-5_mean: 4.56
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 23
    cleaning_beam_agent-0_mean: 4.33
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 270
    cleaning_beam_agent-1_mean: 216.33
    cleaning_beam_agent-1_min: 166
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 3.42
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 18
    cleaning_beam_agent-3_mean: 7.78
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.06
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.29
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-33-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 171.0
  episode_reward_mean: 124.34
  episode_reward_min: 87.0
  episodes_this_iter: 96
  episodes_total: 75456
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11901.401
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2632864713668823
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007973178289830685
        model: {}
        policy_loss: -0.001489169429987669
        total_loss: -0.0018457518890500069
        vf_explained_var: 0.008304089307785034
        vf_loss: 1.0680043697357178
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21711096167564392
        entropy_coeff: 0.0017600000137463212
        kl: 0.001430769101716578
        model: {}
        policy_loss: -0.0015737416688352823
        total_loss: -0.001903096097521484
        vf_explained_var: 0.06882326304912567
        vf_loss: 0.5275996327400208
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.371438592672348
        entropy_coeff: 0.0017600000137463212
        kl: 0.001328248530626297
        model: {}
        policy_loss: -0.001891275867819786
        total_loss: -0.0021633580327033997
        vf_explained_var: 0.021791011095046997
        vf_loss: 3.816490650177002
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4779808223247528
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014581194845959544
        model: {}
        policy_loss: -0.0013927090913057327
        total_loss: -0.00207336712628603
        vf_explained_var: -0.001130133867263794
        vf_loss: 1.6058801412582397
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38502275943756104
        entropy_coeff: 0.0017600000137463212
        kl: 0.000772081024479121
        model: {}
        policy_loss: -0.001117686741054058
        total_loss: -0.001691773533821106
        vf_explained_var: 0.00731891393661499
        vf_loss: 1.0355360507965088
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5510327816009521
        entropy_coeff: 0.0017600000137463212
        kl: 0.001086544943973422
        model: {}
        policy_loss: -0.0015049148350954056
        total_loss: -0.0023500919342041016
        vf_explained_var: 0.01872175931930542
        vf_loss: 1.2464345693588257
    load_time_ms: 14102.908
    num_steps_sampled: 75456000
    num_steps_trained: 75456000
    sample_time_ms: 118928.282
    update_time_ms: 16.335
  iterations_since_restore: 136
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.01923076923077
    ram_util_percent: 12.38028846153846
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 18.0
    agent-2: 66.0
    agent-3: 41.0
    agent-4: 26.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 15.72
    agent-1: 8.41
    agent-2: 45.02
    agent-3: 21.66
    agent-4: 15.5
    agent-5: 18.03
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 29.0
    agent-3: 11.0
    agent-4: 7.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.2105239876835
    mean_inference_ms: 15.094822352979309
    mean_processing_ms: 73.47508049678183
  time_since_restore: 19802.073467969894
  time_this_iter_s: 146.03142786026
  time_total_s: 108738.7515976429
  timestamp: 1637382789
  timesteps_since_restore: 13056000
  timesteps_this_iter: 96000
  timesteps_total: 75456000
  training_iteration: 786
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    786 |           108739 | 75456000 |   124.34 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 6.5
    apples_agent-0_min: 1
    apples_agent-1_max: 24
    apples_agent-1_mean: 3.06
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 28.5
    apples_agent-2_min: 8
    apples_agent-3_max: 24
    apples_agent-3_mean: 8.53
    apples_agent-3_min: 1
    apples_agent-4_max: 25
    apples_agent-4_mean: 9.76
    apples_agent-4_min: 3
    apples_agent-5_max: 26
    apples_agent-5_mean: 3.76
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 3.45
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 263
    cleaning_beam_agent-1_mean: 206.68
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 39
    cleaning_beam_agent-2_mean: 3.64
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 22
    cleaning_beam_agent-3_mean: 8.39
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 15
    cleaning_beam_agent-4_mean: 3.27
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.47
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-35-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 168.0
  episode_reward_mean: 119.83
  episode_reward_min: 72.0
  episodes_this_iter: 96
  episodes_total: 75552
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11948.211
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2618876099586487
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011875631753355265
        model: {}
        policy_loss: -0.0016845744103193283
        total_loss: -0.002044165972620249
        vf_explained_var: 0.00688539445400238
        vf_loss: 1.013310194015503
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21219877898693085
        entropy_coeff: 0.0017600000137463212
        kl: 0.000705295242369175
        model: {}
        policy_loss: -0.0013069170527160168
        total_loss: -0.0016322373412549496
        vf_explained_var: 0.07858887314796448
        vf_loss: 0.48147356510162354
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37373119592666626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011828416027128696
        model: {}
        policy_loss: -0.0015841086860746145
        total_loss: -0.0018717749044299126
        vf_explained_var: 0.018545404076576233
        vf_loss: 3.7010159492492676
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4822932183742523
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016915351152420044
        model: {}
        policy_loss: -0.0016345186159014702
        total_loss: -0.0023180879652500153
        vf_explained_var: 0.0034593045711517334
        vf_loss: 1.6526645421981812
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38555708527565
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004059378115925938
        model: {}
        policy_loss: -0.0010022998321801424
        total_loss: -0.001583425560966134
        vf_explained_var: 0.014957696199417114
        vf_loss: 0.9745131134986877
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5460938215255737
        entropy_coeff: 0.0017600000137463212
        kl: 0.001776507357135415
        model: {}
        policy_loss: -0.001735157798975706
        total_loss: -0.002578150946646929
        vf_explained_var: 0.020853698253631592
        vf_loss: 1.181322455406189
    load_time_ms: 14124.194
    num_steps_sampled: 75552000
    num_steps_trained: 75552000
    sample_time_ms: 118969.965
    update_time_ms: 15.758
  iterations_since_restore: 137
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.096634615384616
    ram_util_percent: 12.449519230769226
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 19.0
    agent-2: 69.0
    agent-3: 36.0
    agent-4: 27.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 15.51
    agent-1: 8.1
    agent-2: 43.46
    agent-3: 21.21
    agent-4: 14.82
    agent-5: 16.73
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 16.0
    agent-3: 6.0
    agent-4: 3.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.212107050583832
    mean_inference_ms: 15.095959191635165
    mean_processing_ms: 73.47849167061206
  time_since_restore: 19948.522571086884
  time_this_iter_s: 146.44910311698914
  time_total_s: 108885.20070075989
  timestamp: 1637382936
  timesteps_since_restore: 13152000
  timesteps_this_iter: 96000
  timesteps_total: 75552000
  training_iteration: 787
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    787 |           108885 | 75552000 |   119.83 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 7.66
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 3.74
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 27.89
    apples_agent-2_min: 16
    apples_agent-3_max: 48
    apples_agent-3_mean: 8.81
    apples_agent-3_min: 2
    apples_agent-4_max: 18
    apples_agent-4_mean: 9.45
    apples_agent-4_min: 3
    apples_agent-5_max: 20
    apples_agent-5_mean: 3.57
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 2.69
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 242
    cleaning_beam_agent-1_mean: 209.11
    cleaning_beam_agent-1_min: 157
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 2.77
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 7.86
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.35
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.99
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-38-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 152.0
  episode_reward_mean: 119.46
  episode_reward_min: 78.0
  episodes_this_iter: 96
  episodes_total: 75648
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11945.372
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26290857791900635
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008588650962337852
        model: {}
        policy_loss: -0.0017209385987371206
        total_loss: -0.0020844624377787113
        vf_explained_var: 0.0016310811042785645
        vf_loss: 0.9919753074645996
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2139940708875656
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017167048063129187
        model: {}
        policy_loss: -0.0017616285476833582
        total_loss: -0.002087386790663004
        vf_explained_var: 0.07724836468696594
        vf_loss: 0.5087376236915588
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3781449794769287
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008397395140491426
        model: {}
        policy_loss: -0.0017269074451178312
        total_loss: -0.0020439783111214638
        vf_explained_var: 0.014436095952987671
        vf_loss: 3.4846391677856445
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48075592517852783
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012331845937296748
        model: {}
        policy_loss: -0.0014675407437607646
        total_loss: -0.0021564664784818888
        vf_explained_var: -0.0009772628545761108
        vf_loss: 1.5720182657241821
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38426610827445984
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007627982413396239
        model: {}
        policy_loss: -0.0011436291970312595
        total_loss: -0.001716196071356535
        vf_explained_var: 0.0036347657442092896
        vf_loss: 1.0373847484588623
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.561792254447937
        entropy_coeff: 0.0017600000137463212
        kl: 0.001973206177353859
        model: {}
        policy_loss: -0.001723680179566145
        total_loss: -0.0025984500534832478
        vf_explained_var: 0.016222715377807617
        vf_loss: 1.1398811340332031
    load_time_ms: 14131.807
    num_steps_sampled: 75648000
    num_steps_trained: 75648000
    sample_time_ms: 119102.42
    update_time_ms: 15.66
  iterations_since_restore: 138
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.897115384615386
    ram_util_percent: 12.370673076923074
  pid: 27065
  policy_reward_max:
    agent-0: 38.0
    agent-1: 17.0
    agent-2: 62.0
    agent-3: 37.0
    agent-4: 31.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.05
    agent-1: 8.06
    agent-2: 42.9
    agent-3: 21.84
    agent-4: 14.97
    agent-5: 16.64
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 30.0
    agent-3: 7.0
    agent-4: 4.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.211936584561975
    mean_inference_ms: 15.096238117980455
    mean_processing_ms: 73.4783403324776
  time_since_restore: 20094.230699777603
  time_this_iter_s: 145.7081286907196
  time_total_s: 109030.90882945061
  timestamp: 1637383081
  timesteps_since_restore: 13248000
  timesteps_this_iter: 96000
  timesteps_total: 75648000
  training_iteration: 788
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    788 |           109031 | 75648000 |   119.46 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 7.35
    apples_agent-0_min: 1
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.29
    apples_agent-1_min: 0
    apples_agent-2_max: 59
    apples_agent-2_mean: 29.3
    apples_agent-2_min: 14
    apples_agent-3_max: 37
    apples_agent-3_mean: 8.65
    apples_agent-3_min: 2
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.07
    apples_agent-4_min: 2
    apples_agent-5_max: 16
    apples_agent-5_mean: 3.66
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 22
    cleaning_beam_agent-0_mean: 4.05
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 282
    cleaning_beam_agent-1_mean: 214.47
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 3.34
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 9.35
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.09
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.86
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-40-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 174.0
  episode_reward_mean: 124.72
  episode_reward_min: 70.0
  episodes_this_iter: 96
  episodes_total: 75744
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11951.045
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2576186954975128
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007755143451504409
        model: {}
        policy_loss: -0.0015568467788398266
        total_loss: -0.0018916300032287836
        vf_explained_var: 0.00705362856388092
        vf_loss: 1.186253547668457
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21300983428955078
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011648716172203422
        model: {}
        policy_loss: -0.0016842198092490435
        total_loss: -0.002003632253035903
        vf_explained_var: 0.0813036859035492
        vf_loss: 0.5548527240753174
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38185274600982666
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011512222699820995
        model: {}
        policy_loss: -0.001811580266803503
        total_loss: -0.0021091634407639503
        vf_explained_var: 0.016754284501075745
        vf_loss: 3.7447669506073
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4857473373413086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014804163947701454
        model: {}
        policy_loss: -0.0014139087870717049
        total_loss: -0.0021033864468336105
        vf_explained_var: -0.0036074817180633545
        vf_loss: 1.6543892621994019
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3801221251487732
        entropy_coeff: 0.0017600000137463212
        kl: 0.0003409521887078881
        model: {}
        policy_loss: -0.0011005388805642724
        total_loss: -0.0016500572673976421
        vf_explained_var: 0.013526037335395813
        vf_loss: 1.194962978363037
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5711387991905212
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014894891064614058
        model: {}
        policy_loss: -0.0016587385907769203
        total_loss: -0.002528098411858082
        vf_explained_var: 0.001698136329650879
        vf_loss: 1.3584578037261963
    load_time_ms: 14126.796
    num_steps_sampled: 75744000
    num_steps_trained: 75744000
    sample_time_ms: 119090.506
    update_time_ms: 15.698
  iterations_since_restore: 139
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.87149758454106
    ram_util_percent: 12.451690821256038
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 17.0
    agent-2: 69.0
    agent-3: 39.0
    agent-4: 32.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 16.48
    agent-1: 8.99
    agent-2: 44.17
    agent-3: 22.26
    agent-4: 15.83
    agent-5: 16.99
  policy_reward_min:
    agent-0: 4.0
    agent-1: 1.0
    agent-2: 26.0
    agent-3: 8.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.211413103444226
    mean_inference_ms: 15.095307893586721
    mean_processing_ms: 73.47472108982438
  time_since_restore: 20239.34456562996
  time_this_iter_s: 145.11386585235596
  time_total_s: 109176.02269530296
  timestamp: 1637383227
  timesteps_since_restore: 13344000
  timesteps_this_iter: 96000
  timesteps_total: 75744000
  training_iteration: 789
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    789 |           109176 | 75744000 |   124.72 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 7.62
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.04
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 29.05
    apples_agent-2_min: 10
    apples_agent-3_max: 38
    apples_agent-3_mean: 9.44
    apples_agent-3_min: 1
    apples_agent-4_max: 26
    apples_agent-4_mean: 10.19
    apples_agent-4_min: 2
    apples_agent-5_max: 46
    apples_agent-5_mean: 4.81
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 23
    cleaning_beam_agent-0_mean: 3.82
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 277
    cleaning_beam_agent-1_mean: 213.48
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 3.52
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 10.3
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.7
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.69
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-42-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 179.0
  episode_reward_mean: 124.66
  episode_reward_min: 64.0
  episodes_this_iter: 96
  episodes_total: 75840
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11988.973
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.25598275661468506
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005305894301272929
        model: {}
        policy_loss: -0.0014041506219655275
        total_loss: -0.0017385510727763176
        vf_explained_var: 0.006540432572364807
        vf_loss: 1.161314606666565
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21520107984542847
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006353704375214875
        model: {}
        policy_loss: -0.001392582431435585
        total_loss: -0.001720178872346878
        vf_explained_var: 0.09472854435443878
        vf_loss: 0.5115921497344971
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37676793336868286
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012126502115279436
        model: {}
        policy_loss: -0.0019923346117138863
        total_loss: -0.002262260764837265
        vf_explained_var: 0.014797762036323547
        vf_loss: 3.931866407394409
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4828091561794281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018886178731918335
        model: {}
        policy_loss: -0.0018992081750184298
        total_loss: -0.002574062207713723
        vf_explained_var: 0.002920791506767273
        vf_loss: 1.7488808631896973
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3828715980052948
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008093828801065683
        model: {}
        policy_loss: -0.0011950135231018066
        total_loss: -0.0017634080722928047
        vf_explained_var: 0.01247665286064148
        vf_loss: 1.0545885562896729
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5797249674797058
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008886652649380267
        model: {}
        policy_loss: -0.0016159971710294485
        total_loss: -0.0025095148012042046
        vf_explained_var: 0.012655258178710938
        vf_loss: 1.2679951190948486
    load_time_ms: 14148.218
    num_steps_sampled: 75840000
    num_steps_trained: 75840000
    sample_time_ms: 119219.118
    update_time_ms: 15.658
  iterations_since_restore: 140
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.93076923076923
    ram_util_percent: 12.457692307692305
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 15.0
    agent-2: 62.0
    agent-3: 35.0
    agent-4: 26.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 16.19
    agent-1: 8.72
    agent-2: 44.74
    agent-3: 22.31
    agent-4: 15.03
    agent-5: 17.67
  policy_reward_min:
    agent-0: 4.0
    agent-1: 3.0
    agent-2: 23.0
    agent-3: 11.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.211613406541357
    mean_inference_ms: 15.09526766092671
    mean_processing_ms: 73.47513934314028
  time_since_restore: 20385.167232513428
  time_this_iter_s: 145.82266688346863
  time_total_s: 109321.84536218643
  timestamp: 1637383373
  timesteps_since_restore: 13440000
  timesteps_this_iter: 96000
  timesteps_total: 75840000
  training_iteration: 790
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    790 |           109322 | 75840000 |   124.66 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 7.07
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 3.19
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 27.66
    apples_agent-2_min: 0
    apples_agent-3_max: 28
    apples_agent-3_mean: 8.2
    apples_agent-3_min: 1
    apples_agent-4_max: 25
    apples_agent-4_mean: 10.21
    apples_agent-4_min: 1
    apples_agent-5_max: 34
    apples_agent-5_mean: 4.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 22
    cleaning_beam_agent-0_mean: 4.61
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 257
    cleaning_beam_agent-1_mean: 209.44
    cleaning_beam_agent-1_min: 73
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 3.35
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 26
    cleaning_beam_agent-3_mean: 8.96
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 19
    cleaning_beam_agent-4_mean: 3.49
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.8
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-45-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 183.0
  episode_reward_mean: 120.07
  episode_reward_min: 54.0
  episodes_this_iter: 96
  episodes_total: 75936
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11979.155
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2589724659919739
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009467678610235453
        model: {}
        policy_loss: -0.0017362413927912712
        total_loss: -0.0020741503685712814
        vf_explained_var: 0.012788325548171997
        vf_loss: 1.1788337230682373
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21122409403324127
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007171986508183181
        model: {}
        policy_loss: -0.0013792463578283787
        total_loss: -0.0016977733466774225
        vf_explained_var: 0.08760249614715576
        vf_loss: 0.5322656631469727
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3802315890789032
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009172565769404173
        model: {}
        policy_loss: -0.0016535134054720402
        total_loss: -0.0019396278075873852
        vf_explained_var: 0.023079857230186462
        vf_loss: 3.830911159515381
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48314663767814636
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011764729861170053
        model: {}
        policy_loss: -0.0014612511731684208
        total_loss: -0.0021448987536132336
        vf_explained_var: 0.007737383246421814
        vf_loss: 1.6669037342071533
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3814312815666199
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006921091116964817
        model: {}
        policy_loss: -0.0013162605464458466
        total_loss: -0.0018784892745316029
        vf_explained_var: 0.010170191526412964
        vf_loss: 1.0908924341201782
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5942529439926147
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012632212601602077
        model: {}
        policy_loss: -0.0016152244061231613
        total_loss: -0.002538158558309078
        vf_explained_var: 0.036442890763282776
        vf_loss: 1.2294931411743164
    load_time_ms: 14140.625
    num_steps_sampled: 75936000
    num_steps_trained: 75936000
    sample_time_ms: 119228.543
    update_time_ms: 15.772
  iterations_since_restore: 141
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.751196172248804
    ram_util_percent: 12.364593301435406
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 23.0
    agent-2: 63.0
    agent-3: 36.0
    agent-4: 26.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 16.15
    agent-1: 8.66
    agent-2: 41.81
    agent-3: 21.72
    agent-4: 15.23
    agent-5: 16.5
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 0.0
    agent-3: 5.0
    agent-4: 7.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.211300725726016
    mean_inference_ms: 15.095560241299063
    mean_processing_ms: 73.47254866708592
  time_since_restore: 20530.16495513916
  time_this_iter_s: 144.99772262573242
  time_total_s: 109466.84308481216
  timestamp: 1637383520
  timesteps_since_restore: 13536000
  timesteps_this_iter: 96000
  timesteps_total: 75936000
  training_iteration: 791
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    791 |           109467 | 75936000 |   120.07 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 7.82
    apples_agent-0_min: 2
    apples_agent-1_max: 17
    apples_agent-1_mean: 3.19
    apples_agent-1_min: 0
    apples_agent-2_max: 62
    apples_agent-2_mean: 29.74
    apples_agent-2_min: 13
    apples_agent-3_max: 25
    apples_agent-3_mean: 8.67
    apples_agent-3_min: 1
    apples_agent-4_max: 27
    apples_agent-4_mean: 10.72
    apples_agent-4_min: 4
    apples_agent-5_max: 29
    apples_agent-5_mean: 4.54
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 19
    cleaning_beam_agent-0_mean: 4.25
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 261
    cleaning_beam_agent-1_mean: 217.51
    cleaning_beam_agent-1_min: 160
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 3.02
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 19
    cleaning_beam_agent-3_mean: 8.35
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 3.29
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.8
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-47-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 166.0
  episode_reward_mean: 124.76
  episode_reward_min: 71.0
  episodes_this_iter: 96
  episodes_total: 76032
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11927.078
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26043444871902466
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008967237081378698
        model: {}
        policy_loss: -0.0019789300858974457
        total_loss: -0.002332535805180669
        vf_explained_var: 0.005583405494689941
        vf_loss: 1.0475987195968628
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2163892537355423
        entropy_coeff: 0.0017600000137463212
        kl: 0.000838112027850002
        model: {}
        policy_loss: -0.0013435343280434608
        total_loss: -0.0016699228435754776
        vf_explained_var: 0.0733848363161087
        vf_loss: 0.5446023344993591
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3741895854473114
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009690509177744389
        model: {}
        policy_loss: -0.0015734974294900894
        total_loss: -0.0018780636601150036
        vf_explained_var: 0.020701661705970764
        vf_loss: 3.5400824546813965
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.475328266620636
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021581221371889114
        model: {}
        policy_loss: -0.001729315146803856
        total_loss: -0.002388147171586752
        vf_explained_var: -0.004533439874649048
        vf_loss: 1.777475357055664
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3824979066848755
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007319583091884851
        model: {}
        policy_loss: -0.001296978909522295
        total_loss: -0.0018625271040946245
        vf_explained_var: 0.012188509106636047
        vf_loss: 1.0764975547790527
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6022982597351074
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010260766139253974
        model: {}
        policy_loss: -0.0018666146788746119
        total_loss: -0.0028058842290192842
        vf_explained_var: 0.0045836567878723145
        vf_loss: 1.2077221870422363
    load_time_ms: 14127.774
    num_steps_sampled: 76032000
    num_steps_trained: 76032000
    sample_time_ms: 119272.207
    update_time_ms: 15.763
  iterations_since_restore: 142
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.846634615384616
    ram_util_percent: 12.452884615384612
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 19.0
    agent-2: 62.0
    agent-3: 37.0
    agent-4: 28.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.73
    agent-1: 8.56
    agent-2: 44.36
    agent-3: 22.94
    agent-4: 15.56
    agent-5: 17.61
  policy_reward_min:
    agent-0: 4.0
    agent-1: 1.0
    agent-2: 25.0
    agent-3: 8.0
    agent-4: 4.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.211329830664667
    mean_inference_ms: 15.095571426408512
    mean_processing_ms: 73.47063715630827
  time_since_restore: 20675.91285920143
  time_this_iter_s: 145.74790406227112
  time_total_s: 109612.59098887444
  timestamp: 1637383665
  timesteps_since_restore: 13632000
  timesteps_this_iter: 96000
  timesteps_total: 76032000
  training_iteration: 792
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    792 |           109613 | 76032000 |   124.76 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 7.7
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 3.69
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 28.96
    apples_agent-2_min: 7
    apples_agent-3_max: 40
    apples_agent-3_mean: 8.87
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 9.71
    apples_agent-4_min: 2
    apples_agent-5_max: 16
    apples_agent-5_mean: 3.99
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 3.99
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 259
    cleaning_beam_agent-1_mean: 214.36
    cleaning_beam_agent-1_min: 59
    cleaning_beam_agent-2_max: 9
    cleaning_beam_agent-2_mean: 2.8
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 25
    cleaning_beam_agent-3_mean: 9.38
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.53
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.67
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-50-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 158.0
  episode_reward_mean: 121.58
  episode_reward_min: 39.0
  episodes_this_iter: 96
  episodes_total: 76128
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11922.399
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2680406868457794
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007003331556916237
        model: {}
        policy_loss: -0.0017708172090351582
        total_loss: -0.0021409126929938793
        vf_explained_var: 0.007875680923461914
        vf_loss: 1.016558051109314
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21172510087490082
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007070288411341608
        model: {}
        policy_loss: -0.0013244915753602982
        total_loss: -0.0016354108229279518
        vf_explained_var: 0.07756990194320679
        vf_loss: 0.6171772480010986
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37189218401908875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008406636188738048
        model: {}
        policy_loss: -0.001378699205815792
        total_loss: -0.0016433782875537872
        vf_explained_var: 0.011759266257286072
        vf_loss: 3.898531675338745
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4873146712779999
        entropy_coeff: 0.0017600000137463212
        kl: 0.001441596308723092
        model: {}
        policy_loss: -0.0015261934604495764
        total_loss: -0.002211227547377348
        vf_explained_var: 0.0006250143051147461
        vf_loss: 1.726410984992981
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3827420473098755
        entropy_coeff: 0.0017600000137463212
        kl: 0.000533962796907872
        model: {}
        policy_loss: -0.0011307575041428208
        total_loss: -0.0017120531992986798
        vf_explained_var: 0.020079657435417175
        vf_loss: 0.9233202934265137
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5964764952659607
        entropy_coeff: 0.0017600000137463212
        kl: 0.002955133095383644
        model: {}
        policy_loss: -0.0020027540158480406
        total_loss: -0.002929546870291233
        vf_explained_var: 0.019137650728225708
        vf_loss: 1.2300705909729004
    load_time_ms: 14133.235
    num_steps_sampled: 76128000
    num_steps_trained: 76128000
    sample_time_ms: 119281.515
    update_time_ms: 15.948
  iterations_since_restore: 143
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.974271844660194
    ram_util_percent: 12.454368932038832
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 17.0
    agent-2: 64.0
    agent-3: 34.0
    agent-4: 25.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 15.79
    agent-1: 7.78
    agent-2: 43.67
    agent-3: 22.45
    agent-4: 14.68
    agent-5: 17.21
  policy_reward_min:
    agent-0: 6.0
    agent-1: -35.0
    agent-2: 12.0
    agent-3: 5.0
    agent-4: 6.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.211835333671342
    mean_inference_ms: 15.09510892346682
    mean_processing_ms: 73.46958586417645
  time_since_restore: 20820.597907543182
  time_this_iter_s: 144.6850483417511
  time_total_s: 109757.27603721619
  timestamp: 1637383810
  timesteps_since_restore: 13728000
  timesteps_this_iter: 96000
  timesteps_total: 76128000
  training_iteration: 793
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    793 |           109757 | 76128000 |   121.58 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 6.95
    apples_agent-0_min: 0
    apples_agent-1_max: 37
    apples_agent-1_mean: 3.35
    apples_agent-1_min: 0
    apples_agent-2_max: 60
    apples_agent-2_mean: 27.28
    apples_agent-2_min: 10
    apples_agent-3_max: 21
    apples_agent-3_mean: 8.03
    apples_agent-3_min: 1
    apples_agent-4_max: 24
    apples_agent-4_mean: 9.46
    apples_agent-4_min: 2
    apples_agent-5_max: 16
    apples_agent-5_mean: 3.79
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 22
    cleaning_beam_agent-0_mean: 4.84
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 275
    cleaning_beam_agent-1_mean: 208.96
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 3.51
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 27
    cleaning_beam_agent-3_mean: 8.88
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.16
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 3.8
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-52-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 171.0
  episode_reward_mean: 118.42
  episode_reward_min: 70.0
  episodes_this_iter: 96
  episodes_total: 76224
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11945.864
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2670474946498871
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008414851617999375
        model: {}
        policy_loss: -0.0017913177143782377
        total_loss: -0.0021576047874987125
        vf_explained_var: 0.006984218955039978
        vf_loss: 1.0371662378311157
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21143415570259094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010505117243155837
        model: {}
        policy_loss: -0.0015520364977419376
        total_loss: -0.0018702116794884205
        vf_explained_var: 0.06669215857982635
        vf_loss: 0.5394750237464905
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.375253826379776
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010977096389979124
        model: {}
        policy_loss: -0.0017004916444420815
        total_loss: -0.0019955462776124477
        vf_explained_var: 0.02373291552066803
        vf_loss: 3.6539182662963867
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4876772165298462
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015072565292939544
        model: {}
        policy_loss: -0.0015220828354358673
        total_loss: -0.0022127614356577396
        vf_explained_var: -0.002986505627632141
        vf_loss: 1.6763416528701782
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38827672600746155
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007878143806010485
        model: {}
        policy_loss: -0.0012362711131572723
        total_loss: -0.0018163016065955162
        vf_explained_var: 0.0043158382177352905
        vf_loss: 1.033337116241455
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5715108513832092
        entropy_coeff: 0.0017600000137463212
        kl: 0.002568861935287714
        model: {}
        policy_loss: -0.0019484073854982853
        total_loss: -0.0028342311270534992
        vf_explained_var: 0.013092473149299622
        vf_loss: 1.2003839015960693
    load_time_ms: 14143.925
    num_steps_sampled: 76224000
    num_steps_trained: 76224000
    sample_time_ms: 119224.568
    update_time_ms: 15.848
  iterations_since_restore: 144
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.966019417475728
    ram_util_percent: 12.46019417475728
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 17.0
    agent-2: 63.0
    agent-3: 39.0
    agent-4: 26.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.53
    agent-1: 8.16
    agent-2: 42.76
    agent-3: 21.26
    agent-4: 14.15
    agent-5: 16.56
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 24.0
    agent-3: 6.0
    agent-4: 4.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.210550217497286
    mean_inference_ms: 15.094226410637392
    mean_processing_ms: 73.46637420924193
  time_since_restore: 20965.084055900574
  time_this_iter_s: 144.48614835739136
  time_total_s: 109901.76218557358
  timestamp: 1637383955
  timesteps_since_restore: 13824000
  timesteps_this_iter: 96000
  timesteps_total: 76224000
  training_iteration: 794
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    794 |           109902 | 76224000 |   118.42 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 6.88
    apples_agent-0_min: 1
    apples_agent-1_max: 34
    apples_agent-1_mean: 3.42
    apples_agent-1_min: 0
    apples_agent-2_max: 59
    apples_agent-2_mean: 28.18
    apples_agent-2_min: 6
    apples_agent-3_max: 29
    apples_agent-3_mean: 8.98
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 9.79
    apples_agent-4_min: 2
    apples_agent-5_max: 20
    apples_agent-5_mean: 3.73
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 23
    cleaning_beam_agent-0_mean: 4.9
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 208.87
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 3.74
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 9.23
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.51
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.06
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-55-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 185.0
  episode_reward_mean: 120.11
  episode_reward_min: 61.0
  episodes_this_iter: 96
  episodes_total: 76320
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11945.352
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2696022391319275
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005622326279990375
        model: {}
        policy_loss: -0.0013230596669018269
        total_loss: -0.0015520774759352207
        vf_explained_var: 0.007666409015655518
        vf_loss: 2.4548189640045166
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20964756608009338
        entropy_coeff: 0.0017600000137463212
        kl: 0.001051956438459456
        model: {}
        policy_loss: -0.0013170846505090594
        total_loss: -0.0016324815806001425
        vf_explained_var: 0.08770355582237244
        vf_loss: 0.5358356833457947
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3760634660720825
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013463583309203386
        model: {}
        policy_loss: -0.0017422360833734274
        total_loss: -0.0020372988656163216
        vf_explained_var: 0.033797621726989746
        vf_loss: 3.668088912963867
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48615217208862305
        entropy_coeff: 0.0017600000137463212
        kl: 0.001153887016698718
        model: {}
        policy_loss: -0.001491054892539978
        total_loss: -0.002171885222196579
        vf_explained_var: -0.0014935135841369629
        vf_loss: 1.7479976415634155
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3922646939754486
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005301197525113821
        model: {}
        policy_loss: -0.001161662396043539
        total_loss: -0.0017451141029596329
        vf_explained_var: 0.008902952075004578
        vf_loss: 1.0693503618240356
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5813183784484863
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016797335119917989
        model: {}
        policy_loss: -0.0017589475028216839
        total_loss: -0.0026652514934539795
        vf_explained_var: 0.01946994662284851
        vf_loss: 1.168144702911377
    load_time_ms: 14134.862
    num_steps_sampled: 76320000
    num_steps_trained: 76320000
    sample_time_ms: 119186.632
    update_time_ms: 15.984
  iterations_since_restore: 145
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.932038834951452
    ram_util_percent: 12.453883495145629
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 18.0
    agent-2: 67.0
    agent-3: 45.0
    agent-4: 30.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.11
    agent-1: 8.6
    agent-2: 43.18
    agent-3: 22.22
    agent-4: 14.57
    agent-5: 16.43
  policy_reward_min:
    agent-0: -30.0
    agent-1: 2.0
    agent-2: 13.0
    agent-3: 11.0
    agent-4: 3.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.20978509202697
    mean_inference_ms: 15.09305867796545
    mean_processing_ms: 73.4613314701954
  time_since_restore: 21109.787208795547
  time_this_iter_s: 144.70315289497375
  time_total_s: 110046.46533846855
  timestamp: 1637384100
  timesteps_since_restore: 13920000
  timesteps_this_iter: 96000
  timesteps_total: 76320000
  training_iteration: 795
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    795 |           110046 | 76320000 |   120.11 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 7.07
    apples_agent-0_min: 1
    apples_agent-1_max: 19
    apples_agent-1_mean: 3.4
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 28.84
    apples_agent-2_min: 17
    apples_agent-3_max: 35
    apples_agent-3_mean: 8.31
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 8.91
    apples_agent-4_min: 3
    apples_agent-5_max: 22
    apples_agent-5_mean: 4.25
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 22
    cleaning_beam_agent-0_mean: 4.61
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 255
    cleaning_beam_agent-1_mean: 213.02
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 3.46
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 21
    cleaning_beam_agent-3_mean: 8.1
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.68
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.57
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-57-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 177.0
  episode_reward_mean: 121.9
  episode_reward_min: 66.0
  episodes_this_iter: 96
  episodes_total: 76416
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11983.542
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26696208119392395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009119947208091617
        model: {}
        policy_loss: -0.0017282550688832998
        total_loss: -0.002096189884468913
        vf_explained_var: 0.009203344583511353
        vf_loss: 1.0191643238067627
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2107030302286148
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009723703842610121
        model: {}
        policy_loss: -0.0013594594784080982
        total_loss: -0.0016800544690340757
        vf_explained_var: 0.08483783900737762
        vf_loss: 0.502426028251648
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37406590580940247
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016557823400944471
        model: {}
        policy_loss: -0.002021646127104759
        total_loss: -0.002257987856864929
        vf_explained_var: 0.015036582946777344
        vf_loss: 4.220161437988281
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48024147748947144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015072913374751806
        model: {}
        policy_loss: -0.0014868439175188541
        total_loss: -0.0021553286351263523
        vf_explained_var: 0.004024311900138855
        vf_loss: 1.7673972845077515
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39262112975120544
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005140854045748711
        model: {}
        policy_loss: -0.0012303930707275867
        total_loss: -0.0018389595206826925
        vf_explained_var: 0.01712629199028015
        vf_loss: 0.82443767786026
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5637323260307312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015094125410541892
        model: {}
        policy_loss: -0.0017649512737989426
        total_loss: -0.0026333476416766644
        vf_explained_var: 0.014567956328392029
        vf_loss: 1.2377245426177979
    load_time_ms: 14115.433
    num_steps_sampled: 76416000
    num_steps_trained: 76416000
    sample_time_ms: 119067.861
    update_time_ms: 16.73
  iterations_since_restore: 146
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.99902912621359
    ram_util_percent: 12.375242718446604
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 22.0
    agent-2: 63.0
    agent-3: 35.0
    agent-4: 22.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 16.01
    agent-1: 8.43
    agent-2: 44.46
    agent-3: 21.99
    agent-4: 14.03
    agent-5: 16.98
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 24.0
    agent-3: 11.0
    agent-4: 5.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.208259340604073
    mean_inference_ms: 15.092457479669031
    mean_processing_ms: 73.46059410002272
  time_since_restore: 21254.786046266556
  time_this_iter_s: 144.9988374710083
  time_total_s: 110191.46417593956
  timestamp: 1637384245
  timesteps_since_restore: 14016000
  timesteps_this_iter: 96000
  timesteps_total: 76416000
  training_iteration: 796
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    796 |           110191 | 76416000 |    121.9 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 6.49
    apples_agent-0_min: 0
    apples_agent-1_max: 26
    apples_agent-1_mean: 3.73
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 28.08
    apples_agent-2_min: 13
    apples_agent-3_max: 39
    apples_agent-3_mean: 8.73
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.84
    apples_agent-4_min: 0
    apples_agent-5_max: 23
    apples_agent-5_mean: 3.6
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 47
    cleaning_beam_agent-0_mean: 3.78
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 264
    cleaning_beam_agent-1_mean: 219.25
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 2.92
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 9.85
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.45
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.15
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-19_23-59-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 123.54
  episode_reward_min: 69.0
  episodes_this_iter: 96
  episodes_total: 76512
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11933.203
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2706817090511322
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010053308214992285
        model: {}
        policy_loss: -0.001877729082480073
        total_loss: -0.0022431896068155766
        vf_explained_var: 0.00536133348941803
        vf_loss: 1.1093885898590088
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21842625737190247
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013815485872328281
        model: {}
        policy_loss: -0.0016222968697547913
        total_loss: -0.001956662628799677
        vf_explained_var: 0.08379849791526794
        vf_loss: 0.5006679892539978
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3750476539134979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016151204472407699
        model: {}
        policy_loss: -0.0018951380625367165
        total_loss: -0.002196591580286622
        vf_explained_var: 0.005955636501312256
        vf_loss: 3.5862975120544434
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47936561703681946
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009590138797648251
        model: {}
        policy_loss: -0.0012800681870430708
        total_loss: -0.0019512881990522146
        vf_explained_var: -0.004650399088859558
        vf_loss: 1.724625587463379
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39698702096939087
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006097761215642095
        model: {}
        policy_loss: -0.0012733633629977703
        total_loss: -0.0018619280308485031
        vf_explained_var: 0.0110863596200943
        vf_loss: 1.1013429164886475
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5645485520362854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011440296657383442
        model: {}
        policy_loss: -0.0017415967304259539
        total_loss: -0.002610983094200492
        vf_explained_var: 0.011355563998222351
        vf_loss: 1.2422151565551758
    load_time_ms: 14111.389
    num_steps_sampled: 76512000
    num_steps_trained: 76512000
    sample_time_ms: 118977.634
    update_time_ms: 16.607
  iterations_since_restore: 147
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.94106280193237
    ram_util_percent: 12.371980676328503
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 65.0
    agent-3: 38.0
    agent-4: 26.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 16.11
    agent-1: 8.67
    agent-2: 43.26
    agent-3: 22.62
    agent-4: 15.4
    agent-5: 17.48
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 24.0
    agent-3: 6.0
    agent-4: 3.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.208626324164015
    mean_inference_ms: 15.092215313640043
    mean_processing_ms: 73.45831819644019
  time_since_restore: 21399.888308763504
  time_this_iter_s: 145.10226249694824
  time_total_s: 110336.56643843651
  timestamp: 1637384390
  timesteps_since_restore: 14112000
  timesteps_this_iter: 96000
  timesteps_total: 76512000
  training_iteration: 797
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    797 |           110337 | 76512000 |   123.54 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 7.27
    apples_agent-0_min: 2
    apples_agent-1_max: 30
    apples_agent-1_mean: 3.47
    apples_agent-1_min: 0
    apples_agent-2_max: 80
    apples_agent-2_mean: 30.74
    apples_agent-2_min: 6
    apples_agent-3_max: 22
    apples_agent-3_mean: 8.19
    apples_agent-3_min: 2
    apples_agent-4_max: 26
    apples_agent-4_mean: 10.33
    apples_agent-4_min: 4
    apples_agent-5_max: 19
    apples_agent-5_mean: 3.63
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 30
    cleaning_beam_agent-0_mean: 3.75
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 276
    cleaning_beam_agent-1_mean: 224.27
    cleaning_beam_agent-1_min: 159
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 2.73
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 9.7
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.96
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.89
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-02-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 127.15
  episode_reward_min: 85.0
  episodes_this_iter: 96
  episodes_total: 76608
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11932.311
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2665066719055176
        entropy_coeff: 0.0017600000137463212
        kl: 0.000725731544662267
        model: {}
        policy_loss: -0.0015347725711762905
        total_loss: -0.0018845084123313427
        vf_explained_var: -0.0027687400579452515
        vf_loss: 1.193104863166809
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2207380086183548
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017382672522217035
        model: {}
        policy_loss: -0.0017847614362835884
        total_loss: -0.002118414267897606
        vf_explained_var: 0.07226510345935822
        vf_loss: 0.5484503507614136
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3685579299926758
        entropy_coeff: 0.0017600000137463212
        kl: 0.001210889546200633
        model: {}
        policy_loss: -0.0017184482421725988
        total_loss: -0.0019794730469584465
        vf_explained_var: 0.003189072012901306
        vf_loss: 3.8763461112976074
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4842716455459595
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013023719657212496
        model: {}
        policy_loss: -0.0015980768948793411
        total_loss: -0.0022994098253548145
        vf_explained_var: 0.007673665881156921
        vf_loss: 1.5098183155059814
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39712923765182495
        entropy_coeff: 0.0017600000137463212
        kl: 0.00040835855179466307
        model: {}
        policy_loss: -0.0011144822929054499
        total_loss: -0.0017055310308933258
        vf_explained_var: 0.009944751858711243
        vf_loss: 1.0789991617202759
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5535982847213745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017068227753043175
        model: {}
        policy_loss: -0.001796984113752842
        total_loss: -0.0026382161304354668
        vf_explained_var: 0.00616416335105896
        vf_loss: 1.3310233354568481
    load_time_ms: 14127.714
    num_steps_sampled: 76608000
    num_steps_trained: 76608000
    sample_time_ms: 118793.064
    update_time_ms: 16.727
  iterations_since_restore: 148
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.088834951456313
    ram_util_percent: 12.45533980582524
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 66.0
    agent-3: 39.0
    agent-4: 30.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 16.87
    agent-1: 8.76
    agent-2: 45.62
    agent-3: 22.51
    agent-4: 15.01
    agent-5: 18.38
  policy_reward_min:
    agent-0: 9.0
    agent-1: 1.0
    agent-2: 11.0
    agent-3: 11.0
    agent-4: 4.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.208699967744078
    mean_inference_ms: 15.091671310827591
    mean_processing_ms: 73.45369662723942
  time_since_restore: 21543.909707784653
  time_this_iter_s: 144.02139902114868
  time_total_s: 110480.58783745766
  timestamp: 1637384534
  timesteps_since_restore: 14208000
  timesteps_this_iter: 96000
  timesteps_total: 76608000
  training_iteration: 798
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    798 |           110481 | 76608000 |   127.15 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 6.69
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 3.01
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 28.56
    apples_agent-2_min: 11
    apples_agent-3_max: 31
    apples_agent-3_mean: 8.11
    apples_agent-3_min: 2
    apples_agent-4_max: 27
    apples_agent-4_mean: 9.5
    apples_agent-4_min: 1
    apples_agent-5_max: 19
    apples_agent-5_mean: 3.31
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 15
    cleaning_beam_agent-0_mean: 4.16
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 295
    cleaning_beam_agent-1_mean: 217.72
    cleaning_beam_agent-1_min: 78
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 2.94
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 26
    cleaning_beam_agent-3_mean: 8.6
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.66
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.52
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-04-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 156.0
  episode_reward_mean: 118.58
  episode_reward_min: 39.0
  episodes_this_iter: 96
  episodes_total: 76704
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11939.23
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26673901081085205
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009753609774634242
        model: {}
        policy_loss: -0.0016828905791044235
        total_loss: -0.002048828173428774
        vf_explained_var: 0.002179160714149475
        vf_loss: 1.0352249145507812
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21938295662403107
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012373097706586123
        model: {}
        policy_loss: -0.0014934111386537552
        total_loss: -0.0018286695703864098
        vf_explained_var: 0.07393030822277069
        vf_loss: 0.5085377097129822
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3677065372467041
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013622320257127285
        model: {}
        policy_loss: -0.001605879981070757
        total_loss: -0.0018622602801769972
        vf_explained_var: 0.01666112244129181
        vf_loss: 3.907820701599121
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4875761866569519
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012867882614955306
        model: {}
        policy_loss: -0.0014135409146547318
        total_loss: -0.0021151783876121044
        vf_explained_var: -0.0026617199182510376
        vf_loss: 1.5649853944778442
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40201854705810547
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011091496562585235
        model: {}
        policy_loss: -0.0013652141205966473
        total_loss: -0.0019802595488727093
        vf_explained_var: 0.00923629105091095
        vf_loss: 0.9250496625900269
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.565061092376709
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021097552962601185
        model: {}
        policy_loss: -0.001988630276173353
        total_loss: -0.0028667403385043144
        vf_explained_var: 0.015732884407043457
        vf_loss: 1.163995385169983
    load_time_ms: 14103.959
    num_steps_sampled: 76704000
    num_steps_trained: 76704000
    sample_time_ms: 118816.94
    update_time_ms: 16.605
  iterations_since_restore: 149
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.871014492753627
    ram_util_percent: 12.368599033816427
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 18.0
    agent-2: 65.0
    agent-3: 35.0
    agent-4: 26.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.17
    agent-1: 7.99
    agent-2: 43.51
    agent-3: 21.31
    agent-4: 14.01
    agent-5: 16.59
  policy_reward_min:
    agent-0: 3.0
    agent-1: 1.0
    agent-2: 13.0
    agent-3: 11.0
    agent-4: 3.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.209196108437574
    mean_inference_ms: 15.090787982872753
    mean_processing_ms: 73.451722137431
  time_since_restore: 21689.05371904373
  time_this_iter_s: 145.14401125907898
  time_total_s: 110625.73184871674
  timestamp: 1637384679
  timesteps_since_restore: 14304000
  timesteps_this_iter: 96000
  timesteps_total: 76704000
  training_iteration: 799
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    799 |           110626 | 76704000 |   118.58 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 6.64
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 3.29
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 27.42
    apples_agent-2_min: 2
    apples_agent-3_max: 29
    apples_agent-3_mean: 8.45
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.4
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 3.97
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 17
    cleaning_beam_agent-0_mean: 4.09
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 281
    cleaning_beam_agent-1_mean: 215.25
    cleaning_beam_agent-1_min: 23
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 2.97
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 27
    cleaning_beam_agent-3_mean: 8.56
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 15
    cleaning_beam_agent-4_mean: 3.11
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 3.51
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-07-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 156.0
  episode_reward_mean: 118.61
  episode_reward_min: 7.0
  episodes_this_iter: 96
  episodes_total: 76800
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11898.353
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26902663707733154
        entropy_coeff: 0.0017600000137463212
        kl: 0.001128736068494618
        model: {}
        policy_loss: -0.0017533448990434408
        total_loss: -0.00212661549448967
        vf_explained_var: 0.0071362704038619995
        vf_loss: 1.002138376235962
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21594490110874176
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009986822260543704
        model: {}
        policy_loss: -0.0015834704972803593
        total_loss: -0.0019093616865575314
        vf_explained_var: 0.09218981862068176
        vf_loss: 0.541717529296875
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3678082525730133
        entropy_coeff: 0.0017600000137463212
        kl: 0.001276958966627717
        model: {}
        policy_loss: -0.0017264774069190025
        total_loss: -0.0020199273712933064
        vf_explained_var: 0.026228025555610657
        vf_loss: 3.538945198059082
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47653666138648987
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019810486119240522
        model: {}
        policy_loss: -0.001779782585799694
        total_loss: -0.0024683750234544277
        vf_explained_var: -0.0012755542993545532
        vf_loss: 1.501118779182434
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40084654092788696
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005342483636923134
        model: {}
        policy_loss: -0.0012951227836310863
        total_loss: -0.0019029746763408184
        vf_explained_var: 0.010711684823036194
        vf_loss: 0.9764041900634766
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5676560401916504
        entropy_coeff: 0.0017600000137463212
        kl: 0.002129267668351531
        model: {}
        policy_loss: -0.0016940278001129627
        total_loss: -0.0025705560110509396
        vf_explained_var: 0.003935679793357849
        vf_loss: 1.2254602909088135
    load_time_ms: 14106.504
    num_steps_sampled: 76800000
    num_steps_trained: 76800000
    sample_time_ms: 118699.031
    update_time_ms: 16.52
  iterations_since_restore: 150
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.964390243902443
    ram_util_percent: 12.454146341463412
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 18.0
    agent-2: 61.0
    agent-3: 37.0
    agent-4: 24.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.32
    agent-1: 8.29
    agent-2: 42.53
    agent-3: 21.36
    agent-4: 14.24
    agent-5: 16.87
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 4.0
    agent-3: 1.0
    agent-4: 0.0
    agent-5: 2.0
  sampler_perf:
    mean_env_wait_ms: 28.208649825119345
    mean_inference_ms: 15.090261254473695
    mean_processing_ms: 73.44849729760398
  time_since_restore: 21833.35944724083
  time_this_iter_s: 144.30572819709778
  time_total_s: 110770.03757691383
  timestamp: 1637384824
  timesteps_since_restore: 14400000
  timesteps_this_iter: 96000
  timesteps_total: 76800000
  training_iteration: 800
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    800 |           110770 | 76800000 |   118.61 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 7.09
    apples_agent-0_min: 1
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.18
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 29.35
    apples_agent-2_min: 16
    apples_agent-3_max: 31
    apples_agent-3_mean: 8.61
    apples_agent-3_min: 2
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.79
    apples_agent-4_min: 3
    apples_agent-5_max: 24
    apples_agent-5_mean: 4.39
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 26
    cleaning_beam_agent-0_mean: 4.11
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 278
    cleaning_beam_agent-1_mean: 217.07
    cleaning_beam_agent-1_min: 151
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 2.97
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 25
    cleaning_beam_agent-3_mean: 9.74
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 14
    cleaning_beam_agent-4_mean: 3.23
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.17
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-09-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 166.0
  episode_reward_mean: 125.35
  episode_reward_min: 74.0
  episodes_this_iter: 96
  episodes_total: 76896
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11894.266
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2745758891105652
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007069635321386158
        model: {}
        policy_loss: -0.0015392053173854947
        total_loss: -0.0019038454629480839
        vf_explained_var: 0.0006448924541473389
        vf_loss: 1.1861252784729004
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2191827893257141
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006737645016983151
        model: {}
        policy_loss: -0.0014341049827635288
        total_loss: -0.00176942627876997
        vf_explained_var: 0.08266982436180115
        vf_loss: 0.5043927431106567
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37030014395713806
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011663527693599463
        model: {}
        policy_loss: -0.0016204970888793468
        total_loss: -0.0019121547229588032
        vf_explained_var: 0.03720349073410034
        vf_loss: 3.600684642791748
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48141157627105713
        entropy_coeff: 0.0017600000137463212
        kl: 0.00159649143461138
        model: {}
        policy_loss: -0.001535705290734768
        total_loss: -0.0022256234660744667
        vf_explained_var: 0.005877986550331116
        vf_loss: 1.5736896991729736
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39018183946609497
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009768593590706587
        model: {}
        policy_loss: -0.0013210251927375793
        total_loss: -0.0018913522362709045
        vf_explained_var: 0.008695334196090698
        vf_loss: 1.1639046669006348
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5641390085220337
        entropy_coeff: 0.0017600000137463212
        kl: 0.002039387822151184
        model: {}
        policy_loss: -0.0022404142655432224
        total_loss: -0.003110096789896488
        vf_explained_var: 0.013177037239074707
        vf_loss: 1.232039213180542
    load_time_ms: 14106.085
    num_steps_sampled: 76896000
    num_steps_trained: 76896000
    sample_time_ms: 118535.704
    update_time_ms: 16.49
  iterations_since_restore: 151
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.73623188405797
    ram_util_percent: 12.376328502415461
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 18.0
    agent-2: 65.0
    agent-3: 34.0
    agent-4: 28.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 16.16
    agent-1: 8.43
    agent-2: 44.97
    agent-3: 22.39
    agent-4: 15.53
    agent-5: 17.87
  policy_reward_min:
    agent-0: 4.0
    agent-1: 3.0
    agent-2: 26.0
    agent-3: 9.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.207361906969364
    mean_inference_ms: 15.089026127669358
    mean_processing_ms: 73.44180037483841
  time_since_restore: 21976.603125333786
  time_this_iter_s: 143.24367809295654
  time_total_s: 110913.28125500679
  timestamp: 1637384969
  timesteps_since_restore: 14496000
  timesteps_this_iter: 96000
  timesteps_total: 76896000
  training_iteration: 801
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    801 |           110913 | 76896000 |   125.35 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 6.97
    apples_agent-0_min: 1
    apples_agent-1_max: 13
    apples_agent-1_mean: 3.01
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 28.41
    apples_agent-2_min: 8
    apples_agent-3_max: 28
    apples_agent-3_mean: 8.45
    apples_agent-3_min: 2
    apples_agent-4_max: 19
    apples_agent-4_mean: 10.39
    apples_agent-4_min: 3
    apples_agent-5_max: 19
    apples_agent-5_mean: 4.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 70
    cleaning_beam_agent-0_mean: 4.4
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 262
    cleaning_beam_agent-1_mean: 213.15
    cleaning_beam_agent-1_min: 44
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 3.06
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 10.24
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.78
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.2
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-11-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 171.0
  episode_reward_mean: 122.29
  episode_reward_min: 42.0
  episodes_this_iter: 96
  episodes_total: 76992
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11891.992
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2755679488182068
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007890151464380324
        model: {}
        policy_loss: -0.0015655038878321648
        total_loss: -0.0019269264303147793
        vf_explained_var: 0.012043386697769165
        vf_loss: 1.2357914447784424
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21648024022579193
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006428061751648784
        model: {}
        policy_loss: -0.0012860968708992004
        total_loss: -0.0016208402812480927
        vf_explained_var: 0.09480330348014832
        vf_loss: 0.46263235807418823
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3736467957496643
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014426237903535366
        model: {}
        policy_loss: -0.00179310142993927
        total_loss: -0.00207229470834136
        vf_explained_var: 0.01989288628101349
        vf_loss: 3.784247398376465
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4869505763053894
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014044280396774411
        model: {}
        policy_loss: -0.0015517659485340118
        total_loss: -0.002247593831270933
        vf_explained_var: 0.008959084749221802
        vf_loss: 1.6120721101760864
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3865007162094116
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006593874422833323
        model: {}
        policy_loss: -0.0012827576138079166
        total_loss: -0.0018530432134866714
        vf_explained_var: 0.012928485870361328
        vf_loss: 1.0995923280715942
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5633478164672852
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017629610374569893
        model: {}
        policy_loss: -0.0016267383471131325
        total_loss: -0.0024782558903098106
        vf_explained_var: 0.02715352177619934
        vf_loss: 1.3997490406036377
    load_time_ms: 14126.328
    num_steps_sampled: 76992000
    num_steps_trained: 76992000
    sample_time_ms: 118458.035
    update_time_ms: 16.419
  iterations_since_restore: 152
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.857004830917862
    ram_util_percent: 12.365700483091787
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 14.0
    agent-2: 65.0
    agent-3: 33.0
    agent-4: 27.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 15.94
    agent-1: 7.67
    agent-2: 43.38
    agent-3: 22.04
    agent-4: 15.19
    agent-5: 18.07
  policy_reward_min:
    agent-0: -39.0
    agent-1: 1.0
    agent-2: 12.0
    agent-3: 6.0
    agent-4: 4.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.2070139240692
    mean_inference_ms: 15.088969949095386
    mean_processing_ms: 73.4372207879624
  time_since_restore: 22121.75086927414
  time_this_iter_s: 145.1477439403534
  time_total_s: 111058.42899894714
  timestamp: 1637385114
  timesteps_since_restore: 14592000
  timesteps_this_iter: 96000
  timesteps_total: 76992000
  training_iteration: 802
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    802 |           111058 | 76992000 |   122.29 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 7.34
    apples_agent-0_min: 1
    apples_agent-1_max: 30
    apples_agent-1_mean: 3.74
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 28.59
    apples_agent-2_min: 15
    apples_agent-3_max: 24
    apples_agent-3_mean: 7.79
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.19
    apples_agent-4_min: 3
    apples_agent-5_max: 18
    apples_agent-5_mean: 4.14
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 21
    cleaning_beam_agent-0_mean: 3.98
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 271
    cleaning_beam_agent-1_mean: 215.58
    cleaning_beam_agent-1_min: 170
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 3.17
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 10.54
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.85
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.08
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-14-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 171.0
  episode_reward_mean: 123.87
  episode_reward_min: 79.0
  episodes_this_iter: 96
  episodes_total: 77088
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11888.22
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27823445200920105
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008965367451310158
        model: {}
        policy_loss: -0.0019210223108530045
        total_loss: -0.0022942014038562775
        vf_explained_var: -0.00040328502655029297
        vf_loss: 1.1651413440704346
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21599873900413513
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006752996705472469
        model: {}
        policy_loss: -0.0013804775662720203
        total_loss: -0.0017057799268513918
        vf_explained_var: 0.08152732253074646
        vf_loss: 0.5485669374465942
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3705672025680542
        entropy_coeff: 0.0017600000137463212
        kl: 0.001613987609744072
        model: {}
        policy_loss: -0.0019641187973320484
        total_loss: -0.0022376305423676968
        vf_explained_var: 0.03615379333496094
        vf_loss: 3.7868731021881104
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48507070541381836
        entropy_coeff: 0.0017600000137463212
        kl: 0.002089402638375759
        model: {}
        policy_loss: -0.0019608866423368454
        total_loss: -0.0026669753715395927
        vf_explained_var: 0.001520693302154541
        vf_loss: 1.476358413696289
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3786262571811676
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005745314992964268
        model: {}
        policy_loss: -0.0012596407905220985
        total_loss: -0.0018235663883388042
        vf_explained_var: 0.0030165761709213257
        vf_loss: 1.02458655834198
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5532160401344299
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024560666643083096
        model: {}
        policy_loss: -0.0020214365795254707
        total_loss: -0.0028568448033183813
        vf_explained_var: 0.018553808331489563
        vf_loss: 1.3825314044952393
    load_time_ms: 14120.291
    num_steps_sampled: 77088000
    num_steps_trained: 77088000
    sample_time_ms: 118463.952
    update_time_ms: 16.346
  iterations_since_restore: 153
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.974757281553394
    ram_util_percent: 12.37621359223301
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 21.0
    agent-2: 63.0
    agent-3: 34.0
    agent-4: 28.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 16.91
    agent-1: 8.35
    agent-2: 43.82
    agent-3: 20.97
    agent-4: 15.44
    agent-5: 18.38
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 28.0
    agent-3: 9.0
    agent-4: 7.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.206700220354374
    mean_inference_ms: 15.08864557353864
    mean_processing_ms: 73.43494297730368
  time_since_restore: 22266.416499853134
  time_this_iter_s: 144.66563057899475
  time_total_s: 111203.09462952614
  timestamp: 1637385259
  timesteps_since_restore: 14688000
  timesteps_this_iter: 96000
  timesteps_total: 77088000
  training_iteration: 803
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    803 |           111203 | 77088000 |   123.87 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 7.04
    apples_agent-0_min: 2
    apples_agent-1_max: 34
    apples_agent-1_mean: 3.75
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 26.82
    apples_agent-2_min: 6
    apples_agent-3_max: 25
    apples_agent-3_mean: 8.26
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.34
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 4.4
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 28
    cleaning_beam_agent-0_mean: 4.58
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 208.16
    cleaning_beam_agent-1_min: 49
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 3.86
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 28
    cleaning_beam_agent-3_mean: 9.49
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.93
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.86
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-16-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 163.0
  episode_reward_mean: 121.25
  episode_reward_min: 34.0
  episodes_this_iter: 96
  episodes_total: 77184
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11870.86
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2796326279640198
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007846584776416421
        model: {}
        policy_loss: -0.00157852191478014
        total_loss: -0.0019511040300130844
        vf_explained_var: 0.000850379467010498
        vf_loss: 1.195692539215088
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21201756596565247
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010490983258932829
        model: {}
        policy_loss: -0.001707861665636301
        total_loss: -0.002023928100243211
        vf_explained_var: 0.08762887120246887
        vf_loss: 0.5708567500114441
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3731270134449005
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012300948146730661
        model: {}
        policy_loss: -0.0017442754469811916
        total_loss: -0.0020104944705963135
        vf_explained_var: 0.02449655532836914
        vf_loss: 3.9048209190368652
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4758126139640808
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018847302999347448
        model: {}
        policy_loss: -0.0015096557326614857
        total_loss: -0.0021798480302095413
        vf_explained_var: 0.0060158222913742065
        vf_loss: 1.6723829507827759
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3863409161567688
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007057720795273781
        model: {}
        policy_loss: -0.0013403662014752626
        total_loss: -0.0019078566692769527
        vf_explained_var: 0.008926868438720703
        vf_loss: 1.1246684789657593
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5420814752578735
        entropy_coeff: 0.0017600000137463212
        kl: 0.002093137940391898
        model: {}
        policy_loss: -0.0016460950719192624
        total_loss: -0.002471812767907977
        vf_explained_var: 0.001156643033027649
        vf_loss: 1.2834649085998535
    load_time_ms: 14096.401
    num_steps_sampled: 77184000
    num_steps_trained: 77184000
    sample_time_ms: 118532.304
    update_time_ms: 16.186
  iterations_since_restore: 154
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.950970873786407
    ram_util_percent: 12.448543689320386
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 19.0
    agent-2: 59.0
    agent-3: 39.0
    agent-4: 28.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 16.42
    agent-1: 9.13
    agent-2: 42.11
    agent-3: 22.2
    agent-4: 14.57
    agent-5: 16.82
  policy_reward_min:
    agent-0: 5.0
    agent-1: 1.0
    agent-2: 10.0
    agent-3: 1.0
    agent-4: 0.0
    agent-5: 2.0
  sampler_perf:
    mean_env_wait_ms: 28.20635773025479
    mean_inference_ms: 15.088216039429394
    mean_processing_ms: 73.43213429574695
  time_since_restore: 22411.164962530136
  time_this_iter_s: 144.74846267700195
  time_total_s: 111347.84309220314
  timestamp: 1637385404
  timesteps_since_restore: 14784000
  timesteps_this_iter: 96000
  timesteps_total: 77184000
  training_iteration: 804
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    804 |           111348 | 77184000 |   121.25 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 7.41
    apples_agent-0_min: 1
    apples_agent-1_max: 14
    apples_agent-1_mean: 3.1
    apples_agent-1_min: 0
    apples_agent-2_max: 60
    apples_agent-2_mean: 29.14
    apples_agent-2_min: 16
    apples_agent-3_max: 78
    apples_agent-3_mean: 9.16
    apples_agent-3_min: 2
    apples_agent-4_max: 25
    apples_agent-4_mean: 10.25
    apples_agent-4_min: 3
    apples_agent-5_max: 27
    apples_agent-5_mean: 5.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 23
    cleaning_beam_agent-0_mean: 5.45
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 263
    cleaning_beam_agent-1_mean: 214.74
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 2.82
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 27
    cleaning_beam_agent-3_mean: 9.01
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.82
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.23
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-19-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 172.0
  episode_reward_mean: 125.79
  episode_reward_min: 83.0
  episodes_this_iter: 96
  episodes_total: 77280
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11880.494
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.26955121755599976
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009445984032936394
        model: {}
        policy_loss: -0.0017696823924779892
        total_loss: -0.00212904904037714
        vf_explained_var: -0.0012063682079315186
        vf_loss: 1.1504137516021729
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21684563159942627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016087768599390984
        model: {}
        policy_loss: -0.0016046329401433468
        total_loss: -0.0019313306547701359
        vf_explained_var: 0.08145852386951447
        vf_loss: 0.5494879484176636
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3645368218421936
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006288561853580177
        model: {}
        policy_loss: -0.0014763178769499063
        total_loss: -0.0017493274062871933
        vf_explained_var: 0.02155393362045288
        vf_loss: 3.6857564449310303
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4785709083080292
        entropy_coeff: 0.0017600000137463212
        kl: 0.00158842618111521
        model: {}
        policy_loss: -0.0016256931703537703
        total_loss: -0.002290728036314249
        vf_explained_var: 0.014236345887184143
        vf_loss: 1.7724997997283936
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3840571641921997
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006945016793906689
        model: {}
        policy_loss: -0.0012254880275577307
        total_loss: -0.0017871172167360783
        vf_explained_var: 0.0076247453689575195
        vf_loss: 1.1431154012680054
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5459444522857666
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013699668925255537
        model: {}
        policy_loss: -0.0016044769436120987
        total_loss: -0.0024436460807919502
        vf_explained_var: 0.011850178241729736
        vf_loss: 1.2169182300567627
    load_time_ms: 14121.822
    num_steps_sampled: 77280000
    num_steps_trained: 77280000
    sample_time_ms: 118545.834
    update_time_ms: 16.314
  iterations_since_restore: 155
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.902898550724636
    ram_util_percent: 12.379227053140097
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 22.0
    agent-2: 63.0
    agent-3: 40.0
    agent-4: 27.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 16.32
    agent-1: 8.89
    agent-2: 44.1
    agent-3: 23.34
    agent-4: 15.78
    agent-5: 17.36
  policy_reward_min:
    agent-0: 4.0
    agent-1: 2.0
    agent-2: 30.0
    agent-3: 10.0
    agent-4: 4.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.207246905795
    mean_inference_ms: 15.087849858460777
    mean_processing_ms: 73.43026750626
  time_since_restore: 22556.31760621071
  time_this_iter_s: 145.1526436805725
  time_total_s: 111492.99573588371
  timestamp: 1637385549
  timesteps_since_restore: 14880000
  timesteps_this_iter: 96000
  timesteps_total: 77280000
  training_iteration: 805
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    805 |           111493 | 77280000 |   125.79 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 7.07
    apples_agent-0_min: 1
    apples_agent-1_max: 14
    apples_agent-1_mean: 3.04
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 29.1
    apples_agent-2_min: 8
    apples_agent-3_max: 78
    apples_agent-3_mean: 9.15
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.59
    apples_agent-4_min: 2
    apples_agent-5_max: 41
    apples_agent-5_mean: 5.25
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 27
    cleaning_beam_agent-0_mean: 5.43
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 256
    cleaning_beam_agent-1_mean: 220.43
    cleaning_beam_agent-1_min: 101
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 3.02
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 25
    cleaning_beam_agent-3_mean: 9.39
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 7
    cleaning_beam_agent-4_mean: 3.0
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.84
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-21-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 178.0
  episode_reward_mean: 126.15
  episode_reward_min: 58.0
  episodes_this_iter: 96
  episodes_total: 77376
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11826.439
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2776261568069458
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009837158722802997
        model: {}
        policy_loss: -0.001547768246382475
        total_loss: -0.0019166739657521248
        vf_explained_var: 0.009810060262680054
        vf_loss: 1.1971940994262695
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2165679931640625
        entropy_coeff: 0.0017600000137463212
        kl: 0.001343372045084834
        model: {}
        policy_loss: -0.0015826523303985596
        total_loss: -0.0019072312861680984
        vf_explained_var: 0.0862148106098175
        vf_loss: 0.5658136010169983
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3736981451511383
        entropy_coeff: 0.0017600000137463212
        kl: 0.000906905741430819
        model: {}
        policy_loss: -0.0015083637554198503
        total_loss: -0.0018035090761259198
        vf_explained_var: 0.021184459328651428
        vf_loss: 3.625614643096924
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47855865955352783
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014569233171641827
        model: {}
        policy_loss: -0.0016600554808974266
        total_loss: -0.0023416252806782722
        vf_explained_var: 0.0033555328845977783
        vf_loss: 1.6069231033325195
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38970130681991577
        entropy_coeff: 0.0017600000137463212
        kl: 0.00044235121458768845
        model: {}
        policy_loss: -0.0011803198140114546
        total_loss: -0.0017621093429625034
        vf_explained_var: 0.01582406461238861
        vf_loss: 1.0408375263214111
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5455586910247803
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013995770132169127
        model: {}
        policy_loss: -0.0016745100729167461
        total_loss: -0.0024945323821157217
        vf_explained_var: 0.017025291919708252
        vf_loss: 1.4016268253326416
    load_time_ms: 14119.136
    num_steps_sampled: 77376000
    num_steps_trained: 77376000
    sample_time_ms: 118561.184
    update_time_ms: 15.49
  iterations_since_restore: 156
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.980582524271846
    ram_util_percent: 12.449029126213592
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 19.0
    agent-2: 71.0
    agent-3: 34.0
    agent-4: 30.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 16.28
    agent-1: 8.95
    agent-2: 44.55
    agent-3: 21.8
    agent-4: 16.03
    agent-5: 18.54
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 17.0
    agent-3: 0.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.207009971695335
    mean_inference_ms: 15.08727219366815
    mean_processing_ms: 73.42791791377964
  time_since_restore: 22700.89019060135
  time_this_iter_s: 144.57258439064026
  time_total_s: 111637.56832027435
  timestamp: 1637385694
  timesteps_since_restore: 14976000
  timesteps_this_iter: 96000
  timesteps_total: 77376000
  training_iteration: 806
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    806 |           111638 | 77376000 |   126.15 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 6.78
    apples_agent-0_min: 2
    apples_agent-1_max: 14
    apples_agent-1_mean: 3.08
    apples_agent-1_min: 0
    apples_agent-2_max: 88
    apples_agent-2_mean: 29.93
    apples_agent-2_min: 9
    apples_agent-3_max: 24
    apples_agent-3_mean: 8.59
    apples_agent-3_min: 1
    apples_agent-4_max: 19
    apples_agent-4_mean: 10.18
    apples_agent-4_min: 2
    apples_agent-5_max: 17
    apples_agent-5_mean: 3.84
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 19
    cleaning_beam_agent-0_mean: 4.72
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 274
    cleaning_beam_agent-1_mean: 217.42
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 3.43
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 38
    cleaning_beam_agent-3_mean: 10.99
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 3.09
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.87
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-23-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 170.0
  episode_reward_mean: 123.36
  episode_reward_min: 49.0
  episodes_this_iter: 96
  episodes_total: 77472
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11834.573
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28455886244773865
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010383700719103217
        model: {}
        policy_loss: -0.0016576473135501146
        total_loss: -0.0020461075473576784
        vf_explained_var: 0.005793750286102295
        vf_loss: 1.1236292123794556
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21730487048625946
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015160483308136463
        model: {}
        policy_loss: -0.001747438684105873
        total_loss: -0.002077726647257805
        vf_explained_var: 0.07395684719085693
        vf_loss: 0.5217095017433167
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37053027749061584
        entropy_coeff: 0.0017600000137463212
        kl: 0.001160355401225388
        model: {}
        policy_loss: -0.0016211725305765867
        total_loss: -0.0019047665409743786
        vf_explained_var: 0.02069389820098877
        vf_loss: 3.685426712036133
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4759538173675537
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013024022337049246
        model: {}
        policy_loss: -0.0015810587210580707
        total_loss: -0.002257132902741432
        vf_explained_var: 0.0074694156646728516
        vf_loss: 1.6160144805908203
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3872094750404358
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006570022087544203
        model: {}
        policy_loss: -0.001243149396032095
        total_loss: -0.0018147416412830353
        vf_explained_var: 0.006162673234939575
        vf_loss: 1.0989844799041748
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5553667545318604
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011008944129571319
        model: {}
        policy_loss: -0.0014145267195999622
        total_loss: -0.0022745225578546524
        vf_explained_var: 0.01151151955127716
        vf_loss: 1.1745083332061768
    load_time_ms: 14108.494
    num_steps_sampled: 77472000
    num_steps_trained: 77472000
    sample_time_ms: 118519.666
    update_time_ms: 15.417
  iterations_since_restore: 157
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.980582524271846
    ram_util_percent: 12.373300970873787
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 18.0
    agent-2: 62.0
    agent-3: 35.0
    agent-4: 31.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.36
    agent-1: 8.57
    agent-2: 44.15
    agent-3: 22.64
    agent-4: 15.38
    agent-5: 17.26
  policy_reward_min:
    agent-0: 5.0
    agent-1: 3.0
    agent-2: 19.0
    agent-3: 5.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.206803136838126
    mean_inference_ms: 15.086604556575155
    mean_processing_ms: 73.42561132225981
  time_since_restore: 22845.50784778595
  time_this_iter_s: 144.61765718460083
  time_total_s: 111782.18597745895
  timestamp: 1637385839
  timesteps_since_restore: 15072000
  timesteps_this_iter: 96000
  timesteps_total: 77472000
  training_iteration: 807
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    807 |           111782 | 77472000 |   123.36 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 7.49
    apples_agent-0_min: 2
    apples_agent-1_max: 11
    apples_agent-1_mean: 3.12
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 29.34
    apples_agent-2_min: 11
    apples_agent-3_max: 24
    apples_agent-3_mean: 8.19
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.82
    apples_agent-4_min: 3
    apples_agent-5_max: 30
    apples_agent-5_mean: 4.4
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 3.67
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 270
    cleaning_beam_agent-1_mean: 219.66
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 3.13
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 25
    cleaning_beam_agent-3_mean: 8.96
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 2.8
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.28
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-26-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 168.0
  episode_reward_mean: 124.4
  episode_reward_min: 66.0
  episodes_this_iter: 96
  episodes_total: 77568
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11834.238
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2854841351509094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009522882173769176
        model: {}
        policy_loss: -0.0016893469728529453
        total_loss: -0.002077990211546421
        vf_explained_var: 0.006989791989326477
        vf_loss: 1.1380999088287354
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21863886713981628
        entropy_coeff: 0.0017600000137463212
        kl: 0.001674176543019712
        model: {}
        policy_loss: -0.0017732183914631605
        total_loss: -0.00210470799356699
        vf_explained_var: 0.07485343515872955
        vf_loss: 0.533179521560669
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36945295333862305
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009431698126718402
        model: {}
        policy_loss: -0.0015913464594632387
        total_loss: -0.0018886602483689785
        vf_explained_var: 0.016036182641983032
        vf_loss: 3.529238224029541
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47688013315200806
        entropy_coeff: 0.0017600000137463212
        kl: 0.00142074772156775
        model: {}
        policy_loss: -0.0013152952305972576
        total_loss: -0.0019870870746672153
        vf_explained_var: 0.015693962574005127
        vf_loss: 1.675145149230957
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3870639204978943
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008954406948760152
        model: {}
        policy_loss: -0.0012588876998052
        total_loss: -0.0018279869109392166
        vf_explained_var: 0.0130033940076828
        vf_loss: 1.1213312149047852
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.543610692024231
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017368640983477235
        model: {}
        policy_loss: -0.0017786873504519463
        total_loss: -0.0026187696494162083
        vf_explained_var: 0.003796696662902832
        vf_loss: 1.1667424440383911
    load_time_ms: 14104.091
    num_steps_sampled: 77568000
    num_steps_trained: 77568000
    sample_time_ms: 118603.639
    update_time_ms: 15.764
  iterations_since_restore: 158
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.025242718446602
    ram_util_percent: 12.44514563106796
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 15.0
    agent-2: 59.0
    agent-3: 37.0
    agent-4: 27.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 16.49
    agent-1: 8.2
    agent-2: 43.63
    agent-3: 22.27
    agent-4: 15.93
    agent-5: 17.88
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 22.0
    agent-3: 10.0
    agent-4: 5.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.206951600682046
    mean_inference_ms: 15.086176847596438
    mean_processing_ms: 73.4247040257079
  time_since_restore: 22990.32233452797
  time_this_iter_s: 144.81448674201965
  time_total_s: 111927.00046420097
  timestamp: 1637385984
  timesteps_since_restore: 15168000
  timesteps_this_iter: 96000
  timesteps_total: 77568000
  training_iteration: 808
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    808 |           111927 | 77568000 |    124.4 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 8.22
    apples_agent-0_min: 1
    apples_agent-1_max: 16
    apples_agent-1_mean: 2.97
    apples_agent-1_min: 0
    apples_agent-2_max: 67
    apples_agent-2_mean: 29.35
    apples_agent-2_min: 18
    apples_agent-3_max: 20
    apples_agent-3_mean: 8.01
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.37
    apples_agent-4_min: 2
    apples_agent-5_max: 30
    apples_agent-5_mean: 4.31
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 19
    cleaning_beam_agent-0_mean: 3.91
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 245
    cleaning_beam_agent-1_mean: 213.13
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 2.83
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 9.59
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 3.05
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.59
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-28-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 186.0
  episode_reward_mean: 124.36
  episode_reward_min: 60.0
  episodes_this_iter: 96
  episodes_total: 77664
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11817.185
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.290039986371994
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009023031452670693
        model: {}
        policy_loss: -0.0019068182446062565
        total_loss: -0.0023029064759612083
        vf_explained_var: -0.0012749433517456055
        vf_loss: 1.143834114074707
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2170664221048355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011956623056903481
        model: {}
        policy_loss: -0.0014362959191203117
        total_loss: -0.0017662858590483665
        vf_explained_var: 0.07972030341625214
        vf_loss: 0.5204439163208008
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.380256712436676
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010898788459599018
        model: {}
        policy_loss: -0.0018676442559808493
        total_loss: -0.002159744966775179
        vf_explained_var: 0.02948547899723053
        vf_loss: 3.771495819091797
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.482116162776947
        entropy_coeff: 0.0017600000137463212
        kl: 0.001977179665118456
        model: {}
        policy_loss: -0.0016784321051090956
        total_loss: -0.002347854431718588
        vf_explained_var: 0.01093512773513794
        vf_loss: 1.7910072803497314
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37348586320877075
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010500553762540221
        model: {}
        policy_loss: -0.001185121014714241
        total_loss: -0.0017241155728697777
        vf_explained_var: 0.006375715136528015
        vf_loss: 1.183397650718689
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5446462035179138
        entropy_coeff: 0.0017600000137463212
        kl: 0.00127468875143677
        model: {}
        policy_loss: -0.0017322944477200508
        total_loss: -0.0025603007525205612
        vf_explained_var: 0.014766409993171692
        vf_loss: 1.3057277202606201
    load_time_ms: 14118.923
    num_steps_sampled: 77664000
    num_steps_trained: 77664000
    sample_time_ms: 118578.005
    update_time_ms: 15.889
  iterations_since_restore: 159
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.915458937198068
    ram_util_percent: 12.37584541062802
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 17.0
    agent-2: 69.0
    agent-3: 39.0
    agent-4: 30.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 16.06
    agent-1: 8.36
    agent-2: 44.01
    agent-3: 22.48
    agent-4: 15.57
    agent-5: 17.88
  policy_reward_min:
    agent-0: 5.0
    agent-1: 1.0
    agent-2: 26.0
    agent-3: 2.0
    agent-4: 4.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.206573923808634
    mean_inference_ms: 15.085638155779002
    mean_processing_ms: 73.42398889817397
  time_since_restore: 23135.191687583923
  time_this_iter_s: 144.86935305595398
  time_total_s: 112071.86981725693
  timestamp: 1637386129
  timesteps_since_restore: 15264000
  timesteps_this_iter: 96000
  timesteps_total: 77664000
  training_iteration: 809
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    809 |           112072 | 77664000 |   124.36 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 7.47
    apples_agent-0_min: 1
    apples_agent-1_max: 22
    apples_agent-1_mean: 3.3
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 30.55
    apples_agent-2_min: 17
    apples_agent-3_max: 52
    apples_agent-3_mean: 8.52
    apples_agent-3_min: 2
    apples_agent-4_max: 26
    apples_agent-4_mean: 10.88
    apples_agent-4_min: 3
    apples_agent-5_max: 42
    apples_agent-5_mean: 5.12
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 23
    cleaning_beam_agent-0_mean: 4.37
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 273
    cleaning_beam_agent-1_mean: 224.09
    cleaning_beam_agent-1_min: 191
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 3.1
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 31
    cleaning_beam_agent-3_mean: 10.18
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 3.16
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 7
    cleaning_beam_agent-5_mean: 2.49
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-31-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 178.0
  episode_reward_mean: 128.88
  episode_reward_min: 88.0
  episodes_this_iter: 96
  episodes_total: 77760
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11832.763
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3033214807510376
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013561061350628734
        model: {}
        policy_loss: -0.0018314709886908531
        total_loss: -0.0022419518791139126
        vf_explained_var: 0.004908010363578796
        vf_loss: 1.2336704730987549
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21880969405174255
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010765937622636557
        model: {}
        policy_loss: -0.001566604245454073
        total_loss: -0.0018909871578216553
        vf_explained_var: 0.06697496771812439
        vf_loss: 0.6072343587875366
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3752635717391968
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012018397683277726
        model: {}
        policy_loss: -0.0017052581533789635
        total_loss: -0.0019976745825260878
        vf_explained_var: 0.02313438057899475
        vf_loss: 3.680478811264038
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4752562940120697
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018777521327137947
        model: {}
        policy_loss: -0.0014544446021318436
        total_loss: -0.0021262974478304386
        vf_explained_var: 0.007170483469963074
        vf_loss: 1.6459951400756836
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3789483308792114
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004174159257672727
        model: {}
        policy_loss: -0.001198401441797614
        total_loss: -0.001762129832059145
        vf_explained_var: 0.01336921751499176
        vf_loss: 1.0321844816207886
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5482444763183594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011599191930145025
        model: {}
        policy_loss: -0.00155263626947999
        total_loss: -0.002386411651968956
        vf_explained_var: 0.012248590588569641
        vf_loss: 1.3113142251968384
    load_time_ms: 14115.22
    num_steps_sampled: 77760000
    num_steps_trained: 77760000
    sample_time_ms: 118615.312
    update_time_ms: 16.168
  iterations_since_restore: 160
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.160194174757283
    ram_util_percent: 12.461650485436891
  pid: 27065
  policy_reward_max:
    agent-0: 33.0
    agent-1: 18.0
    agent-2: 71.0
    agent-3: 33.0
    agent-4: 26.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 17.74
    agent-1: 8.57
    agent-2: 45.3
    agent-3: 22.58
    agent-4: 15.87
    agent-5: 18.82
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 29.0
    agent-3: 11.0
    agent-4: 7.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.20607552925463
    mean_inference_ms: 15.085420836650268
    mean_processing_ms: 73.42143310754916
  time_since_restore: 23279.980674266815
  time_this_iter_s: 144.78898668289185
  time_total_s: 112216.65880393982
  timestamp: 1637386273
  timesteps_since_restore: 15360000
  timesteps_this_iter: 96000
  timesteps_total: 77760000
  training_iteration: 810
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    810 |           112217 | 77760000 |   128.88 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 7.1
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 3.81
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 28.71
    apples_agent-2_min: 0
    apples_agent-3_max: 52
    apples_agent-3_mean: 8.89
    apples_agent-3_min: 0
    apples_agent-4_max: 34
    apples_agent-4_mean: 9.67
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 4.27
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 28
    cleaning_beam_agent-0_mean: 3.47
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 253
    cleaning_beam_agent-1_mean: 214.04
    cleaning_beam_agent-1_min: 0
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 3.23
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 27
    cleaning_beam_agent-3_mean: 9.01
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 14
    cleaning_beam_agent-4_mean: 3.18
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 2.6
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-33-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 172.0
  episode_reward_mean: 122.66
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 77856
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11836.9
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30435705184936523
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007907777326181531
        model: {}
        policy_loss: -0.0016343952156603336
        total_loss: -0.0020585572347044945
        vf_explained_var: 0.013328388333320618
        vf_loss: 1.1150550842285156
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21606776118278503
        entropy_coeff: 0.0017600000137463212
        kl: 0.001058376976288855
        model: {}
        policy_loss: -0.0015820623375475407
        total_loss: -0.0019093826413154602
        vf_explained_var: 0.07740433514118195
        vf_loss: 0.5295876264572144
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39006665349006653
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014228741638362408
        model: {}
        policy_loss: -0.0019057763274759054
        total_loss: -0.0022014109417796135
        vf_explained_var: 0.03011094033718109
        vf_loss: 3.9088363647460938
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47188466787338257
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015929141081869602
        model: {}
        policy_loss: -0.001682058908045292
        total_loss: -0.0023369246628135443
        vf_explained_var: 0.014238983392715454
        vf_loss: 1.7565032243728638
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37198489904403687
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005020397366024554
        model: {}
        policy_loss: -0.0010293335653841496
        total_loss: -0.0015706364065408707
        vf_explained_var: 0.00907684862613678
        vf_loss: 1.1339118480682373
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5500268936157227
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015696777263656259
        model: {}
        policy_loss: -0.001732338103465736
        total_loss: -0.0025669322349131107
        vf_explained_var: 0.014130890369415283
        vf_loss: 1.3345571756362915
    load_time_ms: 14103.944
    num_steps_sampled: 77856000
    num_steps_trained: 77856000
    sample_time_ms: 118791.591
    update_time_ms: 16.025
  iterations_since_restore: 161
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.79904306220096
    ram_util_percent: 12.281818181818185
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 62.0
    agent-3: 37.0
    agent-4: 28.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 15.72
    agent-1: 8.73
    agent-2: 43.48
    agent-3: 22.53
    agent-4: 14.5
    agent-5: 17.7
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 28.207001750278305
    mean_inference_ms: 15.085425278960667
    mean_processing_ms: 73.4209221396534
  time_since_restore: 23424.90580558777
  time_this_iter_s: 144.92513132095337
  time_total_s: 112361.58393526077
  timestamp: 1637386420
  timesteps_since_restore: 15456000
  timesteps_this_iter: 96000
  timesteps_total: 77856000
  training_iteration: 811
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    811 |           112362 | 77856000 |   122.66 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 7.53
    apples_agent-0_min: 1
    apples_agent-1_max: 15
    apples_agent-1_mean: 3.49
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 29.02
    apples_agent-2_min: 18
    apples_agent-3_max: 35
    apples_agent-3_mean: 7.69
    apples_agent-3_min: 2
    apples_agent-4_max: 27
    apples_agent-4_mean: 10.02
    apples_agent-4_min: 4
    apples_agent-5_max: 21
    apples_agent-5_mean: 4.06
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 26
    cleaning_beam_agent-0_mean: 3.17
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 283
    cleaning_beam_agent-1_mean: 215.43
    cleaning_beam_agent-1_min: 144
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 3.05
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 9.63
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.16
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 3.02
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-36-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 171.0
  episode_reward_mean: 122.08
  episode_reward_min: 79.0
  episodes_this_iter: 96
  episodes_total: 77952
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11839.318
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30598175525665283
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008139356505125761
        model: {}
        policy_loss: -0.0016904124058783054
        total_loss: -0.0021158913150429726
        vf_explained_var: 0.012549340724945068
        vf_loss: 1.1304631233215332
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21714253723621368
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009897754061967134
        model: {}
        policy_loss: -0.0016115587204694748
        total_loss: -0.0019380375742912292
        vf_explained_var: 0.07850323617458344
        vf_loss: 0.5569866299629211
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3961423635482788
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010408340021967888
        model: {}
        policy_loss: -0.0016346024349331856
        total_loss: -0.0019652103073894978
        vf_explained_var: 0.006331026554107666
        vf_loss: 3.666013717651367
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4728184938430786
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012431067880243063
        model: {}
        policy_loss: -0.001534594688564539
        total_loss: -0.002213633619248867
        vf_explained_var: -0.0026222020387649536
        vf_loss: 1.5312161445617676
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36838048696517944
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005250070244073868
        model: {}
        policy_loss: -0.0011650430969893932
        total_loss: -0.001706790179014206
        vf_explained_var: 0.014697104692459106
        vf_loss: 1.0660154819488525
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5472869277000427
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014698761515319347
        model: {}
        policy_loss: -0.0016579325310885906
        total_loss: -0.002499470952898264
        vf_explained_var: 0.0009665638208389282
        vf_loss: 1.2168983221054077
    load_time_ms: 14087.472
    num_steps_sampled: 77952000
    num_steps_trained: 77952000
    sample_time_ms: 118720.465
    update_time_ms: 16.018
  iterations_since_restore: 162
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.962135922330102
    ram_util_percent: 12.38543689320388
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 18.0
    agent-2: 66.0
    agent-3: 34.0
    agent-4: 25.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 16.28
    agent-1: 8.82
    agent-2: 43.25
    agent-3: 21.71
    agent-4: 15.14
    agent-5: 16.88
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 27.0
    agent-3: 12.0
    agent-4: 5.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.207563031771926
    mean_inference_ms: 15.08455560508745
    mean_processing_ms: 73.41920559203201
  time_since_restore: 23569.20491361618
  time_this_iter_s: 144.29910802841187
  time_total_s: 112505.88304328918
  timestamp: 1637386565
  timesteps_since_restore: 15552000
  timesteps_this_iter: 96000
  timesteps_total: 77952000
  training_iteration: 812
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    812 |           112506 | 77952000 |   122.08 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 6.79
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 2.97
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 28.58
    apples_agent-2_min: 9
    apples_agent-3_max: 18
    apples_agent-3_mean: 7.85
    apples_agent-3_min: 2
    apples_agent-4_max: 27
    apples_agent-4_mean: 9.95
    apples_agent-4_min: 3
    apples_agent-5_max: 16
    apples_agent-5_mean: 3.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 22
    cleaning_beam_agent-0_mean: 3.76
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 251
    cleaning_beam_agent-1_mean: 208.52
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 3.23
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 59
    cleaning_beam_agent-3_mean: 10.08
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 7
    cleaning_beam_agent-4_mean: 3.01
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.99
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-38-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 161.0
  episode_reward_mean: 120.63
  episode_reward_min: 42.0
  episodes_this_iter: 96
  episodes_total: 78048
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11840.642
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3064568042755127
        entropy_coeff: 0.0017600000137463212
        kl: 0.001074428902938962
        model: {}
        policy_loss: -0.0018939857836812735
        total_loss: -0.002333709504455328
        vf_explained_var: 0.015347316861152649
        vf_loss: 0.9964102506637573
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21462062001228333
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014374881284311414
        model: {}
        policy_loss: -0.0018520142184570432
        total_loss: -0.002173463813960552
        vf_explained_var: 0.07415898144245148
        vf_loss: 0.5628269910812378
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3860287070274353
        entropy_coeff: 0.0017600000137463212
        kl: 0.001581683405674994
        model: {}
        policy_loss: -0.0018262984231114388
        total_loss: -0.002148180268704891
        vf_explained_var: 0.015146195888519287
        vf_loss: 3.5752720832824707
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46775156259536743
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018338410882279277
        model: {}
        policy_loss: -0.001570301130414009
        total_loss: -0.002241258043795824
        vf_explained_var: 0.015277400612831116
        vf_loss: 1.5229119062423706
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3801075518131256
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010016856249421835
        model: {}
        policy_loss: -0.0013023274950683117
        total_loss: -0.0018728068098425865
        vf_explained_var: 0.011432752013206482
        vf_loss: 0.9850963354110718
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5382559895515442
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019559585489332676
        model: {}
        policy_loss: -0.0017730242107063532
        total_loss: -0.002597539685666561
        vf_explained_var: 0.007369816303253174
        vf_loss: 1.2281548976898193
    load_time_ms: 14093.986
    num_steps_sampled: 78048000
    num_steps_trained: 78048000
    sample_time_ms: 118826.215
    update_time_ms: 16.019
  iterations_since_restore: 163
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.874519230769234
    ram_util_percent: 12.228846153846156
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 17.0
    agent-2: 60.0
    agent-3: 37.0
    agent-4: 25.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.83
    agent-1: 7.98
    agent-2: 43.2
    agent-3: 21.78
    agent-4: 14.96
    agent-5: 16.88
  policy_reward_min:
    agent-0: 8.0
    agent-1: -38.0
    agent-2: 14.0
    agent-3: 5.0
    agent-4: 5.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.207572205526372
    mean_inference_ms: 15.084403703972043
    mean_processing_ms: 73.41994491058801
  time_since_restore: 23715.03042268753
  time_this_iter_s: 145.8255090713501
  time_total_s: 112651.70855236053
  timestamp: 1637386711
  timesteps_since_restore: 15648000
  timesteps_this_iter: 96000
  timesteps_total: 78048000
  training_iteration: 813
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    813 |           112652 | 78048000 |   120.63 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 6.51
    apples_agent-0_min: 1
    apples_agent-1_max: 32
    apples_agent-1_mean: 3.46
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 28.86
    apples_agent-2_min: 15
    apples_agent-3_max: 29
    apples_agent-3_mean: 7.74
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.33
    apples_agent-4_min: 1
    apples_agent-5_max: 12
    apples_agent-5_mean: 3.78
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 25
    cleaning_beam_agent-0_mean: 4.13
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 248
    cleaning_beam_agent-1_mean: 208.66
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 3.86
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 23
    cleaning_beam_agent-3_mean: 9.33
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.13
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 2.89
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-40-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 168.0
  episode_reward_mean: 121.41
  episode_reward_min: 68.0
  episodes_this_iter: 96
  episodes_total: 78144
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11851.954
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2994619607925415
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014311086852103472
        model: {}
        policy_loss: -0.0019677714444696903
        total_loss: -0.0024020331911742687
        vf_explained_var: -0.0008344203233718872
        vf_loss: 0.9279260039329529
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21511147916316986
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008608007337898016
        model: {}
        policy_loss: -0.0014417394995689392
        total_loss: -0.0017711413092911243
        vf_explained_var: 0.07935822010040283
        vf_loss: 0.4919217824935913
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3854522705078125
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012958410661667585
        model: {}
        policy_loss: -0.001849074847996235
        total_loss: -0.0021711457520723343
        vf_explained_var: 0.03190338611602783
        vf_loss: 3.563265323638916
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.475968599319458
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015127070946618915
        model: {}
        policy_loss: -0.001673537422902882
        total_loss: -0.002350040478631854
        vf_explained_var: 0.007999852299690247
        vf_loss: 1.6120052337646484
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.381205290555954
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006776762893423438
        model: {}
        policy_loss: -0.0011168657802045345
        total_loss: -0.001691251527518034
        vf_explained_var: 0.00916355848312378
        vf_loss: 0.9653551578521729
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5542617440223694
        entropy_coeff: 0.0017600000137463212
        kl: 0.002234484301880002
        model: {}
        policy_loss: -0.0019273674115538597
        total_loss: -0.00278300978243351
        vf_explained_var: 0.010880053043365479
        vf_loss: 1.1985905170440674
    load_time_ms: 14121.165
    num_steps_sampled: 78144000
    num_steps_trained: 78144000
    sample_time_ms: 118678.043
    update_time_ms: 16.168
  iterations_since_restore: 164
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.86878048780488
    ram_util_percent: 12.365853658536581
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 18.0
    agent-2: 60.0
    agent-3: 39.0
    agent-4: 25.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 14.98
    agent-1: 8.56
    agent-2: 43.68
    agent-3: 22.55
    agent-4: 14.41
    agent-5: 17.23
  policy_reward_min:
    agent-0: 8.0
    agent-1: 2.0
    agent-2: 26.0
    agent-3: 9.0
    agent-4: 3.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.20551459489153
    mean_inference_ms: 15.083393721668864
    mean_processing_ms: 73.41348459988708
  time_since_restore: 23858.686310768127
  time_this_iter_s: 143.65588808059692
  time_total_s: 112795.36444044113
  timestamp: 1637386855
  timesteps_since_restore: 15744000
  timesteps_this_iter: 96000
  timesteps_total: 78144000
  training_iteration: 814
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    814 |           112795 | 78144000 |   121.41 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 6.86
    apples_agent-0_min: 2
    apples_agent-1_max: 13
    apples_agent-1_mean: 3.23
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 28.01
    apples_agent-2_min: 13
    apples_agent-3_max: 33
    apples_agent-3_mean: 8.28
    apples_agent-3_min: 2
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.5
    apples_agent-4_min: 3
    apples_agent-5_max: 31
    apples_agent-5_mean: 4.21
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 49
    cleaning_beam_agent-0_mean: 4.18
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 274
    cleaning_beam_agent-1_mean: 211.18
    cleaning_beam_agent-1_min: 168
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 3.65
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 27
    cleaning_beam_agent-3_mean: 9.36
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.95
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.18
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-43-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 123.51
  episode_reward_min: 90.0
  episodes_this_iter: 96
  episodes_total: 78240
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11837.018
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2990409731864929
        entropy_coeff: 0.0017600000137463212
        kl: 0.000848773866891861
        model: {}
        policy_loss: -0.0017168282065540552
        total_loss: -0.002132206689566374
        vf_explained_var: 0.0076869577169418335
        vf_loss: 1.1093395948410034
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2139551341533661
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011138528352603316
        model: {}
        policy_loss: -0.0016717473044991493
        total_loss: -0.001990105491131544
        vf_explained_var: 0.07260067760944366
        vf_loss: 0.5820251703262329
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37321317195892334
        entropy_coeff: 0.0017600000137463212
        kl: 0.001576044480316341
        model: {}
        policy_loss: -0.0018689311109483242
        total_loss: -0.002163404133170843
        vf_explained_var: 0.026477456092834473
        vf_loss: 3.6238203048706055
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4794245958328247
        entropy_coeff: 0.0017600000137463212
        kl: 0.001455975347198546
        model: {}
        policy_loss: -0.0014449185691773891
        total_loss: -0.00213993969373405
        vf_explained_var: 0.00699065625667572
        vf_loss: 1.4876549243927002
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37853631377220154
        entropy_coeff: 0.0017600000137463212
        kl: 0.000823571695946157
        model: {}
        policy_loss: -0.0013799660373479128
        total_loss: -0.0019362306920811534
        vf_explained_var: 0.01376788318157196
        vf_loss: 1.0995736122131348
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5612005591392517
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018608460668474436
        model: {}
        policy_loss: -0.001882336102426052
        total_loss: -0.0027505564503371716
        vf_explained_var: 0.00661015510559082
        vf_loss: 1.1949093341827393
    load_time_ms: 14104.955
    num_steps_sampled: 78240000
    num_steps_trained: 78240000
    sample_time_ms: 118714.008
    update_time_ms: 15.766
  iterations_since_restore: 165
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.8768115942029
    ram_util_percent: 12.352657004830913
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 21.0
    agent-2: 67.0
    agent-3: 39.0
    agent-4: 30.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 16.42
    agent-1: 9.0
    agent-2: 43.42
    agent-3: 21.74
    agent-4: 15.76
    agent-5: 17.17
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 29.0
    agent-3: 9.0
    agent-4: 5.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.205089783911152
    mean_inference_ms: 15.082826003431114
    mean_processing_ms: 73.41204214662473
  time_since_restore: 24003.915909528732
  time_this_iter_s: 145.22959876060486
  time_total_s: 112940.59403920174
  timestamp: 1637387000
  timesteps_since_restore: 15840000
  timesteps_this_iter: 96000
  timesteps_total: 78240000
  training_iteration: 815
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    815 |           112941 | 78240000 |   123.51 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 6.12
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.32
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 27.64
    apples_agent-2_min: 11
    apples_agent-3_max: 39
    apples_agent-3_mean: 8.34
    apples_agent-3_min: 1
    apples_agent-4_max: 18
    apples_agent-4_mean: 9.68
    apples_agent-4_min: 1
    apples_agent-5_max: 14
    apples_agent-5_mean: 4.32
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 3.08
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 243
    cleaning_beam_agent-1_mean: 207.04
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 4.6
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 9.35
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.36
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.01
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-45-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 118.04
  episode_reward_min: 52.0
  episodes_this_iter: 96
  episodes_total: 78336
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11822.566
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2975118160247803
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009296452044509351
        model: {}
        policy_loss: -0.001676532905548811
        total_loss: -0.0021018730476498604
        vf_explained_var: 0.011897757649421692
        vf_loss: 0.9827955961227417
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21498161554336548
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010851213010028005
        model: {}
        policy_loss: -0.0015166057273745537
        total_loss: -0.0018406985327601433
        vf_explained_var: 0.07708674669265747
        vf_loss: 0.542755663394928
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37801453471183777
        entropy_coeff: 0.0017600000137463212
        kl: 0.001118952059186995
        model: {}
        policy_loss: -0.0017281332984566689
        total_loss: -0.0020398134365677834
        vf_explained_var: 0.03331179916858673
        vf_loss: 3.5362815856933594
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4770939350128174
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016223399434238672
        model: {}
        policy_loss: -0.0015897131524980068
        total_loss: -0.002271845703944564
        vf_explained_var: 0.01886439323425293
        vf_loss: 1.5755313634872437
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37358272075653076
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007581230602227151
        model: {}
        policy_loss: -0.0012025171890854836
        total_loss: -0.0017653033137321472
        vf_explained_var: 0.011592388153076172
        vf_loss: 0.9471899271011353
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5640825033187866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023851613514125347
        model: {}
        policy_loss: -0.0016858915332704782
        total_loss: -0.0025628497824072838
        vf_explained_var: 0.011426880955696106
        vf_loss: 1.1582753658294678
    load_time_ms: 14109.302
    num_steps_sampled: 78336000
    num_steps_trained: 78336000
    sample_time_ms: 118747.879
    update_time_ms: 15.767
  iterations_since_restore: 166
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.928640776699027
    ram_util_percent: 12.367961165048545
  pid: 27065
  policy_reward_max:
    agent-0: 24.0
    agent-1: 16.0
    agent-2: 62.0
    agent-3: 36.0
    agent-4: 32.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 14.56
    agent-1: 8.72
    agent-2: 42.28
    agent-3: 21.47
    agent-4: 14.18
    agent-5: 16.83
  policy_reward_min:
    agent-0: 5.0
    agent-1: 1.0
    agent-2: 21.0
    agent-3: 8.0
    agent-4: 3.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.20403591101823
    mean_inference_ms: 15.082192151569323
    mean_processing_ms: 73.41041871758215
  time_since_restore: 24148.728412151337
  time_this_iter_s: 144.81250262260437
  time_total_s: 113085.40654182434
  timestamp: 1637387145
  timesteps_since_restore: 15936000
  timesteps_this_iter: 96000
  timesteps_total: 78336000
  training_iteration: 816
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    816 |           113085 | 78336000 |   118.04 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 6.74
    apples_agent-0_min: 1
    apples_agent-1_max: 20
    apples_agent-1_mean: 3.36
    apples_agent-1_min: 0
    apples_agent-2_max: 69
    apples_agent-2_mean: 29.02
    apples_agent-2_min: 16
    apples_agent-3_max: 20
    apples_agent-3_mean: 7.96
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.92
    apples_agent-4_min: 1
    apples_agent-5_max: 25
    apples_agent-5_mean: 4.72
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 15
    cleaning_beam_agent-0_mean: 3.22
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 246
    cleaning_beam_agent-1_mean: 207.2
    cleaning_beam_agent-1_min: 168
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 3.5
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 19
    cleaning_beam_agent-3_mean: 9.37
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.42
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.1
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-48-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 174.0
  episode_reward_mean: 121.76
  episode_reward_min: 88.0
  episodes_this_iter: 96
  episodes_total: 78432
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11810.452
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2975504398345947
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008056100341491401
        model: {}
        policy_loss: -0.0016223505372181535
        total_loss: -0.002038988284766674
        vf_explained_var: 0.0002212822437286377
        vf_loss: 1.0705244541168213
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21697762608528137
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011839494109153748
        model: {}
        policy_loss: -0.0016655938234180212
        total_loss: -0.001991549041122198
        vf_explained_var: 0.05839328467845917
        vf_loss: 0.5592201948165894
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36998844146728516
        entropy_coeff: 0.0017600000137463212
        kl: 0.001117861014790833
        model: {}
        policy_loss: -0.0017618280835449696
        total_loss: -0.00205003609880805
        vf_explained_var: 0.0170595645904541
        vf_loss: 3.629728317260742
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46895909309387207
        entropy_coeff: 0.0017600000137463212
        kl: 0.001333328546024859
        model: {}
        policy_loss: -0.0015647541731595993
        total_loss: -0.002245867857709527
        vf_explained_var: -0.00876268744468689
        vf_loss: 1.442548394203186
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39072129130363464
        entropy_coeff: 0.0017600000137463212
        kl: 0.001290441257879138
        model: {}
        policy_loss: -0.001496302429586649
        total_loss: -0.0020770656410604715
        vf_explained_var: 0.010134071111679077
        vf_loss: 1.069050669670105
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5715937614440918
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015694540925323963
        model: {}
        policy_loss: -0.0017834443133324385
        total_loss: -0.002674662508070469
        vf_explained_var: 0.006955176591873169
        vf_loss: 1.1478421688079834
    load_time_ms: 14126.183
    num_steps_sampled: 78432000
    num_steps_trained: 78432000
    sample_time_ms: 118689.404
    update_time_ms: 15.795
  iterations_since_restore: 167
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.90341463414634
    ram_util_percent: 12.279999999999998
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 17.0
    agent-2: 63.0
    agent-3: 36.0
    agent-4: 27.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.83
    agent-1: 8.83
    agent-2: 43.72
    agent-3: 21.2
    agent-4: 14.94
    agent-5: 17.24
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 26.0
    agent-3: 11.0
    agent-4: 3.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.203041271290655
    mean_inference_ms: 15.081361466964172
    mean_processing_ms: 73.40588167654025
  time_since_restore: 24292.873208522797
  time_this_iter_s: 144.14479637145996
  time_total_s: 113229.5513381958
  timestamp: 1637387289
  timesteps_since_restore: 16032000
  timesteps_this_iter: 96000
  timesteps_total: 78432000
  training_iteration: 817
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    817 |           113230 | 78432000 |   121.76 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 6.65
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 3.31
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 28.37
    apples_agent-2_min: 14
    apples_agent-3_max: 25
    apples_agent-3_mean: 8.29
    apples_agent-3_min: 2
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.48
    apples_agent-4_min: 2
    apples_agent-5_max: 32
    apples_agent-5_mean: 5.04
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 3.25
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 259
    cleaning_beam_agent-1_mean: 211.21
    cleaning_beam_agent-1_min: 147
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 3.51
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 25
    cleaning_beam_agent-3_mean: 8.98
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.45
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 3.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-50-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 158.0
  episode_reward_mean: 122.8
  episode_reward_min: 84.0
  episodes_this_iter: 96
  episodes_total: 78528
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11818.539
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2940027713775635
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008819791255518794
        model: {}
        policy_loss: -0.0018526725471019745
        total_loss: -0.0022583508398383856
        vf_explained_var: 0.009733721613883972
        vf_loss: 1.1176457405090332
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21692097187042236
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010096865007653832
        model: {}
        policy_loss: -0.0016139843501150608
        total_loss: -0.0019430718384683132
        vf_explained_var: 0.08331166207790375
        vf_loss: 0.5269350409507751
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36725282669067383
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013181676622480154
        model: {}
        policy_loss: -0.001841920893639326
        total_loss: -0.002135521499440074
        vf_explained_var: 0.03929997980594635
        vf_loss: 3.527655839920044
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4727390706539154
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016265102894976735
        model: {}
        policy_loss: -0.0015535012353211641
        total_loss: -0.0022415192797780037
        vf_explained_var: -0.012741386890411377
        vf_loss: 1.440003514289856
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3920835256576538
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006342529668472707
        model: {}
        policy_loss: -0.0013662524288520217
        total_loss: -0.0019528950797393918
        vf_explained_var: 0.004623591899871826
        vf_loss: 1.034218668937683
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.574555516242981
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010941845830529928
        model: {}
        policy_loss: -0.0017032846808433533
        total_loss: -0.002589469775557518
        vf_explained_var: 0.011987775564193726
        vf_loss: 1.2502763271331787
    load_time_ms: 14117.929
    num_steps_sampled: 78528000
    num_steps_trained: 78528000
    sample_time_ms: 118640.579
    update_time_ms: 15.604
  iterations_since_restore: 168
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.90097087378641
    ram_util_percent: 12.35145631067961
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 64.0
    agent-3: 36.0
    agent-4: 28.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 15.68
    agent-1: 8.76
    agent-2: 44.17
    agent-3: 22.09
    agent-4: 14.68
    agent-5: 17.42
  policy_reward_min:
    agent-0: 3.0
    agent-1: 0.0
    agent-2: 25.0
    agent-3: 5.0
    agent-4: 6.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.20242805553042
    mean_inference_ms: 15.08034920181291
    mean_processing_ms: 73.40337265302009
  time_since_restore: 24437.19389462471
  time_this_iter_s: 144.32068610191345
  time_total_s: 113373.87202429771
  timestamp: 1637387434
  timesteps_since_restore: 16128000
  timesteps_this_iter: 96000
  timesteps_total: 78528000
  training_iteration: 818
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    818 |           113374 | 78528000 |    122.8 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 6.61
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 2.59
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 28.74
    apples_agent-2_min: 14
    apples_agent-3_max: 26
    apples_agent-3_mean: 8.1
    apples_agent-3_min: 1
    apples_agent-4_max: 32
    apples_agent-4_mean: 10.2
    apples_agent-4_min: 2
    apples_agent-5_max: 18
    apples_agent-5_mean: 4.64
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 2.68
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 257
    cleaning_beam_agent-1_mean: 209.62
    cleaning_beam_agent-1_min: 144
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.09
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 38
    cleaning_beam_agent-3_mean: 10.28
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 2.97
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.64
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-52-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 164.0
  episode_reward_mean: 121.17
  episode_reward_min: 5.0
  episodes_this_iter: 96
  episodes_total: 78624
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11823.298
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29132139682769775
        entropy_coeff: 0.0017600000137463212
        kl: 0.00047811021795496345
        model: {}
        policy_loss: -0.0011098627001047134
        total_loss: -0.0013858946040272713
        vf_explained_var: 0.003342241048812866
        vf_loss: 2.366910934448242
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21842150390148163
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011535584926605225
        model: {}
        policy_loss: -0.001563187688589096
        total_loss: -0.0018986715003848076
        vf_explained_var: 0.0783420205116272
        vf_loss: 0.48936229944229126
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3808484673500061
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015763458795845509
        model: {}
        policy_loss: -0.001953328959643841
        total_loss: -0.0021238834597170353
        vf_explained_var: -0.003727450966835022
        vf_loss: 4.997393608093262
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4705902934074402
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015931455418467522
        model: {}
        policy_loss: -0.0016878282185643911
        total_loss: -0.002342614345252514
        vf_explained_var: -0.0011232346296310425
        vf_loss: 1.7345048189163208
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3829227089881897
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011110487394034863
        model: {}
        policy_loss: -0.0013565365225076675
        total_loss: -0.0019336440600454807
        vf_explained_var: 0.010016456246376038
        vf_loss: 0.9683582782745361
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5787034034729004
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014678076840937138
        model: {}
        policy_loss: -0.0017657799180597067
        total_loss: -0.0026592935901135206
        vf_explained_var: 0.007151663303375244
        vf_loss: 1.2500441074371338
    load_time_ms: 14135.658
    num_steps_sampled: 78624000
    num_steps_trained: 78624000
    sample_time_ms: 118676.301
    update_time_ms: 15.539
  iterations_since_restore: 169
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.842995169082126
    ram_util_percent: 12.361835748792267
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 19.0
    agent-2: 70.0
    agent-3: 38.0
    agent-4: 23.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 16.22
    agent-1: 7.99
    agent-2: 42.76
    agent-3: 22.33
    agent-4: 14.83
    agent-5: 17.04
  policy_reward_min:
    agent-0: -38.0
    agent-1: 2.0
    agent-2: -3.0
    agent-3: 10.0
    agent-4: 7.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.202254345044526
    mean_inference_ms: 15.079629804785089
    mean_processing_ms: 73.39984533523156
  time_since_restore: 24582.64602446556
  time_this_iter_s: 145.45212984085083
  time_total_s: 113519.32415413857
  timestamp: 1637387579
  timesteps_since_restore: 16224000
  timesteps_this_iter: 96000
  timesteps_total: 78624000
  training_iteration: 819
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    819 |           113519 | 78624000 |   121.17 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 6.72
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 2.99
    apples_agent-1_min: 0
    apples_agent-2_max: 60
    apples_agent-2_mean: 28.53
    apples_agent-2_min: 12
    apples_agent-3_max: 26
    apples_agent-3_mean: 8.53
    apples_agent-3_min: 1
    apples_agent-4_max: 29
    apples_agent-4_mean: 10.58
    apples_agent-4_min: 3
    apples_agent-5_max: 16
    apples_agent-5_mean: 3.97
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 18
    cleaning_beam_agent-0_mean: 3.11
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 248
    cleaning_beam_agent-1_mean: 208.94
    cleaning_beam_agent-1_min: 62
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 3.52
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 10.49
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.53
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.21
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-55-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 164.0
  episode_reward_mean: 121.14
  episode_reward_min: 52.0
  episodes_this_iter: 96
  episodes_total: 78720
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11805.196
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2946234345436096
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009244461543858051
        model: {}
        policy_loss: -0.0015616696327924728
        total_loss: -0.0019682880956679583
        vf_explained_var: 0.012256935238838196
        vf_loss: 1.1192065477371216
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2152090221643448
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009255011100322008
        model: {}
        policy_loss: -0.0015380214899778366
        total_loss: -0.0018609589897096157
        vf_explained_var: 0.07801750302314758
        vf_loss: 0.5582513213157654
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37780970335006714
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011729017132893205
        model: {}
        policy_loss: -0.0015542805194854736
        total_loss: -0.0018281228840351105
        vf_explained_var: 0.018184080719947815
        vf_loss: 3.911055326461792
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46703046560287476
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013043931685388088
        model: {}
        policy_loss: -0.001491607166826725
        total_loss: -0.0021429434418678284
        vf_explained_var: 2.7999281883239746e-05
        vf_loss: 1.7063697576522827
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3847818374633789
        entropy_coeff: 0.0017600000137463212
        kl: 0.00048568984493613243
        model: {}
        policy_loss: -0.0011456236243247986
        total_loss: -0.0017165085300803185
        vf_explained_var: 0.010461315512657166
        vf_loss: 1.0633199214935303
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5715925693511963
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016300122952088714
        model: {}
        policy_loss: -0.0017941745463758707
        total_loss: -0.002688474953174591
        vf_explained_var: 0.012139961123466492
        vf_loss: 1.1170036792755127
    load_time_ms: 14137.258
    num_steps_sampled: 78720000
    num_steps_trained: 78720000
    sample_time_ms: 118808.984
    update_time_ms: 15.326
  iterations_since_restore: 170
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.02259615384616
    ram_util_percent: 12.363461538461538
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 24.0
    agent-2: 59.0
    agent-3: 37.0
    agent-4: 30.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 15.56
    agent-1: 8.39
    agent-2: 43.01
    agent-3: 22.66
    agent-4: 15.31
    agent-5: 16.21
  policy_reward_min:
    agent-0: -37.0
    agent-1: 1.0
    agent-2: 20.0
    agent-3: 5.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.202842199787025
    mean_inference_ms: 15.079957601316053
    mean_processing_ms: 73.40311521021634
  time_since_restore: 24728.595527887344
  time_this_iter_s: 145.94950342178345
  time_total_s: 113665.27365756035
  timestamp: 1637387725
  timesteps_since_restore: 16320000
  timesteps_this_iter: 96000
  timesteps_total: 78720000
  training_iteration: 820
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    820 |           113665 | 78720000 |   121.14 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 6.88
    apples_agent-0_min: 1
    apples_agent-1_max: 15
    apples_agent-1_mean: 3.1
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 28.0
    apples_agent-2_min: 13
    apples_agent-3_max: 26
    apples_agent-3_mean: 8.14
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.61
    apples_agent-4_min: 3
    apples_agent-5_max: 18
    apples_agent-5_mean: 4.26
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 28
    cleaning_beam_agent-0_mean: 3.33
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 251
    cleaning_beam_agent-1_mean: 209.57
    cleaning_beam_agent-1_min: 157
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 4.17
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 9.32
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.25
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.28
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_00-57-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 166.0
  episode_reward_mean: 122.06
  episode_reward_min: 82.0
  episodes_this_iter: 96
  episodes_total: 78816
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11821.619
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3037678599357605
        entropy_coeff: 0.0017600000137463212
        kl: 0.001271536573767662
        model: {}
        policy_loss: -0.0018132564146071672
        total_loss: -0.002244509756565094
        vf_explained_var: 0.004448428750038147
        vf_loss: 1.033778190612793
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21450845897197723
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012203662190586329
        model: {}
        policy_loss: -0.0014700430911034346
        total_loss: -0.0017966419691219926
        vf_explained_var: 0.09371878206729889
        vf_loss: 0.509357213973999
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37913820147514343
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013106921687722206
        model: {}
        policy_loss: -0.001839479198679328
        total_loss: -0.0021234797313809395
        vf_explained_var: 0.014486938714981079
        vf_loss: 3.8328630924224854
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4630562365055084
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010723958257585764
        model: {}
        policy_loss: -0.0013857221929356456
        total_loss: -0.0020408262498676777
        vf_explained_var: 0.006814122200012207
        vf_loss: 1.598747730255127
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3869345188140869
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006090507959015667
        model: {}
        policy_loss: -0.0012492439709603786
        total_loss: -0.0018271754961460829
        vf_explained_var: 0.006310388445854187
        vf_loss: 1.0307245254516602
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5830976963043213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016772644594311714
        model: {}
        policy_loss: -0.0017800001660361886
        total_loss: -0.0026849540881812572
        vf_explained_var: 0.004038006067276001
        vf_loss: 1.212958574295044
    load_time_ms: 14141.791
    num_steps_sampled: 78816000
    num_steps_trained: 78816000
    sample_time_ms: 118887.375
    update_time_ms: 15.636
  iterations_since_restore: 171
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.553080568720382
    ram_util_percent: 12.364454976303316
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 19.0
    agent-2: 62.0
    agent-3: 38.0
    agent-4: 27.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.81
    agent-1: 8.78
    agent-2: 43.91
    agent-3: 21.85
    agent-4: 14.96
    agent-5: 16.75
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 25.0
    agent-3: 8.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.202554764765154
    mean_inference_ms: 15.079595944367142
    mean_processing_ms: 73.40155117011089
  time_since_restore: 24874.525832414627
  time_this_iter_s: 145.93030452728271
  time_total_s: 113811.20396208763
  timestamp: 1637387873
  timesteps_since_restore: 16416000
  timesteps_this_iter: 96000
  timesteps_total: 78816000
  training_iteration: 821
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    821 |           113811 | 78816000 |   122.06 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 6.41
    apples_agent-0_min: 1
    apples_agent-1_max: 42
    apples_agent-1_mean: 3.57
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 28.01
    apples_agent-2_min: 15
    apples_agent-3_max: 22
    apples_agent-3_mean: 8.49
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 9.7
    apples_agent-4_min: 3
    apples_agent-5_max: 33
    apples_agent-5_mean: 4.46
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 2.74
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 246
    cleaning_beam_agent-1_mean: 212.6
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 4.43
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 10.61
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.59
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.89
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-00-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 160.0
  episode_reward_mean: 121.49
  episode_reward_min: 79.0
  episodes_this_iter: 96
  episodes_total: 78912
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11817.967
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3101747930049896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010064721573144197
        model: {}
        policy_loss: -0.001943741226568818
        total_loss: -0.002391474787145853
        vf_explained_var: 0.0069720447063446045
        vf_loss: 0.9817399382591248
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.213232159614563
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012065268820151687
        model: {}
        policy_loss: -0.0015048496425151825
        total_loss: -0.0018323487602174282
        vf_explained_var: 0.07322724163532257
        vf_loss: 0.4779219925403595
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3781341314315796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012812947388738394
        model: {}
        policy_loss: -0.0017677337164059281
        total_loss: -0.0020630299113690853
        vf_explained_var: 0.017623886466026306
        vf_loss: 3.7022032737731934
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4613330364227295
        entropy_coeff: 0.0017600000137463212
        kl: 0.001640052767470479
        model: {}
        policy_loss: -0.0014828713610768318
        total_loss: -0.0021152906119823456
        vf_explained_var: -0.003408372402191162
        vf_loss: 1.7952734231948853
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37623780965805054
        entropy_coeff: 0.0017600000137463212
        kl: 0.00114639091771096
        model: {}
        policy_loss: -0.0014189648209139705
        total_loss: -0.00198066933080554
        vf_explained_var: 0.012950390577316284
        vf_loss: 1.004767894744873
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5750799179077148
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016998666105791926
        model: {}
        policy_loss: -0.001841933000832796
        total_loss: -0.002742096083238721
        vf_explained_var: 0.010433748364448547
        vf_loss: 1.119796633720398
    load_time_ms: 14175.135
    num_steps_sampled: 78912000
    num_steps_trained: 78912000
    sample_time_ms: 118980.622
    update_time_ms: 15.804
  iterations_since_restore: 172
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.03913043478261
    ram_util_percent: 12.383574879227051
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 15.0
    agent-2: 68.0
    agent-3: 43.0
    agent-4: 25.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 14.87
    agent-1: 8.57
    agent-2: 43.75
    agent-3: 23.22
    agent-4: 14.38
    agent-5: 16.7
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 28.0
    agent-3: 13.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.20273120508612
    mean_inference_ms: 15.078962805713822
    mean_processing_ms: 73.40110354643748
  time_since_restore: 25020.098705768585
  time_this_iter_s: 145.57287335395813
  time_total_s: 113956.77683544159
  timestamp: 1637388019
  timesteps_since_restore: 16512000
  timesteps_this_iter: 96000
  timesteps_total: 78912000
  training_iteration: 822
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    822 |           113957 | 78912000 |   121.49 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 6.47
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.01
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 29.2
    apples_agent-2_min: 15
    apples_agent-3_max: 24
    apples_agent-3_mean: 8.65
    apples_agent-3_min: 2
    apples_agent-4_max: 28
    apples_agent-4_mean: 10.16
    apples_agent-4_min: 2
    apples_agent-5_max: 24
    apples_agent-5_mean: 4.7
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 21
    cleaning_beam_agent-0_mean: 2.61
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 247
    cleaning_beam_agent-1_mean: 209.06
    cleaning_beam_agent-1_min: 176
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 4.51
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 38
    cleaning_beam_agent-3_mean: 10.16
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.47
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.74
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-02-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 165.0
  episode_reward_mean: 122.54
  episode_reward_min: 85.0
  episodes_this_iter: 96
  episodes_total: 79008
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11814.043
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3037114441394806
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008256071014329791
        model: {}
        policy_loss: -0.0017192959785461426
        total_loss: -0.0021582443732768297
        vf_explained_var: 0.004923179745674133
        vf_loss: 0.9558315873146057
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21525481343269348
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007952936575748026
        model: {}
        policy_loss: -0.0014283524360507727
        total_loss: -0.0017509220633655787
        vf_explained_var: 0.08427345752716064
        vf_loss: 0.5627835392951965
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37427473068237305
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013798738364130259
        model: {}
        policy_loss: -0.0016602030955255032
        total_loss: -0.0019550062716007233
        vf_explained_var: 0.026485726237297058
        vf_loss: 3.6392292976379395
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46481800079345703
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014741874765604734
        model: {}
        policy_loss: -0.0012227357365190983
        total_loss: -0.0018741912208497524
        vf_explained_var: 0.00765557587146759
        vf_loss: 1.6662592887878418
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35606780648231506
        entropy_coeff: 0.0017600000137463212
        kl: 0.001219012658111751
        model: {}
        policy_loss: -0.0013293931260704994
        total_loss: -0.0018620556220412254
        vf_explained_var: 0.016348212957382202
        vf_loss: 0.9401679635047913
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5843932628631592
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013215092476457357
        model: {}
        policy_loss: -0.001629323698580265
        total_loss: -0.0025463630445301533
        vf_explained_var: 0.014235347509384155
        vf_loss: 1.1149340867996216
    load_time_ms: 14162.68
    num_steps_sampled: 79008000
    num_steps_trained: 79008000
    sample_time_ms: 118657.975
    update_time_ms: 15.605
  iterations_since_restore: 173
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.879802955665028
    ram_util_percent: 12.37192118226601
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 66.0
    agent-3: 36.0
    agent-4: 26.0
    agent-5: 39.0
  policy_reward_mean:
    agent-0: 15.41
    agent-1: 8.71
    agent-2: 44.33
    agent-3: 22.54
    agent-4: 14.52
    agent-5: 17.03
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 24.0
    agent-3: 13.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.200039116855105
    mean_inference_ms: 15.077494381201333
    mean_processing_ms: 73.39151540941928
  time_since_restore: 25162.495552539825
  time_this_iter_s: 142.39684677124023
  time_total_s: 114099.17368221283
  timestamp: 1637388162
  timesteps_since_restore: 16608000
  timesteps_this_iter: 96000
  timesteps_total: 79008000
  training_iteration: 823
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    823 |           114099 | 79008000 |   122.54 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 7.53
    apples_agent-0_min: 1
    apples_agent-1_max: 13
    apples_agent-1_mean: 2.91
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 28.1
    apples_agent-2_min: 15
    apples_agent-3_max: 28
    apples_agent-3_mean: 8.45
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.04
    apples_agent-4_min: 3
    apples_agent-5_max: 18
    apples_agent-5_mean: 4.55
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 2.5
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 255
    cleaning_beam_agent-1_mean: 213.51
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 4.97
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 31
    cleaning_beam_agent-3_mean: 10.5
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.9
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.47
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-05-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 161.0
  episode_reward_mean: 122.92
  episode_reward_min: 82.0
  episodes_this_iter: 96
  episodes_total: 79104
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11798.665
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30930113792419434
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009292090544477105
        model: {}
        policy_loss: -0.0018341070972383022
        total_loss: -0.0022616456262767315
        vf_explained_var: 0.00641573965549469
        vf_loss: 1.1683290004730225
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21307753026485443
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008375964243896306
        model: {}
        policy_loss: -0.0014879240188747644
        total_loss: -0.0018100510351359844
        vf_explained_var: 0.08657748997211456
        vf_loss: 0.5289063453674316
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38002467155456543
        entropy_coeff: 0.0017600000137463212
        kl: 0.001212647883221507
        model: {}
        policy_loss: -0.0016547669656574726
        total_loss: -0.0019677227828651667
        vf_explained_var: 0.02253904938697815
        vf_loss: 3.5589044094085693
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45896077156066895
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013349395012483
        model: {}
        policy_loss: -0.0013751281658187509
        total_loss: -0.0020299889147281647
        vf_explained_var: -0.00015075504779815674
        vf_loss: 1.5290998220443726
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36319470405578613
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006301157991401851
        model: {}
        policy_loss: -0.0012462788727134466
        total_loss: -0.0017773008439689875
        vf_explained_var: 0.009038642048835754
        vf_loss: 1.082004189491272
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5955437421798706
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024066464975476265
        model: {}
        policy_loss: -0.0019914768636226654
        total_loss: -0.0029271338135004044
        vf_explained_var: 0.013130336999893188
        vf_loss: 1.1250146627426147
    load_time_ms: 14177.012
    num_steps_sampled: 79104000
    num_steps_trained: 79104000
    sample_time_ms: 118726.447
    update_time_ms: 15.704
  iterations_since_restore: 174
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.96359223300971
    ram_util_percent: 12.451456310679612
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 17.0
    agent-2: 59.0
    agent-3: 37.0
    agent-4: 30.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 16.08
    agent-1: 9.07
    agent-2: 43.6
    agent-3: 22.57
    agent-4: 15.17
    agent-5: 16.43
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 24.0
    agent-3: 9.0
    agent-4: 7.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.199139273896566
    mean_inference_ms: 15.07667423101819
    mean_processing_ms: 73.3892645489634
  time_since_restore: 25306.89306473732
  time_this_iter_s: 144.3975121974945
  time_total_s: 114243.57119441032
  timestamp: 1637388306
  timesteps_since_restore: 16704000
  timesteps_this_iter: 96000
  timesteps_total: 79104000
  training_iteration: 824
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    824 |           114244 | 79104000 |   122.92 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 6.91
    apples_agent-0_min: 1
    apples_agent-1_max: 14
    apples_agent-1_mean: 3.16
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 28.5
    apples_agent-2_min: 10
    apples_agent-3_max: 59
    apples_agent-3_mean: 9.02
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.37
    apples_agent-4_min: 1
    apples_agent-5_max: 24
    apples_agent-5_mean: 4.68
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 2.47
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 250
    cleaning_beam_agent-1_mean: 218.4
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 4.53
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 8.79
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.41
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.79
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-07-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 161.0
  episode_reward_mean: 124.34
  episode_reward_min: 43.0
  episodes_this_iter: 96
  episodes_total: 79200
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11803.532
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3138195872306824
        entropy_coeff: 0.0017600000137463212
        kl: 0.001113894977606833
        model: {}
        policy_loss: -0.0020195264369249344
        total_loss: -0.0024657128378748894
        vf_explained_var: 0.016896694898605347
        vf_loss: 1.0614006519317627
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21581827104091644
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008441443205811083
        model: {}
        policy_loss: -0.001440300140529871
        total_loss: -0.0017605433240532875
        vf_explained_var: 0.07063907384872437
        vf_loss: 0.5959470272064209
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3733590841293335
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009912042878568172
        model: {}
        policy_loss: -0.001489161280915141
        total_loss: -0.0017924043349921703
        vf_explained_var: 0.03200852870941162
        vf_loss: 3.538701057434082
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45611774921417236
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015406077727675438
        model: {}
        policy_loss: -0.0013963683741167188
        total_loss: -0.0020312389824539423
        vf_explained_var: 0.00606672465801239
        vf_loss: 1.678947925567627
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3824100196361542
        entropy_coeff: 0.0017600000137463212
        kl: 0.001448889379389584
        model: {}
        policy_loss: -0.0013679942348971963
        total_loss: -0.0019360497826710343
        vf_explained_var: 0.00925283133983612
        vf_loss: 1.0498697757720947
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5838320255279541
        entropy_coeff: 0.0017600000137463212
        kl: 0.001563708414323628
        model: {}
        policy_loss: -0.0019246721640229225
        total_loss: -0.002835690975189209
        vf_explained_var: 0.00870417058467865
        vf_loss: 1.1652488708496094
    load_time_ms: 14165.512
    num_steps_sampled: 79200000
    num_steps_trained: 79200000
    sample_time_ms: 118760.626
    update_time_ms: 15.779
  iterations_since_restore: 175
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.01497584541063
    ram_util_percent: 12.366666666666669
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 17.0
    agent-2: 67.0
    agent-3: 35.0
    agent-4: 30.0
    agent-5: 27.0
  policy_reward_mean:
    agent-0: 16.53
    agent-1: 8.84
    agent-2: 44.27
    agent-3: 22.91
    agent-4: 14.92
    agent-5: 16.87
  policy_reward_min:
    agent-0: 3.0
    agent-1: 0.0
    agent-2: 18.0
    agent-3: 5.0
    agent-4: 6.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.19986783813365
    mean_inference_ms: 15.076948863119057
    mean_processing_ms: 73.38839344979438
  time_since_restore: 25452.32714152336
  time_this_iter_s: 145.43407678604126
  time_total_s: 114389.00527119637
  timestamp: 1637388452
  timesteps_since_restore: 16800000
  timesteps_this_iter: 96000
  timesteps_total: 79200000
  training_iteration: 825
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    825 |           114389 | 79200000 |   124.34 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 7.54
    apples_agent-0_min: 2
    apples_agent-1_max: 26
    apples_agent-1_mean: 3.4
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 29.94
    apples_agent-2_min: 11
    apples_agent-3_max: 32
    apples_agent-3_mean: 8.73
    apples_agent-3_min: 3
    apples_agent-4_max: 29
    apples_agent-4_mean: 10.3
    apples_agent-4_min: 2
    apples_agent-5_max: 20
    apples_agent-5_mean: 3.84
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 2.23
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 258
    cleaning_beam_agent-1_mean: 221.25
    cleaning_beam_agent-1_min: 162
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 4.44
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 21
    cleaning_beam_agent-3_mean: 8.7
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.03
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.58
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-09-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 169.0
  episode_reward_mean: 126.58
  episode_reward_min: 73.0
  episodes_this_iter: 96
  episodes_total: 79296
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11807.778
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32521548867225647
        entropy_coeff: 0.0017600000137463212
        kl: 0.000879908911883831
        model: {}
        policy_loss: -0.0018224949017167091
        total_loss: -0.002286522649228573
        vf_explained_var: 0.0006371289491653442
        vf_loss: 1.083532452583313
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21902765333652496
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009775892831385136
        model: {}
        policy_loss: -0.0015192963182926178
        total_loss: -0.0018532420508563519
        vf_explained_var: 0.07673615217208862
        vf_loss: 0.5154143571853638
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37291014194488525
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016124866669997573
        model: {}
        policy_loss: -0.0017954339273273945
        total_loss: -0.0020776931196451187
        vf_explained_var: 0.022387981414794922
        vf_loss: 3.740612030029297
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4613730013370514
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010107792913913727
        model: {}
        policy_loss: -0.0013245209120213985
        total_loss: -0.00197006668895483
        vf_explained_var: -0.0011993646621704102
        vf_loss: 1.6647213697433472
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3889537453651428
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008014635532163084
        model: {}
        policy_loss: -0.001174152479507029
        total_loss: -0.0017523764399811625
        vf_explained_var: 0.004520431160926819
        vf_loss: 1.0633453130722046
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5960500240325928
        entropy_coeff: 0.0017600000137463212
        kl: 0.001876707305200398
        model: {}
        policy_loss: -0.0018499153666198254
        total_loss: -0.0027770856395363808
        vf_explained_var: 0.010999873280525208
        vf_loss: 1.2187724113464355
    load_time_ms: 14159.836
    num_steps_sampled: 79296000
    num_steps_trained: 79296000
    sample_time_ms: 118772.517
    update_time_ms: 15.764
  iterations_since_restore: 176
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.988888888888887
    ram_util_percent: 12.366666666666667
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 66.0
    agent-3: 33.0
    agent-4: 32.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 16.52
    agent-1: 8.91
    agent-2: 45.5
    agent-3: 23.01
    agent-4: 15.4
    agent-5: 17.24
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 21.0
    agent-3: -20.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.199790677442937
    mean_inference_ms: 15.07676179324353
    mean_processing_ms: 73.38917496306426
  time_since_restore: 25597.283787727356
  time_this_iter_s: 144.95664620399475
  time_total_s: 114533.96191740036
  timestamp: 1637388597
  timesteps_since_restore: 16896000
  timesteps_this_iter: 96000
  timesteps_total: 79296000
  training_iteration: 826
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    826 |           114534 | 79296000 |   126.58 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 6.85
    apples_agent-0_min: 0
    apples_agent-1_max: 39
    apples_agent-1_mean: 3.52
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 30.07
    apples_agent-2_min: 16
    apples_agent-3_max: 25
    apples_agent-3_mean: 8.43
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.83
    apples_agent-4_min: 3
    apples_agent-5_max: 14
    apples_agent-5_mean: 4.19
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 42
    cleaning_beam_agent-0_mean: 3.09
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 256
    cleaning_beam_agent-1_mean: 222.72
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 4.5
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 9.53
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 7
    cleaning_beam_agent-4_mean: 2.74
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.81
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-12-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 172.0
  episode_reward_mean: 124.21
  episode_reward_min: 58.0
  episodes_this_iter: 96
  episodes_total: 79392
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11801.51
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31894806027412415
        entropy_coeff: 0.0017600000137463212
        kl: 0.000974149676039815
        model: {}
        policy_loss: -0.001908021979033947
        total_loss: -0.0023634801618754864
        vf_explained_var: -0.004492849111557007
        vf_loss: 1.0589114427566528
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21439003944396973
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011838226346299052
        model: {}
        policy_loss: -0.0017458220245316625
        total_loss: -0.002069448120892048
        vf_explained_var: 0.07110431790351868
        vf_loss: 0.5369993448257446
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3768942356109619
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012904326431453228
        model: {}
        policy_loss: -0.0017344471998512745
        total_loss: -0.001991206780076027
        vf_explained_var: 0.023005753755569458
        vf_loss: 4.065732479095459
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45842769742012024
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012605955125764012
        model: {}
        policy_loss: -0.0013695459347218275
        total_loss: -0.0020026694983243942
        vf_explained_var: 0.00549355149269104
        vf_loss: 1.7371209859848022
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39530840516090393
        entropy_coeff: 0.0017600000137463212
        kl: 0.000821246299892664
        model: {}
        policy_loss: -0.0013809840893372893
        total_loss: -0.00197515357285738
        vf_explained_var: 0.011840060353279114
        vf_loss: 1.015733003616333
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5994101762771606
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014158084522932768
        model: {}
        policy_loss: -0.0017862692475318909
        total_loss: -0.0027187513187527657
        vf_explained_var: 0.004756897687911987
        vf_loss: 1.2248094081878662
    load_time_ms: 14140.074
    num_steps_sampled: 79392000
    num_steps_trained: 79392000
    sample_time_ms: 119035.342
    update_time_ms: 15.956
  iterations_since_restore: 177
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.802884615384617
    ram_util_percent: 12.356730769230769
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 21.0
    agent-2: 70.0
    agent-3: 40.0
    agent-4: 25.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 14.6
    agent-1: 8.62
    agent-2: 46.61
    agent-3: 22.76
    agent-4: 14.48
    agent-5: 17.14
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 26.0
    agent-3: 6.0
    agent-4: 3.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.199738655788256
    mean_inference_ms: 15.0765418106456
    mean_processing_ms: 73.38744796161797
  time_since_restore: 25743.680272340775
  time_this_iter_s: 146.39648461341858
  time_total_s: 114680.35840201378
  timestamp: 1637388743
  timesteps_since_restore: 16992000
  timesteps_this_iter: 96000
  timesteps_total: 79392000
  training_iteration: 827
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    827 |           114680 | 79392000 |   124.21 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 6.28
    apples_agent-0_min: 0
    apples_agent-1_max: 21
    apples_agent-1_mean: 3.43
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 27.51
    apples_agent-2_min: 1
    apples_agent-3_max: 38
    apples_agent-3_mean: 9.17
    apples_agent-3_min: 0
    apples_agent-4_max: 37
    apples_agent-4_mean: 9.88
    apples_agent-4_min: 2
    apples_agent-5_max: 20
    apples_agent-5_mean: 4.28
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 26
    cleaning_beam_agent-0_mean: 3.01
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 267
    cleaning_beam_agent-1_mean: 214.27
    cleaning_beam_agent-1_min: 15
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 4.95
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 28
    cleaning_beam_agent-3_mean: 10.23
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.84
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 3.95
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-14-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 156.0
  episode_reward_mean: 119.16
  episode_reward_min: 10.0
  episodes_this_iter: 96
  episodes_total: 79488
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11801.491
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.308685302734375
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012752179754897952
        model: {}
        policy_loss: -0.0018398058600723743
        total_loss: -0.002273328136652708
        vf_explained_var: 0.007559940218925476
        vf_loss: 1.0976299047470093
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21151427924633026
        entropy_coeff: 0.0017600000137463212
        kl: 0.000851056189276278
        model: {}
        policy_loss: -0.0016093207523226738
        total_loss: -0.0019305727910250425
        vf_explained_var: 0.08807708323001862
        vf_loss: 0.5101718306541443
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38698816299438477
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016253532376140356
        model: {}
        policy_loss: -0.001880517229437828
        total_loss: -0.002170819789171219
        vf_explained_var: 0.021147921681404114
        vf_loss: 3.907925605773926
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4607362151145935
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015642544021829963
        model: {}
        policy_loss: -0.0014692721888422966
        total_loss: -0.0021203747019171715
        vf_explained_var: 0.0014247149229049683
        vf_loss: 1.5979235172271729
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3881552219390869
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012414094526320696
        model: {}
        policy_loss: -0.0015197591856122017
        total_loss: -0.0020996560342609882
        vf_explained_var: 0.010077401995658875
        vf_loss: 1.0325714349746704
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5969489216804504
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016704679001122713
        model: {}
        policy_loss: -0.0020160931162536144
        total_loss: -0.0029508168809115887
        vf_explained_var: 0.007468134164810181
        vf_loss: 1.1590667963027954
    load_time_ms: 14152.275
    num_steps_sampled: 79488000
    num_steps_trained: 79488000
    sample_time_ms: 119081.815
    update_time_ms: 15.941
  iterations_since_restore: 178
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.92415458937198
    ram_util_percent: 12.370531400966183
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 18.0
    agent-2: 63.0
    agent-3: 32.0
    agent-4: 26.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 15.62
    agent-1: 8.58
    agent-2: 43.24
    agent-3: 21.73
    agent-4: 13.45
    agent-5: 16.54
  policy_reward_min:
    agent-0: 1.0
    agent-1: 1.0
    agent-2: 5.0
    agent-3: 0.0
    agent-4: -35.0
    agent-5: 1.0
  sampler_perf:
    mean_env_wait_ms: 28.200295748390012
    mean_inference_ms: 15.075971720224334
    mean_processing_ms: 73.38728282524148
  time_since_restore: 25888.595690965652
  time_this_iter_s: 144.91541862487793
  time_total_s: 114825.27382063866
  timestamp: 1637388888
  timesteps_since_restore: 17088000
  timesteps_this_iter: 96000
  timesteps_total: 79488000
  training_iteration: 828
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    828 |           114825 | 79488000 |   119.16 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 7.06
    apples_agent-0_min: 1
    apples_agent-1_max: 53
    apples_agent-1_mean: 3.88
    apples_agent-1_min: 0
    apples_agent-2_max: 54
    apples_agent-2_mean: 29.75
    apples_agent-2_min: 15
    apples_agent-3_max: 35
    apples_agent-3_mean: 8.96
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.63
    apples_agent-4_min: 1
    apples_agent-5_max: 16
    apples_agent-5_mean: 4.28
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 19
    cleaning_beam_agent-0_mean: 3.06
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 275
    cleaning_beam_agent-1_mean: 223.29
    cleaning_beam_agent-1_min: 162
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 4.63
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 9.79
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.96
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.07
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-17-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 179.0
  episode_reward_mean: 126.51
  episode_reward_min: 82.0
  episodes_this_iter: 96
  episodes_total: 79584
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11806.648
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30803564190864563
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008315877639688551
        model: {}
        policy_loss: -0.0016643921844661236
        total_loss: -0.0021072644740343094
        vf_explained_var: 0.0032814741134643555
        vf_loss: 0.9927193522453308
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21651852130889893
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012783700367435813
        model: {}
        policy_loss: -0.0016054832376539707
        total_loss: -0.0019271780038252473
        vf_explained_var: 0.07131707668304443
        vf_loss: 0.5937955379486084
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3834502100944519
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011677772272378206
        model: {}
        policy_loss: -0.0018228804692626
        total_loss: -0.002122899517416954
        vf_explained_var: 0.02087894082069397
        vf_loss: 3.7485227584838867
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46145540475845337
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012448353227227926
        model: {}
        policy_loss: -0.0014593726955354214
        total_loss: -0.0021010576747357845
        vf_explained_var: 0.007498413324356079
        vf_loss: 1.7047700881958008
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3790457546710968
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006742806290276349
        model: {}
        policy_loss: -0.0011686371872201562
        total_loss: -0.0017191569786518812
        vf_explained_var: 0.004688575863838196
        vf_loss: 1.1660112142562866
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5937116742134094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015467916382476687
        model: {}
        policy_loss: -0.001696188934147358
        total_loss: -0.0026158709079027176
        vf_explained_var: 0.009292036294937134
        vf_loss: 1.2525149583816528
    load_time_ms: 14134.818
    num_steps_sampled: 79584000
    num_steps_trained: 79584000
    sample_time_ms: 119074.537
    update_time_ms: 16.113
  iterations_since_restore: 179
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.88840579710145
    ram_util_percent: 12.371014492753623
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 21.0
    agent-2: 72.0
    agent-3: 38.0
    agent-4: 27.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 15.78
    agent-1: 9.41
    agent-2: 44.84
    agent-3: 23.15
    agent-4: 15.61
    agent-5: 17.72
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 28.0
    agent-3: 10.0
    agent-4: 4.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.20051325623691
    mean_inference_ms: 15.075410413887182
    mean_processing_ms: 73.38527563729065
  time_since_restore: 26033.936815023422
  time_this_iter_s: 145.34112405776978
  time_total_s: 114970.61494469643
  timestamp: 1637389034
  timesteps_since_restore: 17184000
  timesteps_this_iter: 96000
  timesteps_total: 79584000
  training_iteration: 829
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    829 |           114971 | 79584000 |   126.51 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 7.18
    apples_agent-0_min: 1
    apples_agent-1_max: 7
    apples_agent-1_mean: 2.69
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 29.35
    apples_agent-2_min: 15
    apples_agent-3_max: 37
    apples_agent-3_mean: 8.93
    apples_agent-3_min: 2
    apples_agent-4_max: 26
    apples_agent-4_mean: 10.66
    apples_agent-4_min: 3
    apples_agent-5_max: 19
    apples_agent-5_mean: 4.48
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 23
    cleaning_beam_agent-0_mean: 2.93
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 272
    cleaning_beam_agent-1_mean: 224.04
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.58
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 33
    cleaning_beam_agent-3_mean: 10.76
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.05
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.19
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-19-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 178.0
  episode_reward_mean: 124.95
  episode_reward_min: 47.0
  episodes_this_iter: 96
  episodes_total: 79680
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11804.478
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31028276681900024
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007483555236831307
        model: {}
        policy_loss: -0.0017971217166632414
        total_loss: -0.0022354170214384794
        vf_explained_var: 0.008403435349464417
        vf_loss: 1.0780129432678223
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21585184335708618
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007990439189597964
        model: {}
        policy_loss: -0.0012838449329137802
        total_loss: -0.0016107596457004547
        vf_explained_var: 0.08574818074703217
        vf_loss: 0.5298328399658203
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3913238048553467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012238441267982125
        model: {}
        policy_loss: -0.0016162912361323833
        total_loss: -0.0019330424256622791
        vf_explained_var: 0.04355679452419281
        vf_loss: 3.7198033332824707
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47028231620788574
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013491457793861628
        model: {}
        policy_loss: -0.001467626541852951
        total_loss: -0.002138650044798851
        vf_explained_var: 0.0002515166997909546
        vf_loss: 1.5667312145233154
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37347838282585144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005773688899353147
        model: {}
        policy_loss: -0.0012648948468267918
        total_loss: -0.0018159239552915096
        vf_explained_var: 0.005257338285446167
        vf_loss: 1.062898874282837
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5948206782341003
        entropy_coeff: 0.0017600000137463212
        kl: 0.002029502298682928
        model: {}
        policy_loss: -0.0018206755630671978
        total_loss: -0.002733662724494934
        vf_explained_var: 0.006915748119354248
        vf_loss: 1.3389701843261719
    load_time_ms: 14121.494
    num_steps_sampled: 79680000
    num_steps_trained: 79680000
    sample_time_ms: 118948.675
    update_time_ms: 16.147
  iterations_since_restore: 180
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.97378640776699
    ram_util_percent: 12.369417475728158
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 15.0
    agent-2: 63.0
    agent-3: 35.0
    agent-4: 32.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 15.99
    agent-1: 8.68
    agent-2: 45.18
    agent-3: 22.54
    agent-4: 14.69
    agent-5: 17.87
  policy_reward_min:
    agent-0: 5.0
    agent-1: 1.0
    agent-2: 23.0
    agent-3: 9.0
    agent-4: -37.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.200776293422727
    mean_inference_ms: 15.074754916809567
    mean_processing_ms: 73.38332914581528
  time_since_restore: 26178.43124127388
  time_this_iter_s: 144.49442625045776
  time_total_s: 115115.10937094688
  timestamp: 1637389178
  timesteps_since_restore: 17280000
  timesteps_this_iter: 96000
  timesteps_total: 79680000
  training_iteration: 830
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    830 |           115115 | 79680000 |   124.95 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 7.61
    apples_agent-0_min: 1
    apples_agent-1_max: 18
    apples_agent-1_mean: 3.4
    apples_agent-1_min: 0
    apples_agent-2_max: 55
    apples_agent-2_mean: 30.64
    apples_agent-2_min: 17
    apples_agent-3_max: 28
    apples_agent-3_mean: 9.13
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.8
    apples_agent-4_min: 4
    apples_agent-5_max: 14
    apples_agent-5_mean: 4.36
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 2.97
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 268
    cleaning_beam_agent-1_mean: 231.32
    cleaning_beam_agent-1_min: 184
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 4.33
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 28
    cleaning_beam_agent-3_mean: 10.9
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 2.74
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.98
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-22-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 187.0
  episode_reward_mean: 132.09
  episode_reward_min: 94.0
  episodes_this_iter: 96
  episodes_total: 79776
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11792.139
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3008836805820465
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012827052269130945
        model: {}
        policy_loss: -0.001959670800715685
        total_loss: -0.002380230464041233
        vf_explained_var: 0.005259573459625244
        vf_loss: 1.0899577140808105
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21684962511062622
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010108469286933541
        model: {}
        policy_loss: -0.0014265887439250946
        total_loss: -0.0017499702516943216
        vf_explained_var: 0.07379445433616638
        vf_loss: 0.5827357769012451
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37662196159362793
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011581780854612589
        model: {}
        policy_loss: -0.0017074295319616795
        total_loss: -0.0019627094734460115
        vf_explained_var: 0.016526490449905396
        vf_loss: 4.075758457183838
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47013354301452637
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016686642775312066
        model: {}
        policy_loss: -0.0015453551895916462
        total_loss: -0.002188334707170725
        vf_explained_var: 0.008722975850105286
        vf_loss: 1.8445699214935303
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3783591091632843
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007830359973013401
        model: {}
        policy_loss: -0.0014629173092544079
        total_loss: -0.002026236616075039
        vf_explained_var: 0.007662743330001831
        vf_loss: 1.0259407758712769
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5908427238464355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020607151091098785
        model: {}
        policy_loss: -0.0017978549003601074
        total_loss: -0.0027164011262357235
        vf_explained_var: 0.010182738304138184
        vf_loss: 1.2133636474609375
    load_time_ms: 14129.505
    num_steps_sampled: 79776000
    num_steps_trained: 79776000
    sample_time_ms: 118891.911
    update_time_ms: 15.996
  iterations_since_restore: 181
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.712857142857146
    ram_util_percent: 12.362857142857141
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 19.0
    agent-2: 68.0
    agent-3: 38.0
    agent-4: 29.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 16.75
    agent-1: 9.38
    agent-2: 47.67
    agent-3: 24.38
    agent-4: 15.59
    agent-5: 18.32
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 32.0
    agent-3: 14.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.20130124716784
    mean_inference_ms: 15.074500551763379
    mean_processing_ms: 73.38340478686838
  time_since_restore: 26323.74614548683
  time_this_iter_s: 145.31490421295166
  time_total_s: 115260.42427515984
  timestamp: 1637389326
  timesteps_since_restore: 17376000
  timesteps_this_iter: 96000
  timesteps_total: 79776000
  training_iteration: 831
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    831 |           115260 | 79776000 |   132.09 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 6.76
    apples_agent-0_min: 1
    apples_agent-1_max: 17
    apples_agent-1_mean: 3.27
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 30.32
    apples_agent-2_min: 13
    apples_agent-3_max: 40
    apples_agent-3_mean: 8.85
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.95
    apples_agent-4_min: 3
    apples_agent-5_max: 17
    apples_agent-5_mean: 4.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 20
    cleaning_beam_agent-0_mean: 2.75
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 261
    cleaning_beam_agent-1_mean: 226.96
    cleaning_beam_agent-1_min: 189
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 4.17
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 11.03
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.68
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 4.85
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-24-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 197.0
  episode_reward_mean: 129.09
  episode_reward_min: 91.0
  episodes_this_iter: 96
  episodes_total: 79872
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11793.689
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3024277687072754
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007785398047417402
        model: {}
        policy_loss: -0.0017042262479662895
        total_loss: -0.0021358896046876907
        vf_explained_var: 0.007972195744514465
        vf_loss: 1.006058692932129
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2156284600496292
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010145669803023338
        model: {}
        policy_loss: -0.0014930758625268936
        total_loss: -0.001814347691833973
        vf_explained_var: 0.08788320422172546
        vf_loss: 0.5823688507080078
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3768024742603302
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014161024009808898
        model: {}
        policy_loss: -0.0017812021542340517
        total_loss: -0.0020637090783566236
        vf_explained_var: 0.013065844774246216
        vf_loss: 3.806659460067749
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4630793333053589
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016522654332220554
        model: {}
        policy_loss: -0.0014302795752882957
        total_loss: -0.0020760211627930403
        vf_explained_var: 0.008909150958061218
        vf_loss: 1.6927950382232666
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3863060474395752
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009893105598166585
        model: {}
        policy_loss: -0.001442087348550558
        total_loss: -0.0020065666176378727
        vf_explained_var: 0.012639820575714111
        vf_loss: 1.1541920900344849
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5796322822570801
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017821344081312418
        model: {}
        policy_loss: -0.0018667812692001462
        total_loss: -0.0027479268610477448
        vf_explained_var: 0.013548031449317932
        vf_loss: 1.3900593519210815
    load_time_ms: 14112.926
    num_steps_sampled: 79872000
    num_steps_trained: 79872000
    sample_time_ms: 118841.173
    update_time_ms: 16.066
  iterations_since_restore: 182
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.985436893203882
    ram_util_percent: 12.444660194174755
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 21.0
    agent-2: 69.0
    agent-3: 38.0
    agent-4: 28.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 15.78
    agent-1: 9.66
    agent-2: 46.09
    agent-3: 23.39
    agent-4: 15.51
    agent-5: 18.66
  policy_reward_min:
    agent-0: 6.0
    agent-1: 4.0
    agent-2: 26.0
    agent-3: 11.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.20176904108504
    mean_inference_ms: 15.073905360273805
    mean_processing_ms: 73.38264199262314
  time_since_restore: 26468.625638484955
  time_this_iter_s: 144.87949299812317
  time_total_s: 115405.30376815796
  timestamp: 1637389471
  timesteps_since_restore: 17472000
  timesteps_this_iter: 96000
  timesteps_total: 79872000
  training_iteration: 832
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    832 |           115405 | 79872000 |   129.09 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 7.89
    apples_agent-0_min: 1
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.7
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 30.77
    apples_agent-2_min: 18
    apples_agent-3_max: 32
    apples_agent-3_mean: 8.62
    apples_agent-3_min: 2
    apples_agent-4_max: 33
    apples_agent-4_mean: 11.42
    apples_agent-4_min: 5
    apples_agent-5_max: 22
    apples_agent-5_mean: 3.99
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 17
    cleaning_beam_agent-0_mean: 2.91
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 262
    cleaning_beam_agent-1_mean: 230.32
    cleaning_beam_agent-1_min: 174
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 4.4
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 9.8
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.64
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 5.21
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-26-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 197.0
  episode_reward_mean: 130.05
  episode_reward_min: 85.0
  episodes_this_iter: 96
  episodes_total: 79968
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11802.709
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3003776967525482
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012216335162520409
        model: {}
        policy_loss: -0.0019894083961844444
        total_loss: -0.002414088696241379
        vf_explained_var: 0.002268478274345398
        vf_loss: 1.0398523807525635
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21640130877494812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013169359881430864
        model: {}
        policy_loss: -0.0014823444653302431
        total_loss: -0.0017996442038565874
        vf_explained_var: 0.07135194540023804
        vf_loss: 0.6356582641601562
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38143059611320496
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010596986394375563
        model: {}
        policy_loss: -0.0018227086402475834
        total_loss: -0.0021153753623366356
        vf_explained_var: 0.014641255140304565
        vf_loss: 3.786503553390503
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45601823925971985
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013973002787679434
        model: {}
        policy_loss: -0.0016123843379318714
        total_loss: -0.0022580986842513084
        vf_explained_var: 0.0016956925392150879
        vf_loss: 1.5687716007232666
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.388577401638031
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009927554056048393
        model: {}
        policy_loss: -0.0013993780594319105
        total_loss: -0.001971437595784664
        vf_explained_var: 0.009339302778244019
        vf_loss: 1.1183652877807617
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5728362798690796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017743015196174383
        model: {}
        policy_loss: -0.001704429741948843
        total_loss: -0.0025919731706380844
        vf_explained_var: 0.01989687979221344
        vf_loss: 1.2064704895019531
    load_time_ms: 14145.242
    num_steps_sampled: 79968000
    num_steps_trained: 79968000
    sample_time_ms: 119000.8
    update_time_ms: 16.213
  iterations_since_restore: 183
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.83932038834952
    ram_util_percent: 12.369417475728158
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 19.0
    agent-2: 69.0
    agent-3: 38.0
    agent-4: 28.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 16.58
    agent-1: 9.5
    agent-2: 46.26
    agent-3: 23.46
    agent-4: 16.1
    agent-5: 18.15
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 28.0
    agent-3: 12.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.20191302311897
    mean_inference_ms: 15.073235087694648
    mean_processing_ms: 73.37743924633016
  time_since_restore: 26613.075852155685
  time_this_iter_s: 144.4502136707306
  time_total_s: 115549.75398182869
  timestamp: 1637389615
  timesteps_since_restore: 17568000
  timesteps_this_iter: 96000
  timesteps_total: 79968000
  training_iteration: 833
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    833 |           115550 | 79968000 |   130.05 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 6.81
    apples_agent-0_min: 1
    apples_agent-1_max: 12
    apples_agent-1_mean: 3.18
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 29.68
    apples_agent-2_min: 15
    apples_agent-3_max: 46
    apples_agent-3_mean: 9.72
    apples_agent-3_min: 0
    apples_agent-4_max: 35
    apples_agent-4_mean: 11.14
    apples_agent-4_min: 3
    apples_agent-5_max: 15
    apples_agent-5_mean: 4.23
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 34
    cleaning_beam_agent-0_mean: 3.64
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 277
    cleaning_beam_agent-1_mean: 236.55
    cleaning_beam_agent-1_min: 64
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 4.63
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 28
    cleaning_beam_agent-3_mean: 9.11
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.04
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 4.57
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-29-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 177.0
  episode_reward_mean: 128.31
  episode_reward_min: 69.0
  episodes_this_iter: 96
  episodes_total: 80064
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11807.345
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29570627212524414
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018466291949152946
        model: {}
        policy_loss: -0.001741301268339157
        total_loss: -0.002147614024579525
        vf_explained_var: -0.008075714111328125
        vf_loss: 1.1412969827651978
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21841788291931152
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011989667546004057
        model: {}
        policy_loss: -0.0015872078947722912
        total_loss: -0.001917973393574357
        vf_explained_var: 0.09526017308235168
        vf_loss: 0.5364879369735718
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3762435019016266
        entropy_coeff: 0.0017600000137463212
        kl: 0.001168334623798728
        model: {}
        policy_loss: -0.0017318744212388992
        total_loss: -0.0020121827255934477
        vf_explained_var: 0.03132618963718414
        vf_loss: 3.8188085556030273
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45434099435806274
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016017673769965768
        model: {}
        policy_loss: -0.0016986401751637459
        total_loss: -0.0023171035572886467
        vf_explained_var: 0.00636880099773407
        vf_loss: 1.8117852210998535
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3952077031135559
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010318768909201026
        model: {}
        policy_loss: -0.0012697242200374603
        total_loss: -0.001851721666753292
        vf_explained_var: 0.009316995739936829
        vf_loss: 1.1356745958328247
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5713707208633423
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016409355448558927
        model: {}
        policy_loss: -0.0015219543129205704
        total_loss: -0.0023914100602269173
        vf_explained_var: 0.012074574828147888
        vf_loss: 1.361558198928833
    load_time_ms: 14101.52
    num_steps_sampled: 80064000
    num_steps_trained: 80064000
    sample_time_ms: 119068.433
    update_time_ms: 16.924
  iterations_since_restore: 184
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.27281553398058
    ram_util_percent: 12.364077669902912
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 16.0
    agent-2: 60.0
    agent-3: 41.0
    agent-4: 25.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 15.95
    agent-1: 8.57
    agent-2: 45.85
    agent-3: 23.66
    agent-4: 15.69
    agent-5: 18.59
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 23.0
    agent-3: 7.0
    agent-4: 7.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.2026688988542
    mean_inference_ms: 15.07309132331411
    mean_processing_ms: 73.37633295431345
  time_since_restore: 26757.697563648224
  time_this_iter_s: 144.62171149253845
  time_total_s: 115694.37569332123
  timestamp: 1637389760
  timesteps_since_restore: 17664000
  timesteps_this_iter: 96000
  timesteps_total: 80064000
  training_iteration: 834
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    834 |           115694 | 80064000 |   128.31 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 6.79
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 3.42
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 29.91
    apples_agent-2_min: 13
    apples_agent-3_max: 27
    apples_agent-3_mean: 8.22
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.54
    apples_agent-4_min: 4
    apples_agent-5_max: 21
    apples_agent-5_mean: 4.85
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 2.86
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 277
    cleaning_beam_agent-1_mean: 232.32
    cleaning_beam_agent-1_min: 168
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 4.38
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 20
    cleaning_beam_agent-3_mean: 8.65
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.81
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 4.64
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-31-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 173.0
  episode_reward_mean: 126.79
  episode_reward_min: 77.0
  episodes_this_iter: 96
  episodes_total: 80160
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11803.344
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29152458906173706
        entropy_coeff: 0.0017600000137463212
        kl: 0.000648684857878834
        model: {}
        policy_loss: -0.0015821438282728195
        total_loss: -0.001986926421523094
        vf_explained_var: 0.004906103014945984
        vf_loss: 1.0830249786376953
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22120076417922974
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009194622980430722
        model: {}
        policy_loss: -0.0014822091907262802
        total_loss: -0.001811307854950428
        vf_explained_var: 0.0804792046546936
        vf_loss: 0.602172315120697
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37132754921913147
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013765125768259168
        model: {}
        policy_loss: -0.0017831744626164436
        total_loss: -0.002042011823505163
        vf_explained_var: 0.012865275144577026
        vf_loss: 3.9469685554504395
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4566839933395386
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021361077670007944
        model: {}
        policy_loss: -0.00154868233948946
        total_loss: -0.0021721250377595425
        vf_explained_var: -4.267692565917969e-05
        vf_loss: 1.803191900253296
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38994497060775757
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007574795745313168
        model: {}
        policy_loss: -0.0011125635355710983
        total_loss: -0.0016893026186153293
        vf_explained_var: 0.007716760039329529
        vf_loss: 1.0956244468688965
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.574774444103241
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019374571274966002
        model: {}
        policy_loss: -0.001784149557352066
        total_loss: -0.0026622051373124123
        vf_explained_var: -0.00038658082485198975
        vf_loss: 1.3354871273040771
    load_time_ms: 14118.868
    num_steps_sampled: 80160000
    num_steps_trained: 80160000
    sample_time_ms: 118690.58
    update_time_ms: 16.793
  iterations_since_restore: 185
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.878217821782176
    ram_util_percent: 12.388118811881187
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 19.0
    agent-2: 67.0
    agent-3: 41.0
    agent-4: 29.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 16.12
    agent-1: 9.0
    agent-2: 45.6
    agent-3: 23.05
    agent-4: 15.07
    agent-5: 17.95
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 21.0
    agent-3: 11.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.197777613271764
    mean_inference_ms: 15.07085375222387
    mean_processing_ms: 73.35919109872557
  time_since_restore: 26899.49015903473
  time_this_iter_s: 141.79259538650513
  time_total_s: 115836.16828870773
  timestamp: 1637389902
  timesteps_since_restore: 17760000
  timesteps_this_iter: 96000
  timesteps_total: 80160000
  training_iteration: 835
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    835 |           115836 | 80160000 |   126.79 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 6.66
    apples_agent-0_min: 2
    apples_agent-1_max: 43
    apples_agent-1_mean: 3.76
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 29.03
    apples_agent-2_min: 11
    apples_agent-3_max: 34
    apples_agent-3_mean: 8.72
    apples_agent-3_min: 2
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.3
    apples_agent-4_min: 2
    apples_agent-5_max: 15
    apples_agent-5_mean: 3.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 2.97
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 286
    cleaning_beam_agent-1_mean: 232.89
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 4.41
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 28
    cleaning_beam_agent-3_mean: 9.66
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 7
    cleaning_beam_agent-4_mean: 2.56
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 4.4
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-34-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 164.0
  episode_reward_mean: 127.08
  episode_reward_min: 55.0
  episodes_this_iter: 96
  episodes_total: 80256
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11810.303
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29436588287353516
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008982195286080241
        model: {}
        policy_loss: -0.0017120442353188992
        total_loss: -0.002109324559569359
        vf_explained_var: 0.0038523823022842407
        vf_loss: 1.2080401182174683
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21645736694335938
        entropy_coeff: 0.0017600000137463212
        kl: 0.000783762545324862
        model: {}
        policy_loss: -0.0014911359176039696
        total_loss: -0.0018218662589788437
        vf_explained_var: 0.08972159028053284
        vf_loss: 0.5023277997970581
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.378069669008255
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009942353935912251
        model: {}
        policy_loss: -0.0017032782780006528
        total_loss: -0.0019970694556832314
        vf_explained_var: 0.03923659026622772
        vf_loss: 3.716085910797119
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4570264220237732
        entropy_coeff: 0.0017600000137463212
        kl: 0.001554456539452076
        model: {}
        policy_loss: -0.0012882237788289785
        total_loss: -0.001917817979119718
        vf_explained_var: 0.0050019919872283936
        vf_loss: 1.74770987033844
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3803529441356659
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008914772770367563
        model: {}
        policy_loss: -0.00125191081315279
        total_loss: -0.0018103588372468948
        vf_explained_var: 0.011690497398376465
        vf_loss: 1.109715223312378
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5670763254165649
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011547831818461418
        model: {}
        policy_loss: -0.0015786029398441315
        total_loss: -0.0024473117664456367
        vf_explained_var: 0.015437155961990356
        vf_loss: 1.2934749126434326
    load_time_ms: 14126.277
    num_steps_sampled: 80256000
    num_steps_trained: 80256000
    sample_time_ms: 118696.66
    update_time_ms: 16.97
  iterations_since_restore: 186
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.90434782608696
    ram_util_percent: 12.447826086956521
  pid: 27065
  policy_reward_max:
    agent-0: 33.0
    agent-1: 16.0
    agent-2: 66.0
    agent-3: 42.0
    agent-4: 25.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 16.56
    agent-1: 8.3
    agent-2: 45.3
    agent-3: 23.38
    agent-4: 15.84
    agent-5: 17.7
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 22.0
    agent-3: 6.0
    agent-4: 4.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.197883375101238
    mean_inference_ms: 15.070757873402194
    mean_processing_ms: 73.35674625158012
  time_since_restore: 27044.617087602615
  time_this_iter_s: 145.12692856788635
  time_total_s: 115981.29521727562
  timestamp: 1637390047
  timesteps_since_restore: 17856000
  timesteps_this_iter: 96000
  timesteps_total: 80256000
  training_iteration: 836
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    836 |           115981 | 80256000 |   127.08 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 6.55
    apples_agent-0_min: 2
    apples_agent-1_max: 43
    apples_agent-1_mean: 3.43
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 30.77
    apples_agent-2_min: 16
    apples_agent-3_max: 49
    apples_agent-3_mean: 9.39
    apples_agent-3_min: 2
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.49
    apples_agent-4_min: 2
    apples_agent-5_max: 18
    apples_agent-5_mean: 3.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 36
    cleaning_beam_agent-0_mean: 3.71
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 288
    cleaning_beam_agent-1_mean: 236.88
    cleaning_beam_agent-1_min: 166
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 5.12
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 9.22
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 2.86
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 4.43
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-36-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 168.0
  episode_reward_mean: 130.11
  episode_reward_min: 85.0
  episodes_this_iter: 96
  episodes_total: 80352
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11812.735
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2937587797641754
        entropy_coeff: 0.0017600000137463212
        kl: 0.000815223902463913
        model: {}
        policy_loss: -0.0018022512085735798
        total_loss: -0.0022230935283005238
        vf_explained_var: 0.00812770426273346
        vf_loss: 0.9617267847061157
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22147315740585327
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011087256716564298
        model: {}
        policy_loss: -0.001392727019265294
        total_loss: -0.0017324397340416908
        vf_explained_var: 0.07500936090946198
        vf_loss: 0.5008007884025574
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37035179138183594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008849415462464094
        model: {}
        policy_loss: -0.0017266166396439075
        total_loss: -0.001983232796192169
        vf_explained_var: 0.014343231916427612
        vf_loss: 3.9520559310913086
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46023333072662354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021300686057657003
        model: {}
        policy_loss: -0.0016037644818425179
        total_loss: -0.0022242593113332987
        vf_explained_var: 0.008579224348068237
        vf_loss: 1.8951678276062012
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3797968029975891
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007895687012933195
        model: {}
        policy_loss: -0.0011846597772091627
        total_loss: -0.001750727416947484
        vf_explained_var: 0.004756957292556763
        vf_loss: 1.0237700939178467
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.561906099319458
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012948736548423767
        model: {}
        policy_loss: -0.001669340766966343
        total_loss: -0.002541959285736084
        vf_explained_var: 0.021937206387519836
        vf_loss: 1.1633391380310059
    load_time_ms: 14129.591
    num_steps_sampled: 80352000
    num_steps_trained: 80352000
    sample_time_ms: 118552.136
    update_time_ms: 17.003
  iterations_since_restore: 187
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.92367149758454
    ram_util_percent: 12.369565217391305
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 16.0
    agent-2: 69.0
    agent-3: 38.0
    agent-4: 27.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.92
    agent-1: 8.56
    agent-2: 47.97
    agent-3: 24.42
    agent-4: 15.43
    agent-5: 17.81
  policy_reward_min:
    agent-0: 4.0
    agent-1: 2.0
    agent-2: 28.0
    agent-3: 10.0
    agent-4: 5.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.198300598884977
    mean_inference_ms: 15.07067749147516
    mean_processing_ms: 73.35620271320943
  time_since_restore: 27189.62507390976
  time_this_iter_s: 145.00798630714417
  time_total_s: 116126.30320358276
  timestamp: 1637390192
  timesteps_since_restore: 17952000
  timesteps_this_iter: 96000
  timesteps_total: 80352000
  training_iteration: 837
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    837 |           116126 | 80352000 |   130.11 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 7.5
    apples_agent-0_min: 1
    apples_agent-1_max: 7
    apples_agent-1_mean: 2.85
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 29.96
    apples_agent-2_min: 15
    apples_agent-3_max: 19
    apples_agent-3_mean: 8.21
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 9.91
    apples_agent-4_min: 2
    apples_agent-5_max: 12
    apples_agent-5_mean: 3.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 2.87
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 293
    cleaning_beam_agent-1_mean: 233.01
    cleaning_beam_agent-1_min: 174
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 4.09
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 27
    cleaning_beam_agent-3_mean: 9.36
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 14
    cleaning_beam_agent-4_mean: 3.08
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.65
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-38-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 162.0
  episode_reward_mean: 126.64
  episode_reward_min: 81.0
  episodes_this_iter: 96
  episodes_total: 80448
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11813.751
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2851563096046448
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007205625297501683
        model: {}
        policy_loss: -0.0014071697369217873
        total_loss: -0.0018042884767055511
        vf_explained_var: 0.013961434364318848
        vf_loss: 1.0475678443908691
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21539738774299622
        entropy_coeff: 0.0017600000137463212
        kl: 0.001066771917976439
        model: {}
        policy_loss: -0.001373842591419816
        total_loss: -0.0016942652873694897
        vf_explained_var: 0.08038143813610077
        vf_loss: 0.5867674946784973
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3731463551521301
        entropy_coeff: 0.0017600000137463212
        kl: 0.00083632004680112
        model: {}
        policy_loss: -0.0014808541163802147
        total_loss: -0.0017528580501675606
        vf_explained_var: 0.01900644600391388
        vf_loss: 3.847367763519287
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45887523889541626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011346153914928436
        model: {}
        policy_loss: -0.001565663143992424
        total_loss: -0.0021943501196801662
        vf_explained_var: 0.0038355588912963867
        vf_loss: 1.7893372774124146
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3729986846446991
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005832293536514044
        model: {}
        policy_loss: -0.001330080209299922
        total_loss: -0.0018757233629003167
        vf_explained_var: 0.010931655764579773
        vf_loss: 1.1083333492279053
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5404688715934753
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018740948289632797
        model: {}
        policy_loss: -0.0016834232956171036
        total_loss: -0.0025119115598499775
        vf_explained_var: 0.020713642239570618
        vf_loss: 1.2273882627487183
    load_time_ms: 14113.353
    num_steps_sampled: 80448000
    num_steps_trained: 80448000
    sample_time_ms: 118588.328
    update_time_ms: 16.591
  iterations_since_restore: 188
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.91159420289855
    ram_util_percent: 12.372463768115944
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 17.0
    agent-2: 61.0
    agent-3: 42.0
    agent-4: 25.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 15.64
    agent-1: 8.96
    agent-2: 45.33
    agent-3: 24.13
    agent-4: 14.79
    agent-5: 17.79
  policy_reward_min:
    agent-0: 5.0
    agent-1: 3.0
    agent-2: 26.0
    agent-3: 12.0
    agent-4: 6.0
    agent-5: 10.0
  sampler_perf:
    mean_env_wait_ms: 28.19856815618188
    mean_inference_ms: 15.070741984202769
    mean_processing_ms: 73.35603423290955
  time_since_restore: 27334.787225961685
  time_this_iter_s: 145.16215205192566
  time_total_s: 116271.46535563469
  timestamp: 1637390338
  timesteps_since_restore: 18048000
  timesteps_this_iter: 96000
  timesteps_total: 80448000
  training_iteration: 838
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    838 |           116271 | 80448000 |   126.64 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 6.76
    apples_agent-0_min: 1
    apples_agent-1_max: 14
    apples_agent-1_mean: 3.68
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 29.27
    apples_agent-2_min: 16
    apples_agent-3_max: 24
    apples_agent-3_mean: 8.05
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.5
    apples_agent-4_min: 2
    apples_agent-5_max: 19
    apples_agent-5_mean: 3.97
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 24
    cleaning_beam_agent-0_mean: 3.72
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 272
    cleaning_beam_agent-1_mean: 229.85
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 3.85
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 31
    cleaning_beam_agent-3_mean: 9.98
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 2.9
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.0
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-41-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 165.0
  episode_reward_mean: 127.72
  episode_reward_min: 65.0
  episodes_this_iter: 96
  episodes_total: 80544
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11809.898
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27804285287857056
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007321640150621533
        model: {}
        policy_loss: -0.001596156507730484
        total_loss: -0.0019754888489842415
        vf_explained_var: -0.002330303192138672
        vf_loss: 1.100232720375061
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21641100943088531
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011371776927262545
        model: {}
        policy_loss: -0.0014717342564836144
        total_loss: -0.0017985632875934243
        vf_explained_var: 0.08293521404266357
        vf_loss: 0.5405546426773071
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38418811559677124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019360475707799196
        model: {}
        policy_loss: -0.0020593591034412384
        total_loss: -0.0023468080908060074
        vf_explained_var: 0.014624789357185364
        vf_loss: 3.8872108459472656
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.460440456867218
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015948830405250192
        model: {}
        policy_loss: -0.0014422767562791705
        total_loss: -0.002073804847896099
        vf_explained_var: 0.012133032083511353
        vf_loss: 1.7884666919708252
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3785392940044403
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012147875968366861
        model: {}
        policy_loss: -0.0013570752926170826
        total_loss: -0.0019164644181728363
        vf_explained_var: 0.010732322931289673
        vf_loss: 1.0684070587158203
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5458870530128479
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015580160543322563
        model: {}
        policy_loss: -0.001561922486871481
        total_loss: -0.0023879599757492542
        vf_explained_var: 0.023350030183792114
        vf_loss: 1.347240924835205
    load_time_ms: 14103.836
    num_steps_sampled: 80544000
    num_steps_trained: 80544000
    sample_time_ms: 118533.091
    update_time_ms: 16.525
  iterations_since_restore: 189
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.898058252427184
    ram_util_percent: 12.450485436893201
  pid: 27065
  policy_reward_max:
    agent-0: 33.0
    agent-1: 20.0
    agent-2: 63.0
    agent-3: 42.0
    agent-4: 26.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 16.25
    agent-1: 8.95
    agent-2: 44.91
    agent-3: 23.87
    agent-4: 15.54
    agent-5: 18.2
  policy_reward_min:
    agent-0: 7.0
    agent-1: 3.0
    agent-2: 23.0
    agent-3: 8.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.198317796559746
    mean_inference_ms: 15.070115604509086
    mean_processing_ms: 73.35375144160822
  time_since_restore: 27479.347511529922
  time_this_iter_s: 144.5602855682373
  time_total_s: 116416.02564120293
  timestamp: 1637390482
  timesteps_since_restore: 18144000
  timesteps_this_iter: 96000
  timesteps_total: 80544000
  training_iteration: 839
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    839 |           116416 | 80544000 |   127.72 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 7.05
    apples_agent-0_min: 2
    apples_agent-1_max: 17
    apples_agent-1_mean: 3.62
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 29.85
    apples_agent-2_min: 18
    apples_agent-3_max: 17
    apples_agent-3_mean: 8.11
    apples_agent-3_min: 1
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.36
    apples_agent-4_min: 3
    apples_agent-5_max: 19
    apples_agent-5_mean: 4.34
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 21
    cleaning_beam_agent-0_mean: 3.51
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 289
    cleaning_beam_agent-1_mean: 240.74
    cleaning_beam_agent-1_min: 204
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 3.95
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 30
    cleaning_beam_agent-3_mean: 10.05
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.89
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.07
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-43-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 188.0
  episode_reward_mean: 130.87
  episode_reward_min: 84.0
  episodes_this_iter: 96
  episodes_total: 80640
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11821.076
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2878277003765106
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012144595384597778
        model: {}
        policy_loss: -0.0017971217166632414
        total_loss: -0.002188984304666519
        vf_explained_var: 0.011728957295417786
        vf_loss: 1.1471443176269531
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.22178000211715698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008827775018289685
        model: {}
        policy_loss: -0.001514912350103259
        total_loss: -0.0018504038453102112
        vf_explained_var: 0.08817209303379059
        vf_loss: 0.5484225153923035
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3858039379119873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012640790082514286
        model: {}
        policy_loss: -0.0016442253254354
        total_loss: -0.0019349823705852032
        vf_explained_var: 0.011666297912597656
        vf_loss: 3.8825759887695312
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46470820903778076
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014078079257160425
        model: {}
        policy_loss: -0.0014697574079036713
        total_loss: -0.0021086325868964195
        vf_explained_var: 0.013179510831832886
        vf_loss: 1.7900944948196411
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38881152868270874
        entropy_coeff: 0.0017600000137463212
        kl: 0.001318402006290853
        model: {}
        policy_loss: -0.0014806115068495274
        total_loss: -0.002055300399661064
        vf_explained_var: 0.003597557544708252
        vf_loss: 1.0962073802947998
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5384647846221924
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017811356810852885
        model: {}
        policy_loss: -0.0016654031351208687
        total_loss: -0.0024840948171913624
        vf_explained_var: 0.022734180092811584
        vf_loss: 1.2900513410568237
    load_time_ms: 14114.436
    num_steps_sampled: 80640000
    num_steps_trained: 80640000
    sample_time_ms: 118486.043
    update_time_ms: 16.852
  iterations_since_restore: 190
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.016585365853658
    ram_util_percent: 12.295121951219517
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 20.0
    agent-2: 62.0
    agent-3: 38.0
    agent-4: 28.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 16.87
    agent-1: 9.12
    agent-2: 46.12
    agent-3: 24.41
    agent-4: 15.6
    agent-5: 18.75
  policy_reward_min:
    agent-0: 5.0
    agent-1: 1.0
    agent-2: 28.0
    agent-3: 14.0
    agent-4: 4.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.19839460617222
    mean_inference_ms: 15.069413226696389
    mean_processing_ms: 73.35236952797402
  time_since_restore: 27623.592146635056
  time_this_iter_s: 144.24463510513306
  time_total_s: 116560.27027630806
  timestamp: 1637390627
  timesteps_since_restore: 18240000
  timesteps_this_iter: 96000
  timesteps_total: 80640000
  training_iteration: 840
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    840 |           116560 | 80640000 |   130.87 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 6.48
    apples_agent-0_min: 1
    apples_agent-1_max: 42
    apples_agent-1_mean: 3.91
    apples_agent-1_min: 0
    apples_agent-2_max: 59
    apples_agent-2_mean: 30.94
    apples_agent-2_min: 15
    apples_agent-3_max: 49
    apples_agent-3_mean: 9.73
    apples_agent-3_min: 1
    apples_agent-4_max: 26
    apples_agent-4_mean: 11.24
    apples_agent-4_min: 2
    apples_agent-5_max: 24
    apples_agent-5_mean: 4.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 36
    cleaning_beam_agent-0_mean: 3.89
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 289
    cleaning_beam_agent-1_mean: 236.45
    cleaning_beam_agent-1_min: 184
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 3.88
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 10.43
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 3.29
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.92
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-46-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 203.0
  episode_reward_mean: 132.07
  episode_reward_min: 82.0
  episodes_this_iter: 96
  episodes_total: 80736
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11835.954
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2864200174808502
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006920083542354405
        model: {}
        policy_loss: -0.0017086006700992584
        total_loss: -0.0020960643887519836
        vf_explained_var: 0.010724693536758423
        vf_loss: 1.16636323928833
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21943211555480957
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007361069438047707
        model: {}
        policy_loss: -0.0013596885837614536
        total_loss: -0.0016951351426541805
        vf_explained_var: 0.08653618395328522
        vf_loss: 0.5075496435165405
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38689157366752625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008601969457231462
        model: {}
        policy_loss: -0.0015971530228853226
        total_loss: -0.0018809372559189796
        vf_explained_var: 0.02191266417503357
        vf_loss: 3.9714579582214355
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.454264372587204
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013029924593865871
        model: {}
        policy_loss: -0.0014448657166212797
        total_loss: -0.002033496741205454
        vf_explained_var: 0.0030660927295684814
        vf_loss: 2.108713388442993
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3908534646034241
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008572571678087115
        model: {}
        policy_loss: -0.0012280239025130868
        total_loss: -0.0018064591567963362
        vf_explained_var: 0.010263189673423767
        vf_loss: 1.094665765762329
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5283241271972656
        entropy_coeff: 0.0017600000137463212
        kl: 0.001092450344003737
        model: {}
        policy_loss: -0.0015432746149599552
        total_loss: -0.002337118610739708
        vf_explained_var: 0.032778382301330566
        vf_loss: 1.3600300550460815
    load_time_ms: 14121.742
    num_steps_sampled: 80736000
    num_steps_trained: 80736000
    sample_time_ms: 118414.196
    update_time_ms: 16.753
  iterations_since_restore: 191
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.733492822966507
    ram_util_percent: 12.36267942583732
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 68.0
    agent-3: 53.0
    agent-4: 27.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 16.98
    agent-1: 8.68
    agent-2: 46.95
    agent-3: 25.15
    agent-4: 15.78
    agent-5: 18.53
  policy_reward_min:
    agent-0: 8.0
    agent-1: 3.0
    agent-2: 26.0
    agent-3: 12.0
    agent-4: 7.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.19866691371195
    mean_inference_ms: 15.069052441843741
    mean_processing_ms: 73.35070741078587
  time_since_restore: 27768.452717781067
  time_this_iter_s: 144.86057114601135
  time_total_s: 116705.13084745407
  timestamp: 1637390774
  timesteps_since_restore: 18336000
  timesteps_this_iter: 96000
  timesteps_total: 80736000
  training_iteration: 841
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    841 |           116705 | 80736000 |   132.07 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 6.88
    apples_agent-0_min: 0
    apples_agent-1_max: 53
    apples_agent-1_mean: 4.05
    apples_agent-1_min: 0
    apples_agent-2_max: 54
    apples_agent-2_mean: 30.14
    apples_agent-2_min: 15
    apples_agent-3_max: 19
    apples_agent-3_mean: 8.62
    apples_agent-3_min: 2
    apples_agent-4_max: 19
    apples_agent-4_mean: 10.63
    apples_agent-4_min: 3
    apples_agent-5_max: 19
    apples_agent-5_mean: 3.57
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 23
    cleaning_beam_agent-0_mean: 3.48
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 291
    cleaning_beam_agent-1_mean: 235.82
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 10
    cleaning_beam_agent-2_mean: 3.89
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 9.79
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.05
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 3.1
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-48-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 178.0
  episode_reward_mean: 131.35
  episode_reward_min: 74.0
  episodes_this_iter: 96
  episodes_total: 80832
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11839.391
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2835639417171478
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013226969167590141
        model: {}
        policy_loss: -0.0016795974224805832
        total_loss: -0.0020689135417342186
        vf_explained_var: 0.0033265501260757446
        vf_loss: 1.0975522994995117
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21994799375534058
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006309582386165857
        model: {}
        policy_loss: -0.0013330117799341679
        total_loss: -0.0016577458009123802
        vf_explained_var: 0.08895418047904968
        vf_loss: 0.6237169504165649
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38981887698173523
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011622352758422494
        model: {}
        policy_loss: -0.0017429995350539684
        total_loss: -0.0020124264992773533
        vf_explained_var: 0.016760513186454773
        vf_loss: 4.166550636291504
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45804423093795776
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012474695686250925
        model: {}
        policy_loss: -0.001387274358421564
        total_loss: -0.0020242666359990835
        vf_explained_var: 0.006172165274620056
        vf_loss: 1.6916687488555908
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3839871287345886
        entropy_coeff: 0.0017600000137463212
        kl: 0.000814878789242357
        model: {}
        policy_loss: -0.0013222519773989916
        total_loss: -0.001882839947938919
        vf_explained_var: 0.000908777117729187
        vf_loss: 1.152298927307129
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5323864221572876
        entropy_coeff: 0.0017600000137463212
        kl: 0.00193595583550632
        model: {}
        policy_loss: -0.00187301030382514
        total_loss: -0.002682593185454607
        vf_explained_var: 0.01223476231098175
        vf_loss: 1.2741907835006714
    load_time_ms: 14116.985
    num_steps_sampled: 80832000
    num_steps_trained: 80832000
    sample_time_ms: 118419.692
    update_time_ms: 16.638
  iterations_since_restore: 192
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.907246376811592
    ram_util_percent: 12.371497584541066
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 22.0
    agent-2: 66.0
    agent-3: 38.0
    agent-4: 29.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 16.62
    agent-1: 9.6
    agent-2: 46.08
    agent-3: 24.54
    agent-4: 15.81
    agent-5: 18.7
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 22.0
    agent-3: 13.0
    agent-4: 3.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.198830987622262
    mean_inference_ms: 15.068818313498133
    mean_processing_ms: 73.34822455217744
  time_since_restore: 27913.360030651093
  time_this_iter_s: 144.90731287002563
  time_total_s: 116850.0381603241
  timestamp: 1637390919
  timesteps_since_restore: 18432000
  timesteps_this_iter: 96000
  timesteps_total: 80832000
  training_iteration: 842
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    842 |           116850 | 80832000 |   131.35 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 7.12
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 3.26
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 30.28
    apples_agent-2_min: 0
    apples_agent-3_max: 26
    apples_agent-3_mean: 8.4
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 11.15
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 3.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 23
    cleaning_beam_agent-0_mean: 3.86
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 277
    cleaning_beam_agent-1_mean: 230.45
    cleaning_beam_agent-1_min: 27
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 3.78
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 9.03
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.18
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.0
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-51-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 176.0
  episode_reward_mean: 130.07
  episode_reward_min: 11.0
  episodes_this_iter: 96
  episodes_total: 80928
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11844.787
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2862919270992279
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008182930178008974
        model: {}
        policy_loss: -0.0016308636404573917
        total_loss: -0.0020209220238029957
        vf_explained_var: 0.004289716482162476
        vf_loss: 1.1381070613861084
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21810097992420197
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018757759826257825
        model: {}
        policy_loss: -0.0018347776494920254
        total_loss: -0.0021654050797224045
        vf_explained_var: 0.101065993309021
        vf_loss: 0.5323039889335632
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3917226493358612
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011186072370037436
        model: {}
        policy_loss: -0.0015469258651137352
        total_loss: -0.0018169507384300232
        vf_explained_var: 0.016417592763900757
        vf_loss: 4.1940412521362305
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4557664096355438
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019360597943887115
        model: {}
        policy_loss: -0.0017292096745222807
        total_loss: -0.002349081914871931
        vf_explained_var: 0.00013916194438934326
        vf_loss: 1.822777509689331
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39532098174095154
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016397909494116902
        model: {}
        policy_loss: -0.0015745821874588728
        total_loss: -0.0021603244822472334
        vf_explained_var: 0.010558873414993286
        vf_loss: 1.1002284288406372
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5362493395805359
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014714227290824056
        model: {}
        policy_loss: -0.001468846807256341
        total_loss: -0.002283106790855527
        vf_explained_var: 0.023786142468452454
        vf_loss: 1.2953745126724243
    load_time_ms: 14088.564
    num_steps_sampled: 80928000
    num_steps_trained: 80928000
    sample_time_ms: 118536.204
    update_time_ms: 16.554
  iterations_since_restore: 193
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.96473429951691
    ram_util_percent: 12.369565217391305
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 20.0
    agent-2: 64.0
    agent-3: 39.0
    agent-4: 33.0
    agent-5: 39.0
  policy_reward_mean:
    agent-0: 16.51
    agent-1: 9.16
    agent-2: 45.91
    agent-3: 23.94
    agent-4: 15.95
    agent-5: 18.6
  policy_reward_min:
    agent-0: 0.0
    agent-1: 2.0
    agent-2: 2.0
    agent-3: 4.0
    agent-4: 1.0
    agent-5: 1.0
  sampler_perf:
    mean_env_wait_ms: 28.198940535451968
    mean_inference_ms: 15.068673932837564
    mean_processing_ms: 73.347643716
  time_since_restore: 28058.70092535019
  time_this_iter_s: 145.34089469909668
  time_total_s: 116995.3790550232
  timestamp: 1637391064
  timesteps_since_restore: 18528000
  timesteps_this_iter: 96000
  timesteps_total: 80928000
  training_iteration: 843
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    843 |           116995 | 80928000 |   130.07 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 7.81
    apples_agent-0_min: 2
    apples_agent-1_max: 18
    apples_agent-1_mean: 3.08
    apples_agent-1_min: 0
    apples_agent-2_max: 89
    apples_agent-2_mean: 30.12
    apples_agent-2_min: 18
    apples_agent-3_max: 28
    apples_agent-3_mean: 8.28
    apples_agent-3_min: 2
    apples_agent-4_max: 18
    apples_agent-4_mean: 10.5
    apples_agent-4_min: 1
    apples_agent-5_max: 18
    apples_agent-5_mean: 3.45
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 27
    cleaning_beam_agent-0_mean: 3.88
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 307
    cleaning_beam_agent-1_mean: 234.09
    cleaning_beam_agent-1_min: 178
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 4.37
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 25
    cleaning_beam_agent-3_mean: 8.23
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.88
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.04
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-53-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 183.0
  episode_reward_mean: 130.69
  episode_reward_min: 86.0
  episodes_this_iter: 96
  episodes_total: 81024
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11864.379
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2873232960700989
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010338748106732965
        model: {}
        policy_loss: -0.0016808286309242249
        total_loss: -0.002076278440654278
        vf_explained_var: 0.0041023194789886475
        vf_loss: 1.102339267730713
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21655499935150146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011268379166722298
        model: {}
        policy_loss: -0.0016972313169389963
        total_loss: -0.0020273311529308558
        vf_explained_var: 0.08013510704040527
        vf_loss: 0.5103583335876465
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39982736110687256
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012269809376448393
        model: {}
        policy_loss: -0.0018580311443656683
        total_loss: -0.002154286252334714
        vf_explained_var: 0.012666940689086914
        vf_loss: 4.074411869049072
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4470898509025574
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017971324268728495
        model: {}
        policy_loss: -0.0016715559177100658
        total_loss: -0.002272234298288822
        vf_explained_var: -0.0003543049097061157
        vf_loss: 1.8620127439498901
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3932943642139435
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005382065428420901
        model: {}
        policy_loss: -0.0013056006282567978
        total_loss: -0.001895013265311718
        vf_explained_var: 0.006821542978286743
        vf_loss: 1.027840495109558
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5296080112457275
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009748816955834627
        model: {}
        policy_loss: -0.0016229222528636456
        total_loss: -0.0024166004732251167
        vf_explained_var: 0.009603098034858704
        vf_loss: 1.3843201398849487
    load_time_ms: 14125.954
    num_steps_sampled: 81024000
    num_steps_trained: 81024000
    sample_time_ms: 118548.943
    update_time_ms: 15.769
  iterations_since_restore: 194
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.031400966183572
    ram_util_percent: 12.371980676328503
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 19.0
    agent-2: 65.0
    agent-3: 39.0
    agent-4: 34.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 16.53
    agent-1: 8.76
    agent-2: 46.39
    agent-3: 24.81
    agent-4: 15.57
    agent-5: 18.63
  policy_reward_min:
    agent-0: 8.0
    agent-1: 3.0
    agent-2: 29.0
    agent-3: 13.0
    agent-4: 6.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.199937044913543
    mean_inference_ms: 15.068872238090078
    mean_processing_ms: 73.34807514211657
  time_since_restore: 28204.127740859985
  time_this_iter_s: 145.42681550979614
  time_total_s: 117140.80587053299
  timestamp: 1637391210
  timesteps_since_restore: 18624000
  timesteps_this_iter: 96000
  timesteps_total: 81024000
  training_iteration: 844
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    844 |           117141 | 81024000 |   130.69 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 7.03
    apples_agent-0_min: 0
    apples_agent-1_max: 31
    apples_agent-1_mean: 3.47
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 30.04
    apples_agent-2_min: 13
    apples_agent-3_max: 28
    apples_agent-3_mean: 9.5
    apples_agent-3_min: 3
    apples_agent-4_max: 25
    apples_agent-4_mean: 10.86
    apples_agent-4_min: 4
    apples_agent-5_max: 29
    apples_agent-5_mean: 4.32
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 2.77
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 272
    cleaning_beam_agent-1_mean: 227.14
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 4.47
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 9.29
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.26
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-55-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 179.0
  episode_reward_mean: 130.64
  episode_reward_min: 29.0
  episodes_this_iter: 96
  episodes_total: 81120
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11865.663
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28574270009994507
        entropy_coeff: 0.0017600000137463212
        kl: 0.001048270263709128
        model: {}
        policy_loss: -0.0016874526627361774
        total_loss: -0.0020813094452023506
        vf_explained_var: 0.010128334164619446
        vf_loss: 1.0904871225357056
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21428824961185455
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014708214439451694
        model: {}
        policy_loss: -0.0015989197418093681
        total_loss: -0.0019198842346668243
        vf_explained_var: 0.07319648563861847
        vf_loss: 0.5618135929107666
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39927077293395996
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009449023054912686
        model: {}
        policy_loss: -0.0017667259089648724
        total_loss: -0.0020433105528354645
        vf_explained_var: 0.012800827622413635
        vf_loss: 4.261331558227539
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4486995339393616
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009707691497169435
        model: {}
        policy_loss: -0.0013459944166243076
        total_loss: -0.0019547585397958755
        vf_explained_var: -0.0012141913175582886
        vf_loss: 1.8094897270202637
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3922823667526245
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013867502566426992
        model: {}
        policy_loss: -0.0014406193513423204
        total_loss: -0.0020195713732391596
        vf_explained_var: 0.006860017776489258
        vf_loss: 1.1146634817123413
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5360788106918335
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018920183647423983
        model: {}
        policy_loss: -0.0016997051425278187
        total_loss: -0.0024988334625959396
        vf_explained_var: 0.015875041484832764
        vf_loss: 1.4437236785888672
    load_time_ms: 14119.72
    num_steps_sampled: 81120000
    num_steps_trained: 81120000
    sample_time_ms: 118883.307
    update_time_ms: 16.022
  iterations_since_restore: 195
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.915942028985505
    ram_util_percent: 12.369565217391305
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 17.0
    agent-2: 72.0
    agent-3: 36.0
    agent-4: 26.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 16.2
    agent-1: 8.83
    agent-2: 46.6
    agent-3: 24.5
    agent-4: 15.1
    agent-5: 19.41
  policy_reward_min:
    agent-0: 3.0
    agent-1: 1.0
    agent-2: 15.0
    agent-3: 3.0
    agent-4: -33.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.200773362606938
    mean_inference_ms: 15.068739798164106
    mean_processing_ms: 73.34724120917964
  time_since_restore: 28349.20934343338
  time_this_iter_s: 145.08160257339478
  time_total_s: 117285.88747310638
  timestamp: 1637391355
  timesteps_since_restore: 18720000
  timesteps_this_iter: 96000
  timesteps_total: 81120000
  training_iteration: 845
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    845 |           117286 | 81120000 |   130.64 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 7.84
    apples_agent-0_min: 1
    apples_agent-1_max: 41
    apples_agent-1_mean: 3.38
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 30.39
    apples_agent-2_min: 16
    apples_agent-3_max: 36
    apples_agent-3_mean: 9.54
    apples_agent-3_min: 2
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.53
    apples_agent-4_min: 3
    apples_agent-5_max: 42
    apples_agent-5_mean: 4.18
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 32
    cleaning_beam_agent-0_mean: 3.68
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 256
    cleaning_beam_agent-1_mean: 220.83
    cleaning_beam_agent-1_min: 163
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 4.08
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 8.9
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.25
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.12
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_01-58-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 179.0
  episode_reward_mean: 130.93
  episode_reward_min: 96.0
  episodes_this_iter: 96
  episodes_total: 81216
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11862.524
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28407132625579834
        entropy_coeff: 0.0017600000137463212
        kl: 0.000726930855307728
        model: {}
        policy_loss: -0.0016507720574736595
        total_loss: -0.0020277807489037514
        vf_explained_var: 0.005840480327606201
        vf_loss: 1.2295687198638916
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21496063470840454
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009678484639152884
        model: {}
        policy_loss: -0.0014645546907559037
        total_loss: -0.0017875900957733393
        vf_explained_var: 0.08066770434379578
        vf_loss: 0.5529623031616211
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39048224687576294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009058878058567643
        model: {}
        policy_loss: -0.0016552552115172148
        total_loss: -0.001963402610272169
        vf_explained_var: 0.010270670056343079
        vf_loss: 3.791032075881958
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44362011551856995
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009275467600673437
        model: {}
        policy_loss: -0.0013030976988375187
        total_loss: -0.0018719416111707687
        vf_explained_var: 0.009197741746902466
        vf_loss: 2.1192831993103027
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3886617124080658
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007979933870956302
        model: {}
        policy_loss: -0.0013903090730309486
        total_loss: -0.001966920681297779
        vf_explained_var: 0.005894839763641357
        vf_loss: 1.074333667755127
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5295664072036743
        entropy_coeff: 0.0017600000137463212
        kl: 0.001592734595760703
        model: {}
        policy_loss: -0.0018294753972440958
        total_loss: -0.0026348084211349487
        vf_explained_var: 0.01319316029548645
        vf_loss: 1.2670292854309082
    load_time_ms: 14107.328
    num_steps_sampled: 81216000
    num_steps_trained: 81216000
    sample_time_ms: 118908.849
    update_time_ms: 15.677
  iterations_since_restore: 196
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.955072463768115
    ram_util_percent: 12.364734299516908
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 18.0
    agent-2: 68.0
    agent-3: 47.0
    agent-4: 28.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 16.95
    agent-1: 8.78
    agent-2: 46.11
    agent-3: 25.95
    agent-4: 15.36
    agent-5: 17.78
  policy_reward_min:
    agent-0: 6.0
    agent-1: 4.0
    agent-2: 31.0
    agent-3: 13.0
    agent-4: 5.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.200830345774783
    mean_inference_ms: 15.068542565824513
    mean_processing_ms: 73.34690401625555
  time_since_restore: 28494.430675029755
  time_this_iter_s: 145.2213315963745
  time_total_s: 117431.10880470276
  timestamp: 1637391500
  timesteps_since_restore: 18816000
  timesteps_this_iter: 96000
  timesteps_total: 81216000
  training_iteration: 846
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    846 |           117431 | 81216000 |   130.93 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 6.61
    apples_agent-0_min: 0
    apples_agent-1_max: 27
    apples_agent-1_mean: 3.48
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 30.24
    apples_agent-2_min: 3
    apples_agent-3_max: 36
    apples_agent-3_mean: 8.34
    apples_agent-3_min: 2
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.65
    apples_agent-4_min: 3
    apples_agent-5_max: 18
    apples_agent-5_mean: 4.36
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 3.89
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 270
    cleaning_beam_agent-1_mean: 225.63
    cleaning_beam_agent-1_min: 167
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.37
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 21
    cleaning_beam_agent-3_mean: 8.89
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.08
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.09
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-00-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 184.0
  episode_reward_mean: 129.89
  episode_reward_min: 36.0
  episodes_this_iter: 96
  episodes_total: 81312
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11875.138
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27569228410720825
        entropy_coeff: 0.0017600000137463212
        kl: 0.000567053270060569
        model: {}
        policy_loss: -0.0014438426587730646
        total_loss: -0.001817801734432578
        vf_explained_var: 0.0029880106449127197
        vf_loss: 1.1125785112380981
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2136087864637375
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007555620395578444
        model: {}
        policy_loss: -0.0012941798195242882
        total_loss: -0.0016196544747799635
        vf_explained_var: 0.08694986999034882
        vf_loss: 0.5047831535339355
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3969423174858093
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012232543667778373
        model: {}
        policy_loss: -0.001686714356765151
        total_loss: -0.0019824060145765543
        vf_explained_var: 0.013613343238830566
        vf_loss: 4.029271602630615
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44983136653900146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008262716000899673
        model: {}
        policy_loss: -0.0013741255970671773
        total_loss: -0.00199344614520669
        vf_explained_var: 0.010231181979179382
        vf_loss: 1.72385573387146
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38074636459350586
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006007829797454178
        model: {}
        policy_loss: -0.0011474699713289738
        total_loss: -0.0017072968184947968
        vf_explained_var: 0.009812131524085999
        vf_loss: 1.1028469800949097
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5241042375564575
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021034390665590763
        model: {}
        policy_loss: -0.00174352852627635
        total_loss: -0.0025481656193733215
        vf_explained_var: 0.018145635724067688
        vf_loss: 1.1778600215911865
    load_time_ms: 14133.754
    num_steps_sampled: 81312000
    num_steps_trained: 81312000
    sample_time_ms: 118938.93
    update_time_ms: 16.403
  iterations_since_restore: 197
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.064903846153847
    ram_util_percent: 12.391826923076923
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 18.0
    agent-2: 67.0
    agent-3: 45.0
    agent-4: 29.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 16.17
    agent-1: 9.43
    agent-2: 46.06
    agent-3: 24.53
    agent-4: 15.64
    agent-5: 18.06
  policy_reward_min:
    agent-0: 2.0
    agent-1: 2.0
    agent-2: 11.0
    agent-3: 5.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.20140827057617
    mean_inference_ms: 15.068436091665772
    mean_processing_ms: 73.34598995414082
  time_since_restore: 28640.22595357895
  time_this_iter_s: 145.79527854919434
  time_total_s: 117576.90408325195
  timestamp: 1637391646
  timesteps_since_restore: 18912000
  timesteps_this_iter: 96000
  timesteps_total: 81312000
  training_iteration: 847
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    847 |           117577 | 81312000 |   129.89 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 7.29
    apples_agent-0_min: 0
    apples_agent-1_max: 25
    apples_agent-1_mean: 3.0
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 30.99
    apples_agent-2_min: 10
    apples_agent-3_max: 22
    apples_agent-3_mean: 9.17
    apples_agent-3_min: 2
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.87
    apples_agent-4_min: 2
    apples_agent-5_max: 16
    apples_agent-5_mean: 3.74
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 43
    cleaning_beam_agent-0_mean: 4.92
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 259
    cleaning_beam_agent-1_mean: 222.96
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 4.95
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 8.16
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.39
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 4.14
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-03-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 178.0
  episode_reward_mean: 130.75
  episode_reward_min: 13.0
  episodes_this_iter: 96
  episodes_total: 81408
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11890.445
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27810072898864746
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006816867971792817
        model: {}
        policy_loss: -0.001294226385653019
        total_loss: -0.0015452350489795208
        vf_explained_var: -0.003089606761932373
        vf_loss: 2.3844780921936035
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21483968198299408
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018821393605321646
        model: {}
        policy_loss: -0.0016794279217720032
        total_loss: -0.0020078886300325394
        vf_explained_var: 0.07823078334331512
        vf_loss: 0.4965363144874573
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3980411887168884
        entropy_coeff: 0.0017600000137463212
        kl: 0.001250563538633287
        model: {}
        policy_loss: -0.0017448393628001213
        total_loss: -0.0020421473309397697
        vf_explained_var: 0.012379094958305359
        vf_loss: 4.032459259033203
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.443153440952301
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006523605552501976
        model: {}
        policy_loss: -0.0012656920589506626
        total_loss: -0.001859049778431654
        vf_explained_var: -0.0028283894062042236
        vf_loss: 1.8659261465072632
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.378285676240921
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004850827681366354
        model: {}
        policy_loss: -0.0009430325590074062
        total_loss: -0.0013902881182730198
        vf_explained_var: 0.0017905086278915405
        vf_loss: 2.1852381229400635
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5173252820968628
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016603005351498723
        model: {}
        policy_loss: -0.0018038507550954819
        total_loss: -0.0025808822829276323
        vf_explained_var: 0.01101405918598175
        vf_loss: 1.334609031677246
    load_time_ms: 14133.072
    num_steps_sampled: 81408000
    num_steps_trained: 81408000
    sample_time_ms: 118895.179
    update_time_ms: 16.807
  iterations_since_restore: 198
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.845145631067965
    ram_util_percent: 12.292718446601942
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 70.0
    agent-3: 41.0
    agent-4: 29.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 16.45
    agent-1: 8.71
    agent-2: 47.6
    agent-3: 24.89
    agent-4: 14.57
    agent-5: 18.53
  policy_reward_min:
    agent-0: -38.0
    agent-1: 1.0
    agent-2: 20.0
    agent-3: 8.0
    agent-4: -41.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.20087075060379
    mean_inference_ms: 15.067879799648104
    mean_processing_ms: 73.3418498592784
  time_since_restore: 28785.052920103073
  time_this_iter_s: 144.82696652412415
  time_total_s: 117721.73104977608
  timestamp: 1637391791
  timesteps_since_restore: 19008000
  timesteps_this_iter: 96000
  timesteps_total: 81408000
  training_iteration: 848
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    848 |           117722 | 81408000 |   130.75 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 7.36
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 2.93
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 28.68
    apples_agent-2_min: 10
    apples_agent-3_max: 19
    apples_agent-3_mean: 8.3
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 10.0
    apples_agent-4_min: 3
    apples_agent-5_max: 16
    apples_agent-5_mean: 3.72
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 22
    cleaning_beam_agent-0_mean: 3.8
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 263
    cleaning_beam_agent-1_mean: 214.6
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.44
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 22
    cleaning_beam_agent-3_mean: 8.49
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.26
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 4.8
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-05-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 170.0
  episode_reward_mean: 126.41
  episode_reward_min: 47.0
  episodes_this_iter: 96
  episodes_total: 81504
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11904.799
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2784188389778137
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011492432095110416
        model: {}
        policy_loss: -0.0016515867318958044
        total_loss: -0.002035407815128565
        vf_explained_var: 0.013811945915222168
        vf_loss: 1.0619622468948364
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21303656697273254
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010840795002877712
        model: {}
        policy_loss: -0.0017423080280423164
        total_loss: -0.002066308166831732
        vf_explained_var: 0.08665332198143005
        vf_loss: 0.5094223022460938
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4000096321105957
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011757237371057272
        model: {}
        policy_loss: -0.0018450168427079916
        total_loss: -0.002158140065148473
        vf_explained_var: 0.00787748396396637
        vf_loss: 3.9089298248291016
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44499778747558594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016756991390138865
        model: {}
        policy_loss: -0.001646198332309723
        total_loss: -0.0022558923810720444
        vf_explained_var: 0.0013969242572784424
        vf_loss: 1.7350378036499023
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36970266699790955
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012880335561931133
        model: {}
        policy_loss: -0.0014341790229082108
        total_loss: -0.001990729011595249
        vf_explained_var: 0.013446927070617676
        vf_loss: 0.9412492513656616
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5202028751373291
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015072750393301249
        model: {}
        policy_loss: -0.001606140285730362
        total_loss: -0.0024000322446227074
        vf_explained_var: 0.01982182264328003
        vf_loss: 1.2166651487350464
    load_time_ms: 14163.678
    num_steps_sampled: 81504000
    num_steps_trained: 81504000
    sample_time_ms: 118916.999
    update_time_ms: 16.544
  iterations_since_restore: 199
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.885507246376807
    ram_util_percent: 12.457004830917871
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 18.0
    agent-2: 64.0
    agent-3: 41.0
    agent-4: 24.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.62
    agent-1: 8.53
    agent-2: 45.72
    agent-3: 23.85
    agent-4: 14.6
    agent-5: 18.09
  policy_reward_min:
    agent-0: 2.0
    agent-1: 1.0
    agent-2: 20.0
    agent-3: 8.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.200604401448697
    mean_inference_ms: 15.067763086741891
    mean_processing_ms: 73.34098080954844
  time_since_restore: 28930.34130883217
  time_this_iter_s: 145.28838872909546
  time_total_s: 117867.01943850517
  timestamp: 1637391936
  timesteps_since_restore: 19104000
  timesteps_this_iter: 96000
  timesteps_total: 81504000
  training_iteration: 849
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    849 |           117867 | 81504000 |   126.41 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 7.32
    apples_agent-0_min: 1
    apples_agent-1_max: 20
    apples_agent-1_mean: 3.48
    apples_agent-1_min: 0
    apples_agent-2_max: 55
    apples_agent-2_mean: 29.64
    apples_agent-2_min: 14
    apples_agent-3_max: 22
    apples_agent-3_mean: 8.59
    apples_agent-3_min: 2
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.62
    apples_agent-4_min: 3
    apples_agent-5_max: 22
    apples_agent-5_mean: 3.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 2.83
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 253
    cleaning_beam_agent-1_mean: 215.03
    cleaning_beam_agent-1_min: 144
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 4.52
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 8.23
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 3.1
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 4.41
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-08-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 168.0
  episode_reward_mean: 128.01
  episode_reward_min: 86.0
  episodes_this_iter: 96
  episodes_total: 81600
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11890.361
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28194499015808105
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007932884618639946
        model: {}
        policy_loss: -0.0015962996985763311
        total_loss: -0.001990848919376731
        vf_explained_var: 0.008636534214019775
        vf_loss: 1.016774296760559
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21060232818126678
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010863541392609477
        model: {}
        policy_loss: -0.0013440335169434547
        total_loss: -0.0016610963502898812
        vf_explained_var: 0.09094518423080444
        vf_loss: 0.5360029339790344
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39858150482177734
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013688054168596864
        model: {}
        policy_loss: -0.0019262731075286865
        total_loss: -0.0022431781981140375
        vf_explained_var: 0.005570694804191589
        vf_loss: 3.8459904193878174
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45057815313339233
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012989514507353306
        model: {}
        policy_loss: -0.0014340905472636223
        total_loss: -0.0020535453222692013
        vf_explained_var: 0.002803519368171692
        vf_loss: 1.735641360282898
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36998289823532104
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007185249705798924
        model: {}
        policy_loss: -0.0013599902158603072
        total_loss: -0.001910928520374
        vf_explained_var: 0.005965322256088257
        vf_loss: 1.0023273229599
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5185284614562988
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016134334728121758
        model: {}
        policy_loss: -0.0016246696468442678
        total_loss: -0.002412822563201189
        vf_explained_var: 0.009966224431991577
        vf_loss: 1.244596242904663
    load_time_ms: 14160.055
    num_steps_sampled: 81600000
    num_steps_trained: 81600000
    sample_time_ms: 118880.239
    update_time_ms: 16.472
  iterations_since_restore: 200
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.839024390243903
    ram_util_percent: 12.373658536585365
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 18.0
    agent-2: 77.0
    agent-3: 36.0
    agent-4: 25.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 15.89
    agent-1: 9.21
    agent-2: 46.03
    agent-3: 23.51
    agent-4: 15.23
    agent-5: 18.14
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 27.0
    agent-3: -23.0
    agent-4: 4.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.198887241016838
    mean_inference_ms: 15.06711001953052
    mean_processing_ms: 73.33686643317454
  time_since_restore: 29074.03111600876
  time_this_iter_s: 143.68980717658997
  time_total_s: 118010.70924568176
  timestamp: 1637392080
  timesteps_since_restore: 19200000
  timesteps_this_iter: 96000
  timesteps_total: 81600000
  training_iteration: 850
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    850 |           118011 | 81600000 |   128.01 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 6.46
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.03
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 30.24
    apples_agent-2_min: 18
    apples_agent-3_max: 34
    apples_agent-3_mean: 8.48
    apples_agent-3_min: 2
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.31
    apples_agent-4_min: 2
    apples_agent-5_max: 24
    apples_agent-5_mean: 3.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 26
    cleaning_beam_agent-0_mean: 3.96
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 244
    cleaning_beam_agent-1_mean: 214.08
    cleaning_beam_agent-1_min: 175
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 4.44
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 25
    cleaning_beam_agent-3_mean: 8.63
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.48
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.48
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-10-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 127.56
  episode_reward_min: 87.0
  episodes_this_iter: 96
  episodes_total: 81696
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11883.265
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2793083190917969
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008036606013774872
        model: {}
        policy_loss: -0.0015795258805155754
        total_loss: -0.001971066929399967
        vf_explained_var: 0.01031966507434845
        vf_loss: 1.0004409551620483
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21316657960414886
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011310754343867302
        model: {}
        policy_loss: -0.0015778751112520695
        total_loss: -0.0018995259888470173
        vf_explained_var: 0.08448472619056702
        vf_loss: 0.5352047681808472
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.402288019657135
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012734865304082632
        model: {}
        policy_loss: -0.0019858460873365402
        total_loss: -0.0023094038479030132
        vf_explained_var: 0.003937363624572754
        vf_loss: 3.8447093963623047
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4483431577682495
        entropy_coeff: 0.0017600000137463212
        kl: 0.001371217891573906
        model: {}
        policy_loss: -0.001611590152606368
        total_loss: -0.002237691543996334
        vf_explained_var: -0.0010810494422912598
        vf_loss: 1.6298365592956543
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3845319449901581
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018136424478143454
        model: {}
        policy_loss: -0.0016008454840630293
        total_loss: -0.002179315546527505
        vf_explained_var: 0.0028916597366333008
        vf_loss: 0.9830601811408997
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5182716250419617
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008840016671456397
        model: {}
        policy_loss: -0.0012734122574329376
        total_loss: -0.0020772460848093033
        vf_explained_var: 0.02194686233997345
        vf_loss: 1.0832462310791016
    load_time_ms: 14169.198
    num_steps_sampled: 81696000
    num_steps_trained: 81696000
    sample_time_ms: 118761.566
    update_time_ms: 16.609
  iterations_since_restore: 201
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.650724637681154
    ram_util_percent: 12.297101449275367
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 17.0
    agent-2: 65.0
    agent-3: 35.0
    agent-4: 26.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 15.44
    agent-1: 8.92
    agent-2: 46.55
    agent-3: 23.55
    agent-4: 15.15
    agent-5: 17.95
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 31.0
    agent-3: 13.0
    agent-4: 5.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.197107178558316
    mean_inference_ms: 15.065983809123031
    mean_processing_ms: 73.32958276970656
  time_since_restore: 29217.693530082703
  time_this_iter_s: 143.6624140739441
  time_total_s: 118154.3716597557
  timestamp: 1637392226
  timesteps_since_restore: 19296000
  timesteps_this_iter: 96000
  timesteps_total: 81696000
  training_iteration: 851
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    851 |           118154 | 81696000 |   127.56 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 7.06
    apples_agent-0_min: 1
    apples_agent-1_max: 69
    apples_agent-1_mean: 4.14
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 29.12
    apples_agent-2_min: 15
    apples_agent-3_max: 25
    apples_agent-3_mean: 8.74
    apples_agent-3_min: 2
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.1
    apples_agent-4_min: 2
    apples_agent-5_max: 25
    apples_agent-5_mean: 4.17
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 59
    cleaning_beam_agent-0_mean: 3.97
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 263
    cleaning_beam_agent-1_mean: 213.85
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 4.17
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 31
    cleaning_beam_agent-3_mean: 8.99
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.25
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.28
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-12-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 169.0
  episode_reward_mean: 125.82
  episode_reward_min: 65.0
  episodes_this_iter: 96
  episodes_total: 81792
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11873.395
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2879009246826172
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013416530564427376
        model: {}
        policy_loss: -0.0017760270275175571
        total_loss: -0.0021728803403675556
        vf_explained_var: 0.004340454936027527
        vf_loss: 1.098517894744873
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21147729456424713
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010711844079196453
        model: {}
        policy_loss: -0.0015020119026303291
        total_loss: -0.0018122950568795204
        vf_explained_var: 0.07928365468978882
        vf_loss: 0.6191506385803223
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40322837233543396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012034904211759567
        model: {}
        policy_loss: -0.0019458821043372154
        total_loss: -0.002265368588268757
        vf_explained_var: 0.005998849868774414
        vf_loss: 3.9019289016723633
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4516674280166626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009946427308022976
        model: {}
        policy_loss: -0.0014845672994852066
        total_loss: -0.002095501869916916
        vf_explained_var: 0.0032881200313568115
        vf_loss: 1.8399994373321533
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3865429162979126
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010204031132161617
        model: {}
        policy_loss: -0.0014621303416788578
        total_loss: -0.0020464262925088406
        vf_explained_var: 0.009746119379997253
        vf_loss: 0.9601917266845703
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5173006057739258
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013530128635466099
        model: {}
        policy_loss: -0.0015972205437719822
        total_loss: -0.0023788288235664368
        vf_explained_var: 0.006400883197784424
        vf_loss: 1.2884050607681274
    load_time_ms: 14161.357
    num_steps_sampled: 81792000
    num_steps_trained: 81792000
    sample_time_ms: 118757.208
    update_time_ms: 16.683
  iterations_since_restore: 202
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.998058252427185
    ram_util_percent: 12.365533980582525
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 22.0
    agent-2: 68.0
    agent-3: 46.0
    agent-4: 24.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 16.47
    agent-1: 8.78
    agent-2: 45.37
    agent-3: 23.71
    agent-4: 14.0
    agent-5: 17.49
  policy_reward_min:
    agent-0: 4.0
    agent-1: 3.0
    agent-2: 24.0
    agent-3: 11.0
    agent-4: 5.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.197090210386072
    mean_inference_ms: 15.065779958978037
    mean_processing_ms: 73.32941559537164
  time_since_restore: 29362.42440891266
  time_this_iter_s: 144.73087882995605
  time_total_s: 118299.10253858566
  timestamp: 1637392371
  timesteps_since_restore: 19392000
  timesteps_this_iter: 96000
  timesteps_total: 81792000
  training_iteration: 852
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    852 |           118299 | 81792000 |   125.82 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 7.24
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.17
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 29.14
    apples_agent-2_min: 7
    apples_agent-3_max: 29
    apples_agent-3_mean: 8.33
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.76
    apples_agent-4_min: 2
    apples_agent-5_max: 11
    apples_agent-5_mean: 3.86
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 18
    cleaning_beam_agent-0_mean: 3.33
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 250
    cleaning_beam_agent-1_mean: 212.98
    cleaning_beam_agent-1_min: 57
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.47
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 33
    cleaning_beam_agent-3_mean: 10.4
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.04
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.73
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-15-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 176.0
  episode_reward_mean: 127.19
  episode_reward_min: 41.0
  episodes_this_iter: 96
  episodes_total: 81888
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11871.544
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28291597962379456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008266997174359858
        model: {}
        policy_loss: -0.0017414034809917212
        total_loss: -0.0021211947314441204
        vf_explained_var: -0.0019800662994384766
        vf_loss: 1.1814143657684326
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21063421666622162
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010542756645008922
        model: {}
        policy_loss: -0.0015563652850687504
        total_loss: -0.0018714414909482002
        vf_explained_var: 0.05931934714317322
        vf_loss: 0.556394100189209
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40458565950393677
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010330402292311192
        model: {}
        policy_loss: -0.0016788365319371223
        total_loss: -0.001980886794626713
        vf_explained_var: 0.010264217853546143
        vf_loss: 4.100198745727539
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.454353928565979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014384130481630564
        model: {}
        policy_loss: -0.00169347133487463
        total_loss: -0.0023159501142799854
        vf_explained_var: -0.008277863264083862
        vf_loss: 1.7718279361724854
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38144561648368835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009895612020045519
        model: {}
        policy_loss: -0.0013925919774919748
        total_loss: -0.0019623127300292253
        vf_explained_var: 0.007589071989059448
        vf_loss: 1.0162365436553955
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5091595649719238
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018392124911770225
        model: {}
        policy_loss: -0.001710271928459406
        total_loss: -0.002477454487234354
        vf_explained_var: 0.00165577232837677
        vf_loss: 1.2893919944763184
    load_time_ms: 14168.79
    num_steps_sampled: 81888000
    num_steps_trained: 81888000
    sample_time_ms: 118701.859
    update_time_ms: 16.965
  iterations_since_restore: 203
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.864734299516908
    ram_util_percent: 12.371014492753623
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 16.0
    agent-2: 68.0
    agent-3: 41.0
    agent-4: 25.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 16.18
    agent-1: 8.58
    agent-2: 45.74
    agent-3: 23.87
    agent-4: 15.0
    agent-5: 17.82
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 11.0
    agent-3: 3.0
    agent-4: 6.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.196704765386677
    mean_inference_ms: 15.065355524783435
    mean_processing_ms: 73.3279770259612
  time_since_restore: 29507.272111415863
  time_this_iter_s: 144.84770250320435
  time_total_s: 118443.95024108887
  timestamp: 1637392516
  timesteps_since_restore: 19488000
  timesteps_this_iter: 96000
  timesteps_total: 81888000
  training_iteration: 853
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    853 |           118444 | 81888000 |   127.19 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 7.18
    apples_agent-0_min: 1
    apples_agent-1_max: 34
    apples_agent-1_mean: 3.59
    apples_agent-1_min: 0
    apples_agent-2_max: 71
    apples_agent-2_mean: 30.63
    apples_agent-2_min: 18
    apples_agent-3_max: 23
    apples_agent-3_mean: 8.24
    apples_agent-3_min: 1
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.62
    apples_agent-4_min: 1
    apples_agent-5_max: 22
    apples_agent-5_mean: 4.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 21
    cleaning_beam_agent-0_mean: 2.48
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 256
    cleaning_beam_agent-1_mean: 217.21
    cleaning_beam_agent-1_min: 168
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 4.35
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 10.8
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.33
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.42
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-17-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 163.0
  episode_reward_mean: 129.22
  episode_reward_min: 76.0
  episodes_this_iter: 96
  episodes_total: 81984
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11847.668
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28558677434921265
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007593808695673943
        model: {}
        policy_loss: -0.0017539311666041613
        total_loss: -0.002144167898222804
        vf_explained_var: 0.0011245012283325195
        vf_loss: 1.1239854097366333
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21356752514839172
        entropy_coeff: 0.0017600000137463212
        kl: 0.001213381183333695
        model: {}
        policy_loss: -0.0014758146135136485
        total_loss: -0.0017939190147444606
        vf_explained_var: 0.08008760213851929
        vf_loss: 0.5777702331542969
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4028564691543579
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010645674774423242
        model: {}
        policy_loss: -0.001954667968675494
        total_loss: -0.002305242232978344
        vf_explained_var: 0.012839511036872864
        vf_loss: 3.5845088958740234
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45236894488334656
        entropy_coeff: 0.0017600000137463212
        kl: 0.001701422967016697
        model: {}
        policy_loss: -0.001537504605948925
        total_loss: -0.0021622907370328903
        vf_explained_var: 0.005868569016456604
        vf_loss: 1.7138736248016357
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.389103502035141
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010938020423054695
        model: {}
        policy_loss: -0.0014053403865545988
        total_loss: -0.0019756569527089596
        vf_explained_var: 0.006916075944900513
        vf_loss: 1.145066499710083
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5195730924606323
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019331986550241709
        model: {}
        policy_loss: -0.00169574620667845
        total_loss: -0.0024862189311534166
        vf_explained_var: 0.019268617033958435
        vf_loss: 1.2397825717926025
    load_time_ms: 14148.84
    num_steps_sampled: 81984000
    num_steps_trained: 81984000
    sample_time_ms: 118752.065
    update_time_ms: 16.978
  iterations_since_restore: 204
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.028985507246375
    ram_util_percent: 12.293719806763288
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 18.0
    agent-2: 62.0
    agent-3: 48.0
    agent-4: 26.0
    agent-5: 37.0
  policy_reward_mean:
    agent-0: 16.57
    agent-1: 9.36
    agent-2: 45.96
    agent-3: 23.83
    agent-4: 14.71
    agent-5: 18.79
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 33.0
    agent-3: 11.0
    agent-4: 4.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.197083476509665
    mean_inference_ms: 15.065180581603695
    mean_processing_ms: 73.32924433696246
  time_since_restore: 29652.702704668045
  time_this_iter_s: 145.430593252182
  time_total_s: 118589.38083434105
  timestamp: 1637392661
  timesteps_since_restore: 19584000
  timesteps_this_iter: 96000
  timesteps_total: 81984000
  training_iteration: 854
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    854 |           118589 | 81984000 |   129.22 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 7.25
    apples_agent-0_min: 1
    apples_agent-1_max: 15
    apples_agent-1_mean: 3.4
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 30.46
    apples_agent-2_min: 5
    apples_agent-3_max: 26
    apples_agent-3_mean: 8.99
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.12
    apples_agent-4_min: 1
    apples_agent-5_max: 22
    apples_agent-5_mean: 4.3
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 17
    cleaning_beam_agent-0_mean: 2.69
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 268
    cleaning_beam_agent-1_mean: 219.07
    cleaning_beam_agent-1_min: 24
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 4.75
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 11.63
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.68
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.19
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-20-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 184.0
  episode_reward_mean: 129.53
  episode_reward_min: 16.0
  episodes_this_iter: 96
  episodes_total: 82080
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11852.789
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2856289744377136
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006561088375747204
        model: {}
        policy_loss: -0.0016003134660422802
        total_loss: -0.0019827065989375114
        vf_explained_var: 0.0032080113887786865
        vf_loss: 1.2031506299972534
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2133873999118805
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009064317564480007
        model: {}
        policy_loss: -0.001614126143977046
        total_loss: -0.0019365213811397552
        vf_explained_var: 0.09547655284404755
        vf_loss: 0.5316543579101562
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39768093824386597
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016749637434259057
        model: {}
        policy_loss: -0.0021452726796269417
        total_loss: -0.002444523386657238
        vf_explained_var: 0.02605229616165161
        vf_loss: 4.006687641143799
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45381104946136475
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011892039328813553
        model: {}
        policy_loss: -0.0014091087505221367
        total_loss: -0.0020114220678806305
        vf_explained_var: 0.005148082971572876
        vf_loss: 1.963935375213623
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39165300130844116
        entropy_coeff: 0.0017600000137463212
        kl: 0.000981267774477601
        model: {}
        policy_loss: -0.001314210006967187
        total_loss: -0.0019056901801377535
        vf_explained_var: 0.004878386855125427
        vf_loss: 0.9782865047454834
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5245130658149719
        entropy_coeff: 0.0017600000137463212
        kl: 0.001353603322058916
        model: {}
        policy_loss: -0.0014881775714457035
        total_loss: -0.0022804064210504293
        vf_explained_var: 0.014000609517097473
        vf_loss: 1.3091434240341187
    load_time_ms: 14141.806
    num_steps_sampled: 82080000
    num_steps_trained: 82080000
    sample_time_ms: 118811.261
    update_time_ms: 16.772
  iterations_since_restore: 205
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.920673076923077
    ram_util_percent: 12.365865384615384
  pid: 27065
  policy_reward_max:
    agent-0: 37.0
    agent-1: 16.0
    agent-2: 64.0
    agent-3: 39.0
    agent-4: 31.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 16.83
    agent-1: 9.4
    agent-2: 45.81
    agent-3: 24.59
    agent-4: 14.79
    agent-5: 18.11
  policy_reward_min:
    agent-0: 4.0
    agent-1: 0.0
    agent-2: 5.0
    agent-3: 2.0
    agent-4: 3.0
    agent-5: 2.0
  sampler_perf:
    mean_env_wait_ms: 28.197803394042552
    mean_inference_ms: 15.065303258568484
    mean_processing_ms: 73.32993597069301
  time_since_restore: 29798.353488206863
  time_this_iter_s: 145.65078353881836
  time_total_s: 118735.03161787987
  timestamp: 1637392807
  timesteps_since_restore: 19680000
  timesteps_this_iter: 96000
  timesteps_total: 82080000
  training_iteration: 855
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    855 |           118735 | 82080000 |   129.53 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 7.03
    apples_agent-0_min: 1
    apples_agent-1_max: 12
    apples_agent-1_mean: 3.24
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 29.06
    apples_agent-2_min: 16
    apples_agent-3_max: 26
    apples_agent-3_mean: 8.74
    apples_agent-3_min: 2
    apples_agent-4_max: 27
    apples_agent-4_mean: 10.63
    apples_agent-4_min: 3
    apples_agent-5_max: 27
    apples_agent-5_mean: 4.08
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 20
    cleaning_beam_agent-0_mean: 2.84
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 219.5
    cleaning_beam_agent-1_min: 178
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 4.81
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 38
    cleaning_beam_agent-3_mean: 10.04
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.02
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.13
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-22-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 165.0
  episode_reward_mean: 126.73
  episode_reward_min: 73.0
  episodes_this_iter: 96
  episodes_total: 82176
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11848.783
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2894710898399353
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007866688538342714
        model: {}
        policy_loss: -0.0016578143695369363
        total_loss: -0.0020605982281267643
        vf_explained_var: 0.007466927170753479
        vf_loss: 1.066828966140747
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21805086731910706
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009576515294611454
        model: {}
        policy_loss: -0.0013816156424582005
        total_loss: -0.001711609773337841
        vf_explained_var: 0.0827382504940033
        vf_loss: 0.5377880930900574
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39978674054145813
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010439606849104166
        model: {}
        policy_loss: -0.001639366615563631
        total_loss: -0.001978660933673382
        vf_explained_var: 0.013921171426773071
        vf_loss: 3.6433284282684326
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45203959941864014
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008912947378121316
        model: {}
        policy_loss: -0.0013352874666452408
        total_loss: -0.001947496086359024
        vf_explained_var: 0.005522683262825012
        vf_loss: 1.8338221311569214
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39055195450782776
        entropy_coeff: 0.0017600000137463212
        kl: 0.000646376283839345
        model: {}
        policy_loss: -0.0013100708601996303
        total_loss: -0.0018983646295964718
        vf_explained_var: 0.01201900839805603
        vf_loss: 0.9907807111740112
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5192018747329712
        entropy_coeff: 0.0017600000137463212
        kl: 0.002562060486525297
        model: {}
        policy_loss: -0.0018834248185157776
        total_loss: -0.0026615411043167114
        vf_explained_var: 0.0108795166015625
        vf_loss: 1.3567676544189453
    load_time_ms: 14152.27
    num_steps_sampled: 82176000
    num_steps_trained: 82176000
    sample_time_ms: 118804.916
    update_time_ms: 17.095
  iterations_since_restore: 206
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.82028985507246
    ram_util_percent: 12.445410628019323
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 18.0
    agent-2: 66.0
    agent-3: 43.0
    agent-4: 27.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 16.03
    agent-1: 8.66
    agent-2: 44.82
    agent-3: 23.82
    agent-4: 14.92
    agent-5: 18.48
  policy_reward_min:
    agent-0: 9.0
    agent-1: 2.0
    agent-2: 28.0
    agent-3: 11.0
    agent-4: 4.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.197326632067373
    mean_inference_ms: 15.06485207783911
    mean_processing_ms: 73.32793635187231
  time_since_restore: 29943.615513801575
  time_this_iter_s: 145.2620255947113
  time_total_s: 118880.29364347458
  timestamp: 1637392953
  timesteps_since_restore: 19776000
  timesteps_this_iter: 96000
  timesteps_total: 82176000
  training_iteration: 856
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    856 |           118880 | 82176000 |   126.73 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 6.72
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 3.21
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 28.01
    apples_agent-2_min: 10
    apples_agent-3_max: 25
    apples_agent-3_mean: 8.25
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.56
    apples_agent-4_min: 3
    apples_agent-5_max: 18
    apples_agent-5_mean: 4.12
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 22
    cleaning_beam_agent-0_mean: 2.92
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 250
    cleaning_beam_agent-1_mean: 212.01
    cleaning_beam_agent-1_min: 48
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.52
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 9.92
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.54
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.89
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-24-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 162.0
  episode_reward_mean: 125.53
  episode_reward_min: 32.0
  episodes_this_iter: 96
  episodes_total: 82272
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11830.318
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29477664828300476
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014678144361823797
        model: {}
        policy_loss: -0.0019448272651061416
        total_loss: -0.002361442195251584
        vf_explained_var: 0.010026618838310242
        vf_loss: 1.0219109058380127
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21315760910511017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010817097499966621
        model: {}
        policy_loss: -0.0016896193847060204
        total_loss: -0.0020118989050388336
        vf_explained_var: 0.09239831566810608
        vf_loss: 0.5287972688674927
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39883363246917725
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010929742129519582
        model: {}
        policy_loss: -0.0017479604575783014
        total_loss: -0.0020650068763643503
        vf_explained_var: 0.01824057102203369
        vf_loss: 3.848996877670288
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44530099630355835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015161843039095402
        model: {}
        policy_loss: -0.0015646452084183693
        total_loss: -0.0021718135103583336
        vf_explained_var: -0.0034185051918029785
        vf_loss: 1.7656278610229492
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3874610662460327
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008493115892633796
        model: {}
        policy_loss: -0.0014014641055837274
        total_loss: -0.001982068410143256
        vf_explained_var: 0.01084810495376587
        vf_loss: 1.0133004188537598
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5289953947067261
        entropy_coeff: 0.0017600000137463212
        kl: 0.001756492187269032
        model: {}
        policy_loss: -0.0014219535514712334
        total_loss: -0.0022307545877993107
        vf_explained_var: 0.022796630859375
        vf_loss: 1.222321629524231
    load_time_ms: 14113.812
    num_steps_sampled: 82272000
    num_steps_trained: 82272000
    sample_time_ms: 118655.281
    update_time_ms: 16.326
  iterations_since_restore: 207
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.850980392156863
    ram_util_percent: 12.205882352941176
  pid: 27065
  policy_reward_max:
    agent-0: 33.0
    agent-1: 17.0
    agent-2: 61.0
    agent-3: 40.0
    agent-4: 25.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.1
    agent-1: 9.37
    agent-2: 44.68
    agent-3: 23.87
    agent-4: 14.74
    agent-5: 17.77
  policy_reward_min:
    agent-0: 2.0
    agent-1: 3.0
    agent-2: 17.0
    agent-3: 2.0
    agent-4: 2.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.195241671825926
    mean_inference_ms: 15.063729830645563
    mean_processing_ms: 73.32211497565822
  time_since_restore: 30087.24856352806
  time_this_iter_s: 143.6330497264862
  time_total_s: 119023.92669320107
  timestamp: 1637393096
  timesteps_since_restore: 19872000
  timesteps_this_iter: 96000
  timesteps_total: 82272000
  training_iteration: 857
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    857 |           119024 | 82272000 |   125.53 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 7.01
    apples_agent-0_min: 1
    apples_agent-1_max: 40
    apples_agent-1_mean: 3.5
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 28.47
    apples_agent-2_min: 0
    apples_agent-3_max: 27
    apples_agent-3_mean: 8.08
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.11
    apples_agent-4_min: 1
    apples_agent-5_max: 15
    apples_agent-5_mean: 4.11
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 17
    cleaning_beam_agent-0_mean: 3.15
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 245
    cleaning_beam_agent-1_mean: 207.75
    cleaning_beam_agent-1_min: 48
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.24
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 9.24
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.37
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.48
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-27-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 163.0
  episode_reward_mean: 125.61
  episode_reward_min: 34.0
  episodes_this_iter: 96
  episodes_total: 82368
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11807.623
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2962656617164612
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010671481722965837
        model: {}
        policy_loss: -0.0016918731853365898
        total_loss: -0.002108692191541195
        vf_explained_var: 0.008658453822135925
        vf_loss: 1.0460841655731201
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20904593169689178
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015388091560453176
        model: {}
        policy_loss: -0.0016393885016441345
        total_loss: -0.0019527440890669823
        vf_explained_var: 0.09068837761878967
        vf_loss: 0.5456132888793945
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3902340531349182
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011336163152009249
        model: {}
        policy_loss: -0.0015803647693246603
        total_loss: -0.001883669407106936
        vf_explained_var: 0.005935162305831909
        vf_loss: 3.835092544555664
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4473645091056824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009101044852286577
        model: {}
        policy_loss: -0.0014630553778260946
        total_loss: -0.0020839034114032984
        vf_explained_var: 0.0036266595125198364
        vf_loss: 1.6651637554168701
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39496755599975586
        entropy_coeff: 0.0017600000137463212
        kl: 0.001027062302455306
        model: {}
        policy_loss: -0.0012895306572318077
        total_loss: -0.0018804650753736496
        vf_explained_var: 0.00915306806564331
        vf_loss: 1.042083978652954
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5317442417144775
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013585868291556835
        model: {}
        policy_loss: -0.0016738725826144218
        total_loss: -0.0024851635098457336
        vf_explained_var: 0.010272204875946045
        vf_loss: 1.2457795143127441
    load_time_ms: 14121.27
    num_steps_sampled: 82368000
    num_steps_trained: 82368000
    sample_time_ms: 118611.71
    update_time_ms: 15.999
  iterations_since_restore: 208
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.983980582524268
    ram_util_percent: 12.450970873786407
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 22.0
    agent-2: 65.0
    agent-3: 34.0
    agent-4: 29.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 15.47
    agent-1: 9.31
    agent-2: 45.07
    agent-3: 22.72
    agent-4: 15.06
    agent-5: 17.98
  policy_reward_min:
    agent-0: 4.0
    agent-1: 2.0
    agent-2: 0.0
    agent-3: 6.0
    agent-4: 4.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.194845872420697
    mean_inference_ms: 15.063405130787807
    mean_processing_ms: 73.321352858144
  time_since_restore: 30231.493478536606
  time_this_iter_s: 144.24491500854492
  time_total_s: 119168.17160820961
  timestamp: 1637393241
  timesteps_since_restore: 19968000
  timesteps_this_iter: 96000
  timesteps_total: 82368000
  training_iteration: 858
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    858 |           119168 | 82368000 |   125.61 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 7.43
    apples_agent-0_min: 0
    apples_agent-1_max: 16
    apples_agent-1_mean: 3.28
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 27.75
    apples_agent-2_min: 6
    apples_agent-3_max: 22
    apples_agent-3_mean: 8.14
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.3
    apples_agent-4_min: 1
    apples_agent-5_max: 16
    apples_agent-5_mean: 3.71
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 28
    cleaning_beam_agent-0_mean: 3.6
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 204.29
    cleaning_beam_agent-1_min: 43
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 5.17
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 71
    cleaning_beam_agent-3_mean: 10.57
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.83
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.17
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-29-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 160.0
  episode_reward_mean: 123.51
  episode_reward_min: 29.0
  episodes_this_iter: 96
  episodes_total: 82464
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11792.468
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29798421263694763
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009496060665696859
        model: {}
        policy_loss: -0.001632762374356389
        total_loss: -0.002056289929896593
        vf_explained_var: 0.005926474928855896
        vf_loss: 1.009262204170227
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20745866000652313
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010187857551500201
        model: {}
        policy_loss: -0.001416308106854558
        total_loss: -0.0017284310888499022
        vf_explained_var: 0.09445784986019135
        vf_loss: 0.530039370059967
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4016697406768799
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017505126306787133
        model: {}
        policy_loss: -0.0019457978196442127
        total_loss: -0.0022633865009993315
        vf_explained_var: 0.020938202738761902
        vf_loss: 3.8935084342956543
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.443205326795578
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015609714901074767
        model: {}
        policy_loss: -0.0015466960612684488
        total_loss: -0.0021461353171616793
        vf_explained_var: -0.0005956143140792847
        vf_loss: 1.8060142993927002
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39362943172454834
        entropy_coeff: 0.0017600000137463212
        kl: 0.000653464870993048
        model: {}
        policy_loss: -0.0012884273892268538
        total_loss: -0.0018751878524199128
        vf_explained_var: 0.007014080882072449
        vf_loss: 1.0602692365646362
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5339129567146301
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018301192903891206
        model: {}
        policy_loss: -0.0014687974471598864
        total_loss: -0.0022756727412343025
        vf_explained_var: 0.021038874983787537
        vf_loss: 1.328118920326233
    load_time_ms: 14090.876
    num_steps_sampled: 82464000
    num_steps_trained: 82464000
    sample_time_ms: 118643.386
    update_time_ms: 16.553
  iterations_since_restore: 209
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.002898550724634
    ram_util_percent: 12.278260869565214
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 18.0
    agent-2: 63.0
    agent-3: 41.0
    agent-4: 26.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 15.7
    agent-1: 8.71
    agent-2: 43.75
    agent-3: 23.26
    agent-4: 14.76
    agent-5: 17.33
  policy_reward_min:
    agent-0: 4.0
    agent-1: 1.0
    agent-2: 9.0
    agent-3: 4.0
    agent-4: 5.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.1946686907272
    mean_inference_ms: 15.063365585619641
    mean_processing_ms: 73.3230199011042
  time_since_restore: 30376.636169672012
  time_this_iter_s: 145.1426911354065
  time_total_s: 119313.31429934502
  timestamp: 1637393386
  timesteps_since_restore: 20064000
  timesteps_this_iter: 96000
  timesteps_total: 82464000
  training_iteration: 859
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    859 |           119313 | 82464000 |   123.51 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 7.7
    apples_agent-0_min: 1
    apples_agent-1_max: 19
    apples_agent-1_mean: 3.35
    apples_agent-1_min: 0
    apples_agent-2_max: 55
    apples_agent-2_mean: 28.34
    apples_agent-2_min: 12
    apples_agent-3_max: 35
    apples_agent-3_mean: 9.22
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.87
    apples_agent-4_min: 2
    apples_agent-5_max: 25
    apples_agent-5_mean: 4.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 35
    cleaning_beam_agent-0_mean: 3.83
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 248
    cleaning_beam_agent-1_mean: 211.85
    cleaning_beam_agent-1_min: 172
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 4.79
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 9.73
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.69
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.31
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-32-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 125.87
  episode_reward_min: 70.0
  episodes_this_iter: 96
  episodes_total: 82560
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11840.312
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29483988881111145
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008823360549286008
        model: {}
        policy_loss: -0.0016401363536715508
        total_loss: -0.0020577083341777325
        vf_explained_var: 0.0031224191188812256
        vf_loss: 1.013491153717041
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21255935728549957
        entropy_coeff: 0.0017600000137463212
        kl: 0.000650658505037427
        model: {}
        policy_loss: -0.001306670717895031
        total_loss: -0.0016291268402710557
        vf_explained_var: 0.07930499315261841
        vf_loss: 0.5164795517921448
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39377379417419434
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009784805588424206
        model: {}
        policy_loss: -0.0015667262487113476
        total_loss: -0.0018982779001817107
        vf_explained_var: 0.005572900176048279
        vf_loss: 3.614901542663574
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4407773017883301
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012961647007614374
        model: {}
        policy_loss: -0.0016152448952198029
        total_loss: -0.002221839502453804
        vf_explained_var: -0.0007980614900588989
        vf_loss: 1.6917146444320679
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3916082978248596
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011411659652367234
        model: {}
        policy_loss: -0.0013345247134566307
        total_loss: -0.001926625962369144
        vf_explained_var: 0.006985604763031006
        vf_loss: 0.9712651968002319
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5393039584159851
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015892322408035398
        model: {}
        policy_loss: -0.0015792050398886204
        total_loss: -0.002402800600975752
        vf_explained_var: -0.000620722770690918
        vf_loss: 1.2558493614196777
    load_time_ms: 14089.199
    num_steps_sampled: 82560000
    num_steps_trained: 82560000
    sample_time_ms: 118678.185
    update_time_ms: 16.188
  iterations_since_restore: 210
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.098536585365853
    ram_util_percent: 12.372195121951217
  pid: 27065
  policy_reward_max:
    agent-0: 24.0
    agent-1: 16.0
    agent-2: 70.0
    agent-3: 37.0
    agent-4: 30.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.4
    agent-1: 8.71
    agent-2: 44.94
    agent-3: 23.65
    agent-4: 14.81
    agent-5: 18.36
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 23.0
    agent-3: 7.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.193905671813514
    mean_inference_ms: 15.062798789310325
    mean_processing_ms: 73.320368289052
  time_since_restore: 30521.13858771324
  time_this_iter_s: 144.50241804122925
  time_total_s: 119457.81671738625
  timestamp: 1637393531
  timesteps_since_restore: 20160000
  timesteps_this_iter: 96000
  timesteps_total: 82560000
  training_iteration: 860
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    860 |           119458 | 82560000 |   125.87 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 7.45
    apples_agent-0_min: 1
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.34
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 28.03
    apples_agent-2_min: 15
    apples_agent-3_max: 25
    apples_agent-3_mean: 7.94
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 8.88
    apples_agent-4_min: 2
    apples_agent-5_max: 18
    apples_agent-5_mean: 4.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 20
    cleaning_beam_agent-0_mean: 2.74
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 249
    cleaning_beam_agent-1_mean: 208.85
    cleaning_beam_agent-1_min: 165
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.18
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 33
    cleaning_beam_agent-3_mean: 9.02
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 7
    cleaning_beam_agent-4_mean: 2.58
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 3.68
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-34-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 163.0
  episode_reward_mean: 125.82
  episode_reward_min: 88.0
  episodes_this_iter: 96
  episodes_total: 82656
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11836.425
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28584712743759155
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014894759515300393
        model: {}
        policy_loss: -0.001681748777627945
        total_loss: -0.002088836394250393
        vf_explained_var: 0.008174389600753784
        vf_loss: 0.9600898027420044
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2134980410337448
        entropy_coeff: 0.0017600000137463212
        kl: 0.001131425378844142
        model: {}
        policy_loss: -0.0014749420806765556
        total_loss: -0.0017929058521986008
        vf_explained_var: 0.08271212875843048
        vf_loss: 0.5779321193695068
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3963054418563843
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009721111855469644
        model: {}
        policy_loss: -0.001526426523923874
        total_loss: -0.0018621771596372128
        vf_explained_var: 0.031027331948280334
        vf_loss: 3.6174802780151367
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4318840205669403
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012706994311884046
        model: {}
        policy_loss: -0.0014172691153362393
        total_loss: -0.0020148558542132378
        vf_explained_var: 0.0057717859745025635
        vf_loss: 1.6252710819244385
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39043694734573364
        entropy_coeff: 0.0017600000137463212
        kl: 0.000529050943441689
        model: {}
        policy_loss: -0.001174600562080741
        total_loss: -0.0017638171557337046
        vf_explained_var: 0.0076270997524261475
        vf_loss: 0.9795021414756775
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5312269330024719
        entropy_coeff: 0.0017600000137463212
        kl: 0.002156270667910576
        model: {}
        policy_loss: -0.0016259804833680391
        total_loss: -0.002439154079183936
        vf_explained_var: 0.02626955509185791
        vf_loss: 1.2178215980529785
    load_time_ms: 14086.808
    num_steps_sampled: 82656000
    num_steps_trained: 82656000
    sample_time_ms: 118863.308
    update_time_ms: 16.524
  iterations_since_restore: 211
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.74976303317536
    ram_util_percent: 12.35781990521327
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 20.0
    agent-2: 70.0
    agent-3: 39.0
    agent-4: 30.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 15.31
    agent-1: 8.96
    agent-2: 44.92
    agent-3: 24.14
    agent-4: 14.8
    agent-5: 17.69
  policy_reward_min:
    agent-0: 5.0
    agent-1: 3.0
    agent-2: 23.0
    agent-3: 11.0
    agent-4: 4.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.19410274626731
    mean_inference_ms: 15.063038907985103
    mean_processing_ms: 73.32259142841617
  time_since_restore: 30666.7047188282
  time_this_iter_s: 145.56613111495972
  time_total_s: 119603.3828485012
  timestamp: 1637393678
  timesteps_since_restore: 20256000
  timesteps_this_iter: 96000
  timesteps_total: 82656000
  training_iteration: 861
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    861 |           119603 | 82656000 |   125.82 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 7.41
    apples_agent-0_min: 1
    apples_agent-1_max: 15
    apples_agent-1_mean: 3.2
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 29.0
    apples_agent-2_min: 12
    apples_agent-3_max: 20
    apples_agent-3_mean: 8.34
    apples_agent-3_min: 2
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.76
    apples_agent-4_min: 2
    apples_agent-5_max: 18
    apples_agent-5_mean: 4.2
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 29
    cleaning_beam_agent-0_mean: 3.07
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 247
    cleaning_beam_agent-1_mean: 211.7
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 3.76
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 8.42
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 3.04
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.04
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-37-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 172.0
  episode_reward_mean: 127.57
  episode_reward_min: 65.0
  episodes_this_iter: 96
  episodes_total: 82752
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11848.297
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2874113619327545
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009970355313271284
        model: {}
        policy_loss: -0.0016987668350338936
        total_loss: -0.002100587822496891
        vf_explained_var: 0.004077509045600891
        vf_loss: 1.0402358770370483
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21096360683441162
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008974773809313774
        model: {}
        policy_loss: -0.0014316909946501255
        total_loss: -0.0017567630857229233
        vf_explained_var: 0.09066428244113922
        vf_loss: 0.46222877502441406
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39686116576194763
        entropy_coeff: 0.0017600000137463212
        kl: 0.001171753858216107
        model: {}
        policy_loss: -0.0017657529097050428
        total_loss: -0.002078539226204157
        vf_explained_var: 0.0240604430437088
        vf_loss: 3.8569021224975586
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42524483799934387
        entropy_coeff: 0.0017600000137463212
        kl: 0.001563697587698698
        model: {}
        policy_loss: -0.0014864772092550993
        total_loss: -0.0020699051674455404
        vf_explained_var: 0.0005635470151901245
        vf_loss: 1.6500080823898315
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39228588342666626
        entropy_coeff: 0.0017600000137463212
        kl: 0.00032612288487143815
        model: {}
        policy_loss: -0.001104104332625866
        total_loss: -0.0017006653361022472
        vf_explained_var: 0.011643290519714355
        vf_loss: 0.938611626625061
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.531568169593811
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017333185533061624
        model: {}
        policy_loss: -0.0019281598506495357
        total_loss: -0.0027313088066875935
        vf_explained_var: 0.012075483798980713
        vf_loss: 1.3241249322891235
    load_time_ms: 14086.61
    num_steps_sampled: 82752000
    num_steps_trained: 82752000
    sample_time_ms: 118960.435
    update_time_ms: 16.443
  iterations_since_restore: 212
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.051690821256038
    ram_util_percent: 12.355555555555553
  pid: 27065
  policy_reward_max:
    agent-0: 33.0
    agent-1: 20.0
    agent-2: 69.0
    agent-3: 35.0
    agent-4: 26.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 16.6
    agent-1: 8.35
    agent-2: 45.7
    agent-3: 24.16
    agent-4: 14.46
    agent-5: 18.3
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 22.0
    agent-3: 12.0
    agent-4: 7.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.19470930584911
    mean_inference_ms: 15.063119113163344
    mean_processing_ms: 73.3259344798878
  time_since_restore: 30812.48180961609
  time_this_iter_s: 145.77709078788757
  time_total_s: 119749.1599392891
  timestamp: 1637393824
  timesteps_since_restore: 20352000
  timesteps_this_iter: 96000
  timesteps_total: 82752000
  training_iteration: 862
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    862 |           119749 | 82752000 |   127.57 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 7.2
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.48
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 29.99
    apples_agent-2_min: 11
    apples_agent-3_max: 26
    apples_agent-3_mean: 8.52
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.18
    apples_agent-4_min: 1
    apples_agent-5_max: 23
    apples_agent-5_mean: 4.03
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 36
    cleaning_beam_agent-0_mean: 3.1
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 259
    cleaning_beam_agent-1_mean: 211.25
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.46
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 8.19
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 15
    cleaning_beam_agent-4_mean: 2.67
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.21
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-39-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 177.0
  episode_reward_mean: 128.94
  episode_reward_min: 61.0
  episodes_this_iter: 96
  episodes_total: 82848
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11861.298
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2901681661605835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009011327638290823
        model: {}
        policy_loss: -0.0016194513300433755
        total_loss: -0.0020228514913469553
        vf_explained_var: 0.002215638756752014
        vf_loss: 1.0729786157608032
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20877793431282043
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009290719171985984
        model: {}
        policy_loss: -0.0013496698811650276
        total_loss: -0.001663033850491047
        vf_explained_var: 0.07637001574039459
        vf_loss: 0.5408133268356323
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3904062509536743
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012119632447138429
        model: {}
        policy_loss: -0.0019420089665800333
        total_loss: -0.0022087281104177237
        vf_explained_var: 0.017960861325263977
        vf_loss: 4.2039618492126465
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43236982822418213
        entropy_coeff: 0.0017600000137463212
        kl: 0.001507940818555653
        model: {}
        policy_loss: -0.0016959910281002522
        total_loss: -0.0022699597757309675
        vf_explained_var: 0.0027325302362442017
        vf_loss: 1.870060682296753
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3875846266746521
        entropy_coeff: 0.0017600000137463212
        kl: 0.000711719156242907
        model: {}
        policy_loss: -0.0012113600969314575
        total_loss: -0.0017771576531231403
        vf_explained_var: 0.010927289724349976
        vf_loss: 1.163540005683899
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5454434156417847
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014321167254820466
        model: {}
        policy_loss: -0.0017222375608980656
        total_loss: -0.002565953880548477
        vf_explained_var: 0.026752308011054993
        vf_loss: 1.162617564201355
    load_time_ms: 14103.654
    num_steps_sampled: 82848000
    num_steps_trained: 82848000
    sample_time_ms: 118964.541
    update_time_ms: 16.396
  iterations_since_restore: 213
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.919230769230772
    ram_util_percent: 12.364423076923078
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 16.0
    agent-2: 68.0
    agent-3: 36.0
    agent-4: 27.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.8
    agent-1: 8.88
    agent-2: 46.62
    agent-3: 24.64
    agent-4: 15.65
    agent-5: 17.35
  policy_reward_min:
    agent-0: 5.0
    agent-1: 1.0
    agent-2: 24.0
    agent-3: 6.0
    agent-4: 5.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.194794310520116
    mean_inference_ms: 15.063288037593788
    mean_processing_ms: 73.32665273690627
  time_since_restore: 30957.77298951149
  time_this_iter_s: 145.291179895401
  time_total_s: 119894.4511191845
  timestamp: 1637393970
  timesteps_since_restore: 20448000
  timesteps_this_iter: 96000
  timesteps_total: 82848000
  training_iteration: 863
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    863 |           119894 | 82848000 |   128.94 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 6.72
    apples_agent-0_min: 1
    apples_agent-1_max: 18
    apples_agent-1_mean: 3.43
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 29.63
    apples_agent-2_min: 14
    apples_agent-3_max: 23
    apples_agent-3_mean: 7.92
    apples_agent-3_min: 2
    apples_agent-4_max: 28
    apples_agent-4_mean: 9.68
    apples_agent-4_min: 3
    apples_agent-5_max: 15
    apples_agent-5_mean: 3.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 22
    cleaning_beam_agent-0_mean: 2.82
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 245
    cleaning_beam_agent-1_mean: 211.8
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 4.15
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 47
    cleaning_beam_agent-3_mean: 9.06
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 6
    cleaning_beam_agent-4_mean: 2.64
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.37
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-41-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 176.0
  episode_reward_mean: 127.4
  episode_reward_min: 78.0
  episodes_this_iter: 96
  episodes_total: 82944
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11866.447
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2869151830673218
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015759950038045645
        model: {}
        policy_loss: -0.0018547102808952332
        total_loss: -0.0022475961595773697
        vf_explained_var: 0.01376296579837799
        vf_loss: 1.1208570003509521
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20756888389587402
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009667451377026737
        model: {}
        policy_loss: -0.0015358668752014637
        total_loss: -0.001852299552410841
        vf_explained_var: 0.09757429361343384
        vf_loss: 0.48887014389038086
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39174073934555054
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017021078383550048
        model: {}
        policy_loss: -0.00186627428047359
        total_loss: -0.0021796594373881817
        vf_explained_var: 0.005132898688316345
        vf_loss: 3.760751485824585
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42873913049697876
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011752313002943993
        model: {}
        policy_loss: -0.0013356166891753674
        total_loss: -0.0019115377217531204
        vf_explained_var: 0.000612303614616394
        vf_loss: 1.7866137027740479
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38909247517585754
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013331986265257
        model: {}
        policy_loss: -0.0014412659220397472
        total_loss: -0.0020292396657168865
        vf_explained_var: 0.006827414035797119
        vf_loss: 0.9682909846305847
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5432756543159485
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018990737153217196
        model: {}
        policy_loss: -0.0017312150448560715
        total_loss: -0.0025565270334482193
        vf_explained_var: 0.010260447859764099
        vf_loss: 1.3085429668426514
    load_time_ms: 14102.651
    num_steps_sampled: 82944000
    num_steps_trained: 82944000
    sample_time_ms: 118925.131
    update_time_ms: 16.337
  iterations_since_restore: 214
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.039805825242723
    ram_util_percent: 12.371844660194173
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 18.0
    agent-2: 70.0
    agent-3: 37.0
    agent-4: 28.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 15.97
    agent-1: 8.82
    agent-2: 45.86
    agent-3: 24.56
    agent-4: 14.6
    agent-5: 17.59
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 26.0
    agent-3: 10.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.195245180548888
    mean_inference_ms: 15.063314467253665
    mean_processing_ms: 73.32623956341382
  time_since_restore: 31102.793869495392
  time_this_iter_s: 145.02087998390198
  time_total_s: 120039.4719991684
  timestamp: 1637394115
  timesteps_since_restore: 20544000
  timesteps_this_iter: 96000
  timesteps_total: 82944000
  training_iteration: 864
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    864 |           120039 | 82944000 |    127.4 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 7.22
    apples_agent-0_min: 1
    apples_agent-1_max: 18
    apples_agent-1_mean: 3.64
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 29.48
    apples_agent-2_min: 17
    apples_agent-3_max: 18
    apples_agent-3_mean: 8.62
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.48
    apples_agent-4_min: 2
    apples_agent-5_max: 19
    apples_agent-5_mean: 4.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 20
    cleaning_beam_agent-0_mean: 2.68
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 246
    cleaning_beam_agent-1_mean: 216.31
    cleaning_beam_agent-1_min: 183
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 3.95
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 23
    cleaning_beam_agent-3_mean: 7.64
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 3.01
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.97
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-44-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 176.0
  episode_reward_mean: 132.72
  episode_reward_min: 75.0
  episodes_this_iter: 96
  episodes_total: 83040
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11857.792
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2890692949295044
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008385000401176512
        model: {}
        policy_loss: -0.0016749408096075058
        total_loss: -0.002069985494017601
        vf_explained_var: -0.012634247541427612
        vf_loss: 1.137157917022705
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21088294684886932
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012298573274165392
        model: {}
        policy_loss: -0.001328769139945507
        total_loss: -0.0016397880390286446
        vf_explained_var: 0.07162542641162872
        vf_loss: 0.6013213992118835
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3896010220050812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014439636142924428
        model: {}
        policy_loss: -0.0018988745287060738
        total_loss: -0.0021969694644212723
        vf_explained_var: 0.010736420750617981
        vf_loss: 3.8760151863098145
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4237056374549866
        entropy_coeff: 0.0017600000137463212
        kl: 0.001297592418268323
        model: {}
        policy_loss: -0.0014957059174776077
        total_loss: -0.0020686881616711617
        vf_explained_var: 0.0025519579648971558
        vf_loss: 1.7273743152618408
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39113372564315796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007072244770824909
        model: {}
        policy_loss: -0.0013027614913880825
        total_loss: -0.001885298639535904
        vf_explained_var: 0.003877982497215271
        vf_loss: 1.058565378189087
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5388283133506775
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009208805859088898
        model: {}
        policy_loss: -0.0014570497442036867
        total_loss: -0.002261882647871971
        vf_explained_var: 0.031854256987571716
        vf_loss: 1.4350593090057373
    load_time_ms: 14099.702
    num_steps_sampled: 83040000
    num_steps_trained: 83040000
    sample_time_ms: 118856.622
    update_time_ms: 16.501
  iterations_since_restore: 215
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.92270531400966
    ram_util_percent: 12.26425120772947
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 20.0
    agent-2: 70.0
    agent-3: 38.0
    agent-4: 25.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 16.55
    agent-1: 9.93
    agent-2: 46.83
    agent-3: 25.31
    agent-4: 15.55
    agent-5: 18.55
  policy_reward_min:
    agent-0: 3.0
    agent-1: 3.0
    agent-2: 24.0
    agent-3: 13.0
    agent-4: 7.0
    agent-5: -29.0
  sampler_perf:
    mean_env_wait_ms: 28.195168415604552
    mean_inference_ms: 15.063103648038856
    mean_processing_ms: 73.32600945938681
  time_since_restore: 31247.647170066833
  time_this_iter_s: 144.85330057144165
  time_total_s: 120184.32529973984
  timestamp: 1637394260
  timesteps_since_restore: 20640000
  timesteps_this_iter: 96000
  timesteps_total: 83040000
  training_iteration: 865
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    865 |           120184 | 83040000 |   132.72 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 6.98
    apples_agent-0_min: 1
    apples_agent-1_max: 16
    apples_agent-1_mean: 3.54
    apples_agent-1_min: 0
    apples_agent-2_max: 54
    apples_agent-2_mean: 30.93
    apples_agent-2_min: 15
    apples_agent-3_max: 36
    apples_agent-3_mean: 9.26
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 9.97
    apples_agent-4_min: 2
    apples_agent-5_max: 15
    apples_agent-5_mean: 3.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 23
    cleaning_beam_agent-0_mean: 3.15
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 255
    cleaning_beam_agent-1_mean: 219.0
    cleaning_beam_agent-1_min: 57
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 4.03
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 25
    cleaning_beam_agent-3_mean: 8.51
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 7
    cleaning_beam_agent-4_mean: 2.41
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.48
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-46-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 170.0
  episode_reward_mean: 130.8
  episode_reward_min: 53.0
  episodes_this_iter: 96
  episodes_total: 83136
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11856.752
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2945355176925659
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010549197904765606
        model: {}
        policy_loss: -0.001823132042773068
        total_loss: -0.0022276933304965496
        vf_explained_var: 0.0052680522203445435
        vf_loss: 1.1382122039794922
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21449655294418335
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008385088294744492
        model: {}
        policy_loss: -0.0012934176484122872
        total_loss: -0.00161329610273242
        vf_explained_var: 0.08084340393543243
        vf_loss: 0.5763428211212158
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.384616494178772
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013314331881701946
        model: {}
        policy_loss: -0.0017522657290101051
        total_loss: -0.0020148875191807747
        vf_explained_var: 0.011881515383720398
        vf_loss: 4.143012046813965
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42909324169158936
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016000457108020782
        model: {}
        policy_loss: -0.0016936202300712466
        total_loss: -0.002244360512122512
        vf_explained_var: -0.00510387122631073
        vf_loss: 2.0446395874023438
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3891983926296234
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008839393267408013
        model: {}
        policy_loss: -0.0012742807157337666
        total_loss: -0.0018600584007799625
        vf_explained_var: 0.0025814175605773926
        vf_loss: 0.9921218156814575
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5393786430358887
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012813243083655834
        model: {}
        policy_loss: -0.0015410833293572068
        total_loss: -0.0023639719001948833
        vf_explained_var: 0.011929094791412354
        vf_loss: 1.2641677856445312
    load_time_ms: 14097.824
    num_steps_sampled: 83136000
    num_steps_trained: 83136000
    sample_time_ms: 118864.712
    update_time_ms: 16.289
  iterations_since_restore: 216
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.959420289855075
    ram_util_percent: 12.440096618357485
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 18.0
    agent-2: 65.0
    agent-3: 43.0
    agent-4: 26.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 16.83
    agent-1: 9.09
    agent-2: 46.64
    agent-3: 24.86
    agent-4: 14.91
    agent-5: 18.47
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 22.0
    agent-3: 4.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.195979506484896
    mean_inference_ms: 15.063207531818188
    mean_processing_ms: 73.32667571822668
  time_since_restore: 31392.95106458664
  time_this_iter_s: 145.3038945198059
  time_total_s: 120329.62919425964
  timestamp: 1637394405
  timesteps_since_restore: 20736000
  timesteps_this_iter: 96000
  timesteps_total: 83136000
  training_iteration: 866
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    866 |           120330 | 83136000 |    130.8 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 7.32
    apples_agent-0_min: 2
    apples_agent-1_max: 13
    apples_agent-1_mean: 3.12
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 29.38
    apples_agent-2_min: 16
    apples_agent-3_max: 38
    apples_agent-3_mean: 8.7
    apples_agent-3_min: 1
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.42
    apples_agent-4_min: 3
    apples_agent-5_max: 22
    apples_agent-5_mean: 4.2
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 24
    cleaning_beam_agent-0_mean: 2.4
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 254
    cleaning_beam_agent-1_mean: 215.36
    cleaning_beam_agent-1_min: 164
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 3.49
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 30
    cleaning_beam_agent-3_mean: 9.38
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 2.65
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 3.69
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-49-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 180.0
  episode_reward_mean: 130.51
  episode_reward_min: 92.0
  episodes_this_iter: 96
  episodes_total: 83232
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11865.216
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2975524067878723
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009944499470293522
        model: {}
        policy_loss: -0.0018986656796187162
        total_loss: -0.002312389202415943
        vf_explained_var: 0.0053239911794662476
        vf_loss: 1.0997055768966675
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2117316722869873
        entropy_coeff: 0.0017600000137463212
        kl: 0.001457497593946755
        model: {}
        policy_loss: -0.0014625947223976254
        total_loss: -0.0017841425724327564
        vf_explained_var: 0.09696204960346222
        vf_loss: 0.5109767913818359
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3787779211997986
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013602041872218251
        model: {}
        policy_loss: -0.0019248006865382195
        total_loss: -0.002208249643445015
        vf_explained_var: 0.021163299679756165
        vf_loss: 3.8319990634918213
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42550110816955566
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020971009507775307
        model: {}
        policy_loss: -0.0015506739728152752
        total_loss: -0.002100533340126276
        vf_explained_var: 0.0007866919040679932
        vf_loss: 1.9902329444885254
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3930830955505371
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005765890819020569
        model: {}
        policy_loss: -0.0012599637266248465
        total_loss: -0.0018530036322772503
        vf_explained_var: 0.010616376996040344
        vf_loss: 0.9878622889518738
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5241873264312744
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012971514370292425
        model: {}
        policy_loss: -0.0015492970123887062
        total_loss: -0.002341920044273138
        vf_explained_var: 0.005633443593978882
        vf_loss: 1.2994794845581055
    load_time_ms: 14116.019
    num_steps_sampled: 83232000
    num_steps_trained: 83232000
    sample_time_ms: 119007.097
    update_time_ms: 16.534
  iterations_since_restore: 217
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.073786407766995
    ram_util_percent: 12.445631067961164
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 18.0
    agent-2: 65.0
    agent-3: 42.0
    agent-4: 25.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 16.63
    agent-1: 9.18
    agent-2: 45.3
    agent-3: 26.24
    agent-4: 14.94
    agent-5: 18.22
  policy_reward_min:
    agent-0: 8.0
    agent-1: 1.0
    agent-2: 32.0
    agent-3: 14.0
    agent-4: 3.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.196172971594684
    mean_inference_ms: 15.063168207692877
    mean_processing_ms: 73.32662366579275
  time_since_restore: 31538.277482509613
  time_this_iter_s: 145.32641792297363
  time_total_s: 120474.95561218262
  timestamp: 1637394551
  timesteps_since_restore: 20832000
  timesteps_this_iter: 96000
  timesteps_total: 83232000
  training_iteration: 867
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    867 |           120475 | 83232000 |   130.51 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 7.46
    apples_agent-0_min: 2
    apples_agent-1_max: 19
    apples_agent-1_mean: 3.05
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 29.08
    apples_agent-2_min: 7
    apples_agent-3_max: 30
    apples_agent-3_mean: 8.83
    apples_agent-3_min: 2
    apples_agent-4_max: 25
    apples_agent-4_mean: 10.28
    apples_agent-4_min: 1
    apples_agent-5_max: 13
    apples_agent-5_mean: 3.79
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 19
    cleaning_beam_agent-0_mean: 1.97
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 244
    cleaning_beam_agent-1_mean: 214.08
    cleaning_beam_agent-1_min: 89
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 3.7
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 9.13
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.86
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.04
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-51-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 181.0
  episode_reward_mean: 127.15
  episode_reward_min: 48.0
  episodes_this_iter: 96
  episodes_total: 83328
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11865.67
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2955686151981354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011747785611078143
        model: {}
        policy_loss: -0.0018668975681066513
        total_loss: -0.0022796764969825745
        vf_explained_var: 0.011185705661773682
        vf_loss: 1.0742145776748657
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21174916625022888
        entropy_coeff: 0.0017600000137463212
        kl: 0.001249411259777844
        model: {}
        policy_loss: -0.0015838337130844593
        total_loss: -0.0019047628156840801
        vf_explained_var: 0.07033646106719971
        vf_loss: 0.5174682140350342
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3831402361392975
        entropy_coeff: 0.0017600000137463212
        kl: 0.001124148489907384
        model: {}
        policy_loss: -0.0017893528565764427
        total_loss: -0.0020570517517626286
        vf_explained_var: 0.001016765832901001
        vf_loss: 4.066285133361816
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4326803982257843
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014369315467774868
        model: {}
        policy_loss: -0.0015546446666121483
        total_loss: -0.002126396633684635
        vf_explained_var: 0.008690372109413147
        vf_loss: 1.897676706314087
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3894743323326111
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009881973965093493
        model: {}
        policy_loss: -0.0012893634848296642
        total_loss: -0.0018795952200889587
        vf_explained_var: 0.006718501448631287
        vf_loss: 0.9524471759796143
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5249852538108826
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014859549701213837
        model: {}
        policy_loss: -0.0017590918578207493
        total_loss: -0.0025598760694265366
        vf_explained_var: 0.009220317006111145
        vf_loss: 1.2318904399871826
    load_time_ms: 14133.195
    num_steps_sampled: 83328000
    num_steps_trained: 83328000
    sample_time_ms: 118972.854
    update_time_ms: 16.593
  iterations_since_restore: 218
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.8004854368932
    ram_util_percent: 12.388349514563107
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 17.0
    agent-2: 69.0
    agent-3: 35.0
    agent-4: 24.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.88
    agent-1: 8.82
    agent-2: 46.36
    agent-3: 23.83
    agent-4: 14.48
    agent-5: 17.78
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 14.0
    agent-3: 11.0
    agent-4: 3.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.194991357713562
    mean_inference_ms: 15.06234611963838
    mean_processing_ms: 73.32303323986312
  time_since_restore: 31682.393246650696
  time_this_iter_s: 144.11576414108276
  time_total_s: 120619.0713763237
  timestamp: 1637394695
  timesteps_since_restore: 20928000
  timesteps_this_iter: 96000
  timesteps_total: 83328000
  training_iteration: 868
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    868 |           120619 | 83328000 |   127.15 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 6.92
    apples_agent-0_min: 1
    apples_agent-1_max: 13
    apples_agent-1_mean: 3.78
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 30.7
    apples_agent-2_min: 10
    apples_agent-3_max: 25
    apples_agent-3_mean: 8.65
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.17
    apples_agent-4_min: 2
    apples_agent-5_max: 19
    apples_agent-5_mean: 3.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 20
    cleaning_beam_agent-0_mean: 2.3
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 270
    cleaning_beam_agent-1_mean: 218.75
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 4.42
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 25
    cleaning_beam_agent-3_mean: 8.59
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.84
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 4.99
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-54-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 181.0
  episode_reward_mean: 131.92
  episode_reward_min: 52.0
  episodes_this_iter: 96
  episodes_total: 83424
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11870.683
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2959832549095154
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012498687719926238
        model: {}
        policy_loss: -0.0019730376079678535
        total_loss: -0.0023844661191105843
        vf_explained_var: -0.002474173903465271
        vf_loss: 1.0950372219085693
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2151709496974945
        entropy_coeff: 0.0017600000137463212
        kl: 0.001358588575385511
        model: {}
        policy_loss: -0.001518036238849163
        total_loss: -0.0018378067761659622
        vf_explained_var: 0.09649869799613953
        vf_loss: 0.5893088579177856
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3861900568008423
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013816568534821272
        model: {}
        policy_loss: -0.0018304457189515233
        total_loss: -0.0021260662470012903
        vf_explained_var: 0.017909958958625793
        vf_loss: 3.840766429901123
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4294787347316742
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008005229756236076
        model: {}
        policy_loss: -0.001286040060222149
        total_loss: -0.0018498029094189405
        vf_explained_var: 0.0017099529504776
        vf_loss: 1.9211406707763672
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3945325016975403
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007775768171995878
        model: {}
        policy_loss: -0.0014234166592359543
        total_loss: -0.0020017761271446943
        vf_explained_var: 0.011305317282676697
        vf_loss: 1.1601941585540771
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5201851725578308
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012858659029006958
        model: {}
        policy_loss: -0.0015590274706482887
        total_loss: -0.00233546644449234
        vf_explained_var: 0.018298804759979248
        vf_loss: 1.3909114599227905
    load_time_ms: 14115.499
    num_steps_sampled: 83424000
    num_steps_trained: 83424000
    sample_time_ms: 119066.911
    update_time_ms: 16.544
  iterations_since_restore: 219
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.668750000000003
    ram_util_percent: 12.427884615384613
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 21.0
    agent-2: 71.0
    agent-3: 40.0
    agent-4: 30.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 15.78
    agent-1: 10.1
    agent-2: 46.93
    agent-3: 25.47
    agent-4: 15.08
    agent-5: 18.56
  policy_reward_min:
    agent-0: 8.0
    agent-1: 3.0
    agent-2: -3.0
    agent-3: 13.0
    agent-4: 0.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.195180897873975
    mean_inference_ms: 15.062095362870398
    mean_processing_ms: 73.3220071068058
  time_since_restore: 31828.29955148697
  time_this_iter_s: 145.9063048362732
  time_total_s: 120764.97768115997
  timestamp: 1637394841
  timesteps_since_restore: 21024000
  timesteps_this_iter: 96000
  timesteps_total: 83424000
  training_iteration: 869
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    869 |           120765 | 83424000 |   131.92 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 7.32
    apples_agent-0_min: 2
    apples_agent-1_max: 30
    apples_agent-1_mean: 3.73
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 29.89
    apples_agent-2_min: 13
    apples_agent-3_max: 22
    apples_agent-3_mean: 8.69
    apples_agent-3_min: 1
    apples_agent-4_max: 28
    apples_agent-4_mean: 10.41
    apples_agent-4_min: 4
    apples_agent-5_max: 22
    apples_agent-5_mean: 4.53
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 30
    cleaning_beam_agent-0_mean: 2.23
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 253
    cleaning_beam_agent-1_mean: 221.0
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 3.59
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 9.4
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.73
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.57
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-56-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 184.0
  episode_reward_mean: 132.43
  episode_reward_min: 70.0
  episodes_this_iter: 96
  episodes_total: 83520
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11833.599
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2873064875602722
        entropy_coeff: 0.0017600000137463212
        kl: 0.000971993082202971
        model: {}
        policy_loss: -0.0017406673869118094
        total_loss: -0.00213464698754251
        vf_explained_var: 0.009891748428344727
        vf_loss: 1.1167871952056885
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21601277589797974
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009171523270197213
        model: {}
        policy_loss: -0.0014517353847622871
        total_loss: -0.0017772214487195015
        vf_explained_var: 0.07926228642463684
        vf_loss: 0.546963632106781
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.388047456741333
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010708217741921544
        model: {}
        policy_loss: -0.0017154477536678314
        total_loss: -0.0019987737759947777
        vf_explained_var: 0.020092055201530457
        vf_loss: 3.9963607788085938
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4308774471282959
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012502659810706973
        model: {}
        policy_loss: -0.0014246065402403474
        total_loss: -0.0019784795586019754
        vf_explained_var: 0.0009087622165679932
        vf_loss: 2.0447351932525635
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40086227655410767
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009570288239046931
        model: {}
        policy_loss: -0.0013183392584323883
        total_loss: -0.0019161682575941086
        vf_explained_var: 0.008868411183357239
        vf_loss: 1.0768821239471436
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.523471474647522
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014187331544235349
        model: {}
        policy_loss: -0.0017194451065734029
        total_loss: -0.002514720195904374
        vf_explained_var: 0.013346374034881592
        vf_loss: 1.2603390216827393
    load_time_ms: 14135.633
    num_steps_sampled: 83520000
    num_steps_trained: 83520000
    sample_time_ms: 119109.095
    update_time_ms: 16.385
  iterations_since_restore: 220
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.991747572815534
    ram_util_percent: 12.45631067961165
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 21.0
    agent-2: 67.0
    agent-3: 42.0
    agent-4: 27.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 16.79
    agent-1: 9.33
    agent-2: 46.97
    agent-3: 25.61
    agent-4: 15.38
    agent-5: 18.35
  policy_reward_min:
    agent-0: 9.0
    agent-1: 2.0
    agent-2: 25.0
    agent-3: 10.0
    agent-4: 4.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.194795913032475
    mean_inference_ms: 15.061667186054237
    mean_processing_ms: 73.3196910266128
  time_since_restore: 31973.0911192894
  time_this_iter_s: 144.7915678024292
  time_total_s: 120909.7692489624
  timestamp: 1637394986
  timesteps_since_restore: 21120000
  timesteps_this_iter: 96000
  timesteps_total: 83520000
  training_iteration: 870
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    870 |           120910 | 83520000 |   132.43 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 8.07
    apples_agent-0_min: 1
    apples_agent-1_max: 13
    apples_agent-1_mean: 3.4
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 30.26
    apples_agent-2_min: 19
    apples_agent-3_max: 41
    apples_agent-3_mean: 9.23
    apples_agent-3_min: 3
    apples_agent-4_max: 30
    apples_agent-4_mean: 10.23
    apples_agent-4_min: 2
    apples_agent-5_max: 15
    apples_agent-5_mean: 4.19
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 2.3
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 255
    cleaning_beam_agent-1_mean: 222.46
    cleaning_beam_agent-1_min: 165
    cleaning_beam_agent-2_max: 11
    cleaning_beam_agent-2_mean: 3.42
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 9.17
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.28
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 5.18
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_02-58-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 198.0
  episode_reward_mean: 132.3
  episode_reward_min: 95.0
  episodes_this_iter: 96
  episodes_total: 83616
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11829.926
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2869046628475189
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009595146402716637
        model: {}
        policy_loss: -0.0017065322026610374
        total_loss: -0.002096072304993868
        vf_explained_var: 0.01172386109828949
        vf_loss: 1.1541149616241455
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21773025393486023
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013754584360867739
        model: {}
        policy_loss: -0.0018957809079438448
        total_loss: -0.002227452816441655
        vf_explained_var: 0.08415527641773224
        vf_loss: 0.5153443217277527
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3865283727645874
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015102691249921918
        model: {}
        policy_loss: -0.002092603826895356
        total_loss: -0.0023787980899214745
        vf_explained_var: 0.018209248781204224
        vf_loss: 3.9409713745117188
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4369370937347412
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012677815975621343
        model: {}
        policy_loss: -0.0015266775153577328
        total_loss: -0.0020974292419850826
        vf_explained_var: -0.0028176605701446533
        vf_loss: 1.9825520515441895
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38408949971199036
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012006921460852027
        model: {}
        policy_loss: -0.0013946909457445145
        total_loss: -0.00196714885532856
        vf_explained_var: 0.005718693137168884
        vf_loss: 1.0354282855987549
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5223821997642517
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011752662248909473
        model: {}
        policy_loss: -0.0014387734699994326
        total_loss: -0.0022298130206763744
        vf_explained_var: 0.006142869591712952
        vf_loss: 1.2834968566894531
    load_time_ms: 14112.8
    num_steps_sampled: 83616000
    num_steps_trained: 83616000
    sample_time_ms: 118972.242
    update_time_ms: 15.941
  iterations_since_restore: 221
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.527884615384615
    ram_util_percent: 12.451442307692306
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 17.0
    agent-2: 76.0
    agent-3: 45.0
    agent-4: 26.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 16.95
    agent-1: 8.89
    agent-2: 47.31
    agent-3: 25.74
    agent-4: 14.88
    agent-5: 18.53
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 31.0
    agent-3: 12.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.19406899090166
    mean_inference_ms: 15.060661018139344
    mean_processing_ms: 73.31604215250424
  time_since_restore: 32116.893752336502
  time_this_iter_s: 143.80263304710388
  time_total_s: 121053.5718820095
  timestamp: 1637395132
  timesteps_since_restore: 21216000
  timesteps_this_iter: 96000
  timesteps_total: 83616000
  training_iteration: 871
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    871 |           121054 | 83616000 |    132.3 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 7.38
    apples_agent-0_min: 0
    apples_agent-1_max: 69
    apples_agent-1_mean: 4.13
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 29.7
    apples_agent-2_min: 2
    apples_agent-3_max: 26
    apples_agent-3_mean: 8.77
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.46
    apples_agent-4_min: 1
    apples_agent-5_max: 24
    apples_agent-5_mean: 4.48
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 2.78
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 258
    cleaning_beam_agent-1_mean: 218.71
    cleaning_beam_agent-1_min: 15
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 4.74
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 9.3
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.66
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 5.29
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-01-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 168.0
  episode_reward_mean: 131.61
  episode_reward_min: 6.0
  episodes_this_iter: 96
  episodes_total: 83712
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11839.096
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28464558720588684
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007788266520947218
        model: {}
        policy_loss: -0.001401551067829132
        total_loss: -0.001785325352102518
        vf_explained_var: 0.010810405015945435
        vf_loss: 1.1720458269119263
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21151572465896606
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010644427966326475
        model: {}
        policy_loss: -0.001497924793511629
        total_loss: -0.0018076179549098015
        vf_explained_var: 0.07752580940723419
        vf_loss: 0.6257875561714172
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3954852223396301
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011157800909131765
        model: {}
        policy_loss: -0.001801345031708479
        total_loss: -0.0020488929003477097
        vf_explained_var: 0.01570102572441101
        vf_loss: 4.485044479370117
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4254496693611145
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015071792295202613
        model: {}
        policy_loss: -0.0015601151390001178
        total_loss: -0.0021037652622908354
        vf_explained_var: -0.0020020604133605957
        vf_loss: 2.051384925842285
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39069393277168274
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005961695569567382
        model: {}
        policy_loss: -0.0011606640182435513
        total_loss: -0.0017455066554248333
        vf_explained_var: 0.005608722567558289
        vf_loss: 1.027799129486084
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5200868844985962
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017248319927603006
        model: {}
        policy_loss: -0.0016565509140491486
        total_loss: -0.002442549914121628
        vf_explained_var: 0.022684261202812195
        vf_loss: 1.2935163974761963
    load_time_ms: 14161.284
    num_steps_sampled: 83712000
    num_steps_trained: 83712000
    sample_time_ms: 118808.1
    update_time_ms: 15.904
  iterations_since_restore: 222
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.011165048543685
    ram_util_percent: 12.480582524271844
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 20.0
    agent-2: 67.0
    agent-3: 43.0
    agent-4: 29.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 17.2
    agent-1: 9.45
    agent-2: 46.35
    agent-3: 25.31
    agent-4: 15.36
    agent-5: 17.94
  policy_reward_min:
    agent-0: 1.0
    agent-1: 1.0
    agent-2: 2.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: 1.0
  sampler_perf:
    mean_env_wait_ms: 28.193608949159582
    mean_inference_ms: 15.060032409517676
    mean_processing_ms: 73.31310769629452
  time_since_restore: 32261.660556793213
  time_this_iter_s: 144.76680445671082
  time_total_s: 121198.33868646622
  timestamp: 1637395277
  timesteps_since_restore: 21312000
  timesteps_this_iter: 96000
  timesteps_total: 83712000
  training_iteration: 872
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    872 |           121198 | 83712000 |   131.61 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 7.49
    apples_agent-0_min: 1
    apples_agent-1_max: 18
    apples_agent-1_mean: 3.2
    apples_agent-1_min: 0
    apples_agent-2_max: 65
    apples_agent-2_mean: 31.02
    apples_agent-2_min: 8
    apples_agent-3_max: 34
    apples_agent-3_mean: 8.98
    apples_agent-3_min: 0
    apples_agent-4_max: 25
    apples_agent-4_mean: 10.18
    apples_agent-4_min: 2
    apples_agent-5_max: 21
    apples_agent-5_mean: 4.43
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 2.46
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 251
    cleaning_beam_agent-1_mean: 215.04
    cleaning_beam_agent-1_min: 64
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 4.24
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 9.25
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 6
    cleaning_beam_agent-4_mean: 2.24
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 5.42
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-03-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 176.0
  episode_reward_mean: 131.45
  episode_reward_min: 36.0
  episodes_this_iter: 96
  episodes_total: 83808
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11821.069
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.288485050201416
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011244777124375105
        model: {}
        policy_loss: -0.0019355594413354993
        total_loss: -0.002329786540940404
        vf_explained_var: 0.014639899134635925
        vf_loss: 1.135053277015686
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2113756388425827
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013551021693274379
        model: {}
        policy_loss: -0.0015248460695147514
        total_loss: -0.0018381797708570957
        vf_explained_var: 0.06849850714206696
        vf_loss: 0.5869001746177673
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40009820461273193
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010660933330655098
        model: {}
        policy_loss: -0.0017752908170223236
        total_loss: -0.0020616501569747925
        vf_explained_var: 0.016092345118522644
        vf_loss: 4.178133487701416
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42902448773384094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016080379718914628
        model: {}
        policy_loss: -0.0015736814821138978
        total_loss: -0.002128330059349537
        vf_explained_var: 0.003104090690612793
        vf_loss: 2.0043411254882812
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39549344778060913
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007433050777763128
        model: {}
        policy_loss: -0.0014503650600090623
        total_loss: -0.002044056775048375
        vf_explained_var: 0.012037605047225952
        vf_loss: 1.0237855911254883
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5106226205825806
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013010079273954034
        model: {}
        policy_loss: -0.00164409214630723
        total_loss: -0.0024057896807789803
        vf_explained_var: -0.005888164043426514
        vf_loss: 1.3699936866760254
    load_time_ms: 14133.108
    num_steps_sampled: 83808000
    num_steps_trained: 83808000
    sample_time_ms: 118795.34
    update_time_ms: 16.154
  iterations_since_restore: 223
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.925242718446604
    ram_util_percent: 12.37378640776699
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 20.0
    agent-2: 75.0
    agent-3: 42.0
    agent-4: 27.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 16.24
    agent-1: 9.33
    agent-2: 47.09
    agent-3: 25.03
    agent-4: 15.53
    agent-5: 18.23
  policy_reward_min:
    agent-0: 2.0
    agent-1: 1.0
    agent-2: 12.0
    agent-3: 0.0
    agent-4: 5.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.19267595790117
    mean_inference_ms: 15.059823648270099
    mean_processing_ms: 73.31034179173466
  time_since_restore: 32406.264516830444
  time_this_iter_s: 144.60396003723145
  time_total_s: 121342.94264650345
  timestamp: 1637395421
  timesteps_since_restore: 21408000
  timesteps_this_iter: 96000
  timesteps_total: 83808000
  training_iteration: 873
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    873 |           121343 | 83808000 |   131.45 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 7.23
    apples_agent-0_min: 1
    apples_agent-1_max: 18
    apples_agent-1_mean: 3.17
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 29.05
    apples_agent-2_min: 12
    apples_agent-3_max: 28
    apples_agent-3_mean: 8.54
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.18
    apples_agent-4_min: 2
    apples_agent-5_max: 17
    apples_agent-5_mean: 4.56
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 18
    cleaning_beam_agent-0_mean: 2.52
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 263
    cleaning_beam_agent-1_mean: 214.87
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 4.31
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 8.21
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.4
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 5.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-06-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 177.0
  episode_reward_mean: 129.81
  episode_reward_min: 55.0
  episodes_this_iter: 96
  episodes_total: 83904
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11820.149
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2912088632583618
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006526752840727568
        model: {}
        policy_loss: -0.0014798223273828626
        total_loss: -0.0018826501909643412
        vf_explained_var: 0.0036633461713790894
        vf_loss: 1.09698486328125
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21003331243991852
        entropy_coeff: 0.0017600000137463212
        kl: 0.001162360655143857
        model: {}
        policy_loss: -0.0015758078079670668
        total_loss: -0.001887631369754672
        vf_explained_var: 0.09091222286224365
        vf_loss: 0.5783647298812866
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4003496766090393
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013727564364671707
        model: {}
        policy_loss: -0.0017923121340572834
        total_loss: -0.002135653980076313
        vf_explained_var: 0.01443454623222351
        vf_loss: 3.612746238708496
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4264628291130066
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015272211749106646
        model: {}
        policy_loss: -0.0016344566829502583
        total_loss: -0.002203144133090973
        vf_explained_var: 0.002318143844604492
        vf_loss: 1.8188766241073608
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.390206903219223
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004737117560580373
        model: {}
        policy_loss: -0.001153732999227941
        total_loss: -0.0017367186956107616
        vf_explained_var: 0.008346796035766602
        vf_loss: 1.0377464294433594
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5177077054977417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010783981997519732
        model: {}
        policy_loss: -0.001489361748099327
        total_loss: -0.0022603077813982964
        vf_explained_var: 0.008989140391349792
        vf_loss: 1.4021811485290527
    load_time_ms: 14135.966
    num_steps_sampled: 83904000
    num_steps_trained: 83904000
    sample_time_ms: 118777.844
    update_time_ms: 16.092
  iterations_since_restore: 224
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.95410628019324
    ram_util_percent: 12.457487922705313
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 20.0
    agent-2: 62.0
    agent-3: 43.0
    agent-4: 28.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 16.19
    agent-1: 9.17
    agent-2: 45.39
    agent-3: 24.69
    agent-4: 15.82
    agent-5: 18.55
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 24.0
    agent-3: 5.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.19262673283838
    mean_inference_ms: 15.05956132775074
    mean_processing_ms: 73.31050449068502
  time_since_restore: 32551.127445220947
  time_this_iter_s: 144.86292839050293
  time_total_s: 121487.80557489395
  timestamp: 1637395566
  timesteps_since_restore: 21504000
  timesteps_this_iter: 96000
  timesteps_total: 83904000
  training_iteration: 874
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    874 |           121488 | 83904000 |   129.81 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 7.45
    apples_agent-0_min: 1
    apples_agent-1_max: 16
    apples_agent-1_mean: 3.08
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 30.31
    apples_agent-2_min: 14
    apples_agent-3_max: 26
    apples_agent-3_mean: 8.72
    apples_agent-3_min: 2
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.1
    apples_agent-4_min: 1
    apples_agent-5_max: 15
    apples_agent-5_mean: 4.27
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 2.49
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 255
    cleaning_beam_agent-1_mean: 212.67
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 4.72
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 28
    cleaning_beam_agent-3_mean: 8.15
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.15
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 5.17
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-08-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 131.12
  episode_reward_min: 95.0
  episodes_this_iter: 96
  episodes_total: 84000
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11837.382
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29347336292266846
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009838645346462727
        model: {}
        policy_loss: -0.0017961729317903519
        total_loss: -0.0021987161599099636
        vf_explained_var: 0.006627187132835388
        vf_loss: 1.1396867036819458
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21383216977119446
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006732284091413021
        model: {}
        policy_loss: -0.0013198170345276594
        total_loss: -0.0016402231995016336
        vf_explained_var: 0.08471333980560303
        vf_loss: 0.5594128966331482
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39600667357444763
        entropy_coeff: 0.0017600000137463212
        kl: 0.001044664066284895
        model: {}
        policy_loss: -0.0017724682111293077
        total_loss: -0.002090080641210079
        vf_explained_var: 0.011694252490997314
        vf_loss: 3.793595314025879
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4328697919845581
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014825330581516027
        model: {}
        policy_loss: -0.0014099488034844398
        total_loss: -0.0019879057072103024
        vf_explained_var: -0.003388926386833191
        vf_loss: 1.8389424085617065
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38149070739746094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011066498700529337
        model: {}
        policy_loss: -0.0014060474932193756
        total_loss: -0.0019749184139072895
        vf_explained_var: 0.006962254643440247
        vf_loss: 1.025514841079712
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5325552821159363
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015281846281141043
        model: {}
        policy_loss: -0.0016356767155230045
        total_loss: -0.002458329312503338
        vf_explained_var: 0.0007101893424987793
        vf_loss: 1.1464450359344482
    load_time_ms: 14173.005
    num_steps_sampled: 84000000
    num_steps_trained: 84000000
    sample_time_ms: 118798.727
    update_time_ms: 16.218
  iterations_since_restore: 225
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.91062801932367
    ram_util_percent: 12.45893719806763
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 19.0
    agent-2: 60.0
    agent-3: 39.0
    agent-4: 27.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 16.75
    agent-1: 9.12
    agent-2: 46.7
    agent-3: 24.86
    agent-4: 15.42
    agent-5: 18.27
  policy_reward_min:
    agent-0: 7.0
    agent-1: 3.0
    agent-2: 33.0
    agent-3: 14.0
    agent-4: 5.0
    agent-5: 10.0
  sampler_perf:
    mean_env_wait_ms: 28.19268455033963
    mean_inference_ms: 15.0593823298901
    mean_processing_ms: 73.31048972748535
  time_since_restore: 32696.85236644745
  time_this_iter_s: 145.72492122650146
  time_total_s: 121633.53049612045
  timestamp: 1637395712
  timesteps_since_restore: 21600000
  timesteps_this_iter: 96000
  timesteps_total: 84000000
  training_iteration: 875
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    875 |           121634 | 84000000 |   131.12 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 7.15
    apples_agent-0_min: 2
    apples_agent-1_max: 11
    apples_agent-1_mean: 3.44
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 29.8
    apples_agent-2_min: 18
    apples_agent-3_max: 43
    apples_agent-3_mean: 8.65
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.68
    apples_agent-4_min: 2
    apples_agent-5_max: 22
    apples_agent-5_mean: 4.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 2.46
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 261
    cleaning_beam_agent-1_mean: 210.97
    cleaning_beam_agent-1_min: 168
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 4.28
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 8.79
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 7
    cleaning_beam_agent-4_mean: 2.41
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 5.41
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-10-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 176.0
  episode_reward_mean: 129.15
  episode_reward_min: 58.0
  episodes_this_iter: 96
  episodes_total: 84096
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11839.739
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2954198122024536
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009027225314639509
        model: {}
        policy_loss: -0.0018740524537861347
        total_loss: -0.00228360528126359
        vf_explained_var: 0.0019428879022598267
        vf_loss: 1.1038110256195068
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2116669863462448
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012517075520008802
        model: {}
        policy_loss: -0.0016081980429589748
        total_loss: -0.0019269338808953762
        vf_explained_var: 0.09167245030403137
        vf_loss: 0.5379936695098877
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39581549167633057
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013964107492938638
        model: {}
        policy_loss: -0.0017199814319610596
        total_loss: -0.0020351922139525414
        vf_explained_var: 0.01811833679676056
        vf_loss: 3.8142364025115967
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43842923641204834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017734513385221362
        model: {}
        policy_loss: -0.0016838377341628075
        total_loss: -0.002270554890856147
        vf_explained_var: 0.004195958375930786
        vf_loss: 1.849179983139038
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3855624198913574
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008000402012839913
        model: {}
        policy_loss: -0.001275648595765233
        total_loss: -0.0018453053198754787
        vf_explained_var: 0.010517150163650513
        vf_loss: 1.0893301963806152
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5290302038192749
        entropy_coeff: 0.0017600000137463212
        kl: 0.001526088803075254
        model: {}
        policy_loss: -0.00167904794216156
        total_loss: -0.0024795006029307842
        vf_explained_var: 0.017927363514900208
        vf_loss: 1.3064016103744507
    load_time_ms: 14186.319
    num_steps_sampled: 84096000
    num_steps_trained: 84096000
    sample_time_ms: 118715.563
    update_time_ms: 16.228
  iterations_since_restore: 226
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.927669902912623
    ram_util_percent: 12.45679611650485
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 19.0
    agent-2: 67.0
    agent-3: 37.0
    agent-4: 31.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 15.87
    agent-1: 9.42
    agent-2: 46.42
    agent-3: 24.56
    agent-4: 15.18
    agent-5: 17.7
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 28.0
    agent-3: 14.0
    agent-4: -42.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.19244369433172
    mean_inference_ms: 15.059233100873982
    mean_processing_ms: 73.30886862269885
  time_since_restore: 32841.44782304764
  time_this_iter_s: 144.5954566001892
  time_total_s: 121778.12595272064
  timestamp: 1637395857
  timesteps_since_restore: 21696000
  timesteps_this_iter: 96000
  timesteps_total: 84096000
  training_iteration: 876
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    876 |           121778 | 84096000 |   129.15 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 7.48
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 3.68
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 32.0
    apples_agent-2_min: 20
    apples_agent-3_max: 33
    apples_agent-3_mean: 8.82
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.15
    apples_agent-4_min: 2
    apples_agent-5_max: 26
    apples_agent-5_mean: 4.54
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 29
    cleaning_beam_agent-0_mean: 3.6
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 261
    cleaning_beam_agent-1_mean: 214.33
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 12
    cleaning_beam_agent-2_mean: 4.62
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 8.69
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 2.71
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.5
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-13-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 206.0
  episode_reward_mean: 132.47
  episode_reward_min: 89.0
  episodes_this_iter: 96
  episodes_total: 84192
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11843.739
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29105234146118164
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007494016317650676
        model: {}
        policy_loss: -0.0015553315170109272
        total_loss: -0.0019510271959006786
        vf_explained_var: 0.009641483426094055
        vf_loss: 1.1656036376953125
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21384228765964508
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009922399185597897
        model: {}
        policy_loss: -0.0015777221415191889
        total_loss: -0.0019016488222405314
        vf_explained_var: 0.08001579344272614
        vf_loss: 0.5243816375732422
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40662336349487305
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019508819095790386
        model: {}
        policy_loss: -0.0019504560623317957
        total_loss: -0.002296402584761381
        vf_explained_var: 0.016122356057167053
        vf_loss: 3.6971142292022705
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4420944154262543
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016079636989161372
        model: {}
        policy_loss: -0.0017946981824934483
        total_loss: -0.0023982315324246883
        vf_explained_var: 0.013127908110618591
        vf_loss: 1.7455477714538574
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3804894983768463
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007398424204438925
        model: {}
        policy_loss: -0.0012775349896401167
        total_loss: -0.0018424787558615208
        vf_explained_var: 0.006633087992668152
        vf_loss: 1.0471937656402588
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5273230075836182
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016696047969162464
        model: {}
        policy_loss: -0.0016161836683750153
        total_loss: -0.002410484477877617
        vf_explained_var: 0.006735384464263916
        vf_loss: 1.3378643989562988
    load_time_ms: 14201.066
    num_steps_sampled: 84192000
    num_steps_trained: 84192000
    sample_time_ms: 118748.303
    update_time_ms: 15.958
  iterations_since_restore: 227
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.924519230769228
    ram_util_percent: 12.376442307692306
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 20.0
    agent-2: 72.0
    agent-3: 51.0
    agent-4: 28.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 16.71
    agent-1: 9.07
    agent-2: 47.71
    agent-3: 25.12
    agent-4: 15.25
    agent-5: 18.61
  policy_reward_min:
    agent-0: 4.0
    agent-1: 3.0
    agent-2: 29.0
    agent-3: 14.0
    agent-4: 5.0
    agent-5: 11.0
  sampler_perf:
    mean_env_wait_ms: 28.192745242085252
    mean_inference_ms: 15.059243165967118
    mean_processing_ms: 73.30965357908967
  time_since_restore: 32987.35691809654
  time_this_iter_s: 145.90909504890442
  time_total_s: 121924.03504776955
  timestamp: 1637396003
  timesteps_since_restore: 21792000
  timesteps_this_iter: 96000
  timesteps_total: 84192000
  training_iteration: 877
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    877 |           121924 | 84192000 |   132.47 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 7.4
    apples_agent-0_min: 1
    apples_agent-1_max: 27
    apples_agent-1_mean: 3.24
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 30.06
    apples_agent-2_min: 17
    apples_agent-3_max: 25
    apples_agent-3_mean: 8.46
    apples_agent-3_min: 3
    apples_agent-4_max: 23
    apples_agent-4_mean: 11.13
    apples_agent-4_min: 2
    apples_agent-5_max: 27
    apples_agent-5_mean: 4.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 2.14
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 248
    cleaning_beam_agent-1_mean: 214.13
    cleaning_beam_agent-1_min: 169
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 5.5
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 8.15
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 6
    cleaning_beam_agent-4_mean: 2.69
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.88
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-15-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 177.0
  episode_reward_mean: 129.99
  episode_reward_min: 65.0
  episodes_this_iter: 96
  episodes_total: 84288
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11855.55
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28516334295272827
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010044844821095467
        model: {}
        policy_loss: -0.0018739625811576843
        total_loss: -0.002261941321194172
        vf_explained_var: 0.004016846418380737
        vf_loss: 1.1390849351882935
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21124932169914246
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012120387982577085
        model: {}
        policy_loss: -0.0015447847545146942
        total_loss: -0.001857406459748745
        vf_explained_var: 0.0700884461402893
        vf_loss: 0.5917328000068665
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40666529536247253
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011333093279972672
        model: {}
        policy_loss: -0.001749066635966301
        total_loss: -0.002103122416883707
        vf_explained_var: 0.014820888638496399
        vf_loss: 3.6167638301849365
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4437153935432434
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013065901584923267
        model: {}
        policy_loss: -0.0016195885837078094
        total_loss: -0.002228870987892151
        vf_explained_var: 0.016329243779182434
        vf_loss: 1.716566562652588
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3795406222343445
        entropy_coeff: 0.0017600000137463212
        kl: 0.000544305716175586
        model: {}
        policy_loss: -0.0012031272053718567
        total_loss: -0.0017581451684236526
        vf_explained_var: 0.01202555000782013
        vf_loss: 1.129692554473877
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5258376598358154
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018456552643328905
        model: {}
        policy_loss: -0.0016309336060658097
        total_loss: -0.0024283849634230137
        vf_explained_var: 0.02440011501312256
        vf_loss: 1.2802276611328125
    load_time_ms: 14191.303
    num_steps_sampled: 84288000
    num_steps_trained: 84288000
    sample_time_ms: 118766.631
    update_time_ms: 16.392
  iterations_since_restore: 228
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.88203883495146
    ram_util_percent: 12.383009708737864
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 64.0
    agent-3: 41.0
    agent-4: 27.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 16.42
    agent-1: 9.15
    agent-2: 45.91
    agent-3: 23.93
    agent-4: 15.76
    agent-5: 18.82
  policy_reward_min:
    agent-0: -29.0
    agent-1: 2.0
    agent-2: 29.0
    agent-3: -19.0
    agent-4: 7.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.1919536660217
    mean_inference_ms: 15.058821348003
    mean_processing_ms: 73.30706185607568
  time_since_restore: 33131.636332035065
  time_this_iter_s: 144.27941393852234
  time_total_s: 122068.31446170807
  timestamp: 1637396147
  timesteps_since_restore: 21888000
  timesteps_this_iter: 96000
  timesteps_total: 84288000
  training_iteration: 878
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    878 |           122068 | 84288000 |   129.99 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 7.62
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 3.32
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 28.89
    apples_agent-2_min: 11
    apples_agent-3_max: 42
    apples_agent-3_mean: 9.37
    apples_agent-3_min: 1
    apples_agent-4_max: 18
    apples_agent-4_mean: 10.58
    apples_agent-4_min: 3
    apples_agent-5_max: 22
    apples_agent-5_mean: 4.63
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 2.59
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 247
    cleaning_beam_agent-1_mean: 208.21
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 5.82
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 8.49
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.39
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-18-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 169.0
  episode_reward_mean: 127.95
  episode_reward_min: 47.0
  episodes_this_iter: 96
  episodes_total: 84384
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11869.98
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2895013689994812
        entropy_coeff: 0.0017600000137463212
        kl: 0.001190523966215551
        model: {}
        policy_loss: -0.00182866002433002
        total_loss: -0.002219215966761112
        vf_explained_var: 0.004099562764167786
        vf_loss: 1.1896828413009644
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20835529267787933
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009669906576164067
        model: {}
        policy_loss: -0.0012760700192302465
        total_loss: -0.0015879743732511997
        vf_explained_var: 0.09103341400623322
        vf_loss: 0.5480085611343384
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41455185413360596
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011021525133401155
        model: {}
        policy_loss: -0.001893199747428298
        total_loss: -0.002257982036098838
        vf_explained_var: 0.030514642596244812
        vf_loss: 3.6482901573181152
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4437105059623718
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017884832341223955
        model: {}
        policy_loss: -0.0015027644112706184
        total_loss: -0.0020831720903515816
        vf_explained_var: 0.0021388083696365356
        vf_loss: 2.005220651626587
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38101401925086975
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014427498681470752
        model: {}
        policy_loss: -0.0015623877989128232
        total_loss: -0.00213293032720685
        vf_explained_var: 0.013380542397499084
        vf_loss: 1.0003957748413086
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5255544781684875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017583875451236963
        model: {}
        policy_loss: -0.001587121281772852
        total_loss: -0.002382910344749689
        vf_explained_var: -0.00040040910243988037
        vf_loss: 1.291870355606079
    load_time_ms: 14227.352
    num_steps_sampled: 84384000
    num_steps_trained: 84384000
    sample_time_ms: 118805.27
    update_time_ms: 16.021
  iterations_since_restore: 229
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.946411483253584
    ram_util_percent: 12.450239234449759
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 17.0
    agent-2: 79.0
    agent-3: 42.0
    agent-4: 24.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 16.75
    agent-1: 8.97
    agent-2: 44.8
    agent-3: 24.81
    agent-4: 15.37
    agent-5: 17.25
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 15.0
    agent-3: 2.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.19261257794372
    mean_inference_ms: 15.059365153945823
    mean_processing_ms: 73.30972177566359
  time_since_restore: 33278.52204442024
  time_this_iter_s: 146.8857123851776
  time_total_s: 122215.20017409325
  timestamp: 1637396294
  timesteps_since_restore: 21984000
  timesteps_this_iter: 96000
  timesteps_total: 84384000
  training_iteration: 879
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    879 |           122215 | 84384000 |   127.95 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 7.48
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.4
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 28.48
    apples_agent-2_min: 18
    apples_agent-3_max: 30
    apples_agent-3_mean: 9.11
    apples_agent-3_min: 3
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.1
    apples_agent-4_min: 2
    apples_agent-5_max: 16
    apples_agent-5_mean: 4.39
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.8
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 257
    cleaning_beam_agent-1_mean: 207.48
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 5.84
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 25
    cleaning_beam_agent-3_mean: 6.86
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 6
    cleaning_beam_agent-4_mean: 2.61
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.46
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-20-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 174.0
  episode_reward_mean: 126.36
  episode_reward_min: 86.0
  episodes_this_iter: 96
  episodes_total: 84480
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11869.226
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28406602144241333
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007319075521081686
        model: {}
        policy_loss: -0.0015695649199187756
        total_loss: -0.0019578200299292803
        vf_explained_var: 0.006659090518951416
        vf_loss: 1.117032527923584
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20919643342494965
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007871133857406676
        model: {}
        policy_loss: -0.0013609218876808882
        total_loss: -0.001673164195381105
        vf_explained_var: 0.08619251847267151
        vf_loss: 0.5594169497489929
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41803687810897827
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013316809199750423
        model: {}
        policy_loss: -0.001782035455107689
        total_loss: -0.0021539030130952597
        vf_explained_var: 0.004605308175086975
        vf_loss: 3.6387860774993896
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4421136975288391
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010904620867222548
        model: {}
        policy_loss: -0.0013699163682758808
        total_loss: -0.001978273270651698
        vf_explained_var: 0.005393609404563904
        vf_loss: 1.6976484060287476
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38076186180114746
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008267748635262251
        model: {}
        policy_loss: -0.0013524927198886871
        total_loss: -0.0019253527279943228
        vf_explained_var: 0.007937222719192505
        vf_loss: 0.972768247127533
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5340962409973145
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018989414675161242
        model: {}
        policy_loss: -0.0015682014636695385
        total_loss: -0.0023848870769143105
        vf_explained_var: 0.0012546926736831665
        vf_loss: 1.2332205772399902
    load_time_ms: 14213.105
    num_steps_sampled: 84480000
    num_steps_trained: 84480000
    sample_time_ms: 118850.179
    update_time_ms: 16.428
  iterations_since_restore: 230
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.89275362318841
    ram_util_percent: 12.454589371980674
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 18.0
    agent-2: 62.0
    agent-3: 38.0
    agent-4: 25.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 16.39
    agent-1: 9.13
    agent-2: 43.81
    agent-3: 24.7
    agent-4: 15.05
    agent-5: 17.28
  policy_reward_min:
    agent-0: 8.0
    agent-1: 3.0
    agent-2: 31.0
    agent-3: 11.0
    agent-4: 2.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.192693016950397
    mean_inference_ms: 15.05915506087889
    mean_processing_ms: 73.30852587968951
  time_since_restore: 33423.57379984856
  time_this_iter_s: 145.0517554283142
  time_total_s: 122360.25192952156
  timestamp: 1637396440
  timesteps_since_restore: 22080000
  timesteps_this_iter: 96000
  timesteps_total: 84480000
  training_iteration: 880
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    880 |           122360 | 84480000 |   126.36 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 7.34
    apples_agent-0_min: 1
    apples_agent-1_max: 65
    apples_agent-1_mean: 4.14
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 28.74
    apples_agent-2_min: 17
    apples_agent-3_max: 29
    apples_agent-3_mean: 8.72
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.45
    apples_agent-4_min: 2
    apples_agent-5_max: 16
    apples_agent-5_mean: 4.44
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 1.8
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 251
    cleaning_beam_agent-1_mean: 203.63
    cleaning_beam_agent-1_min: 147
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 5.6
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 7.89
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 7
    cleaning_beam_agent-4_mean: 2.49
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.69
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-23-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 172.0
  episode_reward_mean: 125.98
  episode_reward_min: 72.0
  episodes_this_iter: 96
  episodes_total: 84576
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11857.564
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2871096730232239
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008928999304771423
        model: {}
        policy_loss: -0.0018505698535591364
        total_loss: -0.0022443407215178013
        vf_explained_var: 0.001362815499305725
        vf_loss: 1.115394115447998
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2089119851589203
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009773181518539786
        model: {}
        policy_loss: -0.0014912420883774757
        total_loss: -0.0017997701652348042
        vf_explained_var: 0.09703800082206726
        vf_loss: 0.5915642380714417
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4182160794734955
        entropy_coeff: 0.0017600000137463212
        kl: 0.001138004008680582
        model: {}
        policy_loss: -0.002008099341765046
        total_loss: -0.0023875569459050894
        vf_explained_var: 0.004552438855171204
        vf_loss: 3.5660271644592285
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4406539797782898
        entropy_coeff: 0.0017600000137463212
        kl: 0.001036688219755888
        model: {}
        policy_loss: -0.0015230876160785556
        total_loss: -0.002102859551087022
        vf_explained_var: 0.004518687725067139
        vf_loss: 1.957808017730713
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3879753351211548
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006321606924757361
        model: {}
        policy_loss: -0.0012451261281967163
        total_loss: -0.0018288390710949898
        vf_explained_var: 0.013240635395050049
        vf_loss: 0.9912295937538147
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5233972072601318
        entropy_coeff: 0.0017600000137463212
        kl: 0.001231142901815474
        model: {}
        policy_loss: -0.0016301886644214392
        total_loss: -0.002438121009618044
        vf_explained_var: 0.005010038614273071
        vf_loss: 1.1325056552886963
    load_time_ms: 14221.021
    num_steps_sampled: 84576000
    num_steps_trained: 84576000
    sample_time_ms: 118948.896
    update_time_ms: 16.473
  iterations_since_restore: 231
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.71674641148325
    ram_util_percent: 12.440669856459328
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 18.0
    agent-2: 64.0
    agent-3: 43.0
    agent-4: 26.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 15.55
    agent-1: 9.11
    agent-2: 44.07
    agent-3: 25.66
    agent-4: 14.24
    agent-5: 17.35
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 27.0
    agent-3: 8.0
    agent-4: 3.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.1926351935804
    mean_inference_ms: 15.059104638613487
    mean_processing_ms: 73.30762682772102
  time_since_restore: 33568.33141374588
  time_this_iter_s: 144.7576138973236
  time_total_s: 122505.00954341888
  timestamp: 1637396587
  timesteps_since_restore: 22176000
  timesteps_this_iter: 96000
  timesteps_total: 84576000
  training_iteration: 881
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    881 |           122505 | 84576000 |   125.98 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 7.0
    apples_agent-0_min: 1
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.23
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 29.38
    apples_agent-2_min: 15
    apples_agent-3_max: 24
    apples_agent-3_mean: 8.68
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.45
    apples_agent-4_min: 2
    apples_agent-5_max: 14
    apples_agent-5_mean: 3.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 34
    cleaning_beam_agent-0_mean: 2.54
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 233
    cleaning_beam_agent-1_mean: 203.17
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 5.38
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 8.47
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 2.55
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.64
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-25-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 164.0
  episode_reward_mean: 125.66
  episode_reward_min: 53.0
  episodes_this_iter: 96
  episodes_total: 84672
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11835.617
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2835789918899536
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008709100657142699
        model: {}
        policy_loss: -0.0017122541321441531
        total_loss: -0.002115099923685193
        vf_explained_var: 0.01014779508113861
        vf_loss: 0.9625244140625
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20687705278396606
        entropy_coeff: 0.0017600000137463212
        kl: 0.001074998639523983
        model: {}
        policy_loss: -0.0015055310213938355
        total_loss: -0.0018123371992260218
        vf_explained_var: 0.08187475800514221
        vf_loss: 0.5729501843452454
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4286617636680603
        entropy_coeff: 0.0017600000137463212
        kl: 0.001299594179727137
        model: {}
        policy_loss: -0.0018949629738926888
        total_loss: -0.002273573074489832
        vf_explained_var: 0.03226324915885925
        vf_loss: 3.758324146270752
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4405602216720581
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015632827999070287
        model: {}
        policy_loss: -0.0016422001644968987
        total_loss: -0.0022439472377300262
        vf_explained_var: -0.002847433090209961
        vf_loss: 1.736388921737671
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3817949593067169
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007992716273292899
        model: {}
        policy_loss: -0.0013514200691133738
        total_loss: -0.0019133188761770725
        vf_explained_var: 0.010563313961029053
        vf_loss: 1.1006290912628174
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5355590581893921
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019869059324264526
        model: {}
        policy_loss: -0.0017030630260705948
        total_loss: -0.0025290902704000473
        vf_explained_var: 0.011376872658729553
        vf_loss: 1.165583848953247
    load_time_ms: 14172.405
    num_steps_sampled: 84672000
    num_steps_trained: 84672000
    sample_time_ms: 118851.012
    update_time_ms: 16.59
  iterations_since_restore: 232
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.913725490196075
    ram_util_percent: 12.450980392156861
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 62.0
    agent-3: 43.0
    agent-4: 32.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.66
    agent-1: 9.21
    agent-2: 44.49
    agent-3: 23.93
    agent-4: 15.13
    agent-5: 17.24
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 19.0
    agent-3: 8.0
    agent-4: 4.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.19089609138413
    mean_inference_ms: 15.058510119261161
    mean_processing_ms: 73.30257369976329
  time_since_restore: 33711.40161204338
  time_this_iter_s: 143.0701982975006
  time_total_s: 122648.07974171638
  timestamp: 1637396730
  timesteps_since_restore: 22272000
  timesteps_this_iter: 96000
  timesteps_total: 84672000
  training_iteration: 882
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    882 |           122648 | 84672000 |   125.66 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 7.26
    apples_agent-0_min: 1
    apples_agent-1_max: 12
    apples_agent-1_mean: 3.05
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 29.34
    apples_agent-2_min: 17
    apples_agent-3_max: 25
    apples_agent-3_mean: 8.48
    apples_agent-3_min: 2
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.28
    apples_agent-4_min: 2
    apples_agent-5_max: 18
    apples_agent-5_mean: 4.12
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 2.98
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 235
    cleaning_beam_agent-1_mean: 201.47
    cleaning_beam_agent-1_min: 158
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 5.27
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 7.96
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 7
    cleaning_beam_agent-4_mean: 2.77
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.68
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-27-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 181.0
  episode_reward_mean: 123.31
  episode_reward_min: 88.0
  episodes_this_iter: 96
  episodes_total: 84768
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11837.635
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28466784954071045
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008861942915245891
        model: {}
        policy_loss: -0.0017577710095793009
        total_loss: -0.002157560782507062
        vf_explained_var: 0.005924507975578308
        vf_loss: 1.0122795104980469
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20885688066482544
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013820263557136059
        model: {}
        policy_loss: -0.0016489597037434578
        total_loss: -0.001966293668374419
        vf_explained_var: 0.09185299277305603
        vf_loss: 0.502494215965271
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4360528588294983
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017294808058068156
        model: {}
        policy_loss: -0.0020629693754017353
        total_loss: -0.0024604571517556906
        vf_explained_var: 0.023782387375831604
        vf_loss: 3.699641227722168
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43880319595336914
        entropy_coeff: 0.0017600000137463212
        kl: 0.00147745362482965
        model: {}
        policy_loss: -0.0017617830308154225
        total_loss: -0.002354569733142853
        vf_explained_var: 0.0014141947031021118
        vf_loss: 1.7950676679611206
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37925511598587036
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013639659155160189
        model: {}
        policy_loss: -0.0014674731064587831
        total_loss: -0.002037815749645233
        vf_explained_var: 0.01158037781715393
        vf_loss: 0.971466064453125
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5326495170593262
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012333798222243786
        model: {}
        policy_loss: -0.0015915753319859505
        total_loss: -0.0024082844611257315
        vf_explained_var: 0.020132198929786682
        vf_loss: 1.2075259685516357
    load_time_ms: 14191.91
    num_steps_sampled: 84768000
    num_steps_trained: 84768000
    sample_time_ms: 118895.927
    update_time_ms: 16.223
  iterations_since_restore: 233
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.82125603864734
    ram_util_percent: 12.3743961352657
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 16.0
    agent-2: 62.0
    agent-3: 40.0
    agent-4: 25.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 15.22
    agent-1: 8.78
    agent-2: 43.92
    agent-3: 23.65
    agent-4: 14.45
    agent-5: 17.29
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 28.0
    agent-3: 13.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.19029594969719
    mean_inference_ms: 15.057993835080131
    mean_processing_ms: 73.30099676949195
  time_since_restore: 33856.66503787041
  time_this_iter_s: 145.26342582702637
  time_total_s: 122793.34316754341
  timestamp: 1637396875
  timesteps_since_restore: 22368000
  timesteps_this_iter: 96000
  timesteps_total: 84768000
  training_iteration: 883
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    883 |           122793 | 84768000 |   123.31 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 6.72
    apples_agent-0_min: 1
    apples_agent-1_max: 15
    apples_agent-1_mean: 3.39
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 30.06
    apples_agent-2_min: 16
    apples_agent-3_max: 34
    apples_agent-3_mean: 8.89
    apples_agent-3_min: 2
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.32
    apples_agent-4_min: 3
    apples_agent-5_max: 35
    apples_agent-5_mean: 4.55
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 15
    cleaning_beam_agent-0_mean: 2.4
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 241
    cleaning_beam_agent-1_mean: 207.79
    cleaning_beam_agent-1_min: 167
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 5.13
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 26
    cleaning_beam_agent-3_mean: 8.72
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.71
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 4.26
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-30-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 192.0
  episode_reward_mean: 128.03
  episode_reward_min: 88.0
  episodes_this_iter: 96
  episodes_total: 84864
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11838.793
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2869969308376312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009060502052307129
        model: {}
        policy_loss: -0.0018167784437537193
        total_loss: -0.002220885129645467
        vf_explained_var: -0.0031462013721466064
        vf_loss: 1.0100849866867065
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2098565399646759
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011922450503334403
        model: {}
        policy_loss: -0.0014614881947636604
        total_loss: -0.0017810435965657234
        vf_explained_var: 0.07424335181713104
        vf_loss: 0.4979366958141327
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.429210364818573
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018808196764439344
        model: {}
        policy_loss: -0.0019305283203721046
        total_loss: -0.002290912438184023
        vf_explained_var: 0.01214897632598877
        vf_loss: 3.9502758979797363
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44102609157562256
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010609445162117481
        model: {}
        policy_loss: -0.001268269494175911
        total_loss: -0.0018617702880874276
        vf_explained_var: 0.00366269052028656
        vf_loss: 1.827049732208252
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3901805281639099
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009221778600476682
        model: {}
        policy_loss: -0.0013161820825189352
        total_loss: -0.0019020834006369114
        vf_explained_var: 0.011568307876586914
        vf_loss: 1.0081771612167358
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5284414887428284
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007518832571804523
        model: {}
        policy_loss: -0.001618226058781147
        total_loss: -0.002432365668937564
        vf_explained_var: 0.009000301361083984
        vf_loss: 1.1591641902923584
    load_time_ms: 14209.065
    num_steps_sampled: 84864000
    num_steps_trained: 84864000
    sample_time_ms: 118964.796
    update_time_ms: 16.281
  iterations_since_restore: 234
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.157692307692308
    ram_util_percent: 12.46682692307692
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 67.0
    agent-3: 42.0
    agent-4: 29.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 15.59
    agent-1: 8.96
    agent-2: 45.66
    agent-3: 24.89
    agent-4: 14.97
    agent-5: 17.96
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 27.0
    agent-3: 13.0
    agent-4: 4.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.19069126247547
    mean_inference_ms: 15.05820132024574
    mean_processing_ms: 73.30316588257884
  time_since_restore: 34002.454228401184
  time_this_iter_s: 145.78919053077698
  time_total_s: 122939.13235807419
  timestamp: 1637397021
  timesteps_since_restore: 22464000
  timesteps_this_iter: 96000
  timesteps_total: 84864000
  training_iteration: 884
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    884 |           122939 | 84864000 |   128.03 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 7.08
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 2.83
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 27.39
    apples_agent-2_min: 9
    apples_agent-3_max: 36
    apples_agent-3_mean: 8.36
    apples_agent-3_min: 2
    apples_agent-4_max: 22
    apples_agent-4_mean: 9.75
    apples_agent-4_min: 2
    apples_agent-5_max: 22
    apples_agent-5_mean: 4.28
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 24
    cleaning_beam_agent-0_mean: 2.59
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 233
    cleaning_beam_agent-1_mean: 198.42
    cleaning_beam_agent-1_min: 70
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 5.83
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 38
    cleaning_beam_agent-3_mean: 8.55
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.65
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.21
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-32-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 156.0
  episode_reward_mean: 118.17
  episode_reward_min: 36.0
  episodes_this_iter: 96
  episodes_total: 84960
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11830.147
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28186526894569397
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008189958753064275
        model: {}
        policy_loss: -0.001733726472593844
        total_loss: -0.002126362407580018
        vf_explained_var: 0.006960153579711914
        vf_loss: 1.0344665050506592
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20719365775585175
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011314837029203773
        model: {}
        policy_loss: -0.0014680975582450628
        total_loss: -0.0017825858667492867
        vf_explained_var: 0.09511548280715942
        vf_loss: 0.5016815066337585
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43193045258522034
        entropy_coeff: 0.0017600000137463212
        kl: 0.001596160582266748
        model: {}
        policy_loss: -0.001920066075399518
        total_loss: -0.002294700127094984
        vf_explained_var: 0.033505409955978394
        vf_loss: 3.8555991649627686
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4400240480899811
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011762272333726287
        model: {}
        policy_loss: -0.0013177217915654182
        total_loss: -0.001919051632285118
        vf_explained_var: -0.004218041896820068
        vf_loss: 1.7311291694641113
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3977211117744446
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013526586117222905
        model: {}
        policy_loss: -0.0014887363649904728
        total_loss: -0.0020938916131854057
        vf_explained_var: 0.028328463435173035
        vf_loss: 0.9483379125595093
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5279486775398254
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015661293873563409
        model: {}
        policy_loss: -0.0017530135810375214
        total_loss: -0.002571617253124714
        vf_explained_var: 0.00853206217288971
        vf_loss: 1.1058746576309204
    load_time_ms: 14174.888
    num_steps_sampled: 84960000
    num_steps_trained: 84960000
    sample_time_ms: 119058.734
    update_time_ms: 16.251
  iterations_since_restore: 235
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.86730769230769
    ram_util_percent: 12.446634615384612
  pid: 27065
  policy_reward_max:
    agent-0: 24.0
    agent-1: 18.0
    agent-2: 60.0
    agent-3: 38.0
    agent-4: 26.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.48
    agent-1: 8.11
    agent-2: 41.68
    agent-3: 22.84
    agent-4: 14.06
    agent-5: 16.0
  policy_reward_min:
    agent-0: 4.0
    agent-1: 2.0
    agent-2: 17.0
    agent-3: 5.0
    agent-4: 2.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.19092524812659
    mean_inference_ms: 15.058178551118583
    mean_processing_ms: 73.30419827492278
  time_since_restore: 34148.57834148407
  time_this_iter_s: 146.12411308288574
  time_total_s: 123085.25647115707
  timestamp: 1637397167
  timesteps_since_restore: 22560000
  timesteps_this_iter: 96000
  timesteps_total: 84960000
  training_iteration: 885
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    885 |           123085 | 84960000 |   118.17 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 7.44
    apples_agent-0_min: 1
    apples_agent-1_max: 12
    apples_agent-1_mean: 3.27
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 28.57
    apples_agent-2_min: 17
    apples_agent-3_max: 26
    apples_agent-3_mean: 8.56
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.54
    apples_agent-4_min: 3
    apples_agent-5_max: 27
    apples_agent-5_mean: 4.38
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 17
    cleaning_beam_agent-0_mean: 2.16
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 246
    cleaning_beam_agent-1_mean: 205.21
    cleaning_beam_agent-1_min: 148
    cleaning_beam_agent-2_max: 13
    cleaning_beam_agent-2_mean: 5.27
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 9.11
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 7
    cleaning_beam_agent-4_mean: 2.15
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.93
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-35-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 172.0
  episode_reward_mean: 127.35
  episode_reward_min: 76.0
  episodes_this_iter: 96
  episodes_total: 85056
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11837.77
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2860156297683716
        entropy_coeff: 0.0017600000137463212
        kl: 0.001239617122337222
        model: {}
        policy_loss: -0.0017459392547607422
        total_loss: -0.0021447231993079185
        vf_explained_var: 0.012097641825675964
        vf_loss: 1.0460588932037354
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20808632671833038
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012597581371665
        model: {}
        policy_loss: -0.0013888574903830886
        total_loss: -0.0017033407930284739
        vf_explained_var: 0.09229984879493713
        vf_loss: 0.5175012946128845
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4234200716018677
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010702427243813872
        model: {}
        policy_loss: -0.001724468544125557
        total_loss: -0.0020940909162163734
        vf_explained_var: 0.013066843152046204
        vf_loss: 3.7559895515441895
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43962737917900085
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018417335813865066
        model: {}
        policy_loss: -0.0013953641755506396
        total_loss: -0.0019637420773506165
        vf_explained_var: 0.013082951307296753
        vf_loss: 2.0536675453186035
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40012696385383606
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005513454671017826
        model: {}
        policy_loss: -0.0013316287659108639
        total_loss: -0.0019208462908864021
        vf_explained_var: 0.012717127799987793
        vf_loss: 1.150045394897461
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5338159799575806
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014321571215987206
        model: {}
        policy_loss: -0.0016853949055075645
        total_loss: -0.0024861088022589684
        vf_explained_var: 0.022496595978736877
        vf_loss: 1.3880252838134766
    load_time_ms: 14174.902
    num_steps_sampled: 85056000
    num_steps_trained: 85056000
    sample_time_ms: 119074.946
    update_time_ms: 16.231
  iterations_since_restore: 236
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.8975845410628
    ram_util_percent: 12.456038647342993
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 17.0
    agent-2: 64.0
    agent-3: 43.0
    agent-4: 32.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 15.66
    agent-1: 9.01
    agent-2: 44.38
    agent-3: 24.73
    agent-4: 15.51
    agent-5: 18.06
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 24.0
    agent-3: 10.0
    agent-4: 7.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.19058517955007
    mean_inference_ms: 15.057953266880213
    mean_processing_ms: 73.30371877129893
  time_since_restore: 34293.45727992058
  time_this_iter_s: 144.87893843650818
  time_total_s: 123230.13540959358
  timestamp: 1637397312
  timesteps_since_restore: 22656000
  timesteps_this_iter: 96000
  timesteps_total: 85056000
  training_iteration: 886
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    886 |           123230 | 85056000 |   127.35 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 7.45
    apples_agent-0_min: 1
    apples_agent-1_max: 26
    apples_agent-1_mean: 3.48
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 29.09
    apples_agent-2_min: 18
    apples_agent-3_max: 23
    apples_agent-3_mean: 8.01
    apples_agent-3_min: 2
    apples_agent-4_max: 22
    apples_agent-4_mean: 11.08
    apples_agent-4_min: 2
    apples_agent-5_max: 27
    apples_agent-5_mean: 4.2
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 1.74
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 248
    cleaning_beam_agent-1_mean: 206.98
    cleaning_beam_agent-1_min: 175
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 5.5
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 8.01
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 2.48
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.28
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-37-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 172.0
  episode_reward_mean: 125.52
  episode_reward_min: 89.0
  episodes_this_iter: 96
  episodes_total: 85152
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11842.786
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28042373061180115
        entropy_coeff: 0.0017600000137463212
        kl: 0.001387304626405239
        model: {}
        policy_loss: -0.0018118228763341904
        total_loss: -0.00220358744263649
        vf_explained_var: 0.007641986012458801
        vf_loss: 1.0177932977676392
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21068289875984192
        entropy_coeff: 0.0017600000137463212
        kl: 0.001389145851135254
        model: {}
        policy_loss: -0.0015031673246994615
        total_loss: -0.001823911676183343
        vf_explained_var: 0.09724518656730652
        vf_loss: 0.5006059408187866
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4326137602329254
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018531490350142121
        model: {}
        policy_loss: -0.0019983304664492607
        total_loss: -0.0023982306011021137
        vf_explained_var: 0.03719855844974518
        vf_loss: 3.614987373352051
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43807461857795715
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010683764703571796
        model: {}
        policy_loss: -0.0015915422700345516
        total_loss: -0.002181427087634802
        vf_explained_var: 0.004722490906715393
        vf_loss: 1.8112871646881104
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40709003806114197
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008496194495819509
        model: {}
        policy_loss: -0.0012342960108071566
        total_loss: -0.0018534264527261257
        vf_explained_var: 0.01287965476512909
        vf_loss: 0.9734911918640137
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5346964001655579
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014135540695860982
        model: {}
        policy_loss: -0.0016912533901631832
        total_loss: -0.002517073880881071
        vf_explained_var: 0.021237418055534363
        vf_loss: 1.1524345874786377
    load_time_ms: 14146.711
    num_steps_sampled: 85152000
    num_steps_trained: 85152000
    sample_time_ms: 119045.213
    update_time_ms: 16.424
  iterations_since_restore: 237
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.04202898550724
    ram_util_percent: 12.453140096618355
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 17.0
    agent-2: 65.0
    agent-3: 41.0
    agent-4: 28.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 16.89
    agent-1: 8.5
    agent-2: 43.64
    agent-3: 23.77
    agent-4: 15.4
    agent-5: 17.32
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 25.0
    agent-3: 13.0
    agent-4: 7.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.19101588984498
    mean_inference_ms: 15.05829435126229
    mean_processing_ms: 73.30575048240007
  time_since_restore: 34438.76490139961
  time_this_iter_s: 145.30762147903442
  time_total_s: 123375.44303107262
  timestamp: 1637397458
  timesteps_since_restore: 22752000
  timesteps_this_iter: 96000
  timesteps_total: 85152000
  training_iteration: 887
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    887 |           123375 | 85152000 |   125.52 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 7.07
    apples_agent-0_min: 1
    apples_agent-1_max: 11
    apples_agent-1_mean: 3.31
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 29.63
    apples_agent-2_min: 16
    apples_agent-3_max: 19
    apples_agent-3_mean: 8.36
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.94
    apples_agent-4_min: 4
    apples_agent-5_max: 15
    apples_agent-5_mean: 4.34
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 1.96
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 248
    cleaning_beam_agent-1_mean: 207.44
    cleaning_beam_agent-1_min: 166
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 5.88
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 44
    cleaning_beam_agent-3_mean: 8.73
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.96
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.23
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-40-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 128.73
  episode_reward_min: 92.0
  episodes_this_iter: 96
  episodes_total: 85248
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11838.759
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2783135175704956
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008886828436516225
        model: {}
        policy_loss: -0.0016457319725304842
        total_loss: -0.0020277337171137333
        vf_explained_var: 0.014021918177604675
        vf_loss: 1.0783264636993408
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20837387442588806
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007615307113155723
        model: {}
        policy_loss: -0.0013444505166262388
        total_loss: -0.0016617474611848593
        vf_explained_var: 0.09334772825241089
        vf_loss: 0.49442681670188904
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42639005184173584
        entropy_coeff: 0.0017600000137463212
        kl: 0.001304629142396152
        model: {}
        policy_loss: -0.001747960690408945
        total_loss: -0.002118995413184166
        vf_explained_var: 0.02106638252735138
        vf_loss: 3.794140577316284
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43830811977386475
        entropy_coeff: 0.0017600000137463212
        kl: 0.001603575306944549
        model: {}
        policy_loss: -0.0013980125077068806
        total_loss: -0.0019936240278184414
        vf_explained_var: 0.0017306208610534668
        vf_loss: 1.7580686807632446
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41280999779701233
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005438872030936182
        model: {}
        policy_loss: -0.0011416380293667316
        total_loss: -0.001777167897671461
        vf_explained_var: 0.020861178636550903
        vf_loss: 0.9101337194442749
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5235590934753418
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013241285923868418
        model: {}
        policy_loss: -0.001690036617219448
        total_loss: -0.002479841932654381
        vf_explained_var: 0.008271053433418274
        vf_loss: 1.3165807723999023
    load_time_ms: 14149.259
    num_steps_sampled: 85248000
    num_steps_trained: 85248000
    sample_time_ms: 119214.143
    update_time_ms: 16.04
  iterations_since_restore: 238
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.98076923076923
    ram_util_percent: 12.456730769230765
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 18.0
    agent-2: 67.0
    agent-3: 38.0
    agent-4: 24.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 16.21
    agent-1: 8.82
    agent-2: 45.85
    agent-3: 25.11
    agent-4: 14.35
    agent-5: 18.39
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 28.0
    agent-3: 13.0
    agent-4: 5.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.191790545274692
    mean_inference_ms: 15.058547340412993
    mean_processing_ms: 73.3073654639766
  time_since_restore: 34584.75569367409
  time_this_iter_s: 145.9907922744751
  time_total_s: 123521.43382334709
  timestamp: 1637397604
  timesteps_since_restore: 22848000
  timesteps_this_iter: 96000
  timesteps_total: 85248000
  training_iteration: 888
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    888 |           123521 | 85248000 |   128.73 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 6.77
    apples_agent-0_min: 0
    apples_agent-1_max: 23
    apples_agent-1_mean: 4.06
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 29.75
    apples_agent-2_min: 18
    apples_agent-3_max: 22
    apples_agent-3_mean: 8.66
    apples_agent-3_min: 1
    apples_agent-4_max: 27
    apples_agent-4_mean: 10.8
    apples_agent-4_min: 3
    apples_agent-5_max: 21
    apples_agent-5_mean: 4.45
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 2.31
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 262
    cleaning_beam_agent-1_mean: 208.28
    cleaning_beam_agent-1_min: 159
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 6.26
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 8.61
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 7
    cleaning_beam_agent-4_mean: 2.82
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 3.68
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-42-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 172.0
  episode_reward_mean: 128.23
  episode_reward_min: 79.0
  episodes_this_iter: 96
  episodes_total: 85344
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11823.559
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2791080176830292
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008867664146237075
        model: {}
        policy_loss: -0.001773950643837452
        total_loss: -0.0021615028381347656
        vf_explained_var: 0.014134854078292847
        vf_loss: 1.0368013381958008
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21086926758289337
        entropy_coeff: 0.0017600000137463212
        kl: 0.000968863838352263
        model: {}
        policy_loss: -0.0015375977382063866
        total_loss: -0.0018498441204428673
        vf_explained_var: 0.09600509703159332
        vf_loss: 0.5887951254844666
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.420424222946167
        entropy_coeff: 0.0017600000137463212
        kl: 0.001576714450493455
        model: {}
        policy_loss: -0.0018443510634824634
        total_loss: -0.002224394353106618
        vf_explained_var: 0.01312856376171112
        vf_loss: 3.5990195274353027
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4339720606803894
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017106093000620604
        model: {}
        policy_loss: -0.0015969621017575264
        total_loss: -0.002179602859541774
        vf_explained_var: 0.011157050728797913
        vf_loss: 1.8115124702453613
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4169500470161438
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008966652676463127
        model: {}
        policy_loss: -0.0013931621797382832
        total_loss: -0.0020223362371325493
        vf_explained_var: 0.020879849791526794
        vf_loss: 1.0465720891952515
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5306363105773926
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014912469778209925
        model: {}
        policy_loss: -0.0017177043482661247
        total_loss: -0.0025223817210644484
        vf_explained_var: 0.015309110283851624
        vf_loss: 1.2923932075500488
    load_time_ms: 14122.301
    num_steps_sampled: 85344000
    num_steps_trained: 85344000
    sample_time_ms: 119106.658
    update_time_ms: 16.586
  iterations_since_restore: 239
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.999033816425126
    ram_util_percent: 12.450724637681159
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 17.0
    agent-2: 61.0
    agent-3: 41.0
    agent-4: 27.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.85
    agent-1: 9.5
    agent-2: 45.13
    agent-3: 24.86
    agent-4: 14.71
    agent-5: 18.18
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 32.0
    agent-3: 11.0
    agent-4: 3.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.192363386623654
    mean_inference_ms: 15.058840388840599
    mean_processing_ms: 73.30875680410726
  time_since_restore: 34730.06674575806
  time_this_iter_s: 145.31105208396912
  time_total_s: 123666.74487543106
  timestamp: 1637397749
  timesteps_since_restore: 22944000
  timesteps_this_iter: 96000
  timesteps_total: 85344000
  training_iteration: 889
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    889 |           123667 | 85344000 |   128.23 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 6.66
    apples_agent-0_min: 2
    apples_agent-1_max: 17
    apples_agent-1_mean: 3.61
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 28.56
    apples_agent-2_min: 12
    apples_agent-3_max: 37
    apples_agent-3_mean: 9.06
    apples_agent-3_min: 2
    apples_agent-4_max: 31
    apples_agent-4_mean: 10.44
    apples_agent-4_min: 2
    apples_agent-5_max: 40
    apples_agent-5_mean: 4.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 21
    cleaning_beam_agent-0_mean: 1.85
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 262
    cleaning_beam_agent-1_mean: 210.35
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 6.67
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 67
    cleaning_beam_agent-3_mean: 8.17
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 2.91
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-44-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 170.0
  episode_reward_mean: 128.42
  episode_reward_min: 69.0
  episodes_this_iter: 96
  episodes_total: 85440
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11846.689
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27326464653015137
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007759854197502136
        model: {}
        policy_loss: -0.0017176661640405655
        total_loss: -0.0020910045132040977
        vf_explained_var: 0.004235431551933289
        vf_loss: 1.0760583877563477
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21043449640274048
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009430019417777658
        model: {}
        policy_loss: -0.0015302780084311962
        total_loss: -0.0018383751157671213
        vf_explained_var: 0.07790733873844147
        vf_loss: 0.6226891279220581
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4165996313095093
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012997856829315424
        model: {}
        policy_loss: -0.0017930679023265839
        total_loss: -0.002139180898666382
        vf_explained_var: 0.026386186480522156
        vf_loss: 3.871030569076538
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4278830289840698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014907736331224442
        model: {}
        policy_loss: -0.001504457090049982
        total_loss: -0.0020777154713869095
        vf_explained_var: 0.008983686566352844
        vf_loss: 1.7981739044189453
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40789803862571716
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009489423828199506
        model: {}
        policy_loss: -0.0014678789302706718
        total_loss: -0.0020776549354195595
        vf_explained_var: 0.016223087906837463
        vf_loss: 1.0812795162200928
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5297209620475769
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019795093685388565
        model: {}
        policy_loss: -0.0014646174386143684
        total_loss: -0.0022563976235687733
        vf_explained_var: 0.019054323434829712
        vf_loss: 1.4053056240081787
    load_time_ms: 14147.12
    num_steps_sampled: 85440000
    num_steps_trained: 85440000
    sample_time_ms: 119071.026
    update_time_ms: 16.216
  iterations_since_restore: 240
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.861352657004826
    ram_util_percent: 12.389371980676328
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 23.0
    agent-2: 58.0
    agent-3: 41.0
    agent-4: 27.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 16.03
    agent-1: 9.46
    agent-2: 43.63
    agent-3: 25.05
    agent-4: 15.25
    agent-5: 19.0
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 25.0
    agent-3: 12.0
    agent-4: 5.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.192085645561676
    mean_inference_ms: 15.058763980123931
    mean_processing_ms: 73.30831954173016
  time_since_restore: 34875.36581373215
  time_this_iter_s: 145.29906797409058
  time_total_s: 123812.04394340515
  timestamp: 1637397895
  timesteps_since_restore: 23040000
  timesteps_this_iter: 96000
  timesteps_total: 85440000
  training_iteration: 890
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    890 |           123812 | 85440000 |   128.42 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 7.63
    apples_agent-0_min: 2
    apples_agent-1_max: 19
    apples_agent-1_mean: 3.28
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 29.73
    apples_agent-2_min: 16
    apples_agent-3_max: 34
    apples_agent-3_mean: 9.8
    apples_agent-3_min: 2
    apples_agent-4_max: 31
    apples_agent-4_mean: 11.36
    apples_agent-4_min: 4
    apples_agent-5_max: 16
    apples_agent-5_mean: 4.31
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 1.47
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 253
    cleaning_beam_agent-1_mean: 217.56
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 6.14
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 8.49
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.17
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.17
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-47-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 174.0
  episode_reward_mean: 131.56
  episode_reward_min: 73.0
  episodes_this_iter: 96
  episodes_total: 85536
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11850.564
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.27217724919319153
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011546993628144264
        model: {}
        policy_loss: -0.0016725622117519379
        total_loss: -0.002023762557655573
        vf_explained_var: 0.008425310254096985
        vf_loss: 1.2783139944076538
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21110789477825165
        entropy_coeff: 0.0017600000137463212
        kl: 0.001160659478046
        model: {}
        policy_loss: -0.0013664178550243378
        total_loss: -0.0016817315481603146
        vf_explained_var: 0.09567523002624512
        vf_loss: 0.5623983144760132
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41124987602233887
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013387036742642522
        model: {}
        policy_loss: -0.0019170725718140602
        total_loss: -0.0022285394370555878
        vf_explained_var: 0.024951428174972534
        vf_loss: 4.12333869934082
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.429395854473114
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018197669414803386
        model: {}
        policy_loss: -0.0015733521431684494
        total_loss: -0.0021297764033079147
        vf_explained_var: 0.007108420133590698
        vf_loss: 1.9931423664093018
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4080738127231598
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008361655054613948
        model: {}
        policy_loss: -0.0015190085396170616
        total_loss: -0.002135244198143482
        vf_explained_var: 0.013609454035758972
        vf_loss: 1.019729495048523
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5149661898612976
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009483160683885217
        model: {}
        policy_loss: -0.0013335589319467545
        total_loss: -0.002109150867909193
        vf_explained_var: 0.010576069355010986
        vf_loss: 1.3074724674224854
    load_time_ms: 14153.301
    num_steps_sampled: 85536000
    num_steps_trained: 85536000
    sample_time_ms: 119117.853
    update_time_ms: 16.231
  iterations_since_restore: 241
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.732380952380954
    ram_util_percent: 12.439999999999998
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 16.0
    agent-2: 69.0
    agent-3: 41.0
    agent-4: 27.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 17.05
    agent-1: 9.09
    agent-2: 45.47
    agent-3: 26.39
    agent-4: 15.66
    agent-5: 17.9
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 25.0
    agent-3: 16.0
    agent-4: 8.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.19254971600099
    mean_inference_ms: 15.05888596823462
    mean_processing_ms: 73.30865399021282
  time_since_restore: 35020.68915891647
  time_this_iter_s: 145.32334518432617
  time_total_s: 123957.36728858948
  timestamp: 1637398042
  timesteps_since_restore: 23136000
  timesteps_this_iter: 96000
  timesteps_total: 85536000
  training_iteration: 891
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    891 |           123957 | 85536000 |   131.56 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 6.67
    apples_agent-0_min: 1
    apples_agent-1_max: 20
    apples_agent-1_mean: 3.66
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 29.9
    apples_agent-2_min: 13
    apples_agent-3_max: 34
    apples_agent-3_mean: 8.47
    apples_agent-3_min: 1
    apples_agent-4_max: 25
    apples_agent-4_mean: 11.19
    apples_agent-4_min: 2
    apples_agent-5_max: 16
    apples_agent-5_mean: 4.48
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 1.72
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 253
    cleaning_beam_agent-1_mean: 214.29
    cleaning_beam_agent-1_min: 176
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 5.75
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 44
    cleaning_beam_agent-3_mean: 9.04
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.25
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.08
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-49-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 180.0
  episode_reward_mean: 128.25
  episode_reward_min: 89.0
  episodes_this_iter: 96
  episodes_total: 85632
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11861.409
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2862723171710968
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017157417023554444
        model: {}
        policy_loss: -0.001912793144583702
        total_loss: -0.002317998558282852
        vf_explained_var: 0.016408398747444153
        vf_loss: 0.9863576889038086
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.21115665137767792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014567025937139988
        model: {}
        policy_loss: -0.0016657141968607903
        total_loss: -0.00197782926261425
        vf_explained_var: 0.07446913421154022
        vf_loss: 0.5952063202857971
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42099764943122864
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012832353822886944
        model: {}
        policy_loss: -0.0018024640157818794
        total_loss: -0.0021862895227968693
        vf_explained_var: 0.007355943322181702
        vf_loss: 3.5712766647338867
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43867820501327515
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010256018722429872
        model: {}
        policy_loss: -0.0014822408556938171
        total_loss: -0.0020695223938673735
        vf_explained_var: 0.010205626487731934
        vf_loss: 1.8478941917419434
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4136272668838501
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009724521660245955
        model: {}
        policy_loss: -0.0016198409721255302
        total_loss: -0.002245028503239155
        vf_explained_var: 0.012261554598808289
        vf_loss: 1.0279252529144287
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5064616799354553
        entropy_coeff: 0.0017600000137463212
        kl: 0.001346280099824071
        model: {}
        policy_loss: -0.0016548731364309788
        total_loss: -0.0024298932403326035
        vf_explained_var: 0.01701889932155609
        vf_loss: 1.163529634475708
    load_time_ms: 14150.445
    num_steps_sampled: 85632000
    num_steps_trained: 85632000
    sample_time_ms: 119285.956
    update_time_ms: 16.155
  iterations_since_restore: 242
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.96699029126214
    ram_util_percent: 12.45388349514563
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 22.0
    agent-2: 64.0
    agent-3: 48.0
    agent-4: 28.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 15.72
    agent-1: 9.51
    agent-2: 44.95
    agent-3: 24.72
    agent-4: 15.16
    agent-5: 18.19
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 27.0
    agent-3: 1.0
    agent-4: 6.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.192509100420285
    mean_inference_ms: 15.05868680351156
    mean_processing_ms: 73.30851502307172
  time_since_restore: 35165.47920680046
  time_this_iter_s: 144.79004788398743
  time_total_s: 124102.15733647346
  timestamp: 1637398187
  timesteps_since_restore: 23232000
  timesteps_this_iter: 96000
  timesteps_total: 85632000
  training_iteration: 892
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    892 |           124102 | 85632000 |   128.25 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 6.48
    apples_agent-0_min: 0
    apples_agent-1_max: 21
    apples_agent-1_mean: 3.67
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 29.06
    apples_agent-2_min: 1
    apples_agent-3_max: 22
    apples_agent-3_mean: 8.06
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.01
    apples_agent-4_min: 0
    apples_agent-5_max: 22
    apples_agent-5_mean: 5.36
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.16
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 238
    cleaning_beam_agent-1_mean: 201.88
    cleaning_beam_agent-1_min: 16
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 6.21
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 9.79
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 3.0
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.59
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-52-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 175.0
  episode_reward_mean: 124.36
  episode_reward_min: 14.0
  episodes_this_iter: 96
  episodes_total: 85728
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11866.909
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28672540187835693
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008669572416692972
        model: {}
        policy_loss: -0.001582648605108261
        total_loss: -0.0019852533005177975
        vf_explained_var: 0.006777539849281311
        vf_loss: 1.0202832221984863
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20371827483177185
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010908037656918168
        model: {}
        policy_loss: -0.0013337773270905018
        total_loss: -0.001633406849578023
        vf_explained_var: 0.08537648618221283
        vf_loss: 0.5891327857971191
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42591068148612976
        entropy_coeff: 0.0017600000137463212
        kl: 0.001451008371077478
        model: {}
        policy_loss: -0.0020875874906778336
        total_loss: -0.002465243451297283
        vf_explained_var: 0.033623963594436646
        vf_loss: 3.7194747924804688
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4454076886177063
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010585144627839327
        model: {}
        policy_loss: -0.0015481645241379738
        total_loss: -0.002142108976840973
        vf_explained_var: 0.00606749951839447
        vf_loss: 1.8996977806091309
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41630226373672485
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009138914756476879
        model: {}
        policy_loss: -0.0015011951327323914
        total_loss: -0.00213273661211133
        vf_explained_var: 0.012659117579460144
        vf_loss: 1.0115103721618652
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.510926365852356
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016929305857047439
        model: {}
        policy_loss: -0.001582680270075798
        total_loss: -0.002354467287659645
        vf_explained_var: 0.0023473501205444336
        vf_loss: 1.2744630575180054
    load_time_ms: 14163.094
    num_steps_sampled: 85728000
    num_steps_trained: 85728000
    sample_time_ms: 119252.242
    update_time_ms: 16.36
  iterations_since_restore: 243
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.016990291262136
    ram_util_percent: 12.45679611650485
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 21.0
    agent-2: 61.0
    agent-3: 38.0
    agent-4: 25.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 15.22
    agent-1: 9.17
    agent-2: 44.07
    agent-3: 23.94
    agent-4: 14.34
    agent-5: 17.62
  policy_reward_min:
    agent-0: 0.0
    agent-1: 2.0
    agent-2: 3.0
    agent-3: 2.0
    agent-4: 1.0
    agent-5: 1.0
  sampler_perf:
    mean_env_wait_ms: 28.192083681785384
    mean_inference_ms: 15.058508052233483
    mean_processing_ms: 73.30779154412535
  time_since_restore: 35310.64407372475
  time_this_iter_s: 145.1648669242859
  time_total_s: 124247.32220339775
  timestamp: 1637398332
  timesteps_since_restore: 23328000
  timesteps_this_iter: 96000
  timesteps_total: 85728000
  training_iteration: 893
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    893 |           124247 | 85728000 |   124.36 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 7.17
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.21
    apples_agent-1_min: 0
    apples_agent-2_max: 68
    apples_agent-2_mean: 30.63
    apples_agent-2_min: 5
    apples_agent-3_max: 29
    apples_agent-3_mean: 9.31
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 11.27
    apples_agent-4_min: 5
    apples_agent-5_max: 19
    apples_agent-5_mean: 4.35
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 1.59
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 237
    cleaning_beam_agent-1_mean: 202.45
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 14
    cleaning_beam_agent-2_mean: 5.42
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 44
    cleaning_beam_agent-3_mean: 10.45
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 3.2
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.01
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-54-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 166.0
  episode_reward_mean: 127.82
  episode_reward_min: 52.0
  episodes_this_iter: 96
  episodes_total: 85824
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11863.663
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.28682592511177063
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007883505895733833
        model: {}
        policy_loss: -0.0017359284684062004
        total_loss: -0.002126384526491165
        vf_explained_var: 0.001473739743232727
        vf_loss: 1.1435785293579102
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20168356597423553
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016104771057143807
        model: {}
        policy_loss: -0.0017281481996178627
        total_loss: -0.002026238478720188
        vf_explained_var: 0.09632650017738342
        vf_loss: 0.5687311291694641
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42067092657089233
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016081981593742967
        model: {}
        policy_loss: -0.0020613207016140223
        total_loss: -0.0024197145830839872
        vf_explained_var: 0.025663986802101135
        vf_loss: 3.819866180419922
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4429378807544708
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011324284132570028
        model: {}
        policy_loss: -0.0013235913356766105
        total_loss: -0.001928150886669755
        vf_explained_var: 0.010621026158332825
        vf_loss: 1.7501214742660522
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4133152365684509
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007497508777305484
        model: {}
        policy_loss: -0.0013077538460493088
        total_loss: -0.0019365991465747356
        vf_explained_var: 0.012308120727539062
        vf_loss: 0.9858739972114563
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4949163496494293
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015621987404301763
        model: {}
        policy_loss: -0.0018094961997121572
        total_loss: -0.0025576248299330473
        vf_explained_var: 0.014393553137779236
        vf_loss: 1.22920823097229
    load_time_ms: 14141.986
    num_steps_sampled: 85824000
    num_steps_trained: 85824000
    sample_time_ms: 119204.974
    update_time_ms: 16.309
  iterations_since_restore: 244
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.99227053140097
    ram_util_percent: 12.375362318840581
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 17.0
    agent-2: 69.0
    agent-3: 38.0
    agent-4: 25.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 16.09
    agent-1: 9.2
    agent-2: 45.47
    agent-3: 24.24
    agent-4: 15.37
    agent-5: 17.45
  policy_reward_min:
    agent-0: 7.0
    agent-1: 3.0
    agent-2: 12.0
    agent-3: 12.0
    agent-4: 4.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.19214632326012
    mean_inference_ms: 15.058577213682543
    mean_processing_ms: 73.30817251871694
  time_since_restore: 35455.6676235199
  time_this_iter_s: 145.02354979515076
  time_total_s: 124392.3457531929
  timestamp: 1637398478
  timesteps_since_restore: 23424000
  timesteps_this_iter: 96000
  timesteps_total: 85824000
  training_iteration: 894
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    894 |           124392 | 85824000 |   127.82 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 6.85
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 3.46
    apples_agent-1_min: 0
    apples_agent-2_max: 68
    apples_agent-2_mean: 28.14
    apples_agent-2_min: 16
    apples_agent-3_max: 29
    apples_agent-3_mean: 8.33
    apples_agent-3_min: 2
    apples_agent-4_max: 19
    apples_agent-4_mean: 10.16
    apples_agent-4_min: 0
    apples_agent-5_max: 30
    apples_agent-5_mean: 5.66
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 1.24
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 235
    cleaning_beam_agent-1_mean: 196.48
    cleaning_beam_agent-1_min: 58
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 6.46
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 10.18
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 3.24
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.69
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-57-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 166.0
  episode_reward_mean: 121.37
  episode_reward_min: 53.0
  episodes_this_iter: 96
  episodes_total: 85920
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11864.435
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2898634076118469
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008430481539107859
        model: {}
        policy_loss: -0.0017835719045251608
        total_loss: -0.0021906369365751743
        vf_explained_var: 0.0033727437257766724
        vf_loss: 1.0309579372406006
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20207534730434418
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012521692551672459
        model: {}
        policy_loss: -0.0014236127026379108
        total_loss: -0.0017249961383640766
        vf_explained_var: 0.07531891763210297
        vf_loss: 0.5426992774009705
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42610692977905273
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011345493840053678
        model: {}
        policy_loss: -0.0018510321388021111
        total_loss: -0.0022316486574709415
        vf_explained_var: 0.03231492638587952
        vf_loss: 3.69331955909729
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4431838095188141
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013668722240254283
        model: {}
        policy_loss: -0.0014134766533970833
        total_loss: -0.002010982483625412
        vf_explained_var: -0.00013245642185211182
        vf_loss: 1.8249849081039429
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4197741448879242
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012162287021055818
        model: {}
        policy_loss: -0.0014871377497911453
        total_loss: -0.002123870886862278
        vf_explained_var: 0.014942049980163574
        vf_loss: 1.0207160711288452
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4967983663082123
        entropy_coeff: 0.0017600000137463212
        kl: 0.00143861910328269
        model: {}
        policy_loss: -0.001491363625973463
        total_loss: -0.002254885621368885
        vf_explained_var: 0.01793096959590912
        vf_loss: 1.108449101448059
    load_time_ms: 14153.007
    num_steps_sampled: 85920000
    num_steps_trained: 85920000
    sample_time_ms: 119036.257
    update_time_ms: 16.43
  iterations_since_restore: 245
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.927669902912623
    ram_util_percent: 12.451941747572812
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 64.0
    agent-3: 36.0
    agent-4: 26.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 15.44
    agent-1: 8.63
    agent-2: 43.4
    agent-3: 23.03
    agent-4: 14.22
    agent-5: 16.65
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 21.0
    agent-3: 8.0
    agent-4: 3.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.191420283438685
    mean_inference_ms: 15.058345278128204
    mean_processing_ms: 73.3067303037626
  time_since_restore: 35600.25985097885
  time_this_iter_s: 144.59222745895386
  time_total_s: 124536.93798065186
  timestamp: 1637398622
  timesteps_since_restore: 23520000
  timesteps_this_iter: 96000
  timesteps_total: 85920000
  training_iteration: 895
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    895 |           124537 | 85920000 |   121.37 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 7.38
    apples_agent-0_min: 2
    apples_agent-1_max: 67
    apples_agent-1_mean: 4.1
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 28.81
    apples_agent-2_min: 14
    apples_agent-3_max: 38
    apples_agent-3_mean: 8.75
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.13
    apples_agent-4_min: 2
    apples_agent-5_max: 15
    apples_agent-5_mean: 4.5
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 1.01
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 235
    cleaning_beam_agent-1_mean: 206.25
    cleaning_beam_agent-1_min: 162
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 5.86
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 22
    cleaning_beam_agent-3_mean: 7.77
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 7
    cleaning_beam_agent-4_mean: 2.51
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.19
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_03-59-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 174.0
  episode_reward_mean: 126.62
  episode_reward_min: 78.0
  episodes_this_iter: 96
  episodes_total: 86016
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11884.103
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.296321839094162
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014713665004819632
        model: {}
        policy_loss: -0.001792430179193616
        total_loss: -0.0022003506310284138
        vf_explained_var: 0.015224665403366089
        vf_loss: 1.1360669136047363
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20499563217163086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013767816126346588
        model: {}
        policy_loss: -0.0015640261117368937
        total_loss: -0.0018636155873537064
        vf_explained_var: 0.07396699488162994
        vf_loss: 0.6120346188545227
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42277032136917114
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016473387368023396
        model: {}
        policy_loss: -0.0020224321633577347
        total_loss: -0.0024020615965127945
        vf_explained_var: 0.009064808487892151
        vf_loss: 3.6444830894470215
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44208574295043945
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010134342592209578
        model: {}
        policy_loss: -0.0013701335992664099
        total_loss: -0.0019611776806414127
        vf_explained_var: 0.0053887516260147095
        vf_loss: 1.8702566623687744
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4011076092720032
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018976362189278007
        model: {}
        policy_loss: -0.0017130686901509762
        total_loss: -0.0023223133757710457
        vf_explained_var: 0.012760207056999207
        vf_loss: 0.9670250415802002
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5016656517982483
        entropy_coeff: 0.0017600000137463212
        kl: 0.001957549015060067
        model: {}
        policy_loss: -0.0016772323288023472
        total_loss: -0.0024412733037024736
        vf_explained_var: 0.011129498481750488
        vf_loss: 1.1888912916183472
    load_time_ms: 14135.518
    num_steps_sampled: 86016000
    num_steps_trained: 86016000
    sample_time_ms: 119083.153
    update_time_ms: 16.538
  iterations_since_restore: 246
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.997584541062803
    ram_util_percent: 12.452173913043476
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 20.0
    agent-2: 67.0
    agent-3: 43.0
    agent-4: 27.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 15.94
    agent-1: 9.31
    agent-2: 44.5
    agent-3: 25.28
    agent-4: 14.06
    agent-5: 17.53
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 23.0
    agent-3: 7.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.191392678994735
    mean_inference_ms: 15.058375937880529
    mean_processing_ms: 73.30780023246635
  time_since_restore: 35745.589452028275
  time_this_iter_s: 145.32960104942322
  time_total_s: 124682.26758170128
  timestamp: 1637398768
  timesteps_since_restore: 23616000
  timesteps_this_iter: 96000
  timesteps_total: 86016000
  training_iteration: 896
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    896 |           124682 | 86016000 |   126.62 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 7.36
    apples_agent-0_min: 1
    apples_agent-1_max: 23
    apples_agent-1_mean: 3.73
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 29.68
    apples_agent-2_min: 17
    apples_agent-3_max: 32
    apples_agent-3_mean: 8.55
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.7
    apples_agent-4_min: 2
    apples_agent-5_max: 14
    apples_agent-5_mean: 4.5
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.26
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 240
    cleaning_beam_agent-1_mean: 209.22
    cleaning_beam_agent-1_min: 175
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 6.65
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 9.52
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.69
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.06
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-01-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 166.0
  episode_reward_mean: 129.8
  episode_reward_min: 89.0
  episodes_this_iter: 96
  episodes_total: 86112
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11875.202
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29705703258514404
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013484255177900195
        model: {}
        policy_loss: -0.0018906733021140099
        total_loss: -0.0023038052022457123
        vf_explained_var: -0.002138480544090271
        vf_loss: 1.096904993057251
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20433123409748077
        entropy_coeff: 0.0017600000137463212
        kl: 0.000967781525105238
        model: {}
        policy_loss: -0.0014092829078435898
        total_loss: -0.001715073361992836
        vf_explained_var: 0.09174776077270508
        vf_loss: 0.53829026222229
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42677536606788635
        entropy_coeff: 0.0017600000137463212
        kl: 0.001423223060555756
        model: {}
        policy_loss: -0.0017850808799266815
        total_loss: -0.0021701091900467873
        vf_explained_var: 0.008506640791893005
        vf_loss: 3.660958766937256
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44493722915649414
        entropy_coeff: 0.0017600000137463212
        kl: 0.001353294006548822
        model: {}
        policy_loss: -0.0013935386668890715
        total_loss: -0.001997290411964059
        vf_explained_var: 0.006307005882263184
        vf_loss: 1.793376088142395
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3933568298816681
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009244935354217887
        model: {}
        policy_loss: -0.00125062745064497
        total_loss: -0.0018484191969037056
        vf_explained_var: 0.008422896265983582
        vf_loss: 0.9451776742935181
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5090104341506958
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017469247104600072
        model: {}
        policy_loss: -0.0014760526828467846
        total_loss: -0.0022389055229723454
        vf_explained_var: 0.010303378105163574
        vf_loss: 1.3300327062606812
    load_time_ms: 14136.311
    num_steps_sampled: 86112000
    num_steps_trained: 86112000
    sample_time_ms: 118950.247
    update_time_ms: 16.786
  iterations_since_restore: 247
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.110731707317075
    ram_util_percent: 12.467317073170728
  pid: 27065
  policy_reward_max:
    agent-0: 34.0
    agent-1: 18.0
    agent-2: 60.0
    agent-3: 42.0
    agent-4: 24.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 16.22
    agent-1: 9.51
    agent-2: 45.74
    agent-3: 25.35
    agent-4: 14.61
    agent-5: 18.37
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 25.0
    agent-3: 12.0
    agent-4: 6.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.19038161311637
    mean_inference_ms: 15.058103262200804
    mean_processing_ms: 73.30565184971425
  time_since_restore: 35889.4931576252
  time_this_iter_s: 143.90370559692383
  time_total_s: 124826.1712872982
  timestamp: 1637398912
  timesteps_since_restore: 23712000
  timesteps_this_iter: 96000
  timesteps_total: 86112000
  training_iteration: 897
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    897 |           124826 | 86112000 |    129.8 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 6.8
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.03
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 29.75
    apples_agent-2_min: 9
    apples_agent-3_max: 25
    apples_agent-3_mean: 8.81
    apples_agent-3_min: 2
    apples_agent-4_max: 29
    apples_agent-4_mean: 11.41
    apples_agent-4_min: 3
    apples_agent-5_max: 20
    apples_agent-5_mean: 4.24
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 1.11
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 235
    cleaning_beam_agent-1_mean: 206.27
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 6.54
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 10.34
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.57
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.61
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-04-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 170.0
  episode_reward_mean: 127.68
  episode_reward_min: 60.0
  episodes_this_iter: 96
  episodes_total: 86208
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11868.424
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.295162171125412
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011418040376156569
        model: {}
        policy_loss: -0.0016615702770650387
        total_loss: -0.00207439623773098
        vf_explained_var: 0.013821229338645935
        vf_loss: 1.0665884017944336
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20268544554710388
        entropy_coeff: 0.0017600000137463212
        kl: 0.001582925207912922
        model: {}
        policy_loss: -0.0017938213422894478
        total_loss: -0.002094971714541316
        vf_explained_var: 0.06944939494132996
        vf_loss: 0.5557617545127869
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4234684109687805
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014568476472049952
        model: {}
        policy_loss: -0.0019203370902687311
        total_loss: -0.0022902446798980236
        vf_explained_var: 0.025478526949882507
        vf_loss: 3.7540078163146973
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4419984519481659
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016687012976035476
        model: {}
        policy_loss: -0.0017809736309573054
        total_loss: -0.0023871504236012697
        vf_explained_var: 0.005327582359313965
        vf_loss: 1.7173930406570435
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38927406072616577
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013205413706600666
        model: {}
        policy_loss: -0.0015891417860984802
        total_loss: -0.0021779448725283146
        vf_explained_var: 0.026529580354690552
        vf_loss: 0.9631770253181458
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4998782277107239
        entropy_coeff: 0.0017600000137463212
        kl: 0.002314723329618573
        model: {}
        policy_loss: -0.0016681734705343843
        total_loss: -0.00242170924320817
        vf_explained_var: 0.021462485194206238
        vf_loss: 1.2625014781951904
    load_time_ms: 14132.279
    num_steps_sampled: 86208000
    num_steps_trained: 86208000
    sample_time_ms: 118924.359
    update_time_ms: 16.987
  iterations_since_restore: 248
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.790865384615383
    ram_util_percent: 12.398076923076921
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 17.0
    agent-2: 72.0
    agent-3: 45.0
    agent-4: 31.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 16.56
    agent-1: 8.72
    agent-2: 45.36
    agent-3: 24.17
    agent-4: 15.36
    agent-5: 17.51
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 25.0
    agent-3: 9.0
    agent-4: 7.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.19047136475214
    mean_inference_ms: 15.057858560627004
    mean_processing_ms: 73.30392024020975
  time_since_restore: 36035.124331474304
  time_this_iter_s: 145.63117384910583
  time_total_s: 124971.80246114731
  timestamp: 1637399058
  timesteps_since_restore: 23808000
  timesteps_this_iter: 96000
  timesteps_total: 86208000
  training_iteration: 898
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    898 |           124972 | 86208000 |   127.68 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 7.75
    apples_agent-0_min: 1
    apples_agent-1_max: 6
    apples_agent-1_mean: 3.08
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 30.5
    apples_agent-2_min: 11
    apples_agent-3_max: 32
    apples_agent-3_mean: 9.03
    apples_agent-3_min: 0
    apples_agent-4_max: 29
    apples_agent-4_mean: 11.09
    apples_agent-4_min: 3
    apples_agent-5_max: 19
    apples_agent-5_mean: 4.85
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 26
    cleaning_beam_agent-0_mean: 1.71
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 259
    cleaning_beam_agent-1_mean: 210.17
    cleaning_beam_agent-1_min: 48
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 6.64
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 53
    cleaning_beam_agent-3_mean: 11.35
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 2.62
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.44
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-06-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 179.0
  episode_reward_mean: 130.91
  episode_reward_min: 38.0
  episodes_this_iter: 96
  episodes_total: 86304
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11876.717
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2996317744255066
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010840492323040962
        model: {}
        policy_loss: -0.0018956671701744199
        total_loss: -0.0023045113775879145
        vf_explained_var: 0.013666898012161255
        vf_loss: 1.185089349746704
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20474226772785187
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006028609350323677
        model: {}
        policy_loss: -0.0013152938336133957
        total_loss: -0.001623670570552349
        vf_explained_var: 0.07726810872554779
        vf_loss: 0.5197008848190308
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42990344762802124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012905783951282501
        model: {}
        policy_loss: -0.001906949793919921
        total_loss: -0.002264745067805052
        vf_explained_var: 0.007739052176475525
        vf_loss: 3.9883615970611572
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4479924142360687
        entropy_coeff: 0.0017600000137463212
        kl: 0.001688683871179819
        model: {}
        policy_loss: -0.0016259653493762016
        total_loss: -0.00221962109208107
        vf_explained_var: 0.008887648582458496
        vf_loss: 1.9481204748153687
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3943687677383423
        entropy_coeff: 0.0017600000137463212
        kl: 0.000846870883833617
        model: {}
        policy_loss: -0.0013678241521120071
        total_loss: -0.001962419133633375
        vf_explained_var: 0.02014070749282837
        vf_loss: 0.9949454069137573
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5102951526641846
        entropy_coeff: 0.0017600000137463212
        kl: 0.001697840983979404
        model: {}
        policy_loss: -0.0018531506648287177
        total_loss: -0.0026168394833803177
        vf_explained_var: 0.01424679160118103
        vf_loss: 1.344336748123169
    load_time_ms: 14150.351
    num_steps_sampled: 86304000
    num_steps_trained: 86304000
    sample_time_ms: 118797.853
    update_time_ms: 16.658
  iterations_since_restore: 249
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.911165048543687
    ram_util_percent: 12.450970873786407
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 17.0
    agent-2: 72.0
    agent-3: 39.0
    agent-4: 23.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 17.08
    agent-1: 8.84
    agent-2: 45.96
    agent-3: 25.98
    agent-4: 14.75
    agent-5: 18.3
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 16.0
    agent-3: 7.0
    agent-4: 3.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.189994159156182
    mean_inference_ms: 15.05776227857083
    mean_processing_ms: 73.30210093033524
  time_since_restore: 36179.4320025444
  time_this_iter_s: 144.30767107009888
  time_total_s: 125116.1101322174
  timestamp: 1637399202
  timesteps_since_restore: 23904000
  timesteps_this_iter: 96000
  timesteps_total: 86304000
  training_iteration: 899
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    899 |           125116 | 86304000 |   130.91 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 6.74
    apples_agent-0_min: 1
    apples_agent-1_max: 12
    apples_agent-1_mean: 3.44
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 29.66
    apples_agent-2_min: 14
    apples_agent-3_max: 56
    apples_agent-3_mean: 10.1
    apples_agent-3_min: 2
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.68
    apples_agent-4_min: 2
    apples_agent-5_max: 13
    apples_agent-5_mean: 4.62
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.41
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 248
    cleaning_beam_agent-1_mean: 210.93
    cleaning_beam_agent-1_min: 178
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 6.42
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 9.99
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 2.82
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.4
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-09-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 184.0
  episode_reward_mean: 131.78
  episode_reward_min: 92.0
  episodes_this_iter: 96
  episodes_total: 86400
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11848.434
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2949120104312897
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011120381532236934
        model: {}
        policy_loss: -0.0017689375672489405
        total_loss: -0.0021657764445990324
        vf_explained_var: 0.013438194990158081
        vf_loss: 1.2220603227615356
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20648616552352905
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009803497232496738
        model: {}
        policy_loss: -0.0014435434713959694
        total_loss: -0.0017504154238849878
        vf_explained_var: 0.08797673881053925
        vf_loss: 0.565449059009552
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4270886182785034
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013851418625563383
        model: {}
        policy_loss: -0.0018577403388917446
        total_loss: -0.002257706131786108
        vf_explained_var: 0.026348784565925598
        vf_loss: 3.51711368560791
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4506446421146393
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016469163820147514
        model: {}
        policy_loss: -0.0016864018980413675
        total_loss: -0.0022701164707541466
        vf_explained_var: 0.006980180740356445
        vf_loss: 2.094193458557129
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.388894647359848
        entropy_coeff: 0.0017600000137463212
        kl: 0.001049958635121584
        model: {}
        policy_loss: -0.0013541321968659759
        total_loss: -0.0019387579523026943
        vf_explained_var: 0.011312559247016907
        vf_loss: 0.9982750415802002
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5120127201080322
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024939547292888165
        model: {}
        policy_loss: -0.0017897681100293994
        total_loss: -0.002559407614171505
        vf_explained_var: 0.008268684148788452
        vf_loss: 1.3150441646575928
    load_time_ms: 14129.03
    num_steps_sampled: 86400000
    num_steps_trained: 86400000
    sample_time_ms: 118744.366
    update_time_ms: 16.768
  iterations_since_restore: 250
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.973170731707317
    ram_util_percent: 12.377073170731709
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 17.0
    agent-2: 70.0
    agent-3: 45.0
    agent-4: 25.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 17.18
    agent-1: 9.63
    agent-2: 44.75
    agent-3: 26.53
    agent-4: 14.89
    agent-5: 18.8
  policy_reward_min:
    agent-0: 6.0
    agent-1: 4.0
    agent-2: 30.0
    agent-3: 13.0
    agent-4: 4.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.189924921421678
    mean_inference_ms: 15.05747873610693
    mean_processing_ms: 73.29996783443295
  time_since_restore: 36323.61950349808
  time_this_iter_s: 144.18750095367432
  time_total_s: 125260.29763317108
  timestamp: 1637399346
  timesteps_since_restore: 24000000
  timesteps_this_iter: 96000
  timesteps_total: 86400000
  training_iteration: 900
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    900 |           125260 | 86400000 |   131.78 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 6.76
    apples_agent-0_min: 2
    apples_agent-1_max: 12
    apples_agent-1_mean: 3.27
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 30.26
    apples_agent-2_min: 14
    apples_agent-3_max: 31
    apples_agent-3_mean: 8.98
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.98
    apples_agent-4_min: 3
    apples_agent-5_max: 14
    apples_agent-5_mean: 4.34
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 1.32
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 253
    cleaning_beam_agent-1_mean: 211.92
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 5.76
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 30
    cleaning_beam_agent-3_mean: 9.08
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 2.67
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.05
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-11-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 190.0
  episode_reward_mean: 129.05
  episode_reward_min: 89.0
  episodes_this_iter: 96
  episodes_total: 86496
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11870.603
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30015450716018677
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007648679311387241
        model: {}
        policy_loss: -0.0016744467429816723
        total_loss: -0.0021016490645706654
        vf_explained_var: 0.008840754628181458
        vf_loss: 1.0107002258300781
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20865315198898315
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010178431402891874
        model: {}
        policy_loss: -0.001610486418940127
        total_loss: -0.0019247086020186543
        vf_explained_var: 0.08805033564567566
        vf_loss: 0.5300490856170654
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4350831210613251
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010194246424362063
        model: {}
        policy_loss: -0.0017530835466459394
        total_loss: -0.0021463395096361637
        vf_explained_var: 0.016452327370643616
        vf_loss: 3.724876880645752
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44856613874435425
        entropy_coeff: 0.0017600000137463212
        kl: 0.001420752378180623
        model: {}
        policy_loss: -0.0014537181705236435
        total_loss: -0.002045779023319483
        vf_explained_var: 0.0032957643270492554
        vf_loss: 1.97416090965271
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38860026001930237
        entropy_coeff: 0.0017600000137463212
        kl: 0.000728953629732132
        model: {}
        policy_loss: -0.0013654681388288736
        total_loss: -0.0019494168227538466
        vf_explained_var: 0.02311524748802185
        vf_loss: 0.9998854994773865
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5164397358894348
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015765615971758962
        model: {}
        policy_loss: -0.0017041030805557966
        total_loss: -0.002488147234544158
        vf_explained_var: 0.01709027588367462
        vf_loss: 1.248873233795166
    load_time_ms: 14132.497
    num_steps_sampled: 86496000
    num_steps_trained: 86496000
    sample_time_ms: 118713.389
    update_time_ms: 16.915
  iterations_since_restore: 251
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.74238095238095
    ram_util_percent: 12.446190476190473
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 21.0
    agent-2: 66.0
    agent-3: 39.0
    agent-4: 25.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 15.45
    agent-1: 9.35
    agent-2: 45.42
    agent-3: 25.16
    agent-4: 15.09
    agent-5: 18.58
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 27.0
    agent-3: 8.0
    agent-4: 5.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.190087492621913
    mean_inference_ms: 15.05745431948162
    mean_processing_ms: 73.30002469060874
  time_since_restore: 36468.89683055878
  time_this_iter_s: 145.27732706069946
  time_total_s: 125405.57496023178
  timestamp: 1637399494
  timesteps_since_restore: 24096000
  timesteps_this_iter: 96000
  timesteps_total: 86496000
  training_iteration: 901
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    901 |           125406 | 86496000 |   129.05 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 7.61
    apples_agent-0_min: 2
    apples_agent-1_max: 7
    apples_agent-1_mean: 2.6
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 29.47
    apples_agent-2_min: 16
    apples_agent-3_max: 31
    apples_agent-3_mean: 8.83
    apples_agent-3_min: 2
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.72
    apples_agent-4_min: 3
    apples_agent-5_max: 19
    apples_agent-5_mean: 5.03
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.55
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 253
    cleaning_beam_agent-1_mean: 210.39
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 5.74
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 11.57
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.78
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.15
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-13-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 190.0
  episode_reward_mean: 127.99
  episode_reward_min: 95.0
  episodes_this_iter: 96
  episodes_total: 86592
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11879.251
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29885557293891907
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006767415907233953
        model: {}
        policy_loss: -0.0013776925625279546
        total_loss: -0.001792133436538279
        vf_explained_var: 0.015850558876991272
        vf_loss: 1.1154530048370361
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.205938920378685
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007664544973522425
        model: {}
        policy_loss: -0.0012992247939109802
        total_loss: -0.001610802486538887
        vf_explained_var: 0.09054812788963318
        vf_loss: 0.5087745785713196
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4337787628173828
        entropy_coeff: 0.0017600000137463212
        kl: 0.001286862650886178
        model: {}
        policy_loss: -0.0017866443376988173
        total_loss: -0.002203532960265875
        vf_explained_var: 0.023556753993034363
        vf_loss: 3.4656031131744385
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4420589208602905
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017356902826577425
        model: {}
        policy_loss: -0.0014026123099029064
        total_loss: -0.001991452183574438
        vf_explained_var: 0.006354227662086487
        vf_loss: 1.8918123245239258
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38917064666748047
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006729087326675653
        model: {}
        policy_loss: -0.0013491481076925993
        total_loss: -0.0019382090540602803
        vf_explained_var: 0.00512339174747467
        vf_loss: 0.9588091373443604
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5186253786087036
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007558873039670289
        model: {}
        policy_loss: -0.0013863155618309975
        total_loss: -0.002170668914914131
        vf_explained_var: 0.015604659914970398
        vf_loss: 1.2843031883239746
    load_time_ms: 14160.476
    num_steps_sampled: 86592000
    num_steps_trained: 86592000
    sample_time_ms: 118664.265
    update_time_ms: 16.926
  iterations_since_restore: 252
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.001456310679607
    ram_util_percent: 12.37378640776699
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 20.0
    agent-2: 64.0
    agent-3: 43.0
    agent-4: 25.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 16.42
    agent-1: 8.89
    agent-2: 44.43
    agent-3: 25.36
    agent-4: 14.74
    agent-5: 18.15
  policy_reward_min:
    agent-0: 7.0
    agent-1: 3.0
    agent-2: 29.0
    agent-3: 13.0
    agent-4: 7.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.19036079948392
    mean_inference_ms: 15.05727541841148
    mean_processing_ms: 73.29871650509743
  time_since_restore: 36613.64091587067
  time_this_iter_s: 144.74408531188965
  time_total_s: 125550.31904554367
  timestamp: 1637399639
  timesteps_since_restore: 24192000
  timesteps_this_iter: 96000
  timesteps_total: 86592000
  training_iteration: 902
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    902 |           125550 | 86592000 |   127.99 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 6.92
    apples_agent-0_min: 0
    apples_agent-1_max: 34
    apples_agent-1_mean: 3.78
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 29.85
    apples_agent-2_min: 9
    apples_agent-3_max: 27
    apples_agent-3_mean: 8.34
    apples_agent-3_min: 1
    apples_agent-4_max: 26
    apples_agent-4_mean: 9.93
    apples_agent-4_min: 3
    apples_agent-5_max: 14
    apples_agent-5_mean: 4.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.5
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 248
    cleaning_beam_agent-1_mean: 210.12
    cleaning_beam_agent-1_min: 54
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 6.09
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 11.28
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 2.69
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.97
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-16-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 161.0
  episode_reward_mean: 126.34
  episode_reward_min: 42.0
  episodes_this_iter: 96
  episodes_total: 86688
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11872.12
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3018896281719208
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007349393563345075
        model: {}
        policy_loss: -0.0014732049312442541
        total_loss: -0.0018910891376435757
        vf_explained_var: 0.011651560664176941
        vf_loss: 1.134382963180542
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20542091131210327
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014232064131647348
        model: {}
        policy_loss: -0.0014785584062337875
        total_loss: -0.0017797071486711502
        vf_explained_var: 0.06929653882980347
        vf_loss: 0.6039489507675171
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44036465883255005
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012193979928269982
        model: {}
        policy_loss: -0.0017295833677053452
        total_loss: -0.0021600816398859024
        vf_explained_var: 0.009960830211639404
        vf_loss: 3.445464611053467
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43765121698379517
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015776827931404114
        model: {}
        policy_loss: -0.0014926185831427574
        total_loss: -0.002082967199385166
        vf_explained_var: 0.008044049143791199
        vf_loss: 1.7991811037063599
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3861721158027649
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008515543886460364
        model: {}
        policy_loss: -0.001490788534283638
        total_loss: -0.0020647309720516205
        vf_explained_var: 0.01996973156929016
        vf_loss: 1.0571986436843872
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5121905207633972
        entropy_coeff: 0.0017600000137463212
        kl: 0.001430740812793374
        model: {}
        policy_loss: -0.0016069966368377209
        total_loss: -0.0023823154624551535
        vf_explained_var: 0.01832282543182373
        vf_loss: 1.2613835334777832
    load_time_ms: 14146.671
    num_steps_sampled: 86688000
    num_steps_trained: 86688000
    sample_time_ms: 118630.872
    update_time_ms: 16.861
  iterations_since_restore: 253
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.978155339805824
    ram_util_percent: 12.355825242718444
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 19.0
    agent-2: 62.0
    agent-3: 37.0
    agent-4: 26.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 16.61
    agent-1: 8.92
    agent-2: 44.7
    agent-3: 23.47
    agent-4: 14.77
    agent-5: 17.87
  policy_reward_min:
    agent-0: 2.0
    agent-1: 2.0
    agent-2: 18.0
    agent-3: 6.0
    agent-4: 5.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.190833531077164
    mean_inference_ms: 15.05710603307596
    mean_processing_ms: 73.29843347659984
  time_since_restore: 36758.200681209564
  time_this_iter_s: 144.5597653388977
  time_total_s: 125694.87881088257
  timestamp: 1637399783
  timesteps_since_restore: 24288000
  timesteps_this_iter: 96000
  timesteps_total: 86688000
  training_iteration: 903
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    903 |           125695 | 86688000 |   126.34 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 6.22
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 2.98
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 29.45
    apples_agent-2_min: 14
    apples_agent-3_max: 29
    apples_agent-3_mean: 8.9
    apples_agent-3_min: 2
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.87
    apples_agent-4_min: 3
    apples_agent-5_max: 22
    apples_agent-5_mean: 4.22
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 1.21
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 255
    cleaning_beam_agent-1_mean: 209.68
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 15
    cleaning_beam_agent-2_mean: 6.44
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 10.71
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.15
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.5
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-18-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 126.05
  episode_reward_min: 75.0
  episodes_this_iter: 96
  episodes_total: 86784
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11877.017
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2975302040576935
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009999745525419712
        model: {}
        policy_loss: -0.001649755984544754
        total_loss: -0.0020778151229023933
        vf_explained_var: 0.007822230458259583
        vf_loss: 0.9559224843978882
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20593075454235077
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010198790114372969
        model: {}
        policy_loss: -0.0015214644372463226
        total_loss: -0.0018354300409555435
        vf_explained_var: 0.08228249847888947
        vf_loss: 0.48471951484680176
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44873592257499695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015932549722492695
        model: {}
        policy_loss: -0.0017641945742070675
        total_loss: -0.002193818334490061
        vf_explained_var: 0.005648508667945862
        vf_loss: 3.6015210151672363
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44063252210617065
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011925608851015568
        model: {}
        policy_loss: -0.001419994980096817
        total_loss: -0.00201991549693048
        vf_explained_var: 0.008566811680793762
        vf_loss: 1.755950689315796
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37757259607315063
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011725452495738864
        model: {}
        policy_loss: -0.001514660194516182
        total_loss: -0.002084028907120228
        vf_explained_var: 0.006319999694824219
        vf_loss: 0.9515973329544067
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5136122703552246
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014911865582689643
        model: {}
        policy_loss: -0.0014900187961757183
        total_loss: -0.0022651050239801407
        vf_explained_var: 0.004548594355583191
        vf_loss: 1.2886863946914673
    load_time_ms: 14137.022
    num_steps_sampled: 86784000
    num_steps_trained: 86784000
    sample_time_ms: 118731.321
    update_time_ms: 16.937
  iterations_since_restore: 254
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.91346153846154
    ram_util_percent: 12.36298076923077
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 18.0
    agent-2: 62.0
    agent-3: 41.0
    agent-4: 26.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 15.51
    agent-1: 8.88
    agent-2: 43.9
    agent-3: 24.99
    agent-4: 14.75
    agent-5: 18.02
  policy_reward_min:
    agent-0: 5.0
    agent-1: 3.0
    agent-2: 27.0
    agent-3: 12.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.190950325228723
    mean_inference_ms: 15.057251113571809
    mean_processing_ms: 73.29813075689192
  time_since_restore: 36904.17981481552
  time_this_iter_s: 145.97913360595703
  time_total_s: 125840.85794448853
  timestamp: 1637399929
  timesteps_since_restore: 24384000
  timesteps_this_iter: 96000
  timesteps_total: 86784000
  training_iteration: 904
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    904 |           125841 | 86784000 |   126.05 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 7.07
    apples_agent-0_min: 0
    apples_agent-1_max: 24
    apples_agent-1_mean: 3.24
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 29.33
    apples_agent-2_min: 12
    apples_agent-3_max: 34
    apples_agent-3_mean: 9.04
    apples_agent-3_min: 2
    apples_agent-4_max: 19
    apples_agent-4_mean: 10.71
    apples_agent-4_min: 4
    apples_agent-5_max: 29
    apples_agent-5_mean: 4.61
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 9
    cleaning_beam_agent-0_mean: 1.3
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 242
    cleaning_beam_agent-1_mean: 211.11
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 5.61
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 12.04
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 2.88
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.53
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-21-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 170.0
  episode_reward_mean: 127.62
  episode_reward_min: 57.0
  episodes_this_iter: 96
  episodes_total: 86880
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11863.304
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3074309825897217
        entropy_coeff: 0.0017600000137463212
        kl: 0.001333886757493019
        model: {}
        policy_loss: -0.0018993450794368982
        total_loss: -0.002337159588932991
        vf_explained_var: 0.00135001540184021
        vf_loss: 1.0326279401779175
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20789310336112976
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010789227671921253
        model: {}
        policy_loss: -0.0014765476807951927
        total_loss: -0.001795480027794838
        vf_explained_var: 0.08123238384723663
        vf_loss: 0.46960270404815674
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4517870545387268
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011826488189399242
        model: {}
        policy_loss: -0.001671756966970861
        total_loss: -0.002079191617667675
        vf_explained_var: 0.00705409049987793
        vf_loss: 3.8771142959594727
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.448230504989624
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018580188043415546
        model: {}
        policy_loss: -0.001746349735185504
        total_loss: -0.0023561285343021154
        vf_explained_var: 0.005674928426742554
        vf_loss: 1.79106605052948
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3745805621147156
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008951930794864893
        model: {}
        policy_loss: -0.001393808051943779
        total_loss: -0.001958676613867283
        vf_explained_var: 0.006139159202575684
        vf_loss: 0.9439294338226318
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5209872126579285
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019119544886052608
        model: {}
        policy_loss: -0.001783330924808979
        total_loss: -0.0025767339393496513
        vf_explained_var: 0.010513842105865479
        vf_loss: 1.2353421449661255
    load_time_ms: 14139.344
    num_steps_sampled: 86880000
    num_steps_trained: 86880000
    sample_time_ms: 118720.314
    update_time_ms: 16.96
  iterations_since_restore: 255
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.979126213592238
    ram_util_percent: 12.373300970873789
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 15.0
    agent-2: 65.0
    agent-3: 38.0
    agent-4: 27.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 16.08
    agent-1: 8.27
    agent-2: 45.14
    agent-3: 25.48
    agent-4: 15.13
    agent-5: 17.52
  policy_reward_min:
    agent-0: 5.0
    agent-1: 0.0
    agent-2: 20.0
    agent-3: 8.0
    agent-4: 7.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.190750150403893
    mean_inference_ms: 15.05701727986595
    mean_processing_ms: 73.29666204126232
  time_since_restore: 37048.565657138824
  time_this_iter_s: 144.38584232330322
  time_total_s: 125985.24378681183
  timestamp: 1637400074
  timesteps_since_restore: 24480000
  timesteps_this_iter: 96000
  timesteps_total: 86880000
  training_iteration: 905
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    905 |           125985 | 86880000 |   127.62 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 6.34
    apples_agent-0_min: 1
    apples_agent-1_max: 19
    apples_agent-1_mean: 3.62
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 28.45
    apples_agent-2_min: 14
    apples_agent-3_max: 27
    apples_agent-3_mean: 8.26
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.92
    apples_agent-4_min: 3
    apples_agent-5_max: 19
    apples_agent-5_mean: 4.6
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 1.22
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 259
    cleaning_beam_agent-1_mean: 209.74
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 17
    cleaning_beam_agent-2_mean: 6.14
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 12.03
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.8
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 4.28
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-23-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 176.0
  episode_reward_mean: 127.72
  episode_reward_min: 60.0
  episodes_this_iter: 96
  episodes_total: 86976
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11860.589
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3052771985530853
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007597411167807877
        model: {}
        policy_loss: -0.0015684445388615131
        total_loss: -0.001989285461604595
        vf_explained_var: 0.010356470942497253
        vf_loss: 1.1644423007965088
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20448940992355347
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007747667841613293
        model: {}
        policy_loss: -0.0013910690322518349
        total_loss: -0.0016967696137726307
        vf_explained_var: 0.1014934778213501
        vf_loss: 0.5420123338699341
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44002771377563477
        entropy_coeff: 0.0017600000137463212
        kl: 0.001025527366437018
        model: {}
        policy_loss: -0.0017957799136638641
        total_loss: -0.002208392135798931
        vf_explained_var: 0.029605314135551453
        vf_loss: 3.6183712482452393
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4383997321128845
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009327162406407297
        model: {}
        policy_loss: -0.0013219574466347694
        total_loss: -0.0018926141783595085
        vf_explained_var: 0.00792701542377472
        vf_loss: 2.009263515472412
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3797345459461212
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008986489847302437
        model: {}
        policy_loss: -0.0013795653358101845
        total_loss: -0.0019510481506586075
        vf_explained_var: 0.010366693139076233
        vf_loss: 0.968471884727478
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5222583413124084
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013139074435457587
        model: {}
        policy_loss: -0.0015887150075286627
        total_loss: -0.002378554316237569
        vf_explained_var: 0.01669520139694214
        vf_loss: 1.293371558189392
    load_time_ms: 14150.33
    num_steps_sampled: 86976000
    num_steps_trained: 86976000
    sample_time_ms: 118795.1
    update_time_ms: 16.903
  iterations_since_restore: 256
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.842583732057417
    ram_util_percent: 12.380861244019139
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 17.0
    agent-2: 62.0
    agent-3: 38.0
    agent-4: 24.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 16.71
    agent-1: 9.07
    agent-2: 44.07
    agent-3: 25.49
    agent-4: 14.68
    agent-5: 17.7
  policy_reward_min:
    agent-0: 6.0
    agent-1: 4.0
    agent-2: 22.0
    agent-3: 5.0
    agent-4: 7.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.190768735230016
    mean_inference_ms: 15.056997153555931
    mean_processing_ms: 73.29729262207698
  time_since_restore: 37194.73294377327
  time_this_iter_s: 146.1672866344452
  time_total_s: 126131.41107344627
  timestamp: 1637400220
  timesteps_since_restore: 24576000
  timesteps_this_iter: 96000
  timesteps_total: 86976000
  training_iteration: 906
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    906 |           126131 | 86976000 |   127.72 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 6.37
    apples_agent-0_min: 0
    apples_agent-1_max: 45
    apples_agent-1_mean: 3.85
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 28.44
    apples_agent-2_min: 2
    apples_agent-3_max: 47
    apples_agent-3_mean: 8.85
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.67
    apples_agent-4_min: 0
    apples_agent-5_max: 17
    apples_agent-5_mean: 4.55
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 1.33
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 253
    cleaning_beam_agent-1_mean: 209.64
    cleaning_beam_agent-1_min: 17
    cleaning_beam_agent-2_max: 16
    cleaning_beam_agent-2_mean: 6.77
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 10.65
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 2.91
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.48
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-26-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 170.0
  episode_reward_mean: 125.71
  episode_reward_min: 17.0
  episodes_this_iter: 96
  episodes_total: 87072
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11872.477
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30439192056655884
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012718241196125746
        model: {}
        policy_loss: -0.0017799311317503452
        total_loss: -0.002209234284237027
        vf_explained_var: 0.00562189519405365
        vf_loss: 1.0642387866973877
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20263758301734924
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009145060903392732
        model: {}
        policy_loss: -0.0015640214551240206
        total_loss: -0.0018723014509305358
        vf_explained_var: 0.09264056384563446
        vf_loss: 0.48359957337379456
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43967530131340027
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011391283478587866
        model: {}
        policy_loss: -0.0016406448557972908
        total_loss: -0.0020314869470894337
        vf_explained_var: 0.026503846049308777
        vf_loss: 3.829906702041626
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43554258346557617
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014676724094897509
        model: {}
        policy_loss: -0.0014618365094065666
        total_loss: -0.0020458176732063293
        vf_explained_var: 0.007090479135513306
        vf_loss: 1.8257447481155396
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3695138692855835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011389506980776787
        model: {}
        policy_loss: -0.0015149093233048916
        total_loss: -0.0020538410171866417
        vf_explained_var: 0.01078563928604126
        vf_loss: 1.1141250133514404
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5293204188346863
        entropy_coeff: 0.0017600000137463212
        kl: 0.001994191436097026
        model: {}
        policy_loss: -0.001957111991941929
        total_loss: -0.0027653612196445465
        vf_explained_var: 0.01783144474029541
        vf_loss: 1.233569860458374
    load_time_ms: 14175.891
    num_steps_sampled: 87072000
    num_steps_trained: 87072000
    sample_time_ms: 118866.479
    update_time_ms: 16.591
  iterations_since_restore: 257
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.81111111111111
    ram_util_percent: 12.454106280193235
  pid: 27065
  policy_reward_max:
    agent-0: 24.0
    agent-1: 20.0
    agent-2: 65.0
    agent-3: 37.0
    agent-4: 32.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 15.88
    agent-1: 8.5
    agent-2: 43.38
    agent-3: 24.64
    agent-4: 15.41
    agent-5: 17.9
  policy_reward_min:
    agent-0: 1.0
    agent-1: 2.0
    agent-2: 3.0
    agent-3: 5.0
    agent-4: 2.0
    agent-5: 2.0
  sampler_perf:
    mean_env_wait_ms: 28.190610826144017
    mean_inference_ms: 15.05660571799451
    mean_processing_ms: 73.2957845753686
  time_since_restore: 37339.78306674957
  time_this_iter_s: 145.0501229763031
  time_total_s: 126276.46119642258
  timestamp: 1637400365
  timesteps_since_restore: 24672000
  timesteps_this_iter: 96000
  timesteps_total: 87072000
  training_iteration: 907
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    907 |           126276 | 87072000 |   125.71 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 7.21
    apples_agent-0_min: 2
    apples_agent-1_max: 44
    apples_agent-1_mean: 3.75
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 30.2
    apples_agent-2_min: 11
    apples_agent-3_max: 33
    apples_agent-3_mean: 9.55
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.41
    apples_agent-4_min: 3
    apples_agent-5_max: 24
    apples_agent-5_mean: 4.79
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.1
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 261
    cleaning_beam_agent-1_mean: 213.11
    cleaning_beam_agent-1_min: 101
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 6.16
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 9.18
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 2.59
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 4.29
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-28-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 164.0
  episode_reward_mean: 128.1
  episode_reward_min: 50.0
  episodes_this_iter: 96
  episodes_total: 87168
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11890.161
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30945590138435364
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011120149865746498
        model: {}
        policy_loss: -0.0017862403765320778
        total_loss: -0.0022319909185171127
        vf_explained_var: 0.0031445473432540894
        vf_loss: 0.9888978600502014
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20590531826019287
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009589038672856987
        model: {}
        policy_loss: -0.001391097204759717
        total_loss: -0.001696188235655427
        vf_explained_var: 0.058257266879081726
        vf_loss: 0.5729875564575195
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4357215464115143
        entropy_coeff: 0.0017600000137463212
        kl: 0.001293035689741373
        model: {}
        policy_loss: -0.0017984749283641577
        total_loss: -0.0021948223002254963
        vf_explained_var: 0.01362195611000061
        vf_loss: 3.705214023590088
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4431881308555603
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013743999879807234
        model: {}
        policy_loss: -0.0013421217445284128
        total_loss: -0.0019481750205159187
        vf_explained_var: -9.937584400177002e-05
        vf_loss: 1.7395799160003662
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36503905057907104
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010932650184258819
        model: {}
        policy_loss: -0.001352897146716714
        total_loss: -0.0018889950588345528
        vf_explained_var: 0.01757064461708069
        vf_loss: 1.063650369644165
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5306764841079712
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016246166778728366
        model: {}
        policy_loss: -0.0016696914099156857
        total_loss: -0.0024748085997998714
        vf_explained_var: 0.015847980976104736
        vf_loss: 1.2887028455734253
    load_time_ms: 14160.147
    num_steps_sampled: 87168000
    num_steps_trained: 87168000
    sample_time_ms: 118851.457
    update_time_ms: 16.44
  iterations_since_restore: 258
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.017391304347818
    ram_util_percent: 12.450724637681159
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 20.0
    agent-2: 61.0
    agent-3: 37.0
    agent-4: 28.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 16.37
    agent-1: 8.82
    agent-2: 45.46
    agent-3: 24.24
    agent-4: 14.96
    agent-5: 18.25
  policy_reward_min:
    agent-0: 5.0
    agent-1: 1.0
    agent-2: 16.0
    agent-3: 8.0
    agent-4: 5.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.19154952761432
    mean_inference_ms: 15.056542312176386
    mean_processing_ms: 73.29723278568027
  time_since_restore: 37485.23430085182
  time_this_iter_s: 145.45123410224915
  time_total_s: 126421.91243052483
  timestamp: 1637400511
  timesteps_since_restore: 24768000
  timesteps_this_iter: 96000
  timesteps_total: 87168000
  training_iteration: 908
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    908 |           126422 | 87168000 |    128.1 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 6.86
    apples_agent-0_min: 1
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.66
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 30.28
    apples_agent-2_min: 13
    apples_agent-3_max: 40
    apples_agent-3_mean: 8.98
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.27
    apples_agent-4_min: 3
    apples_agent-5_max: 14
    apples_agent-5_mean: 4.95
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 1.33
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 260
    cleaning_beam_agent-1_mean: 218.65
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 7.54
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 10.82
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.27
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 5.17
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-30-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 170.0
  episode_reward_mean: 130.86
  episode_reward_min: 70.0
  episodes_this_iter: 96
  episodes_total: 87264
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11883.918
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3067183494567871
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008444737177342176
        model: {}
        policy_loss: -0.0016576743219047785
        total_loss: -0.0020894536282867193
        vf_explained_var: 0.007471427321434021
        vf_loss: 1.0804264545440674
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20787453651428223
        entropy_coeff: 0.0017600000137463212
        kl: 0.001586703467182815
        model: {}
        policy_loss: -0.0015478618443012238
        total_loss: -0.0018571531400084496
        vf_explained_var: 0.10614576935768127
        vf_loss: 0.5656650066375732
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4306979477405548
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022006737999618053
        model: {}
        policy_loss: -0.002090341877192259
        total_loss: -0.0024603093042969704
        vf_explained_var: 0.010431304574012756
        vf_loss: 3.880626678466797
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.440657377243042
        entropy_coeff: 0.0017600000137463212
        kl: 0.001867785002104938
        model: {}
        policy_loss: -0.0015152706764638424
        total_loss: -0.0021063610911369324
        vf_explained_var: 0.009039655327796936
        vf_loss: 1.8446640968322754
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3772175908088684
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012269699946045876
        model: {}
        policy_loss: -0.0014926660805940628
        total_loss: -0.0020671123638749123
        vf_explained_var: 0.013123735785484314
        vf_loss: 0.8945499062538147
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5291868448257446
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014618607237935066
        model: {}
        policy_loss: -0.0016954648308455944
        total_loss: -0.002501088660210371
        vf_explained_var: 0.011159539222717285
        vf_loss: 1.257447600364685
    load_time_ms: 14157.73
    num_steps_sampled: 87264000
    num_steps_trained: 87264000
    sample_time_ms: 119041.975
    update_time_ms: 16.357
  iterations_since_restore: 259
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.029326923076926
    ram_util_percent: 12.385576923076922
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 22.0
    agent-2: 67.0
    agent-3: 42.0
    agent-4: 23.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 16.88
    agent-1: 9.63
    agent-2: 46.71
    agent-3: 25.23
    agent-4: 14.39
    agent-5: 18.02
  policy_reward_min:
    agent-0: 5.0
    agent-1: 4.0
    agent-2: 19.0
    agent-3: 0.0
    agent-4: 8.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.192284863596043
    mean_inference_ms: 15.05642085687823
    mean_processing_ms: 73.29736921395731
  time_since_restore: 37631.42571544647
  time_this_iter_s: 146.19141459465027
  time_total_s: 126568.10384511948
  timestamp: 1637400657
  timesteps_since_restore: 24864000
  timesteps_this_iter: 96000
  timesteps_total: 87264000
  training_iteration: 909
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    909 |           126568 | 87264000 |   130.86 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 7.0
    apples_agent-0_min: 2
    apples_agent-1_max: 13
    apples_agent-1_mean: 3.32
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 30.93
    apples_agent-2_min: 18
    apples_agent-3_max: 22
    apples_agent-3_mean: 8.89
    apples_agent-3_min: 1
    apples_agent-4_max: 35
    apples_agent-4_mean: 10.67
    apples_agent-4_min: 4
    apples_agent-5_max: 18
    apples_agent-5_mean: 5.39
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 15
    cleaning_beam_agent-0_mean: 1.23
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 249
    cleaning_beam_agent-1_mean: 214.99
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 6.99
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 9.7
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 2.51
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 5.3
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-33-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 169.0
  episode_reward_mean: 131.64
  episode_reward_min: 96.0
  episodes_this_iter: 96
  episodes_total: 87360
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11885.422
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30324143171310425
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005419211811386049
        model: {}
        policy_loss: -0.001583250006660819
        total_loss: -0.002004672773182392
        vf_explained_var: 0.011982068419456482
        vf_loss: 1.1228296756744385
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2066592276096344
        entropy_coeff: 0.0017600000137463212
        kl: 0.000831175479106605
        model: {}
        policy_loss: -0.0014849265571683645
        total_loss: -0.001797166420146823
        vf_explained_var: 0.08474934101104736
        vf_loss: 0.5147987008094788
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42623209953308105
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010734016541391611
        model: {}
        policy_loss: -0.0018984219059348106
        total_loss: -0.002267767209559679
        vf_explained_var: 0.02129530906677246
        vf_loss: 3.8082456588745117
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4388439655303955
        entropy_coeff: 0.0017600000137463212
        kl: 0.001643097260966897
        model: {}
        policy_loss: -0.0015130983665585518
        total_loss: -0.002108263783156872
        vf_explained_var: 0.0010241270065307617
        vf_loss: 1.771999478340149
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38448771834373474
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007724624592810869
        model: {}
        policy_loss: -0.0013543451204895973
        total_loss: -0.0019305869936943054
        vf_explained_var: 0.01477748155593872
        vf_loss: 1.0045584440231323
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5336505770683289
        entropy_coeff: 0.0017600000137463212
        kl: 0.001663635135628283
        model: {}
        policy_loss: -0.0016358080320060253
        total_loss: -0.0024371137842535973
        vf_explained_var: 0.004069685935974121
        vf_loss: 1.3791967630386353
    load_time_ms: 14153.053
    num_steps_sampled: 87360000
    num_steps_trained: 87360000
    sample_time_ms: 119060.892
    update_time_ms: 16.245
  iterations_since_restore: 260
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.79854368932039
    ram_util_percent: 12.454368932038832
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 17.0
    agent-2: 70.0
    agent-3: 41.0
    agent-4: 26.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 17.61
    agent-1: 8.72
    agent-2: 46.84
    agent-3: 24.37
    agent-4: 15.31
    agent-5: 18.79
  policy_reward_min:
    agent-0: 10.0
    agent-1: 1.0
    agent-2: 30.0
    agent-3: 11.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.19159951306542
    mean_inference_ms: 15.055869458988052
    mean_processing_ms: 73.29372409704106
  time_since_restore: 37775.72160720825
  time_this_iter_s: 144.29589176177979
  time_total_s: 126712.39973688126
  timestamp: 1637400802
  timesteps_since_restore: 24960000
  timesteps_this_iter: 96000
  timesteps_total: 87360000
  training_iteration: 910
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    910 |           126712 | 87360000 |   131.64 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 6.17
    apples_agent-0_min: 0
    apples_agent-1_max: 58
    apples_agent-1_mean: 3.55
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 27.8
    apples_agent-2_min: 9
    apples_agent-3_max: 27
    apples_agent-3_mean: 9.24
    apples_agent-3_min: 1
    apples_agent-4_max: 28
    apples_agent-4_mean: 10.26
    apples_agent-4_min: 2
    apples_agent-5_max: 20
    apples_agent-5_mean: 4.74
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 1.14
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 257
    cleaning_beam_agent-1_mean: 209.97
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 6.8
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 10.29
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.95
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 5.83
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-35-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 171.0
  episode_reward_mean: 124.64
  episode_reward_min: 52.0
  episodes_this_iter: 96
  episodes_total: 87456
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11872.094
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2985250651836395
        entropy_coeff: 0.0017600000137463212
        kl: 0.000920725055038929
        model: {}
        policy_loss: -0.0016940634232014418
        total_loss: -0.0021115210838615894
        vf_explained_var: 0.004145592451095581
        vf_loss: 1.0794484615325928
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2062447965145111
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011194408871233463
        model: {}
        policy_loss: -0.0016562645323574543
        total_loss: -0.001971249468624592
        vf_explained_var: 0.09260866045951843
        vf_loss: 0.4800575077533722
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43597325682640076
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010493859881535172
        model: {}
        policy_loss: -0.001859341049566865
        total_loss: -0.0022585697006434202
        vf_explained_var: 0.013520017266273499
        vf_loss: 3.680868625640869
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44299131631851196
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017822561785578728
        model: {}
        policy_loss: -0.001742357388138771
        total_loss: -0.0023185003083199263
        vf_explained_var: 0.009781047701835632
        vf_loss: 2.035220146179199
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3732355535030365
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011632894165813923
        model: {}
        policy_loss: -0.0015047502238303423
        total_loss: -0.0020651351660490036
        vf_explained_var: 0.01486186683177948
        vf_loss: 0.9650890827178955
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5267212390899658
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012593383435159922
        model: {}
        policy_loss: -0.0015675239264965057
        total_loss: -0.002370505826547742
        vf_explained_var: 0.010412216186523438
        vf_loss: 1.2404817342758179
    load_time_ms: 14145.731
    num_steps_sampled: 87456000
    num_steps_trained: 87456000
    sample_time_ms: 118971.642
    update_time_ms: 16.299
  iterations_since_restore: 261
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.80817307692308
    ram_util_percent: 12.451923076923073
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 61.0
    agent-3: 50.0
    agent-4: 23.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 16.16
    agent-1: 8.43
    agent-2: 42.88
    agent-3: 25.77
    agent-4: 14.14
    agent-5: 17.26
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 12.0
    agent-3: 11.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.19137990588586
    mean_inference_ms: 15.05560405003097
    mean_processing_ms: 73.29294971885496
  time_since_restore: 37919.89713096619
  time_this_iter_s: 144.17552375793457
  time_total_s: 126856.57526063919
  timestamp: 1637400948
  timesteps_since_restore: 25056000
  timesteps_this_iter: 96000
  timesteps_total: 87456000
  training_iteration: 911
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    911 |           126857 | 87456000 |   124.64 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 7.33
    apples_agent-0_min: 1
    apples_agent-1_max: 33
    apples_agent-1_mean: 4.2
    apples_agent-1_min: 0
    apples_agent-2_max: 54
    apples_agent-2_mean: 29.55
    apples_agent-2_min: 10
    apples_agent-3_max: 36
    apples_agent-3_mean: 9.3
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.69
    apples_agent-4_min: 1
    apples_agent-5_max: 21
    apples_agent-5_mean: 4.23
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.47
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 250
    cleaning_beam_agent-1_mean: 213.14
    cleaning_beam_agent-1_min: 46
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 6.44
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 23
    cleaning_beam_agent-3_mean: 10.43
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.82
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.61
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-38-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 177.0
  episode_reward_mean: 130.08
  episode_reward_min: 32.0
  episodes_this_iter: 96
  episodes_total: 87552
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11870.437
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29701074957847595
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015677197370678186
        model: {}
        policy_loss: -0.0017727520316839218
        total_loss: -0.0021810056641697884
        vf_explained_var: 0.006749317049980164
        vf_loss: 1.1448853015899658
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20159319043159485
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007033655419945717
        model: {}
        policy_loss: -0.0013011414557695389
        total_loss: -0.0015981164760887623
        vf_explained_var: 0.08043944835662842
        vf_loss: 0.5782940983772278
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4363012909889221
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015108076622709632
        model: {}
        policy_loss: -0.0020131664350628853
        total_loss: -0.0023790630511939526
        vf_explained_var: 0.028530403971672058
        vf_loss: 4.019921779632568
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44472309947013855
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011748343240469694
        model: {}
        policy_loss: -0.0013370243832468987
        total_loss: -0.0019198870286345482
        vf_explained_var: 0.0015666931867599487
        vf_loss: 1.9985032081604004
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36376309394836426
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010407138615846634
        model: {}
        policy_loss: -0.0014413774479180574
        total_loss: -0.0019678939133882523
        vf_explained_var: 0.0032802075147628784
        vf_loss: 1.1370511054992676
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5253626704216003
        entropy_coeff: 0.0017600000137463212
        kl: 0.002002927241846919
        model: {}
        policy_loss: -0.00156440120190382
        total_loss: -0.002365092048421502
        vf_explained_var: 0.010576024651527405
        vf_loss: 1.2394866943359375
    load_time_ms: 14121.896
    num_steps_sampled: 87552000
    num_steps_trained: 87552000
    sample_time_ms: 119087.038
    update_time_ms: 16.272
  iterations_since_restore: 262
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.987019230769228
    ram_util_percent: 12.368749999999999
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 19.0
    agent-2: 71.0
    agent-3: 38.0
    agent-4: 30.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 16.71
    agent-1: 9.08
    agent-2: 44.76
    agent-3: 26.15
    agent-4: 15.7
    agent-5: 17.68
  policy_reward_min:
    agent-0: 3.0
    agent-1: 2.0
    agent-2: 13.0
    agent-3: 2.0
    agent-4: 2.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.191971712634086
    mean_inference_ms: 15.055777341758342
    mean_processing_ms: 73.29335682086582
  time_since_restore: 38065.50178742409
  time_this_iter_s: 145.604656457901
  time_total_s: 127002.17991709709
  timestamp: 1637401094
  timesteps_since_restore: 25152000
  timesteps_this_iter: 96000
  timesteps_total: 87552000
  training_iteration: 912
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    912 |           127002 | 87552000 |   130.08 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 7.4
    apples_agent-0_min: 0
    apples_agent-1_max: 33
    apples_agent-1_mean: 3.61
    apples_agent-1_min: 0
    apples_agent-2_max: 55
    apples_agent-2_mean: 29.84
    apples_agent-2_min: 15
    apples_agent-3_max: 51
    apples_agent-3_mean: 9.87
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.82
    apples_agent-4_min: 2
    apples_agent-5_max: 14
    apples_agent-5_mean: 4.44
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 1.4
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 279
    cleaning_beam_agent-1_mean: 214.58
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 7.3
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 12.15
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 18
    cleaning_beam_agent-4_mean: 3.16
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.46
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-40-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 170.0
  episode_reward_mean: 130.37
  episode_reward_min: 81.0
  episodes_this_iter: 96
  episodes_total: 87648
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11862.002
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3009703457355499
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009115368593484163
        model: {}
        policy_loss: -0.0015911189839243889
        total_loss: -0.0020179366692900658
        vf_explained_var: 0.005814149975776672
        vf_loss: 1.0288926362991333
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20695166289806366
        entropy_coeff: 0.0017600000137463212
        kl: 0.00114806171040982
        model: {}
        policy_loss: -0.0015248628333210945
        total_loss: -0.0018327012658119202
        vf_explained_var: 0.07710099220275879
        vf_loss: 0.5639585852622986
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4401915371417999
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016423733904957771
        model: {}
        policy_loss: -0.0020210600923746824
        total_loss: -0.0024025533348321915
        vf_explained_var: 0.022339284420013428
        vf_loss: 3.932420015335083
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4476124048233032
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016679232940077782
        model: {}
        policy_loss: -0.001500487793236971
        total_loss: -0.002090423833578825
        vf_explained_var: 0.0038764476776123047
        vf_loss: 1.9785981178283691
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3664679527282715
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005021098768338561
        model: {}
        policy_loss: -0.0011876854114234447
        total_loss: -0.0017266510985791683
        vf_explained_var: 0.01137840747833252
        vf_loss: 1.0601292848587036
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5333458185195923
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011763432994484901
        model: {}
        policy_loss: -0.0016585171688348055
        total_loss: -0.002470777602866292
        vf_explained_var: 0.020347073674201965
        vf_loss: 1.2642757892608643
    load_time_ms: 14111.698
    num_steps_sampled: 87648000
    num_steps_trained: 87648000
    sample_time_ms: 119054.493
    update_time_ms: 16.259
  iterations_since_restore: 263
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.036585365853657
    ram_util_percent: 12.458048780487804
  pid: 27065
  policy_reward_max:
    agent-0: 25.0
    agent-1: 23.0
    agent-2: 67.0
    agent-3: 39.0
    agent-4: 29.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 16.15
    agent-1: 9.15
    agent-2: 45.7
    agent-3: 26.14
    agent-4: 15.03
    agent-5: 18.2
  policy_reward_min:
    agent-0: 3.0
    agent-1: 4.0
    agent-2: 24.0
    agent-3: 12.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.191529256697333
    mean_inference_ms: 15.055519433613535
    mean_processing_ms: 73.29151577131297
  time_since_restore: 38209.55436420441
  time_this_iter_s: 144.0525767803192
  time_total_s: 127146.23249387741
  timestamp: 1637401238
  timesteps_since_restore: 25248000
  timesteps_this_iter: 96000
  timesteps_total: 87648000
  training_iteration: 913
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    913 |           127146 | 87648000 |   130.37 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 6.46
    apples_agent-0_min: 1
    apples_agent-1_max: 23
    apples_agent-1_mean: 3.28
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 29.26
    apples_agent-2_min: 10
    apples_agent-3_max: 59
    apples_agent-3_mean: 10.16
    apples_agent-3_min: 1
    apples_agent-4_max: 25
    apples_agent-4_mean: 10.4
    apples_agent-4_min: 2
    apples_agent-5_max: 16
    apples_agent-5_mean: 5.02
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 1.12
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 256
    cleaning_beam_agent-1_mean: 210.0
    cleaning_beam_agent-1_min: 50
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 8.62
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 11.13
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 25
    cleaning_beam_agent-4_mean: 3.93
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 4.32
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-43-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 165.0
  episode_reward_mean: 127.77
  episode_reward_min: 40.0
  episodes_this_iter: 96
  episodes_total: 87744
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11862.038
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3000136613845825
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009528006776235998
        model: {}
        policy_loss: -0.0018209267873317003
        total_loss: -0.0022415434941649437
        vf_explained_var: 0.0033161044120788574
        vf_loss: 1.0740699768066406
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2013472616672516
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012032855302095413
        model: {}
        policy_loss: -0.0015844902954995632
        total_loss: -0.0018802331760525703
        vf_explained_var: 0.08921988308429718
        vf_loss: 0.5863009095191956
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4448450803756714
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013719041598960757
        model: {}
        policy_loss: -0.0018382142297923565
        total_loss: -0.002222636714577675
        vf_explained_var: 0.01163870096206665
        vf_loss: 3.985050916671753
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44668853282928467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016253743087872863
        model: {}
        policy_loss: -0.0015530707314610481
        total_loss: -0.0021581463515758514
        vf_explained_var: 0.0024368464946746826
        vf_loss: 1.810987949371338
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3731136620044708
        entropy_coeff: 0.0017600000137463212
        kl: 0.000874800025485456
        model: {}
        policy_loss: -0.0015354612842202187
        total_loss: -0.002094677649438381
        vf_explained_var: 0.01526273787021637
        vf_loss: 0.9746595621109009
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5443879961967468
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019816444255411625
        model: {}
        policy_loss: -0.0017703371122479439
        total_loss: -0.0025958726182579994
        vf_explained_var: 0.013820558786392212
        vf_loss: 1.3258874416351318
    load_time_ms: 14135.031
    num_steps_sampled: 87744000
    num_steps_trained: 87744000
    sample_time_ms: 119026.837
    update_time_ms: 16.281
  iterations_since_restore: 264
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.90576923076923
    ram_util_percent: 12.459615384615383
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 64.0
    agent-3: 41.0
    agent-4: 25.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 16.36
    agent-1: 9.11
    agent-2: 44.87
    agent-3: 24.93
    agent-4: 14.09
    agent-5: 18.41
  policy_reward_min:
    agent-0: 4.0
    agent-1: 2.0
    agent-2: 11.0
    agent-3: 8.0
    agent-4: 3.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.191902993887982
    mean_inference_ms: 15.055568078691376
    mean_processing_ms: 73.29240101609221
  time_since_restore: 38355.524530887604
  time_this_iter_s: 145.97016668319702
  time_total_s: 127292.20266056061
  timestamp: 1637401384
  timesteps_since_restore: 25344000
  timesteps_this_iter: 96000
  timesteps_total: 87744000
  training_iteration: 914
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    914 |           127292 | 87744000 |   127.77 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 6.85
    apples_agent-0_min: 1
    apples_agent-1_max: 37
    apples_agent-1_mean: 3.92
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 29.6
    apples_agent-2_min: 17
    apples_agent-3_max: 31
    apples_agent-3_mean: 8.8
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.01
    apples_agent-4_min: 3
    apples_agent-5_max: 14
    apples_agent-5_mean: 4.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.36
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 247
    cleaning_beam_agent-1_mean: 211.29
    cleaning_beam_agent-1_min: 150
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 8.22
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 30
    cleaning_beam_agent-3_mean: 11.22
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 2.95
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.57
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-45-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 173.0
  episode_reward_mean: 128.22
  episode_reward_min: 88.0
  episodes_this_iter: 96
  episodes_total: 87840
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11861.317
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2966141700744629
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009505792986601591
        model: {}
        policy_loss: -0.0017298590391874313
        total_loss: -0.0021382274571806192
        vf_explained_var: 0.007261753082275391
        vf_loss: 1.1367454528808594
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20604433119297028
        entropy_coeff: 0.0017600000137463212
        kl: 0.001072385930456221
        model: {}
        policy_loss: -0.0015345591818913817
        total_loss: -0.0018427885370329022
        vf_explained_var: 0.07998929917812347
        vf_loss: 0.5440889596939087
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4459396004676819
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013091058935970068
        model: {}
        policy_loss: -0.0018521170131862164
        total_loss: -0.002264943439513445
        vf_explained_var: 0.016458794474601746
        vf_loss: 3.7202746868133545
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4427071213722229
        entropy_coeff: 0.0017600000137463212
        kl: 0.001638659741729498
        model: {}
        policy_loss: -0.0016279155388474464
        total_loss: -0.0022145742550492287
        vf_explained_var: 0.0043957531452178955
        vf_loss: 1.925081729888916
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36845093965530396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005374207976274192
        model: {}
        policy_loss: -0.0012643937952816486
        total_loss: -0.0018137102015316486
        vf_explained_var: 0.01773644983768463
        vf_loss: 0.9916200637817383
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.552789568901062
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014601154252886772
        model: {}
        policy_loss: -0.0018931562080979347
        total_loss: -0.0027411109767854214
        vf_explained_var: 0.023773401975631714
        vf_loss: 1.249556303024292
    load_time_ms: 14117.37
    num_steps_sampled: 87840000
    num_steps_trained: 87840000
    sample_time_ms: 119077.175
    update_time_ms: 16.658
  iterations_since_restore: 265
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.96699029126214
    ram_util_percent: 12.465048543689317
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 19.0
    agent-2: 62.0
    agent-3: 44.0
    agent-4: 22.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 16.37
    agent-1: 8.86
    agent-2: 44.69
    agent-3: 25.45
    agent-4: 14.44
    agent-5: 18.41
  policy_reward_min:
    agent-0: 8.0
    agent-1: 3.0
    agent-2: 27.0
    agent-3: 14.0
    agent-4: 5.0
    agent-5: 10.0
  sampler_perf:
    mean_env_wait_ms: 28.192443785346182
    mean_inference_ms: 15.055416278016418
    mean_processing_ms: 73.29110893030752
  time_since_restore: 38500.17279148102
  time_this_iter_s: 144.6482605934143
  time_total_s: 127436.85092115402
  timestamp: 1637401529
  timesteps_since_restore: 25440000
  timesteps_this_iter: 96000
  timesteps_total: 87840000
  training_iteration: 915
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    915 |           127437 | 87840000 |   128.22 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 7.46
    apples_agent-0_min: 1
    apples_agent-1_max: 21
    apples_agent-1_mean: 3.73
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 31.13
    apples_agent-2_min: 15
    apples_agent-3_max: 19
    apples_agent-3_mean: 8.4
    apples_agent-3_min: 2
    apples_agent-4_max: 19
    apples_agent-4_mean: 10.6
    apples_agent-4_min: 3
    apples_agent-5_max: 15
    apples_agent-5_mean: 4.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.21
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 247
    cleaning_beam_agent-1_mean: 215.17
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 8.94
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 11.78
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.2
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.6
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-47-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 173.0
  episode_reward_mean: 130.99
  episode_reward_min: 44.0
  episodes_this_iter: 96
  episodes_total: 87936
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11837.974
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30136018991470337
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009361988049931824
        model: {}
        policy_loss: -0.0016529819695279002
        total_loss: -0.0020743473432958126
        vf_explained_var: 0.009573489427566528
        vf_loss: 1.0902773141860962
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20298349857330322
        entropy_coeff: 0.0017600000137463212
        kl: 0.001057082205079496
        model: {}
        policy_loss: -0.0016965526156127453
        total_loss: -0.0019974461756646633
        vf_explained_var: 0.07073065638542175
        vf_loss: 0.5635834336280823
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.434097021818161
        entropy_coeff: 0.0017600000137463212
        kl: 0.001589663326740265
        model: {}
        policy_loss: -0.0019224912393838167
        total_loss: -0.0022989967837929726
        vf_explained_var: 0.022848740220069885
        vf_loss: 3.8750417232513428
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.443971186876297
        entropy_coeff: 0.0017600000137463212
        kl: 0.001037082402035594
        model: {}
        policy_loss: -0.001455487683415413
        total_loss: -0.0020560873672366142
        vf_explained_var: 0.0036736875772476196
        vf_loss: 1.8078835010528564
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3580193519592285
        entropy_coeff: 0.0017600000137463212
        kl: 0.001118428073823452
        model: {}
        policy_loss: -0.0015211200807243586
        total_loss: -0.0020542005077004433
        vf_explained_var: 0.011599630117416382
        vf_loss: 0.9703385233879089
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5552116632461548
        entropy_coeff: 0.0017600000137463212
        kl: 0.002563092391937971
        model: {}
        policy_loss: -0.0019910933915525675
        total_loss: -0.0028337242547422647
        vf_explained_var: 0.016844511032104492
        vf_loss: 1.345360517501831
    load_time_ms: 14122.71
    num_steps_sampled: 87936000
    num_steps_trained: 87936000
    sample_time_ms: 118958.724
    update_time_ms: 16.481
  iterations_since_restore: 266
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.9495145631068
    ram_util_percent: 12.452427184466018
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 65.0
    agent-3: 40.0
    agent-4: 27.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 17.04
    agent-1: 9.51
    agent-2: 46.28
    agent-3: 24.89
    agent-4: 15.06
    agent-5: 18.21
  policy_reward_min:
    agent-0: 2.0
    agent-1: 2.0
    agent-2: 21.0
    agent-3: 10.0
    agent-4: 5.0
    agent-5: 2.0
  sampler_perf:
    mean_env_wait_ms: 28.19284550063344
    mean_inference_ms: 15.05499219428936
    mean_processing_ms: 73.29038161654799
  time_since_restore: 38644.99512887001
  time_this_iter_s: 144.8223373889923
  time_total_s: 127581.67325854301
  timestamp: 1637401674
  timesteps_since_restore: 25536000
  timesteps_this_iter: 96000
  timesteps_total: 87936000
  training_iteration: 916
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    916 |           127582 | 87936000 |   130.99 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 6.56
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 3.54
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 29.51
    apples_agent-2_min: 7
    apples_agent-3_max: 45
    apples_agent-3_mean: 8.87
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 9.46
    apples_agent-4_min: 2
    apples_agent-5_max: 17
    apples_agent-5_mean: 5.08
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.36
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 265
    cleaning_beam_agent-1_mean: 210.55
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 9.37
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 33
    cleaning_beam_agent-3_mean: 12.16
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 14
    cleaning_beam_agent-4_mean: 3.23
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 4.79
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-50-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 181.0
  episode_reward_mean: 126.69
  episode_reward_min: 36.0
  episodes_this_iter: 96
  episodes_total: 88032
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11817.554
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30233073234558105
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009372124914079905
        model: {}
        policy_loss: -0.0015979981981217861
        total_loss: -0.0020201136358082294
        vf_explained_var: 0.01099829375743866
        vf_loss: 1.0998361110687256
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19991537928581238
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014105462469160557
        model: {}
        policy_loss: -0.0015626443782821298
        total_loss: -0.0018540456658229232
        vf_explained_var: 0.08012309670448303
        vf_loss: 0.6045275330543518
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4426984190940857
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009789816103875637
        model: {}
        policy_loss: -0.001802629791200161
        total_loss: -0.0021854564547538757
        vf_explained_var: 0.017985761165618896
        vf_loss: 3.96321177482605
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4438306391239166
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013810405507683754
        model: {}
        policy_loss: -0.0015464936150237918
        total_loss: -0.0021278243511915207
        vf_explained_var: -0.0035418272018432617
        vf_loss: 1.9981356859207153
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35979777574539185
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006824078154750168
        model: {}
        policy_loss: -0.0014722957275807858
        total_loss: -0.002004374749958515
        vf_explained_var: 0.01569904386997223
        vf_loss: 1.011668086051941
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.568088173866272
        entropy_coeff: 0.0017600000137463212
        kl: 0.002193177817389369
        model: {}
        policy_loss: -0.0018115377752110362
        total_loss: -0.002690231893211603
        vf_explained_var: 0.01301787793636322
        vf_loss: 1.2114198207855225
    load_time_ms: 14105.838
    num_steps_sampled: 88032000
    num_steps_trained: 88032000
    sample_time_ms: 119027.229
    update_time_ms: 16.609
  iterations_since_restore: 267
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.995169082125603
    ram_util_percent: 12.457004830917871
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 67.0
    agent-3: 40.0
    agent-4: 32.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 16.6
    agent-1: 8.92
    agent-2: 45.32
    agent-3: 24.55
    agent-4: 14.15
    agent-5: 17.15
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 13.0
    agent-3: 6.0
    agent-4: 1.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.193276698382707
    mean_inference_ms: 15.055003877881338
    mean_processing_ms: 73.29114899946384
  time_since_restore: 38790.294543504715
  time_this_iter_s: 145.2994146347046
  time_total_s: 127726.97267317772
  timestamp: 1637401819
  timesteps_since_restore: 25632000
  timesteps_this_iter: 96000
  timesteps_total: 88032000
  training_iteration: 917
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    917 |           127727 | 88032000 |   126.69 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 7.27
    apples_agent-0_min: 1
    apples_agent-1_max: 14
    apples_agent-1_mean: 3.35
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 31.51
    apples_agent-2_min: 20
    apples_agent-3_max: 30
    apples_agent-3_mean: 9.25
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.69
    apples_agent-4_min: 2
    apples_agent-5_max: 21
    apples_agent-5_mean: 5.42
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.42
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 271
    cleaning_beam_agent-1_mean: 217.66
    cleaning_beam_agent-1_min: 155
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 8.18
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 13.05
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.18
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 5.15
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-52-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 170.0
  episode_reward_mean: 133.72
  episode_reward_min: 101.0
  episodes_this_iter: 96
  episodes_total: 88128
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11814.107
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29691174626350403
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009800426196306944
        model: {}
        policy_loss: -0.0018077099230140448
        total_loss: -0.002230330603197217
        vf_explained_var: 0.010929793119430542
        vf_loss: 0.9994422793388367
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20317915081977844
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011478892993181944
        model: {}
        policy_loss: -0.0013718437403440475
        total_loss: -0.0016719880513846874
        vf_explained_var: 0.07633084058761597
        vf_loss: 0.5745097994804382
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4409496784210205
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011677357833832502
        model: {}
        policy_loss: -0.0018340782262384892
        total_loss: -0.00219359016045928
        vf_explained_var: 0.019236087799072266
        vf_loss: 4.165627479553223
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44232407212257385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011354601010680199
        model: {}
        policy_loss: -0.0012447199551388621
        total_loss: -0.0018097758293151855
        vf_explained_var: 0.007528096437454224
        vf_loss: 2.134345531463623
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3601701259613037
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008203407051041722
        model: {}
        policy_loss: -0.0015058373101055622
        total_loss: -0.0020280538592487574
        vf_explained_var: 0.011619359254837036
        vf_loss: 1.1167941093444824
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5699396133422852
        entropy_coeff: 0.0017600000137463212
        kl: 0.002409342909231782
        model: {}
        policy_loss: -0.0018740948289632797
        total_loss: -0.002745053730905056
        vf_explained_var: 0.018551424145698547
        vf_loss: 1.3213744163513184
    load_time_ms: 14145.145
    num_steps_sampled: 88128000
    num_steps_trained: 88128000
    sample_time_ms: 119004.069
    update_time_ms: 16.49
  iterations_since_restore: 268
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.872115384615384
    ram_util_percent: 12.462019230769227
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 68.0
    agent-3: 50.0
    agent-4: 25.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 16.3
    agent-1: 9.33
    agent-2: 47.82
    agent-3: 26.22
    agent-4: 15.3
    agent-5: 18.75
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 32.0
    agent-3: 6.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.19379513417235
    mean_inference_ms: 15.054949762572864
    mean_processing_ms: 73.29055476135358
  time_since_restore: 38935.92034339905
  time_this_iter_s: 145.62579989433289
  time_total_s: 127872.59847307205
  timestamp: 1637401965
  timesteps_since_restore: 25728000
  timesteps_this_iter: 96000
  timesteps_total: 88128000
  training_iteration: 918
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    918 |           127873 | 88128000 |   133.72 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 7.28
    apples_agent-0_min: 1
    apples_agent-1_max: 17
    apples_agent-1_mean: 3.32
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 30.38
    apples_agent-2_min: 4
    apples_agent-3_max: 44
    apples_agent-3_mean: 9.39
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.11
    apples_agent-4_min: 2
    apples_agent-5_max: 21
    apples_agent-5_mean: 5.05
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 1.64
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 307
    cleaning_beam_agent-1_mean: 221.34
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 8.36
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 13.36
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.27
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 5.9
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-55-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 176.0
  episode_reward_mean: 130.63
  episode_reward_min: 50.0
  episodes_this_iter: 96
  episodes_total: 88224
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11801.571
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.300682008266449
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009303209371864796
        model: {}
        policy_loss: -0.0017733033746480942
        total_loss: -0.002194963861256838
        vf_explained_var: 0.011136576533317566
        vf_loss: 1.0754239559173584
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2028062492609024
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012749724555760622
        model: {}
        policy_loss: -0.001575808273628354
        total_loss: -0.001874337438493967
        vf_explained_var: 0.08622674643993378
        vf_loss: 0.5840691328048706
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4413589835166931
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014876234345138073
        model: {}
        policy_loss: -0.001919290516525507
        total_loss: -0.0022918712347745895
        vf_explained_var: 0.012420400977134705
        vf_loss: 4.042113304138184
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44777873158454895
        entropy_coeff: 0.0017600000137463212
        kl: 0.00197619735263288
        model: {}
        policy_loss: -0.0018605063669383526
        total_loss: -0.0024564925115555525
        vf_explained_var: 0.01026831567287445
        vf_loss: 1.9210047721862793
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3610246181488037
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007072122534736991
        model: {}
        policy_loss: -0.0014570336788892746
        total_loss: -0.0020007435232400894
        vf_explained_var: 0.016210824251174927
        vf_loss: 0.9169376492500305
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5765483379364014
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017035547643899918
        model: {}
        policy_loss: -0.0016919891349971294
        total_loss: -0.0025743742007762194
        vf_explained_var: 0.01954418420791626
        vf_loss: 1.3233753442764282
    load_time_ms: 14162.34
    num_steps_sampled: 88224000
    num_steps_trained: 88224000
    sample_time_ms: 118899.945
    update_time_ms: 16.464
  iterations_since_restore: 269
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.017475728155343
    ram_util_percent: 12.451941747572812
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 18.0
    agent-2: 66.0
    agent-3: 42.0
    agent-4: 24.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 16.85
    agent-1: 9.36
    agent-2: 46.21
    agent-3: 25.32
    agent-4: 14.69
    agent-5: 18.2
  policy_reward_min:
    agent-0: 8.0
    agent-1: 3.0
    agent-2: 11.0
    agent-3: 9.0
    agent-4: 5.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.19410241470192
    mean_inference_ms: 15.05487156068338
    mean_processing_ms: 73.29055557868477
  time_since_restore: 39081.046999931335
  time_this_iter_s: 145.1266565322876
  time_total_s: 128017.72512960434
  timestamp: 1637402110
  timesteps_since_restore: 25824000
  timesteps_this_iter: 96000
  timesteps_total: 88224000
  training_iteration: 919
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    919 |           128018 | 88224000 |   130.63 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 8.07
    apples_agent-0_min: 2
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.5
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 31.96
    apples_agent-2_min: 9
    apples_agent-3_max: 44
    apples_agent-3_mean: 9.26
    apples_agent-3_min: 3
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.9
    apples_agent-4_min: 5
    apples_agent-5_max: 31
    apples_agent-5_mean: 5.49
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.1
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 260
    cleaning_beam_agent-1_mean: 225.44
    cleaning_beam_agent-1_min: 176
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 7.96
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 13.29
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.31
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 17
    cleaning_beam_agent-5_mean: 6.28
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_04-57-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 179.0
  episode_reward_mean: 137.1
  episode_reward_min: 96.0
  episodes_this_iter: 96
  episodes_total: 88320
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11801.606
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29724863171577454
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007136346539482474
        model: {}
        policy_loss: -0.00158797949552536
        total_loss: -0.001991956029087305
        vf_explained_var: 0.007213905453681946
        vf_loss: 1.1918102502822876
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20554645359516144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009718980873003602
        model: {}
        policy_loss: -0.0015987892402336001
        total_loss: -0.001903144409880042
        vf_explained_var: 0.0811760276556015
        vf_loss: 0.5740595459938049
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43465423583984375
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011073604691773653
        model: {}
        policy_loss: -0.0018358533270657063
        total_loss: -0.00218617869541049
        vf_explained_var: -0.0008079856634140015
        vf_loss: 4.146666049957275
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44405364990234375
        entropy_coeff: 0.0017600000137463212
        kl: 0.001362063456326723
        model: {}
        policy_loss: -0.0015166671946644783
        total_loss: -0.0021042772568762302
        vf_explained_var: 0.01581716537475586
        vf_loss: 1.9392144680023193
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.358267605304718
        entropy_coeff: 0.0017600000137463212
        kl: 0.000500120222568512
        model: {}
        policy_loss: -0.001240994781255722
        total_loss: -0.0017617162084206939
        vf_explained_var: 0.019221216440200806
        vf_loss: 1.0982850790023804
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5753434896469116
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016637439839541912
        model: {}
        policy_loss: -0.0016866056248545647
        total_loss: -0.002568366937339306
        vf_explained_var: 0.006727144122123718
        vf_loss: 1.3084276914596558
    load_time_ms: 14158.748
    num_steps_sampled: 88320000
    num_steps_trained: 88320000
    sample_time_ms: 118900.887
    update_time_ms: 16.914
  iterations_since_restore: 270
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.991747572815534
    ram_util_percent: 12.460679611650484
  pid: 27065
  policy_reward_max:
    agent-0: 34.0
    agent-1: 21.0
    agent-2: 64.0
    agent-3: 43.0
    agent-4: 32.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 17.94
    agent-1: 9.46
    agent-2: 48.3
    agent-3: 26.63
    agent-4: 16.12
    agent-5: 18.65
  policy_reward_min:
    agent-0: 9.0
    agent-1: 2.0
    agent-2: 14.0
    agent-3: 13.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.19446525542653
    mean_inference_ms: 15.054669326917681
    mean_processing_ms: 73.29039888486777
  time_since_restore: 39225.32986330986
  time_this_iter_s: 144.28286337852478
  time_total_s: 128162.00799298286
  timestamp: 1637402255
  timesteps_since_restore: 25920000
  timesteps_this_iter: 96000
  timesteps_total: 88320000
  training_iteration: 920
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    920 |           128162 | 88320000 |    137.1 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 6.9
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.08
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 31.63
    apples_agent-2_min: 16
    apples_agent-3_max: 20
    apples_agent-3_mean: 8.56
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.38
    apples_agent-4_min: 2
    apples_agent-5_max: 31
    apples_agent-5_mean: 5.17
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.17
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 256
    cleaning_beam_agent-1_mean: 224.81
    cleaning_beam_agent-1_min: 193
    cleaning_beam_agent-2_max: 30
    cleaning_beam_agent-2_mean: 8.37
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 11.0
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.54
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 5.98
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-00-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 191.0
  episode_reward_mean: 134.12
  episode_reward_min: 74.0
  episodes_this_iter: 96
  episodes_total: 88416
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11805.774
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3118072748184204
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013057723408564925
        model: {}
        policy_loss: -0.0017527146264910698
        total_loss: -0.002186593133956194
        vf_explained_var: 0.006406649947166443
        vf_loss: 1.14902925491333
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20427802205085754
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009131142869591713
        model: {}
        policy_loss: -0.001313050277531147
        total_loss: -0.001618571113795042
        vf_explained_var: 0.08350072801113129
        vf_loss: 0.5400673151016235
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44601574540138245
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011352702276781201
        model: {}
        policy_loss: -0.0017991787753999233
        total_loss: -0.0021848436444997787
        vf_explained_var: 0.01857037842273712
        vf_loss: 3.9932382106781006
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4398499131202698
        entropy_coeff: 0.0017600000137463212
        kl: 0.000915602024178952
        model: {}
        policy_loss: -0.0014234217815101147
        total_loss: -0.0020111692138016224
        vf_explained_var: 0.006272479891777039
        vf_loss: 1.863898515701294
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3608860969543457
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007923527155071497
        model: {}
        policy_loss: -0.0013669747859239578
        total_loss: -0.0019049905240535736
        vf_explained_var: 0.02660021185874939
        vf_loss: 0.9714450836181641
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5644974112510681
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018320163944736123
        model: {}
        policy_loss: -0.0015800311230123043
        total_loss: -0.002439658623188734
        vf_explained_var: 0.014015048742294312
        vf_loss: 1.3388793468475342
    load_time_ms: 14162.452
    num_steps_sampled: 88416000
    num_steps_trained: 88416000
    sample_time_ms: 119027.212
    update_time_ms: 16.731
  iterations_since_restore: 271
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.657819905213266
    ram_util_percent: 12.446445497630329
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 16.0
    agent-2: 64.0
    agent-3: 40.0
    agent-4: 26.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 17.13
    agent-1: 9.23
    agent-2: 47.6
    agent-3: 26.34
    agent-4: 14.84
    agent-5: 18.98
  policy_reward_min:
    agent-0: 9.0
    agent-1: 1.0
    agent-2: 24.0
    agent-3: 9.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.194922457892122
    mean_inference_ms: 15.054860913187085
    mean_processing_ms: 73.29050921220932
  time_since_restore: 39370.89193677902
  time_this_iter_s: 145.562073469162
  time_total_s: 128307.57006645203
  timestamp: 1637402402
  timesteps_since_restore: 26016000
  timesteps_this_iter: 96000
  timesteps_total: 88416000
  training_iteration: 921
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    921 |           128308 | 88416000 |   134.12 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 7.19
    apples_agent-0_min: 1
    apples_agent-1_max: 16
    apples_agent-1_mean: 3.47
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 30.45
    apples_agent-2_min: 16
    apples_agent-3_max: 21
    apples_agent-3_mean: 9.78
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.24
    apples_agent-4_min: 1
    apples_agent-5_max: 18
    apples_agent-5_mean: 5.27
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.26
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 264
    cleaning_beam_agent-1_mean: 221.07
    cleaning_beam_agent-1_min: 59
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 9.71
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 11.33
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.17
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 6.43
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-02-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 182.0
  episode_reward_mean: 132.65
  episode_reward_min: 63.0
  episodes_this_iter: 96
  episodes_total: 88512
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11785.681
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3138538599014282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010266356403008103
        model: {}
        policy_loss: -0.0016667312011122704
        total_loss: -0.0021026604808866978
        vf_explained_var: 0.0055478960275650024
        vf_loss: 1.164543867111206
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2016952931880951
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012449921341612935
        model: {}
        policy_loss: -0.0015997348818928003
        total_loss: -0.0018974279519170523
        vf_explained_var: 0.07976311445236206
        vf_loss: 0.5729390382766724
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4416946470737457
        entropy_coeff: 0.0017600000137463212
        kl: 0.001125522656366229
        model: {}
        policy_loss: -0.001669188030064106
        total_loss: -0.0020507704466581345
        vf_explained_var: 0.02607123553752899
        vf_loss: 3.9580326080322266
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43950462341308594
        entropy_coeff: 0.0017600000137463212
        kl: 0.001327920239418745
        model: {}
        policy_loss: -0.0015057427808642387
        total_loss: -0.0020627612248063087
        vf_explained_var: 0.013456776738166809
        vf_loss: 2.1651031970977783
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35624152421951294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015766870928928256
        model: {}
        policy_loss: -0.0016872738488018513
        total_loss: -0.002212825696915388
        vf_explained_var: 0.013361960649490356
        vf_loss: 1.0143373012542725
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5704047679901123
        entropy_coeff: 0.0017600000137463212
        kl: 0.001165391760878265
        model: {}
        policy_loss: -0.0016819098964333534
        total_loss: -0.0025557270273566246
        vf_explained_var: 0.02283337712287903
        vf_loss: 1.3009979724884033
    load_time_ms: 14159.223
    num_steps_sampled: 88512000
    num_steps_trained: 88512000
    sample_time_ms: 118838.246
    update_time_ms: 16.779
  iterations_since_restore: 272
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.175980392156863
    ram_util_percent: 12.482352941176469
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 21.0
    agent-2: 62.0
    agent-3: 41.0
    agent-4: 26.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 16.93
    agent-1: 9.46
    agent-2: 46.53
    agent-3: 27.07
    agent-4: 14.18
    agent-5: 18.48
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 24.0
    agent-3: 4.0
    agent-4: 6.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.19416963390971
    mean_inference_ms: 15.054709646783845
    mean_processing_ms: 73.2877348653442
  time_since_restore: 39514.32943511009
  time_this_iter_s: 143.43749833106995
  time_total_s: 128451.0075647831
  timestamp: 1637402546
  timesteps_since_restore: 26112000
  timesteps_this_iter: 96000
  timesteps_total: 88512000
  training_iteration: 922
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    922 |           128451 | 88512000 |   132.65 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 7.35
    apples_agent-0_min: 2
    apples_agent-1_max: 11
    apples_agent-1_mean: 2.98
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 30.6
    apples_agent-2_min: 17
    apples_agent-3_max: 18
    apples_agent-3_mean: 9.4
    apples_agent-3_min: 1
    apples_agent-4_max: 17
    apples_agent-4_mean: 9.63
    apples_agent-4_min: 2
    apples_agent-5_max: 18
    apples_agent-5_mean: 4.71
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.41
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 287
    cleaning_beam_agent-1_mean: 223.05
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 8.51
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 33
    cleaning_beam_agent-3_mean: 11.16
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 3.78
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 6.47
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-04-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 191.0
  episode_reward_mean: 132.59
  episode_reward_min: 65.0
  episodes_this_iter: 96
  episodes_total: 88608
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11810.184
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3113550543785095
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008420782396569848
        model: {}
        policy_loss: -0.0016138418577611446
        total_loss: -0.0020434216130524874
        vf_explained_var: 0.007463932037353516
        vf_loss: 1.184025764465332
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19932560622692108
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014113453216850758
        model: {}
        policy_loss: -0.0016126632690429688
        total_loss: -0.0019122613593935966
        vf_explained_var: 0.07633531093597412
        vf_loss: 0.5121660232543945
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4524056017398834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011254106648266315
        model: {}
        policy_loss: -0.0018572157714515924
        total_loss: -0.0022668796591460705
        vf_explained_var: 0.015809789299964905
        vf_loss: 3.865692138671875
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4302492141723633
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015088464133441448
        model: {}
        policy_loss: -0.0013777990825474262
        total_loss: -0.001918985741212964
        vf_explained_var: 0.005731433629989624
        vf_loss: 2.1605453491210938
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3613806664943695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011470213066786528
        model: {}
        policy_loss: -0.0014511792687699199
        total_loss: -0.0019894358702003956
        vf_explained_var: 0.011052414774894714
        vf_loss: 0.9777250289916992
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5674516558647156
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016890785191208124
        model: {}
        policy_loss: -0.0015364987775683403
        total_loss: -0.0024058427661657333
        vf_explained_var: 0.02494858205318451
        vf_loss: 1.2937231063842773
    load_time_ms: 14187.148
    num_steps_sampled: 88608000
    num_steps_trained: 88608000
    sample_time_ms: 118859.531
    update_time_ms: 16.768
  iterations_since_restore: 273
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.89368932038835
    ram_util_percent: 12.461650485436891
  pid: 27065
  policy_reward_max:
    agent-0: 34.0
    agent-1: 18.0
    agent-2: 64.0
    agent-3: 46.0
    agent-4: 30.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 16.76
    agent-1: 8.62
    agent-2: 46.67
    agent-3: 27.46
    agent-4: 14.64
    agent-5: 18.44
  policy_reward_min:
    agent-0: 5.0
    agent-1: 3.0
    agent-2: 26.0
    agent-3: 10.0
    agent-4: 4.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.194339201111525
    mean_inference_ms: 15.054342419231507
    mean_processing_ms: 73.28541550934766
  time_since_restore: 39659.15728807449
  time_this_iter_s: 144.82785296440125
  time_total_s: 128595.8354177475
  timestamp: 1637402691
  timesteps_since_restore: 26208000
  timesteps_this_iter: 96000
  timesteps_total: 88608000
  training_iteration: 923
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    923 |           128596 | 88608000 |   132.59 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 7.78
    apples_agent-0_min: 1
    apples_agent-1_max: 18
    apples_agent-1_mean: 3.28
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 32.7
    apples_agent-2_min: 18
    apples_agent-3_max: 21
    apples_agent-3_mean: 9.06
    apples_agent-3_min: 2
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.71
    apples_agent-4_min: 2
    apples_agent-5_max: 19
    apples_agent-5_mean: 4.88
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 22
    cleaning_beam_agent-0_mean: 1.52
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 264
    cleaning_beam_agent-1_mean: 226.58
    cleaning_beam_agent-1_min: 183
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 10.1
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 11.87
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 3.54
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 5.66
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-07-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 175.0
  episode_reward_mean: 136.72
  episode_reward_min: 21.0
  episodes_this_iter: 96
  episodes_total: 88704
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11864.206
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30905696749687195
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014994759112596512
        model: {}
        policy_loss: -0.001868530409410596
        total_loss: -0.002298106672242284
        vf_explained_var: 0.015412136912345886
        vf_loss: 1.1436469554901123
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20119836926460266
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009088521474041045
        model: {}
        policy_loss: -0.001387820579111576
        total_loss: -0.0016872510313987732
        vf_explained_var: 0.08688017725944519
        vf_loss: 0.5467785000801086
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4396054744720459
        entropy_coeff: 0.0017600000137463212
        kl: 0.001549309235997498
        model: {}
        policy_loss: -0.0019782548770308495
        total_loss: -0.002340462524443865
        vf_explained_var: 0.02251836657524109
        vf_loss: 4.114979267120361
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4395558834075928
        entropy_coeff: 0.0017600000137463212
        kl: 0.00122389558237046
        model: {}
        policy_loss: -0.0014302761992439628
        total_loss: -0.002013565506786108
        vf_explained_var: 0.00785413384437561
        vf_loss: 1.9033088684082031
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36277079582214355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005802352679893374
        model: {}
        policy_loss: -0.0012993474956601858
        total_loss: -0.001825055805966258
        vf_explained_var: 0.014514327049255371
        vf_loss: 1.1276832818984985
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5696624517440796
        entropy_coeff: 0.0017600000137463212
        kl: 0.001546943443827331
        model: {}
        policy_loss: -0.001812809961847961
        total_loss: -0.0026811915449798107
        vf_explained_var: 0.016762837767601013
        vf_loss: 1.3422437906265259
    load_time_ms: 14190.073
    num_steps_sampled: 88704000
    num_steps_trained: 88704000
    sample_time_ms: 118836.853
    update_time_ms: 16.749
  iterations_since_restore: 274
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.94114832535885
    ram_util_percent: 12.464114832535882
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 16.0
    agent-2: 68.0
    agent-3: 38.0
    agent-4: 25.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 17.02
    agent-1: 9.15
    agent-2: 49.31
    agent-3: 25.81
    agent-4: 16.4
    agent-5: 19.03
  policy_reward_min:
    agent-0: -38.0
    agent-1: 2.0
    agent-2: 32.0
    agent-3: -21.0
    agent-4: 9.0
    agent-5: 10.0
  sampler_perf:
    mean_env_wait_ms: 28.19515540671297
    mean_inference_ms: 15.054722833611372
    mean_processing_ms: 73.2866606160992
  time_since_restore: 39805.434889793396
  time_this_iter_s: 146.2776017189026
  time_total_s: 128742.1130194664
  timestamp: 1637402837
  timesteps_since_restore: 26304000
  timesteps_this_iter: 96000
  timesteps_total: 88704000
  training_iteration: 924
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    924 |           128742 | 88704000 |   136.72 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 7.05
    apples_agent-0_min: 1
    apples_agent-1_max: 14
    apples_agent-1_mean: 3.41
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 29.66
    apples_agent-2_min: 20
    apples_agent-3_max: 34
    apples_agent-3_mean: 9.04
    apples_agent-3_min: 1
    apples_agent-4_max: 26
    apples_agent-4_mean: 10.37
    apples_agent-4_min: 2
    apples_agent-5_max: 29
    apples_agent-5_mean: 5.02
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 1.48
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 256
    cleaning_beam_agent-1_mean: 220.21
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 37
    cleaning_beam_agent-2_mean: 10.16
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 30
    cleaning_beam_agent-3_mean: 10.73
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.18
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 5.47
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-09-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 167.0
  episode_reward_mean: 129.59
  episode_reward_min: 74.0
  episodes_this_iter: 96
  episodes_total: 88800
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11861.903
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29903456568717957
        entropy_coeff: 0.0017600000137463212
        kl: 0.001540791243314743
        model: {}
        policy_loss: -0.0018662838265299797
        total_loss: -0.0022783319000154734
        vf_explained_var: 0.00922161340713501
        vf_loss: 1.142510175704956
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1993914246559143
        entropy_coeff: 0.0017600000137463212
        kl: 0.00144998449832201
        model: {}
        policy_loss: -0.0015999535098671913
        total_loss: -0.0018959827721118927
        vf_explained_var: 0.09520597755908966
        vf_loss: 0.5490220189094543
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44209933280944824
        entropy_coeff: 0.0017600000137463212
        kl: 0.001539332326501608
        model: {}
        policy_loss: -0.001963811693713069
        total_loss: -0.002356516430154443
        vf_explained_var: 0.01300695538520813
        vf_loss: 3.853919506072998
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4362104833126068
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012060641311109066
        model: {}
        policy_loss: -0.001474443357437849
        total_loss: -0.0020513529889285564
        vf_explained_var: 0.006099730730056763
        vf_loss: 1.908200740814209
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35455095767974854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008045297581702471
        model: {}
        policy_loss: -0.0016213890630751848
        total_loss: -0.0021403564605861902
        vf_explained_var: 0.02597615122795105
        vf_loss: 1.0504015684127808
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5717771053314209
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025344938039779663
        model: {}
        policy_loss: -0.0017711948603391647
        total_loss: -0.0026549051981419325
        vf_explained_var: 0.011569902300834656
        vf_loss: 1.2261515855789185
    load_time_ms: 14196.311
    num_steps_sampled: 88800000
    num_steps_trained: 88800000
    sample_time_ms: 118884.759
    update_time_ms: 16.286
  iterations_since_restore: 275
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.032038834951454
    ram_util_percent: 12.380582524271842
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 63.0
    agent-3: 41.0
    agent-4: 25.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 17.09
    agent-1: 8.98
    agent-2: 45.16
    agent-3: 26.18
    agent-4: 15.17
    agent-5: 17.01
  policy_reward_min:
    agent-0: 9.0
    agent-1: 4.0
    agent-2: 29.0
    agent-3: 12.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.19557182181711
    mean_inference_ms: 15.054996395218144
    mean_processing_ms: 73.2865791414598
  time_since_restore: 39950.59874391556
  time_this_iter_s: 145.16385412216187
  time_total_s: 128887.27687358856
  timestamp: 1637402983
  timesteps_since_restore: 26400000
  timesteps_this_iter: 96000
  timesteps_total: 88800000
  training_iteration: 925
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    925 |           128887 | 88800000 |   129.59 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 6.62
    apples_agent-0_min: 0
    apples_agent-1_max: 22
    apples_agent-1_mean: 3.57
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 30.89
    apples_agent-2_min: 7
    apples_agent-3_max: 41
    apples_agent-3_mean: 9.94
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.63
    apples_agent-4_min: 1
    apples_agent-5_max: 27
    apples_agent-5_mean: 5.19
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 20
    cleaning_beam_agent-0_mean: 1.5
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 268
    cleaning_beam_agent-1_mean: 228.83
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 9.46
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 44
    cleaning_beam_agent-3_mean: 11.34
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.2
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 6.98
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-12-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 172.0
  episode_reward_mean: 134.77
  episode_reward_min: 27.0
  episodes_this_iter: 96
  episodes_total: 88896
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11868.556
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.29936596751213074
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009650870924815536
        model: {}
        policy_loss: -0.0018484622705727816
        total_loss: -0.0022634747438132763
        vf_explained_var: 0.011760175228118896
        vf_loss: 1.1187057495117188
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20171815156936646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016196274664252996
        model: {}
        policy_loss: -0.001995546743273735
        total_loss: -0.002297590486705303
        vf_explained_var: 0.1039077490568161
        vf_loss: 0.5297536849975586
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.451538622379303
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013036989839747548
        model: {}
        policy_loss: -0.001895308494567871
        total_loss: -0.0022870320826768875
        vf_explained_var: 0.0051368772983551025
        vf_loss: 4.029839515686035
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43816959857940674
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015536686405539513
        model: {}
        policy_loss: -0.0016598051879554987
        total_loss: -0.002236122963950038
        vf_explained_var: 0.0009488016366958618
        vf_loss: 1.9486181735992432
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3618375062942505
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009494270198047161
        model: {}
        policy_loss: -0.0014694196870550513
        total_loss: -0.0020018047653138638
        vf_explained_var: 0.010000795125961304
        vf_loss: 1.0444931983947754
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5725682973861694
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019559250213205814
        model: {}
        policy_loss: -0.0017214291729032993
        total_loss: -0.0025994153693318367
        vf_explained_var: 0.014684364199638367
        vf_loss: 1.2973507642745972
    load_time_ms: 14197.541
    num_steps_sampled: 88896000
    num_steps_trained: 88896000
    sample_time_ms: 118922.628
    update_time_ms: 16.423
  iterations_since_restore: 276
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.948792270531403
    ram_util_percent: 12.453623188405796
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 18.0
    agent-2: 69.0
    agent-3: 40.0
    agent-4: 31.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 17.18
    agent-1: 9.48
    agent-2: 47.21
    agent-3: 26.87
    agent-4: 15.62
    agent-5: 18.41
  policy_reward_min:
    agent-0: 5.0
    agent-1: 0.0
    agent-2: 8.0
    agent-3: 5.0
    agent-4: 3.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.196010313725164
    mean_inference_ms: 15.055093863627171
    mean_processing_ms: 73.28635691973672
  time_since_restore: 40095.89331841469
  time_this_iter_s: 145.29457449913025
  time_total_s: 129032.57144808769
  timestamp: 1637403128
  timesteps_since_restore: 26496000
  timesteps_this_iter: 96000
  timesteps_total: 88896000
  training_iteration: 926
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    926 |           129033 | 88896000 |   134.77 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 7.51
    apples_agent-0_min: 2
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.17
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 30.9
    apples_agent-2_min: 9
    apples_agent-3_max: 37
    apples_agent-3_mean: 9.81
    apples_agent-3_min: 1
    apples_agent-4_max: 28
    apples_agent-4_mean: 11.04
    apples_agent-4_min: 1
    apples_agent-5_max: 16
    apples_agent-5_mean: 4.77
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 5
    cleaning_beam_agent-0_mean: 1.37
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 255
    cleaning_beam_agent-1_mean: 222.75
    cleaning_beam_agent-1_min: 150
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 9.89
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 11.53
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.1
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 6.71
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-14-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 189.0
  episode_reward_mean: 135.91
  episode_reward_min: 80.0
  episodes_this_iter: 96
  episodes_total: 88992
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11878.122
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.306032657623291
        entropy_coeff: 0.0017600000137463212
        kl: 0.001401271321810782
        model: {}
        policy_loss: -0.0018893955275416374
        total_loss: -0.002311361487954855
        vf_explained_var: -0.003687947988510132
        vf_loss: 1.1665006875991821
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20045331120491028
        entropy_coeff: 0.0017600000137463212
        kl: 0.001081767724826932
        model: {}
        policy_loss: -0.001466742716729641
        total_loss: -0.0017572315409779549
        vf_explained_var: 0.07746854424476624
        vf_loss: 0.62306809425354
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45174652338027954
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011245256755501032
        model: {}
        policy_loss: -0.0017113881185650826
        total_loss: -0.002095904666930437
        vf_explained_var: 0.01898139715194702
        vf_loss: 4.105574607849121
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4297011196613312
        entropy_coeff: 0.0017600000137463212
        kl: 0.001403073314577341
        model: {}
        policy_loss: -0.001430665492080152
        total_loss: -0.0019671828486025333
        vf_explained_var: 0.015034615993499756
        vf_loss: 2.1975674629211426
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3616607189178467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011226460337638855
        model: {}
        policy_loss: -0.0013565393164753914
        total_loss: -0.0018849316984415054
        vf_explained_var: 0.019893601536750793
        vf_loss: 1.0812939405441284
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5825908780097961
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014165511820465326
        model: {}
        policy_loss: -0.0017263181507587433
        total_loss: -0.002626091241836548
        vf_explained_var: 0.012762874364852905
        vf_loss: 1.2558565139770508
    load_time_ms: 14194.443
    num_steps_sampled: 88992000
    num_steps_trained: 88992000
    sample_time_ms: 118895.199
    update_time_ms: 16.244
  iterations_since_restore: 277
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.91594202898551
    ram_util_percent: 12.374879227053142
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 21.0
    agent-2: 67.0
    agent-3: 50.0
    agent-4: 26.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 17.89
    agent-1: 9.38
    agent-2: 47.45
    agent-3: 27.26
    agent-4: 15.55
    agent-5: 18.38
  policy_reward_min:
    agent-0: 9.0
    agent-1: 2.0
    agent-2: 15.0
    agent-3: 8.0
    agent-4: 4.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.196550343335687
    mean_inference_ms: 15.054791327845644
    mean_processing_ms: 73.2865195310741
  time_since_restore: 40240.98140954971
  time_this_iter_s: 145.08809113502502
  time_total_s: 129177.65953922272
  timestamp: 1637403273
  timesteps_since_restore: 26592000
  timesteps_this_iter: 96000
  timesteps_total: 88992000
  training_iteration: 927
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    927 |           129178 | 88992000 |   135.91 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 7.54
    apples_agent-0_min: 2
    apples_agent-1_max: 13
    apples_agent-1_mean: 3.18
    apples_agent-1_min: 0
    apples_agent-2_max: 55
    apples_agent-2_mean: 32.65
    apples_agent-2_min: 18
    apples_agent-3_max: 35
    apples_agent-3_mean: 9.81
    apples_agent-3_min: 3
    apples_agent-4_max: 25
    apples_agent-4_mean: 11.14
    apples_agent-4_min: 1
    apples_agent-5_max: 17
    apples_agent-5_mean: 4.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.49
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 260
    cleaning_beam_agent-1_mean: 227.44
    cleaning_beam_agent-1_min: 174
    cleaning_beam_agent-2_max: 26
    cleaning_beam_agent-2_mean: 10.54
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 10.61
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.38
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 16
    cleaning_beam_agent-5_mean: 8.12
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-16-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 188.0
  episode_reward_mean: 138.16
  episode_reward_min: 80.0
  episodes_this_iter: 96
  episodes_total: 89088
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11861.348
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3144283592700958
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016962356166914105
        model: {}
        policy_loss: -0.0020293104462325573
        total_loss: -0.002464300487190485
        vf_explained_var: 0.011169672012329102
        vf_loss: 1.184051752090454
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20320506393909454
        entropy_coeff: 0.0017600000137463212
        kl: 0.00119307660497725
        model: {}
        policy_loss: -0.0016144877299666405
        total_loss: -0.0019132699817419052
        vf_explained_var: 0.07325500249862671
        vf_loss: 0.5885827541351318
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43901991844177246
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012744789710268378
        model: {}
        policy_loss: -0.0019997870549559593
        total_loss: -0.0023400690406560898
        vf_explained_var: 0.010026872158050537
        vf_loss: 4.323916912078857
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4274730682373047
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011571471113711596
        model: {}
        policy_loss: -0.0014745378866791725
        total_loss: -0.0020316168665885925
        vf_explained_var: 0.00044621527194976807
        vf_loss: 1.95274019241333
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3598722219467163
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012229292187839746
        model: {}
        policy_loss: -0.0014845486730337143
        total_loss: -0.002005272079259157
        vf_explained_var: 0.009699955582618713
        vf_loss: 1.1264960765838623
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5864667892456055
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018202858045697212
        model: {}
        policy_loss: -0.001807916909456253
        total_loss: -0.002702341414988041
        vf_explained_var: 0.006778255105018616
        vf_loss: 1.3775724172592163
    load_time_ms: 14157.619
    num_steps_sampled: 89088000
    num_steps_trained: 89088000
    sample_time_ms: 118895.541
    update_time_ms: 16.181
  iterations_since_restore: 278
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.890338164251208
    ram_util_percent: 12.458937198067632
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 23.0
    agent-2: 70.0
    agent-3: 42.0
    agent-4: 32.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 17.5
    agent-1: 9.15
    agent-2: 49.0
    agent-3: 26.89
    agent-4: 15.98
    agent-5: 19.64
  policy_reward_min:
    agent-0: 9.0
    agent-1: 2.0
    agent-2: 30.0
    agent-3: 12.0
    agent-4: 4.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.197026546650445
    mean_inference_ms: 15.054658136422786
    mean_processing_ms: 73.28573420776567
  time_since_restore: 40386.02416539192
  time_this_iter_s: 145.04275584220886
  time_total_s: 129322.70229506493
  timestamp: 1637403418
  timesteps_since_restore: 26688000
  timesteps_this_iter: 96000
  timesteps_total: 89088000
  training_iteration: 928
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    928 |           129323 | 89088000 |   138.16 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 7.65
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.11
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 31.69
    apples_agent-2_min: 14
    apples_agent-3_max: 53
    apples_agent-3_mean: 9.53
    apples_agent-3_min: 3
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.75
    apples_agent-4_min: 4
    apples_agent-5_max: 16
    apples_agent-5_mean: 4.67
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 1.62
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 257
    cleaning_beam_agent-1_mean: 223.41
    cleaning_beam_agent-1_min: 173
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 10.1
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 11.94
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.82
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 6.83
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-19-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 207.0
  episode_reward_mean: 137.59
  episode_reward_min: 98.0
  episodes_this_iter: 96
  episodes_total: 89184
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11877.005
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3120349049568176
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011130126658827066
        model: {}
        policy_loss: -0.0018063029274344444
        total_loss: -0.0022336971014738083
        vf_explained_var: 0.01566864550113678
        vf_loss: 1.2178764343261719
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20110692083835602
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014660587767139077
        model: {}
        policy_loss: -0.001480155624449253
        total_loss: -0.001779455691576004
        vf_explained_var: 0.09219479560852051
        vf_loss: 0.5464946627616882
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4388625919818878
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010384699562564492
        model: {}
        policy_loss: -0.0017021445091813803
        total_loss: -0.00207662396132946
        vf_explained_var: 0.03440368175506592
        vf_loss: 3.9791932106018066
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42323392629623413
        entropy_coeff: 0.0017600000137463212
        kl: 0.001684032496996224
        model: {}
        policy_loss: -0.0013081831857562065
        total_loss: -0.0018460566643625498
        vf_explained_var: 0.010413914918899536
        vf_loss: 2.0701804161071777
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3608352243900299
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006612013094127178
        model: {}
        policy_loss: -0.0013186633586883545
        total_loss: -0.0018420307897031307
        vf_explained_var: 0.01604810357093811
        vf_loss: 1.1170248985290527
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5926595330238342
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020488803274929523
        model: {}
        policy_loss: -0.0016586161218583584
        total_loss: -0.0025732705835253
        vf_explained_var: 0.02134278416633606
        vf_loss: 1.284222960472107
    load_time_ms: 14137.772
    num_steps_sampled: 89184000
    num_steps_trained: 89184000
    sample_time_ms: 118890.082
    update_time_ms: 16.154
  iterations_since_restore: 279
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.89033816425121
    ram_util_percent: 12.462801932367146
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 18.0
    agent-2: 75.0
    agent-3: 48.0
    agent-4: 30.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 18.22
    agent-1: 9.33
    agent-2: 47.93
    agent-3: 27.26
    agent-4: 15.82
    agent-5: 19.03
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 28.0
    agent-3: 12.0
    agent-4: 6.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.197394908924377
    mean_inference_ms: 15.05438130889494
    mean_processing_ms: 73.28534776388769
  time_since_restore: 40531.09781384468
  time_this_iter_s: 145.0736484527588
  time_total_s: 129467.77594351768
  timestamp: 1637403564
  timesteps_since_restore: 26784000
  timesteps_this_iter: 96000
  timesteps_total: 89184000
  training_iteration: 929
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    929 |           129468 | 89184000 |   137.59 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 7.37
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 3.36
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 30.36
    apples_agent-2_min: 0
    apples_agent-3_max: 35
    apples_agent-3_mean: 9.7
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.68
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 4.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 1.38
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 256
    cleaning_beam_agent-1_mean: 211.61
    cleaning_beam_agent-1_min: 1
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 9.26
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 45
    cleaning_beam_agent-3_mean: 10.79
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.25
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 6.99
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-21-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 178.0
  episode_reward_mean: 132.76
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 89280
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11892.797
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.313726007938385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008274192223325372
        model: {}
        policy_loss: -0.0016279062256217003
        total_loss: -0.0020702360197901726
        vf_explained_var: 0.0002488940954208374
        vf_loss: 1.0982816219329834
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19620421528816223
        entropy_coeff: 0.0017600000137463212
        kl: 0.000744406133890152
        model: {}
        policy_loss: -0.001477050594985485
        total_loss: -0.0017600636929273605
        vf_explained_var: 0.09846916794776917
        vf_loss: 0.6230472326278687
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44578099250793457
        entropy_coeff: 0.0017600000137463212
        kl: 0.00115574907977134
        model: {}
        policy_loss: -0.0020062520634382963
        total_loss: -0.0023543164134025574
        vf_explained_var: 0.03631077706813812
        vf_loss: 4.365108013153076
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41928279399871826
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012523930054157972
        model: {}
        policy_loss: -0.0015331287868320942
        total_loss: -0.002052582334727049
        vf_explained_var: 0.009431988000869751
        vf_loss: 2.184835910797119
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3710109293460846
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008408588473685086
        model: {}
        policy_loss: -0.0012323936680331826
        total_loss: -0.0017677518771961331
        vf_explained_var: 0.014329120516777039
        vf_loss: 1.1762142181396484
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6000004410743713
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013076228788122535
        model: {}
        policy_loss: -0.0017816540785133839
        total_loss: -0.0027100711595267057
        vf_explained_var: 0.03680543601512909
        vf_loss: 1.27580726146698
    load_time_ms: 14143.126
    num_steps_sampled: 89280000
    num_steps_trained: 89280000
    sample_time_ms: 118914.446
    update_time_ms: 15.89
  iterations_since_restore: 280
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.007766990291262
    ram_util_percent: 12.457281553398058
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 17.0
    agent-2: 67.0
    agent-3: 44.0
    agent-4: 29.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 16.4
    agent-1: 9.82
    agent-2: 46.25
    agent-3: 26.6
    agent-4: 15.39
    agent-5: 18.3
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 28.19756410213563
    mean_inference_ms: 15.05430507235188
    mean_processing_ms: 73.2850878158538
  time_since_restore: 40675.826708078384
  time_this_iter_s: 144.7288942337036
  time_total_s: 129612.50483775139
  timestamp: 1637403708
  timesteps_since_restore: 26880000
  timesteps_this_iter: 96000
  timesteps_total: 89280000
  training_iteration: 930
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    930 |           129613 | 89280000 |   132.76 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 7.36
    apples_agent-0_min: 1
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.66
    apples_agent-1_min: 0
    apples_agent-2_max: 158
    apples_agent-2_mean: 31.92
    apples_agent-2_min: 11
    apples_agent-3_max: 34
    apples_agent-3_mean: 8.77
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.69
    apples_agent-4_min: 2
    apples_agent-5_max: 16
    apples_agent-5_mean: 4.3
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 15
    cleaning_beam_agent-0_mean: 2.19
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 213.9
    cleaning_beam_agent-1_min: 100
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 9.29
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 26
    cleaning_beam_agent-3_mean: 9.73
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 2.55
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 7.16
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-24-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 178.0
  episode_reward_mean: 134.72
  episode_reward_min: 41.0
  episodes_this_iter: 96
  episodes_total: 89376
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11901.896
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3152703046798706
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011671341490000486
        model: {}
        policy_loss: -0.0018921559676527977
        total_loss: -0.00232043769210577
        vf_explained_var: -0.0006134957075119019
        vf_loss: 1.2659549713134766
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19868382811546326
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010527417762205005
        model: {}
        policy_loss: -0.0015296651981770992
        total_loss: -0.0018200133927166462
        vf_explained_var: 0.0702025294303894
        vf_loss: 0.593345582485199
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4317760765552521
        entropy_coeff: 0.0017600000137463212
        kl: 0.00138343486469239
        model: {}
        policy_loss: -0.001978030428290367
        total_loss: -0.002345554530620575
        vf_explained_var: 0.0006413310766220093
        vf_loss: 3.924072027206421
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41859009861946106
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010800210293382406
        model: {}
        policy_loss: -0.0014881726820021868
        total_loss: -0.0020096316002309322
        vf_explained_var: 0.00040221214294433594
        vf_loss: 2.152608871459961
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37112700939178467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012469810899347067
        model: {}
        policy_loss: -0.0017058593221008778
        total_loss: -0.0022558816708624363
        vf_explained_var: 0.01288902759552002
        vf_loss: 1.0316084623336792
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5923780798912048
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018310312880203128
        model: {}
        policy_loss: -0.001611625775694847
        total_loss: -0.0025197425857186317
        vf_explained_var: 0.012487813830375671
        vf_loss: 1.3446704149246216
    load_time_ms: 14149.928
    num_steps_sampled: 89376000
    num_steps_trained: 89376000
    sample_time_ms: 118862.785
    update_time_ms: 16.155
  iterations_since_restore: 281
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.72476190476191
    ram_util_percent: 12.380952380952381
  pid: 27065
  policy_reward_max:
    agent-0: 35.0
    agent-1: 17.0
    agent-2: 64.0
    agent-3: 47.0
    agent-4: 25.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 17.48
    agent-1: 9.73
    agent-2: 47.01
    agent-3: 26.95
    agent-4: 15.17
    agent-5: 18.38
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 15.0
    agent-3: 7.0
    agent-4: 6.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.19799941309856
    mean_inference_ms: 15.054249051082179
    mean_processing_ms: 73.28579853646681
  time_since_restore: 40821.0453619957
  time_this_iter_s: 145.21865391731262
  time_total_s: 129757.7234916687
  timestamp: 1637403856
  timesteps_since_restore: 26976000
  timesteps_this_iter: 96000
  timesteps_total: 89376000
  training_iteration: 931
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    931 |           129758 | 89376000 |   134.72 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 6.86
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.47
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 30.07
    apples_agent-2_min: 16
    apples_agent-3_max: 25
    apples_agent-3_mean: 8.64
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 9.87
    apples_agent-4_min: 2
    apples_agent-5_max: 21
    apples_agent-5_mean: 4.59
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.66
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 256
    cleaning_beam_agent-1_mean: 213.19
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 20
    cleaning_beam_agent-2_mean: 8.84
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 28
    cleaning_beam_agent-3_mean: 9.17
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.73
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 6.53
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-26-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 173.0
  episode_reward_mean: 131.35
  episode_reward_min: 63.0
  episodes_this_iter: 96
  episodes_total: 89472
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11909.476
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3144102990627289
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010347893694415689
        model: {}
        policy_loss: -0.001863377634435892
        total_loss: -0.0023159561678767204
        vf_explained_var: 0.001762986183166504
        vf_loss: 1.0078258514404297
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19725202023983002
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011317187454551458
        model: {}
        policy_loss: -0.0015276167541742325
        total_loss: -0.0018216520547866821
        vf_explained_var: 0.09001928567886353
        vf_loss: 0.5312833189964294
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4278673529624939
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013940968783572316
        model: {}
        policy_loss: -0.0018214230658486485
        total_loss: -0.0021832515485584736
        vf_explained_var: 0.009619027376174927
        vf_loss: 3.9121651649475098
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4163030683994293
        entropy_coeff: 0.0017600000137463212
        kl: 0.001246739411726594
        model: {}
        policy_loss: -0.001422483241185546
        total_loss: -0.0019614663906395435
        vf_explained_var: -0.001589462161064148
        vf_loss: 1.93708074092865
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37246501445770264
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004708819033112377
        model: {}
        policy_loss: -0.001120289321988821
        total_loss: -0.001675696112215519
        vf_explained_var: 0.01538608968257904
        vf_loss: 1.0012812614440918
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5742223262786865
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019205703865736723
        model: {}
        policy_loss: -0.0020309442188590765
        total_loss: -0.002922806655988097
        vf_explained_var: 0.016780659556388855
        vf_loss: 1.1876918077468872
    load_time_ms: 14156.709
    num_steps_sampled: 89472000
    num_steps_trained: 89472000
    sample_time_ms: 119014.623
    update_time_ms: 16.255
  iterations_since_restore: 282
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.941062801932365
    ram_util_percent: 12.457971014492752
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 20.0
    agent-2: 65.0
    agent-3: 46.0
    agent-4: 31.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 16.8
    agent-1: 9.69
    agent-2: 46.5
    agent-3: 26.2
    agent-4: 14.32
    agent-5: 17.84
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 26.0
    agent-3: 13.0
    agent-4: 6.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.19809723064368
    mean_inference_ms: 15.05447951406194
    mean_processing_ms: 73.28586653514469
  time_since_restore: 40966.14667224884
  time_this_iter_s: 145.1013102531433
  time_total_s: 129902.82480192184
  timestamp: 1637404001
  timesteps_since_restore: 27072000
  timesteps_this_iter: 96000
  timesteps_total: 89472000
  training_iteration: 932
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    932 |           129903 | 89472000 |   131.35 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 6.21
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.38
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 30.98
    apples_agent-2_min: 17
    apples_agent-3_max: 31
    apples_agent-3_mean: 9.57
    apples_agent-3_min: 2
    apples_agent-4_max: 32
    apples_agent-4_mean: 10.44
    apples_agent-4_min: 4
    apples_agent-5_max: 20
    apples_agent-5_mean: 5.14
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 1.64
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 258
    cleaning_beam_agent-1_mean: 218.46
    cleaning_beam_agent-1_min: 171
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 8.72
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 9.87
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 2.78
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 6.28
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-29-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 178.0
  episode_reward_mean: 134.65
  episode_reward_min: 91.0
  episodes_this_iter: 96
  episodes_total: 89568
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11881.771
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3055928647518158
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009636203176341951
        model: {}
        policy_loss: -0.0017096679657697678
        total_loss: -0.0021426775492727757
        vf_explained_var: 0.018189236521720886
        vf_loss: 1.0483477115631104
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19805745780467987
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012590716360136867
        model: {}
        policy_loss: -0.0015973993577063084
        total_loss: -0.0018844702281057835
        vf_explained_var: 0.07351307570934296
        vf_loss: 0.6151106357574463
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41377848386764526
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014413970056921244
        model: {}
        policy_loss: -0.0016695749945938587
        total_loss: -0.002003523986786604
        vf_explained_var: 0.023948952555656433
        vf_loss: 3.943005084991455
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41770318150520325
        entropy_coeff: 0.0017600000137463212
        kl: 0.001735066995024681
        model: {}
        policy_loss: -0.0016784155741333961
        total_loss: -0.0022277431562542915
        vf_explained_var: 0.004144847393035889
        vf_loss: 1.8582789897918701
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3726671636104584
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005951285711489618
        model: {}
        policy_loss: -0.0012967390939593315
        total_loss: -0.001849612221121788
        vf_explained_var: 0.017131716012954712
        vf_loss: 1.0302129983901978
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5630278587341309
        entropy_coeff: 0.0017600000137463212
        kl: 0.001368364435620606
        model: {}
        policy_loss: -0.0016302285948768258
        total_loss: -0.0024895137175917625
        vf_explained_var: 0.01990242302417755
        vf_loss: 1.3164615631103516
    load_time_ms: 14126.99
    num_steps_sampled: 89568000
    num_steps_trained: 89568000
    sample_time_ms: 119008.535
    update_time_ms: 16.098
  iterations_since_restore: 283
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.936097560975607
    ram_util_percent: 12.463414634146337
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 73.0
    agent-3: 41.0
    agent-4: 25.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 16.13
    agent-1: 9.64
    agent-2: 48.18
    agent-3: 26.8
    agent-4: 15.09
    agent-5: 18.81
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 26.0
    agent-3: 15.0
    agent-4: 7.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.198035474803717
    mean_inference_ms: 15.054087210256997
    mean_processing_ms: 73.28430543138948
  time_since_restore: 41110.29507851601
  time_this_iter_s: 144.14840626716614
  time_total_s: 130046.97320818901
  timestamp: 1637404146
  timesteps_since_restore: 27168000
  timesteps_this_iter: 96000
  timesteps_total: 89568000
  training_iteration: 933
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    933 |           130047 | 89568000 |   134.65 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 7.12
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.45
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 30.29
    apples_agent-2_min: 17
    apples_agent-3_max: 32
    apples_agent-3_mean: 8.89
    apples_agent-3_min: 2
    apples_agent-4_max: 27
    apples_agent-4_mean: 10.77
    apples_agent-4_min: 2
    apples_agent-5_max: 18
    apples_agent-5_mean: 4.78
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.56
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 249
    cleaning_beam_agent-1_mean: 216.85
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 9.52
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 10.97
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.51
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 6.29
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-31-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 185.0
  episode_reward_mean: 133.86
  episode_reward_min: 59.0
  episodes_this_iter: 96
  episodes_total: 89664
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11840.314
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31195783615112305
        entropy_coeff: 0.0017600000137463212
        kl: 0.001460482832044363
        model: {}
        policy_loss: -0.0017744218930602074
        total_loss: -0.0022079264745116234
        vf_explained_var: 0.0050100237131118774
        vf_loss: 1.1554278135299683
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19827395677566528
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008205134072341025
        model: {}
        policy_loss: -0.0012714770855382085
        total_loss: -0.0015640933997929096
        vf_explained_var: 0.09172049164772034
        vf_loss: 0.5634718537330627
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41699787974357605
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012684037210419774
        model: {}
        policy_loss: -0.0018361443653702736
        total_loss: -0.002174566499888897
        vf_explained_var: 0.014480426907539368
        vf_loss: 3.9549310207366943
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4177812933921814
        entropy_coeff: 0.0017600000137463212
        kl: 0.001115891500376165
        model: {}
        policy_loss: -0.0015675099566578865
        total_loss: -0.0021007023751735687
        vf_explained_var: 0.005319863557815552
        vf_loss: 2.0210559368133545
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3746790289878845
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008755425224080682
        model: {}
        policy_loss: -0.0013303366722539067
        total_loss: -0.0018826026935130358
        vf_explained_var: 0.009409144520759583
        vf_loss: 1.0716888904571533
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5685310959815979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013670852640643716
        model: {}
        policy_loss: -0.0015126371290534735
        total_loss: -0.0023786253295838833
        vf_explained_var: 0.020825445652008057
        vf_loss: 1.3462631702423096
    load_time_ms: 14143.24
    num_steps_sampled: 89664000
    num_steps_trained: 89664000
    sample_time_ms: 118949.873
    update_time_ms: 16.11
  iterations_since_restore: 284
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.012980769230772
    ram_util_percent: 12.397115384615383
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 19.0
    agent-2: 72.0
    agent-3: 42.0
    agent-4: 26.0
    agent-5: 37.0
  policy_reward_mean:
    agent-0: 16.83
    agent-1: 9.46
    agent-2: 47.4
    agent-3: 26.32
    agent-4: 15.15
    agent-5: 18.7
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 22.0
    agent-3: 11.0
    agent-4: 5.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.198601599726768
    mean_inference_ms: 15.053906726862374
    mean_processing_ms: 73.28344100731822
  time_since_restore: 41255.84459257126
  time_this_iter_s: 145.54951405525208
  time_total_s: 130192.52272224426
  timestamp: 1637404291
  timesteps_since_restore: 27264000
  timesteps_this_iter: 96000
  timesteps_total: 89664000
  training_iteration: 934
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    934 |           130193 | 89664000 |   133.86 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 7.95
    apples_agent-0_min: 1
    apples_agent-1_max: 13
    apples_agent-1_mean: 3.65
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 30.18
    apples_agent-2_min: 14
    apples_agent-3_max: 26
    apples_agent-3_mean: 8.77
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 10.43
    apples_agent-4_min: 4
    apples_agent-5_max: 51
    apples_agent-5_mean: 5.32
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 24
    cleaning_beam_agent-0_mean: 2.12
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 260
    cleaning_beam_agent-1_mean: 217.89
    cleaning_beam_agent-1_min: 154
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 8.69
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 28
    cleaning_beam_agent-3_mean: 8.47
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 14
    cleaning_beam_agent-4_mean: 2.47
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 5.83
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-33-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 170.0
  episode_reward_mean: 132.79
  episode_reward_min: 87.0
  episodes_this_iter: 96
  episodes_total: 89760
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11854.625
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31221169233322144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009172849822789431
        model: {}
        policy_loss: -0.0016099513741210103
        total_loss: -0.002048521302640438
        vf_explained_var: 0.011072561144828796
        vf_loss: 1.1092499494552612
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19825613498687744
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010684231529012322
        model: {}
        policy_loss: -0.0014489709865301847
        total_loss: -0.0017467525321990252
        vf_explained_var: 0.08764134347438812
        vf_loss: 0.5114970207214355
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41669172048568726
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015202370705083013
        model: {}
        policy_loss: -0.0018338989466428757
        total_loss: -0.002174623543396592
        vf_explained_var: 0.011582031846046448
        vf_loss: 3.9265308380126953
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41569051146507263
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012659792555496097
        model: {}
        policy_loss: -0.0014850734733045101
        total_loss: -0.00202310923486948
        vf_explained_var: 0.0061957091093063354
        vf_loss: 1.935777187347412
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3724307119846344
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008021214161999524
        model: {}
        policy_loss: -0.001274040900170803
        total_loss: -0.0018316642381250858
        vf_explained_var: 0.011280328035354614
        vf_loss: 0.9785383939743042
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5600458383560181
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017353198491036892
        model: {}
        policy_loss: -0.0015539131127297878
        total_loss: -0.0023948464076966047
        vf_explained_var: 0.010113820433616638
        vf_loss: 1.4474728107452393
    load_time_ms: 14132.109
    num_steps_sampled: 89760000
    num_steps_trained: 89760000
    sample_time_ms: 119055.362
    update_time_ms: 15.953
  iterations_since_restore: 285
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.088405797101444
    ram_util_percent: 12.369565217391305
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 19.0
    agent-2: 66.0
    agent-3: 39.0
    agent-4: 30.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 16.91
    agent-1: 9.27
    agent-2: 46.09
    agent-3: 26.45
    agent-4: 15.66
    agent-5: 18.41
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 0.0
    agent-3: 12.0
    agent-4: 7.0
    agent-5: -23.0
  sampler_perf:
    mean_env_wait_ms: 28.19958660978646
    mean_inference_ms: 15.054174455517822
    mean_processing_ms: 73.28580187909603
  time_since_restore: 41402.08949828148
  time_this_iter_s: 146.24490571022034
  time_total_s: 130338.76762795448
  timestamp: 1637404438
  timesteps_since_restore: 27360000
  timesteps_this_iter: 96000
  timesteps_total: 89760000
  training_iteration: 935
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    935 |           130339 | 89760000 |   132.79 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 7.0
    apples_agent-0_min: 1
    apples_agent-1_max: 20
    apples_agent-1_mean: 3.12
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 29.51
    apples_agent-2_min: 16
    apples_agent-3_max: 23
    apples_agent-3_mean: 8.98
    apples_agent-3_min: 2
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.32
    apples_agent-4_min: 2
    apples_agent-5_max: 18
    apples_agent-5_mean: 4.35
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.28
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 218.04
    cleaning_beam_agent-1_min: 178
    cleaning_beam_agent-2_max: 26
    cleaning_beam_agent-2_mean: 8.79
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 8.84
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 6
    cleaning_beam_agent-4_mean: 2.35
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 5.36
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-36-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 173.0
  episode_reward_mean: 131.13
  episode_reward_min: 95.0
  episodes_this_iter: 96
  episodes_total: 89856
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11841.076
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31512051820755005
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007493166485801339
        model: {}
        policy_loss: -0.0015793165657669306
        total_loss: -0.002029252937063575
        vf_explained_var: 0.013775721192359924
        vf_loss: 1.046754240989685
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20082302391529083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011374622117727995
        model: {}
        policy_loss: -0.0015734350308775902
        total_loss: -0.0018788236193358898
        vf_explained_var: 0.08979925513267517
        vf_loss: 0.48061466217041016
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43367815017700195
        entropy_coeff: 0.0017600000137463212
        kl: 0.00134175643324852
        model: {}
        policy_loss: -0.0017817304469645023
        total_loss: -0.0021763613913208246
        vf_explained_var: 0.015191048383712769
        vf_loss: 3.686422824859619
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4177989065647125
        entropy_coeff: 0.0017600000137463212
        kl: 0.00133718631695956
        model: {}
        policy_loss: -0.0015096822753548622
        total_loss: -0.002054411917924881
        vf_explained_var: -0.0029218047857284546
        vf_loss: 1.9059841632843018
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3725510537624359
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007184307323768735
        model: {}
        policy_loss: -0.0013605025596916676
        total_loss: -0.0019095325842499733
        vf_explained_var: 0.012016713619232178
        vf_loss: 1.0665957927703857
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5625227689743042
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026236670091748238
        model: {}
        policy_loss: -0.0016899064648896456
        total_loss: -0.002565797884017229
        vf_explained_var: 0.01260000467300415
        vf_loss: 1.1414716243743896
    load_time_ms: 14118.103
    num_steps_sampled: 89856000
    num_steps_trained: 89856000
    sample_time_ms: 119077.124
    update_time_ms: 16.15
  iterations_since_restore: 286
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.906730769230766
    ram_util_percent: 12.41105769230769
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 15.0
    agent-2: 68.0
    agent-3: 43.0
    agent-4: 29.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 16.81
    agent-1: 8.83
    agent-2: 46.35
    agent-3: 26.31
    agent-4: 15.55
    agent-5: 17.28
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 30.0
    agent-3: 12.0
    agent-4: 6.0
    agent-5: -23.0
  sampler_perf:
    mean_env_wait_ms: 28.200210415508312
    mean_inference_ms: 15.054198032373257
    mean_processing_ms: 73.28553266460791
  time_since_restore: 41547.288080215454
  time_this_iter_s: 145.19858193397522
  time_total_s: 130483.96620988846
  timestamp: 1637404583
  timesteps_since_restore: 27456000
  timesteps_this_iter: 96000
  timesteps_total: 89856000
  training_iteration: 936
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    936 |           130484 | 89856000 |   131.13 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 6.83
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 3.34
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 29.86
    apples_agent-2_min: 15
    apples_agent-3_max: 43
    apples_agent-3_mean: 9.7
    apples_agent-3_min: 2
    apples_agent-4_max: 25
    apples_agent-4_mean: 10.45
    apples_agent-4_min: 3
    apples_agent-5_max: 19
    apples_agent-5_mean: 4.66
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 1.63
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 248
    cleaning_beam_agent-1_mean: 218.07
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 9.51
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 38
    cleaning_beam_agent-3_mean: 8.89
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.14
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 5.2
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-38-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 180.0
  episode_reward_mean: 133.98
  episode_reward_min: 70.0
  episodes_this_iter: 96
  episodes_total: 89952
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11838.978
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31885212659835815
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011206691851839423
        model: {}
        policy_loss: -0.001762116327881813
        total_loss: -0.002207116223871708
        vf_explained_var: 0.006382837891578674
        vf_loss: 1.1618402004241943
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1961580514907837
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007845945656299591
        model: {}
        policy_loss: -0.0013491874560713768
        total_loss: -0.0016370508819818497
        vf_explained_var: 0.0950440764427185
        vf_loss: 0.5737327337265015
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4335152804851532
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015359470853582025
        model: {}
        policy_loss: -0.0020263539627194405
        total_loss: -0.0024033216759562492
        vf_explained_var: 0.01632794737815857
        vf_loss: 3.8601725101470947
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41483068466186523
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017899398226290941
        model: {}
        policy_loss: -0.0015680068172514439
        total_loss: -0.0020863693207502365
        vf_explained_var: 0.006113126873970032
        vf_loss: 2.117382526397705
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3728362023830414
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009630349813960493
        model: {}
        policy_loss: -0.0014139435952529311
        total_loss: -0.001969854347407818
        vf_explained_var: 0.010759368538856506
        vf_loss: 1.002830982208252
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5632781982421875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014963406138122082
        model: {}
        policy_loss: -0.0016917758621275425
        total_loss: -0.0025513875298202038
        vf_explained_var: 0.009475350379943848
        vf_loss: 1.317563772201538
    load_time_ms: 14121.024
    num_steps_sampled: 89952000
    num_steps_trained: 89952000
    sample_time_ms: 119157.418
    update_time_ms: 15.998
  iterations_since_restore: 287
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.897115384615386
    ram_util_percent: 12.44759615384615
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 21.0
    agent-2: 69.0
    agent-3: 43.0
    agent-4: 31.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 17.18
    agent-1: 9.12
    agent-2: 47.53
    agent-3: 26.72
    agent-4: 15.15
    agent-5: 18.28
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 23.0
    agent-3: 13.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.20084153282942
    mean_inference_ms: 15.054231840536584
    mean_processing_ms: 73.28521387875712
  time_since_restore: 41693.27634215355
  time_this_iter_s: 145.9882619380951
  time_total_s: 130629.95447182655
  timestamp: 1637404729
  timesteps_since_restore: 27552000
  timesteps_this_iter: 96000
  timesteps_total: 89952000
  training_iteration: 937
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    937 |           130630 | 89952000 |   133.98 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 7.03
    apples_agent-0_min: 1
    apples_agent-1_max: 51
    apples_agent-1_mean: 3.97
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 29.08
    apples_agent-2_min: 4
    apples_agent-3_max: 30
    apples_agent-3_mean: 8.76
    apples_agent-3_min: 3
    apples_agent-4_max: 25
    apples_agent-4_mean: 10.59
    apples_agent-4_min: 2
    apples_agent-5_max: 19
    apples_agent-5_mean: 4.87
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.72
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 257
    cleaning_beam_agent-1_mean: 221.23
    cleaning_beam_agent-1_min: 182
    cleaning_beam_agent-2_max: 50
    cleaning_beam_agent-2_mean: 10.39
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 10.07
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.62
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 6.02
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-41-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 193.0
  episode_reward_mean: 134.96
  episode_reward_min: 94.0
  episodes_this_iter: 96
  episodes_total: 90048
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11849.73
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3274190425872803
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014295836444944143
        model: {}
        policy_loss: -0.0018815468065440655
        total_loss: -0.00234037428162992
        vf_explained_var: 0.01719944179058075
        vf_loss: 1.1742953062057495
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20086322724819183
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008376892074011266
        model: {}
        policy_loss: -0.0013803099282085896
        total_loss: -0.0016793478280305862
        vf_explained_var: 0.09649601578712463
        vf_loss: 0.5448105335235596
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4320819675922394
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012905072653666139
        model: {}
        policy_loss: -0.0018144201021641493
        total_loss: -0.002161359414458275
        vf_explained_var: 0.0091448575258255
        vf_loss: 4.1352434158325195
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4210672974586487
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012476929696276784
        model: {}
        policy_loss: -0.0014664968475699425
        total_loss: -0.0019953667651861906
        vf_explained_var: 0.002579912543296814
        vf_loss: 2.122091293334961
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3802800476551056
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005519138649106026
        model: {}
        policy_loss: -0.0012278679059818387
        total_loss: -0.0017873679753392935
        vf_explained_var: 0.019688040018081665
        vf_loss: 1.0979219675064087
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5646724104881287
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013355596456676722
        model: {}
        policy_loss: -0.0017805323004722595
        total_loss: -0.002641154918819666
        vf_explained_var: 0.020143702626228333
        vf_loss: 1.3320459127426147
    load_time_ms: 14131.497
    num_steps_sampled: 90048000
    num_steps_trained: 90048000
    sample_time_ms: 119187.283
    update_time_ms: 16.11
  iterations_since_restore: 288
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.85314009661835
    ram_util_percent: 12.381159420289858
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 22.0
    agent-2: 68.0
    agent-3: 40.0
    agent-4: 27.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 17.32
    agent-1: 9.2
    agent-2: 46.71
    agent-3: 27.02
    agent-4: 15.51
    agent-5: 19.2
  policy_reward_min:
    agent-0: 8.0
    agent-1: 3.0
    agent-2: 7.0
    agent-3: 15.0
    agent-4: 6.0
    agent-5: 10.0
  sampler_perf:
    mean_env_wait_ms: 28.201076491094373
    mean_inference_ms: 15.054106009256177
    mean_processing_ms: 73.28359323414948
  time_since_restore: 41838.83634972572
  time_this_iter_s: 145.56000757217407
  time_total_s: 130775.51447939873
  timestamp: 1637404875
  timesteps_since_restore: 27648000
  timesteps_this_iter: 96000
  timesteps_total: 90048000
  training_iteration: 938
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    938 |           130776 | 90048000 |   134.96 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 6.59
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.1
    apples_agent-1_min: 0
    apples_agent-2_max: 77
    apples_agent-2_mean: 31.23
    apples_agent-2_min: 12
    apples_agent-3_max: 29
    apples_agent-3_mean: 8.98
    apples_agent-3_min: 2
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.22
    apples_agent-4_min: 3
    apples_agent-5_max: 20
    apples_agent-5_mean: 4.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 2.37
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 256
    cleaning_beam_agent-1_mean: 217.55
    cleaning_beam_agent-1_min: 59
    cleaning_beam_agent-2_max: 25
    cleaning_beam_agent-2_mean: 8.37
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 9.5
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 18
    cleaning_beam_agent-4_mean: 2.81
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 5.14
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-43-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 176.0
  episode_reward_mean: 133.06
  episode_reward_min: 45.0
  episodes_this_iter: 96
  episodes_total: 90144
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11846.777
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31632959842681885
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015621185302734375
        model: {}
        policy_loss: -0.0017974507063627243
        total_loss: -0.0022368435747921467
        vf_explained_var: -0.0009080469608306885
        vf_loss: 1.1734789609909058
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1968253254890442
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010963021777570248
        model: {}
        policy_loss: -0.0015025176107883453
        total_loss: -0.0017929973546415567
        vf_explained_var: 0.08056683838367462
        vf_loss: 0.5593201518058777
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43001189827919006
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010048288386315107
        model: {}
        policy_loss: -0.0017654018010944128
        total_loss: -0.00211897655390203
        vf_explained_var: 0.02146473526954651
        vf_loss: 4.032474994659424
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41796183586120605
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016598186921328306
        model: {}
        policy_loss: -0.0016655512154102325
        total_loss: -0.0022116880863904953
        vf_explained_var: -0.0006734728813171387
        vf_loss: 1.8947458267211914
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37613731622695923
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008123550796881318
        model: {}
        policy_loss: -0.0012989146634936333
        total_loss: -0.001859742682427168
        vf_explained_var: 0.013504669070243835
        vf_loss: 1.0117478370666504
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5560236573219299
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015242225490510464
        model: {}
        policy_loss: -0.0019356990233063698
        total_loss: -0.0027701258659362793
        vf_explained_var: 0.016873225569725037
        vf_loss: 1.4417691230773926
    load_time_ms: 14146.403
    num_steps_sampled: 90144000
    num_steps_trained: 90144000
    sample_time_ms: 119174.145
    update_time_ms: 16.156
  iterations_since_restore: 289
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.03236714975845
    ram_util_percent: 12.47149758454106
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 16.0
    agent-2: 72.0
    agent-3: 40.0
    agent-4: 26.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 17.03
    agent-1: 9.31
    agent-2: 46.91
    agent-3: 26.07
    agent-4: 15.07
    agent-5: 18.67
  policy_reward_min:
    agent-0: 2.0
    agent-1: 2.0
    agent-2: 18.0
    agent-3: 11.0
    agent-4: 4.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.201892699858373
    mean_inference_ms: 15.05435338430567
    mean_processing_ms: 73.28346766983869
  time_since_restore: 41983.926258802414
  time_this_iter_s: 145.08990907669067
  time_total_s: 130920.60438847542
  timestamp: 1637405020
  timesteps_since_restore: 27744000
  timesteps_this_iter: 96000
  timesteps_total: 90144000
  training_iteration: 939
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    939 |           130921 | 90144000 |   133.06 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 7.69
    apples_agent-0_min: 1
    apples_agent-1_max: 25
    apples_agent-1_mean: 3.79
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 30.08
    apples_agent-2_min: 15
    apples_agent-3_max: 32
    apples_agent-3_mean: 9.59
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.9
    apples_agent-4_min: 3
    apples_agent-5_max: 17
    apples_agent-5_mean: 4.67
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 18
    cleaning_beam_agent-0_mean: 1.86
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 251
    cleaning_beam_agent-1_mean: 223.05
    cleaning_beam_agent-1_min: 183
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 8.39
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 31
    cleaning_beam_agent-3_mean: 9.36
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 2.58
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 5.45
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-46-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 188.0
  episode_reward_mean: 134.71
  episode_reward_min: 79.0
  episodes_this_iter: 96
  episodes_total: 90240
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11837.831
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.312040239572525
        entropy_coeff: 0.0017600000137463212
        kl: 0.001208169967867434
        model: {}
        policy_loss: -0.0019037985475733876
        total_loss: -0.0023413156159222126
        vf_explained_var: 0.005984991788864136
        vf_loss: 1.1167445182800293
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20220816135406494
        entropy_coeff: 0.0017600000137463212
        kl: 0.001078232889994979
        model: {}
        policy_loss: -0.0015085737686604261
        total_loss: -0.001813515555113554
        vf_explained_var: 0.06880606710910797
        vf_loss: 0.5094555020332336
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4319366216659546
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015925501938909292
        model: {}
        policy_loss: -0.0019144169054925442
        total_loss: -0.002285565249621868
        vf_explained_var: 0.007644861936569214
        vf_loss: 3.8905608654022217
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4123333692550659
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018228691769763827
        model: {}
        policy_loss: -0.001560913398861885
        total_loss: -0.002096387557685375
        vf_explained_var: 0.001329675316810608
        vf_loss: 1.902327537536621
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3735106885433197
        entropy_coeff: 0.0017600000137463212
        kl: 0.000863226072397083
        model: {}
        policy_loss: -0.0013113943859934807
        total_loss: -0.0018562325276434422
        vf_explained_var: 0.016106098890304565
        vf_loss: 1.1254208087921143
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5428290963172913
        entropy_coeff: 0.0017600000137463212
        kl: 0.002317585051059723
        model: {}
        policy_loss: -0.001785995438694954
        total_loss: -0.0026088664308190346
        vf_explained_var: 0.028577223420143127
        vf_loss: 1.3250867128372192
    load_time_ms: 14143.646
    num_steps_sampled: 90240000
    num_steps_trained: 90240000
    sample_time_ms: 119194.217
    update_time_ms: 16.139
  iterations_since_restore: 290
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.972330097087376
    ram_util_percent: 12.455825242718445
  pid: 27065
  policy_reward_max:
    agent-0: 33.0
    agent-1: 17.0
    agent-2: 66.0
    agent-3: 42.0
    agent-4: 33.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 17.24
    agent-1: 9.08
    agent-2: 46.58
    agent-3: 26.99
    agent-4: 15.98
    agent-5: 18.84
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 28.0
    agent-3: 15.0
    agent-4: 4.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.202374228126175
    mean_inference_ms: 15.05423069371025
    mean_processing_ms: 73.282219999841
  time_since_restore: 42128.73701095581
  time_this_iter_s: 144.8107521533966
  time_total_s: 131065.41514062881
  timestamp: 1637405165
  timesteps_since_restore: 27840000
  timesteps_this_iter: 96000
  timesteps_total: 90240000
  training_iteration: 940
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    940 |           131065 | 90240000 |   134.71 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 7.41
    apples_agent-0_min: 2
    apples_agent-1_max: 50
    apples_agent-1_mean: 4.0
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 30.7
    apples_agent-2_min: 15
    apples_agent-3_max: 22
    apples_agent-3_mean: 9.28
    apples_agent-3_min: 3
    apples_agent-4_max: 28
    apples_agent-4_mean: 11.2
    apples_agent-4_min: 2
    apples_agent-5_max: 16
    apples_agent-5_mean: 4.78
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 25
    cleaning_beam_agent-0_mean: 2.13
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 258
    cleaning_beam_agent-1_mean: 226.53
    cleaning_beam_agent-1_min: 198
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 7.84
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 47
    cleaning_beam_agent-3_mean: 9.12
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 2.61
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 5.71
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-48-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 177.0
  episode_reward_mean: 136.45
  episode_reward_min: 93.0
  episodes_this_iter: 96
  episodes_total: 90336
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11821.073
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30642861127853394
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011205164482817054
        model: {}
        policy_loss: -0.0017302591586485505
        total_loss: -0.0021637477912008762
        vf_explained_var: 0.01279476284980774
        vf_loss: 1.0582736730575562
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19876845180988312
        entropy_coeff: 0.0017600000137463212
        kl: 0.001212994335219264
        model: {}
        policy_loss: -0.0015462489100173116
        total_loss: -0.0018422403372824192
        vf_explained_var: 0.09544375538825989
        vf_loss: 0.5383981466293335
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41834861040115356
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008065582369454205
        model: {}
        policy_loss: -0.0015177102759480476
        total_loss: -0.0018649548292160034
        vf_explained_var: 0.027407661080360413
        vf_loss: 3.8904714584350586
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4143539071083069
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012262584641575813
        model: {}
        policy_loss: -0.001454322598874569
        total_loss: -0.0019687251187860966
        vf_explained_var: 0.006259620189666748
        vf_loss: 2.148608446121216
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3757447898387909
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008225304773077369
        model: {}
        policy_loss: -0.0014257319271564484
        total_loss: -0.0019761044532060623
        vf_explained_var: 0.019374847412109375
        vf_loss: 1.1093592643737793
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5409368872642517
        entropy_coeff: 0.0017600000137463212
        kl: 0.001804673345759511
        model: {}
        policy_loss: -0.0017457585781812668
        total_loss: -0.0025480585172772408
        vf_explained_var: 0.0075006186962127686
        vf_loss: 1.4974846839904785
    load_time_ms: 14130.677
    num_steps_sampled: 90336000
    num_steps_trained: 90336000
    sample_time_ms: 119209.712
    update_time_ms: 15.973
  iterations_since_restore: 291
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.802380952380954
    ram_util_percent: 12.463809523809521
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 20.0
    agent-2: 67.0
    agent-3: 48.0
    agent-4: 28.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 17.43
    agent-1: 9.26
    agent-2: 46.96
    agent-3: 28.1
    agent-4: 16.1
    agent-5: 18.6
  policy_reward_min:
    agent-0: 7.0
    agent-1: 3.0
    agent-2: 32.0
    agent-3: 12.0
    agent-4: 6.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.2034498162041
    mean_inference_ms: 15.054274374446184
    mean_processing_ms: 73.28295317831922
  time_since_restore: 42273.75767111778
  time_this_iter_s: 145.02066016197205
  time_total_s: 131210.4358007908
  timestamp: 1637405312
  timesteps_since_restore: 27936000
  timesteps_this_iter: 96000
  timesteps_total: 90336000
  training_iteration: 941
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    941 |           131210 | 90336000 |   136.45 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 6.76
    apples_agent-0_min: 1
    apples_agent-1_max: 15
    apples_agent-1_mean: 3.51
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 31.24
    apples_agent-2_min: 17
    apples_agent-3_max: 22
    apples_agent-3_mean: 9.45
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.37
    apples_agent-4_min: 3
    apples_agent-5_max: 20
    apples_agent-5_mean: 4.96
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 19
    cleaning_beam_agent-0_mean: 1.91
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 259
    cleaning_beam_agent-1_mean: 224.17
    cleaning_beam_agent-1_min: 192
    cleaning_beam_agent-2_max: 26
    cleaning_beam_agent-2_mean: 7.62
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 10.73
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.64
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 5.53
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-50-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 187.0
  episode_reward_mean: 136.9
  episode_reward_min: 100.0
  episodes_this_iter: 96
  episodes_total: 90432
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11820.412
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3110102415084839
        entropy_coeff: 0.0017600000137463212
        kl: 0.000920501712244004
        model: {}
        policy_loss: -0.001613793894648552
        total_loss: -0.002053653821349144
        vf_explained_var: 0.012646526098251343
        vf_loss: 1.075161099433899
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19926458597183228
        entropy_coeff: 0.0017600000137463212
        kl: 0.001130573102273047
        model: {}
        policy_loss: -0.0016224198043346405
        total_loss: -0.001915457658469677
        vf_explained_var: 0.08451759815216064
        vf_loss: 0.5766723155975342
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41811707615852356
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012434364762157202
        model: {}
        policy_loss: -0.00177847221493721
        total_loss: -0.002119363285601139
        vf_explained_var: 0.02784307301044464
        vf_loss: 3.9499173164367676
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4161093831062317
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011127616744488478
        model: {}
        policy_loss: -0.0012424623128026724
        total_loss: -0.0017812149599194527
        vf_explained_var: 0.0074959397315979
        vf_loss: 1.9359992742538452
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3804795742034912
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007706951582804322
        model: {}
        policy_loss: -0.001364007475785911
        total_loss: -0.0019251438789069653
        vf_explained_var: 0.013892441987991333
        vf_loss: 1.0850982666015625
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5487564206123352
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019005101639777422
        model: {}
        policy_loss: -0.0014685431960970163
        total_loss: -0.0022887936793267727
        vf_explained_var: 0.025061801075935364
        vf_loss: 1.4555904865264893
    load_time_ms: 14131.148
    num_steps_sampled: 90432000
    num_steps_trained: 90432000
    sample_time_ms: 119237.111
    update_time_ms: 16.02
  iterations_since_restore: 292
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.90096618357488
    ram_util_percent: 12.37632850241546
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 69.0
    agent-3: 41.0
    agent-4: 28.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 16.47
    agent-1: 9.65
    agent-2: 47.67
    agent-3: 27.41
    agent-4: 15.77
    agent-5: 19.93
  policy_reward_min:
    agent-0: 7.0
    agent-1: 3.0
    agent-2: 31.0
    agent-3: 13.0
    agent-4: 7.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.203920460271384
    mean_inference_ms: 15.054287759589899
    mean_processing_ms: 73.28149596546982
  time_since_restore: 42419.191850185394
  time_this_iter_s: 145.4341790676117
  time_total_s: 131355.8699798584
  timestamp: 1637405458
  timesteps_since_restore: 28032000
  timesteps_this_iter: 96000
  timesteps_total: 90432000
  training_iteration: 942
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    942 |           131356 | 90432000 |    136.9 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 6.77
    apples_agent-0_min: 1
    apples_agent-1_max: 18
    apples_agent-1_mean: 3.77
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 30.04
    apples_agent-2_min: 16
    apples_agent-3_max: 24
    apples_agent-3_mean: 9.12
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.5
    apples_agent-4_min: 3
    apples_agent-5_max: 22
    apples_agent-5_mean: 4.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 17
    cleaning_beam_agent-0_mean: 1.9
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 254
    cleaning_beam_agent-1_mean: 222.22
    cleaning_beam_agent-1_min: 167
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 7.85
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 63
    cleaning_beam_agent-3_mean: 9.61
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.29
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 6.0
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-53-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 185.0
  episode_reward_mean: 131.88
  episode_reward_min: 42.0
  episodes_this_iter: 96
  episodes_total: 90528
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11903.958
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3139267563819885
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011079097166657448
        model: {}
        policy_loss: -0.0017521984409540892
        total_loss: -0.0020639337599277496
        vf_explained_var: 0.003881603479385376
        vf_loss: 2.4077537059783936
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20255893468856812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011533823562785983
        model: {}
        policy_loss: -0.0016249697655439377
        total_loss: -0.001924877054989338
        vf_explained_var: 0.09330369532108307
        vf_loss: 0.5659729242324829
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4254131615161896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016055229352787137
        model: {}
        policy_loss: -0.001806706190109253
        total_loss: -0.002161540789529681
        vf_explained_var: 0.022070541977882385
        vf_loss: 3.938920497894287
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4139633774757385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013327749911695719
        model: {}
        policy_loss: -0.0013044063234701753
        total_loss: -0.001829021261073649
        vf_explained_var: 0.0015039443969726562
        vf_loss: 2.0396158695220947
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37241190671920776
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008228740189224482
        model: {}
        policy_loss: -0.0014620604924857616
        total_loss: -0.002023369073867798
        vf_explained_var: 0.009015902876853943
        vf_loss: 0.94139564037323
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5510196685791016
        entropy_coeff: 0.0017600000137463212
        kl: 0.001923764357343316
        model: {}
        policy_loss: -0.0017127037281170487
        total_loss: -0.0025537237524986267
        vf_explained_var: 0.0158565491437912
        vf_loss: 1.2877401113510132
    load_time_ms: 14136.792
    num_steps_sampled: 90528000
    num_steps_trained: 90528000
    sample_time_ms: 119243.983
    update_time_ms: 16.242
  iterations_since_restore: 293
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.886956521739133
    ram_util_percent: 12.468115942028984
  pid: 27065
  policy_reward_max:
    agent-0: 34.0
    agent-1: 17.0
    agent-2: 63.0
    agent-3: 47.0
    agent-4: 27.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 15.91
    agent-1: 9.72
    agent-2: 46.32
    agent-3: 26.88
    agent-4: 14.31
    agent-5: 18.74
  policy_reward_min:
    agent-0: -35.0
    agent-1: 3.0
    agent-2: 28.0
    agent-3: 14.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.203923727831143
    mean_inference_ms: 15.054011069416912
    mean_processing_ms: 73.28032880338823
  time_since_restore: 42564.303968667984
  time_this_iter_s: 145.11211848258972
  time_total_s: 131500.982098341
  timestamp: 1637405603
  timesteps_since_restore: 28128000
  timesteps_this_iter: 96000
  timesteps_total: 90528000
  training_iteration: 943
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    943 |           131501 | 90528000 |   131.88 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 6.83
    apples_agent-0_min: 1
    apples_agent-1_max: 15
    apples_agent-1_mean: 3.64
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 30.54
    apples_agent-2_min: 8
    apples_agent-3_max: 40
    apples_agent-3_mean: 10.16
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.73
    apples_agent-4_min: 2
    apples_agent-5_max: 21
    apples_agent-5_mean: 4.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.71
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 242
    cleaning_beam_agent-1_mean: 216.43
    cleaning_beam_agent-1_min: 56
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 6.93
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 47
    cleaning_beam_agent-3_mean: 10.21
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.48
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 5.86
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-55-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 182.0
  episode_reward_mean: 134.54
  episode_reward_min: 41.0
  episodes_this_iter: 96
  episodes_total: 90624
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11897.405
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3113434910774231
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012969481758773327
        model: {}
        policy_loss: -0.0019229361787438393
        total_loss: -0.0023578666150569916
        vf_explained_var: 0.016197964549064636
        vf_loss: 1.1303305625915527
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19731968641281128
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010898311156779528
        model: {}
        policy_loss: -0.0013034215662628412
        total_loss: -0.0015924999024719
        vf_explained_var: 0.09136936068534851
        vf_loss: 0.5820412039756775
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4304572343826294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012744413688778877
        model: {}
        policy_loss: -0.0018915832042694092
        total_loss: -0.0022528693079948425
        vf_explained_var: 0.016669660806655884
        vf_loss: 3.963179588317871
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41919636726379395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015063543105497956
        model: {}
        policy_loss: -0.0016089181881397963
        total_loss: -0.0021447595208883286
        vf_explained_var: -0.002164795994758606
        vf_loss: 2.019432544708252
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3705177307128906
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008012314792722464
        model: {}
        policy_loss: -0.0012293439358472824
        total_loss: -0.0017707236111164093
        vf_explained_var: 0.015790477395057678
        vf_loss: 1.107312798500061
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5558781623840332
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014618816785514355
        model: {}
        policy_loss: -0.0017009926959872246
        total_loss: -0.0025470061227679253
        vf_explained_var: 0.01714746654033661
        vf_loss: 1.3233246803283691
    load_time_ms: 14115.286
    num_steps_sampled: 90624000
    num_steps_trained: 90624000
    sample_time_ms: 119396.247
    update_time_ms: 16.059
  iterations_since_restore: 294
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.844976076555024
    ram_util_percent: 12.377033492822967
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 21.0
    agent-2: 62.0
    agent-3: 44.0
    agent-4: 30.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 16.95
    agent-1: 9.56
    agent-2: 47.13
    agent-3: 27.36
    agent-4: 15.15
    agent-5: 18.39
  policy_reward_min:
    agent-0: 8.0
    agent-1: 2.0
    agent-2: 16.0
    agent-3: 7.0
    agent-4: 5.0
    agent-5: 2.0
  sampler_perf:
    mean_env_wait_ms: 28.20439117729708
    mean_inference_ms: 15.054358545180744
    mean_processing_ms: 73.28093321204742
  time_since_restore: 42711.02858519554
  time_this_iter_s: 146.72461652755737
  time_total_s: 131647.70671486855
  timestamp: 1637405750
  timesteps_since_restore: 28224000
  timesteps_this_iter: 96000
  timesteps_total: 90624000
  training_iteration: 944
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    944 |           131648 | 90624000 |   134.54 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 7.29
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.33
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 30.49
    apples_agent-2_min: 17
    apples_agent-3_max: 33
    apples_agent-3_mean: 9.01
    apples_agent-3_min: 3
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.46
    apples_agent-4_min: 1
    apples_agent-5_max: 16
    apples_agent-5_mean: 5.31
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 15
    cleaning_beam_agent-0_mean: 1.89
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 254
    cleaning_beam_agent-1_mean: 218.48
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 9.1
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 9.63
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.6
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 5.79
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_05-58-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 180.0
  episode_reward_mean: 136.18
  episode_reward_min: 98.0
  episodes_this_iter: 96
  episodes_total: 90720
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11891.32
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31870701909065247
        entropy_coeff: 0.0017600000137463212
        kl: 0.001281907083466649
        model: {}
        policy_loss: -0.0018975809216499329
        total_loss: -0.0023442967794835567
        vf_explained_var: 0.010543018579483032
        vf_loss: 1.142069697380066
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19931364059448242
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011426741257309914
        model: {}
        policy_loss: -0.0013962676748633385
        total_loss: -0.0016917751636356115
        vf_explained_var: 0.08326065540313721
        vf_loss: 0.5528290867805481
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43658414483070374
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013961507938802242
        model: {}
        policy_loss: -0.0017747925594449043
        total_loss: -0.0021681669168174267
        vf_explained_var: 0.01807764172554016
        vf_loss: 3.7501156330108643
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4115962088108063
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014044975396245718
        model: {}
        policy_loss: -0.0015977313742041588
        total_loss: -0.0021435755770653486
        vf_explained_var: -0.0005808770656585693
        vf_loss: 1.785651445388794
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.367844820022583
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009875607211142778
        model: {}
        policy_loss: -0.0013138935901224613
        total_loss: -0.001858644187450409
        vf_explained_var: 0.02463182806968689
        vf_loss: 1.0265474319458008
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5464532971382141
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017522646812722087
        model: {}
        policy_loss: -0.001545976847410202
        total_loss: -0.002362467348575592
        vf_explained_var: 0.008445009589195251
        vf_loss: 1.452658772468567
    load_time_ms: 14139.866
    num_steps_sampled: 90720000
    num_steps_trained: 90720000
    sample_time_ms: 119200.573
    update_time_ms: 16.105
  iterations_since_restore: 295
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.85145631067961
    ram_util_percent: 12.468932038834948
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 20.0
    agent-2: 66.0
    agent-3: 43.0
    agent-4: 29.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 17.4
    agent-1: 9.51
    agent-2: 47.35
    agent-3: 26.85
    agent-4: 15.18
    agent-5: 19.89
  policy_reward_min:
    agent-0: 9.0
    agent-1: 3.0
    agent-2: 34.0
    agent-3: 14.0
    agent-4: 7.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.20424181846293
    mean_inference_ms: 15.053694755365745
    mean_processing_ms: 73.27939348470031
  time_since_restore: 42855.501757860184
  time_this_iter_s: 144.47317266464233
  time_total_s: 131792.1798875332
  timestamp: 1637405894
  timesteps_since_restore: 28320000
  timesteps_this_iter: 96000
  timesteps_total: 90720000
  training_iteration: 945
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    945 |           131792 | 90720000 |   136.18 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 7.73
    apples_agent-0_min: 1
    apples_agent-1_max: 12
    apples_agent-1_mean: 3.47
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 31.61
    apples_agent-2_min: 20
    apples_agent-3_max: 23
    apples_agent-3_mean: 9.36
    apples_agent-3_min: 3
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.73
    apples_agent-4_min: 3
    apples_agent-5_max: 15
    apples_agent-5_mean: 4.88
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 22
    cleaning_beam_agent-0_mean: 2.13
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 245
    cleaning_beam_agent-1_mean: 217.85
    cleaning_beam_agent-1_min: 166
    cleaning_beam_agent-2_max: 25
    cleaning_beam_agent-2_mean: 8.8
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 10.83
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 7
    cleaning_beam_agent-4_mean: 2.16
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 5.75
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-00-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 180.0
  episode_reward_mean: 136.56
  episode_reward_min: 97.0
  episodes_this_iter: 96
  episodes_total: 90816
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11919.394
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31970763206481934
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013422062620520592
        model: {}
        policy_loss: -0.0018392931669950485
        total_loss: -0.0022870837710797787
        vf_explained_var: 0.0015145987272262573
        vf_loss: 1.1489288806915283
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19870999455451965
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012391081545501947
        model: {}
        policy_loss: -0.0017255269922316074
        total_loss: -0.00202000280842185
        vf_explained_var: 0.09360413253307343
        vf_loss: 0.5525588393211365
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4314730167388916
        entropy_coeff: 0.0017600000137463212
        kl: 0.001214774209074676
        model: {}
        policy_loss: -0.001641489565372467
        total_loss: -0.0020170630887150764
        vf_explained_var: 0.030172929167747498
        vf_loss: 3.83819580078125
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41600245237350464
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009724079282023013
        model: {}
        policy_loss: -0.0014266506768763065
        total_loss: -0.0019643865525722504
        vf_explained_var: 0.005188450217247009
        vf_loss: 1.9443132877349854
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3714244067668915
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008139950805343688
        model: {}
        policy_loss: -0.0011719252215698361
        total_loss: -0.0017152591608464718
        vf_explained_var: 0.006010264158248901
        vf_loss: 1.1037150621414185
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5490727424621582
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022464676294475794
        model: {}
        policy_loss: -0.0020025409758090973
        total_loss: -0.0028283922001719475
        vf_explained_var: 0.023938491940498352
        vf_loss: 1.4051669836044312
    load_time_ms: 14167.771
    num_steps_sampled: 90816000
    num_steps_trained: 90816000
    sample_time_ms: 119250.193
    update_time_ms: 15.869
  iterations_since_restore: 296
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.950239234449757
    ram_util_percent: 12.479904306220094
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 69.0
    agent-3: 42.0
    agent-4: 30.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 17.23
    agent-1: 9.53
    agent-2: 47.7
    agent-3: 26.7
    agent-4: 15.96
    agent-5: 19.44
  policy_reward_min:
    agent-0: 8.0
    agent-1: 2.0
    agent-2: 22.0
    agent-3: 16.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.20480142166436
    mean_inference_ms: 15.053491826436368
    mean_processing_ms: 73.27965058101286
  time_since_restore: 43001.86800098419
  time_this_iter_s: 146.36624312400818
  time_total_s: 131938.5461306572
  timestamp: 1637406041
  timesteps_since_restore: 28416000
  timesteps_this_iter: 96000
  timesteps_total: 90816000
  training_iteration: 946
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    946 |           131939 | 90816000 |   136.56 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 7.16
    apples_agent-0_min: 0
    apples_agent-1_max: 54
    apples_agent-1_mean: 4.13
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 30.99
    apples_agent-2_min: 19
    apples_agent-3_max: 43
    apples_agent-3_mean: 9.18
    apples_agent-3_min: 2
    apples_agent-4_max: 19
    apples_agent-4_mean: 10.03
    apples_agent-4_min: 2
    apples_agent-5_max: 14
    apples_agent-5_mean: 5.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 1.97
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 249
    cleaning_beam_agent-1_mean: 216.57
    cleaning_beam_agent-1_min: 181
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 10.31
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 9.83
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 7
    cleaning_beam_agent-4_mean: 2.48
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 5.56
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-03-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 190.0
  episode_reward_mean: 136.7
  episode_reward_min: 91.0
  episodes_this_iter: 96
  episodes_total: 90912
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11988.846
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3194632828235626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010055601596832275
        model: {}
        policy_loss: -0.001931808888912201
        total_loss: -0.0023726467043161392
        vf_explained_var: 0.000914454460144043
        vf_loss: 1.2141625881195068
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19657444953918457
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012314028572291136
        model: {}
        policy_loss: -0.0015615758020430803
        total_loss: -0.0018441599095240235
        vf_explained_var: 0.08061370253562927
        vf_loss: 0.633869469165802
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43328428268432617
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015063477912917733
        model: {}
        policy_loss: -0.0019228383898735046
        total_loss: -0.0022707898169755936
        vf_explained_var: 0.010360151529312134
        vf_loss: 4.14627742767334
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41623157262802124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014248867519199848
        model: {}
        policy_loss: -0.0016710646450519562
        total_loss: -0.002210915321484208
        vf_explained_var: 0.001992836594581604
        vf_loss: 1.9271700382232666
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3726688623428345
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007903505465947092
        model: {}
        policy_loss: -0.0013291232753545046
        total_loss: -0.0018763563130050898
        vf_explained_var: 0.011870518326759338
        vf_loss: 1.086644172668457
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5464909672737122
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018723254324868321
        model: {}
        policy_loss: -0.0016455798177048564
        total_loss: -0.0024644760414958
        vf_explained_var: 0.01883511245250702
        vf_loss: 1.429300308227539
    load_time_ms: 14159.7
    num_steps_sampled: 90912000
    num_steps_trained: 90912000
    sample_time_ms: 119134.865
    update_time_ms: 16.125
  iterations_since_restore: 297
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.97729468599034
    ram_util_percent: 12.470048309178743
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 20.0
    agent-2: 72.0
    agent-3: 39.0
    agent-4: 33.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 16.9
    agent-1: 10.51
    agent-2: 48.08
    agent-3: 26.57
    agent-4: 15.32
    agent-5: 19.32
  policy_reward_min:
    agent-0: 7.0
    agent-1: 4.0
    agent-2: 32.0
    agent-3: 15.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.205410795824754
    mean_inference_ms: 15.0532396638785
    mean_processing_ms: 73.27986887265142
  time_since_restore: 43147.22646021843
  time_this_iter_s: 145.35845923423767
  time_total_s: 132083.90458989143
  timestamp: 1637406186
  timesteps_since_restore: 28512000
  timesteps_this_iter: 96000
  timesteps_total: 90912000
  training_iteration: 947
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    947 |           132084 | 90912000 |    136.7 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 7.03
    apples_agent-0_min: 1
    apples_agent-1_max: 17
    apples_agent-1_mean: 3.51
    apples_agent-1_min: 1
    apples_agent-2_max: 53
    apples_agent-2_mean: 30.37
    apples_agent-2_min: 10
    apples_agent-3_max: 42
    apples_agent-3_mean: 9.81
    apples_agent-3_min: 3
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.84
    apples_agent-4_min: 3
    apples_agent-5_max: 12
    apples_agent-5_mean: 4.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 18
    cleaning_beam_agent-0_mean: 1.99
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 251
    cleaning_beam_agent-1_mean: 219.69
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 9.93
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 38
    cleaning_beam_agent-3_mean: 10.88
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 2.5
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 5.65
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-05-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 189.0
  episode_reward_mean: 133.41
  episode_reward_min: 80.0
  episodes_this_iter: 96
  episodes_total: 91008
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11990.302
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3160944879055023
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015046608168631792
        model: {}
        policy_loss: -0.0020912950858473778
        total_loss: -0.0025354530662298203
        vf_explained_var: 0.007662028074264526
        vf_loss: 1.1217067241668701
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19867351651191711
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007964674732647836
        model: {}
        policy_loss: -0.0013343968894332647
        total_loss: -0.0016260852571576834
        vf_explained_var: 0.08777837455272675
        vf_loss: 0.5797597169876099
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4404638409614563
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011992952786386013
        model: {}
        policy_loss: -0.001689741387963295
        total_loss: -0.0020947731100022793
        vf_explained_var: 0.005953282117843628
        vf_loss: 3.7018375396728516
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4191487431526184
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015933474060148
        model: {}
        policy_loss: -0.0016991436714306474
        total_loss: -0.002251274883747101
        vf_explained_var: 0.006157219409942627
        vf_loss: 1.8557112216949463
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3827744126319885
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012825868325307965
        model: {}
        policy_loss: -0.0014445977285504341
        total_loss: -0.002014417666941881
        vf_explained_var: 0.012595459818840027
        vf_loss: 1.038625955581665
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5464106202125549
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018754589837044477
        model: {}
        policy_loss: -0.001674783881753683
        total_loss: -0.002511298283934593
        vf_explained_var: 0.011494860053062439
        vf_loss: 1.251664638519287
    load_time_ms: 14180.001
    num_steps_sampled: 91008000
    num_steps_trained: 91008000
    sample_time_ms: 119082.111
    update_time_ms: 15.982
  iterations_since_restore: 298
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.956521739130434
    ram_util_percent: 12.461835748792268
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 19.0
    agent-2: 66.0
    agent-3: 41.0
    agent-4: 29.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 16.77
    agent-1: 9.53
    agent-2: 46.46
    agent-3: 27.08
    agent-4: 14.92
    agent-5: 18.65
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 12.0
    agent-3: 15.0
    agent-4: 6.0
    agent-5: 10.0
  sampler_perf:
    mean_env_wait_ms: 28.20605545475283
    mean_inference_ms: 15.053283569921893
    mean_processing_ms: 73.28000273093832
  time_since_restore: 43292.52784204483
  time_this_iter_s: 145.30138182640076
  time_total_s: 132229.20597171783
  timestamp: 1637406332
  timesteps_since_restore: 28608000
  timesteps_this_iter: 96000
  timesteps_total: 91008000
  training_iteration: 948
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    948 |           132229 | 91008000 |   133.41 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 7.42
    apples_agent-0_min: 2
    apples_agent-1_max: 27
    apples_agent-1_mean: 3.53
    apples_agent-1_min: 0
    apples_agent-2_max: 54
    apples_agent-2_mean: 31.36
    apples_agent-2_min: 5
    apples_agent-3_max: 20
    apples_agent-3_mean: 9.36
    apples_agent-3_min: 2
    apples_agent-4_max: 27
    apples_agent-4_mean: 9.04
    apples_agent-4_min: 1
    apples_agent-5_max: 21
    apples_agent-5_mean: 5.44
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 2.04
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 272
    cleaning_beam_agent-1_mean: 217.32
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 47
    cleaning_beam_agent-2_mean: 10.3
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 12.39
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.16
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 5.5
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-07-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 169.0
  episode_reward_mean: 134.58
  episode_reward_min: 63.0
  episodes_this_iter: 96
  episodes_total: 91104
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11982.28
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31011611223220825
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008356604957953095
        model: {}
        policy_loss: -0.0018481258302927017
        total_loss: -0.0022738634143024683
        vf_explained_var: 0.007372379302978516
        vf_loss: 1.2006622552871704
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19956746697425842
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011834775796160102
        model: {}
        policy_loss: -0.0016140020452439785
        total_loss: -0.0019082152284681797
        vf_explained_var: 0.07733489573001862
        vf_loss: 0.5702667236328125
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43692123889923096
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017141490243375301
        model: {}
        policy_loss: -0.0020476861391216516
        total_loss: -0.0023814861197024584
        vf_explained_var: 0.0012843012809753418
        vf_loss: 4.3518290519714355
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4205018877983093
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013289795024320483
        model: {}
        policy_loss: -0.0013933978043496609
        total_loss: -0.001927973236888647
        vf_explained_var: 0.004992157220840454
        vf_loss: 2.0550942420959473
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3872106373310089
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008862901013344526
        model: {}
        policy_loss: -0.0014174890238791704
        total_loss: -0.00200391816906631
        vf_explained_var: 0.022684171795845032
        vf_loss: 0.9505804777145386
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.544278621673584
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008909573662094772
        model: {}
        policy_loss: -0.0015012454241514206
        total_loss: -0.002319144085049629
        vf_explained_var: 0.016408100724220276
        vf_loss: 1.4002885818481445
    load_time_ms: 14158.058
    num_steps_sampled: 91104000
    num_steps_trained: 91104000
    sample_time_ms: 119030.864
    update_time_ms: 16.127
  iterations_since_restore: 299
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.958048780487808
    ram_util_percent: 12.31707317073171
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 21.0
    agent-2: 71.0
    agent-3: 38.0
    agent-4: 29.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 17.44
    agent-1: 9.26
    agent-2: 47.87
    agent-3: 26.96
    agent-4: 14.23
    agent-5: 18.82
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 9.0
    agent-3: 9.0
    agent-4: 5.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.20621659867884
    mean_inference_ms: 15.052931813041132
    mean_processing_ms: 73.27798106253763
  time_since_restore: 43436.72905135155
  time_this_iter_s: 144.20120930671692
  time_total_s: 132373.40718102455
  timestamp: 1637406476
  timesteps_since_restore: 28704000
  timesteps_this_iter: 96000
  timesteps_total: 91104000
  training_iteration: 949
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    949 |           132373 | 91104000 |   134.58 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 6.78
    apples_agent-0_min: 2
    apples_agent-1_max: 15
    apples_agent-1_mean: 3.5
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 30.58
    apples_agent-2_min: 14
    apples_agent-3_max: 52
    apples_agent-3_mean: 9.51
    apples_agent-3_min: 2
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.34
    apples_agent-4_min: 2
    apples_agent-5_max: 15
    apples_agent-5_mean: 5.1
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 30
    cleaning_beam_agent-0_mean: 2.58
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 219.73
    cleaning_beam_agent-1_min: 147
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 9.77
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 11.63
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.56
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.99
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-10-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 191.0
  episode_reward_mean: 136.18
  episode_reward_min: 79.0
  episodes_this_iter: 96
  episodes_total: 91200
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11972.4
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3021739721298218
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008248359663411975
        model: {}
        policy_loss: -0.0016774097457528114
        total_loss: -0.0020952261984348297
        vf_explained_var: 0.0072605907917022705
        vf_loss: 1.1400792598724365
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20032323896884918
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010297524277120829
        model: {}
        policy_loss: -0.0014157183468341827
        total_loss: -0.0017115818336606026
        vf_explained_var: 0.09094768762588501
        vf_loss: 0.5670535564422607
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4453141987323761
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017226337222382426
        model: {}
        policy_loss: -0.0019675572402775288
        total_loss: -0.002366130705922842
        vf_explained_var: 0.027245596051216125
        vf_loss: 3.851811170578003
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4250800609588623
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016850506654009223
        model: {}
        policy_loss: -0.0016686292365193367
        total_loss: -0.0022033534478396177
        vf_explained_var: 0.0011518895626068115
        vf_loss: 2.1341543197631836
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38125765323638916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009693599422462285
        model: {}
        policy_loss: -0.0015541985630989075
        total_loss: -0.0021222345530986786
        vf_explained_var: 0.01434256136417389
        vf_loss: 1.029773473739624
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5449643135070801
        entropy_coeff: 0.0017600000137463212
        kl: 0.00121951918117702
        model: {}
        policy_loss: -0.0015073493123054504
        total_loss: -0.0023183999583125114
        vf_explained_var: 0.010489329695701599
        vf_loss: 1.4808838367462158
    load_time_ms: 14166.26
    num_steps_sampled: 91200000
    num_steps_trained: 91200000
    sample_time_ms: 118975.918
    update_time_ms: 15.872
  iterations_since_restore: 300
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.960194174757277
    ram_util_percent: 12.407281553398056
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 18.0
    agent-2: 73.0
    agent-3: 45.0
    agent-4: 25.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 17.09
    agent-1: 9.42
    agent-2: 46.91
    agent-3: 27.89
    agent-4: 15.15
    agent-5: 19.72
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 23.0
    agent-3: 8.0
    agent-4: 4.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.20640635494592
    mean_inference_ms: 15.052791423066319
    mean_processing_ms: 73.2767676315853
  time_since_restore: 43580.98340725899
  time_this_iter_s: 144.25435590744019
  time_total_s: 132517.661536932
  timestamp: 1637406620
  timesteps_since_restore: 28800000
  timesteps_this_iter: 96000
  timesteps_total: 91200000
  training_iteration: 950
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    950 |           132518 | 91200000 |   136.18 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 7.05
    apples_agent-0_min: 1
    apples_agent-1_max: 21
    apples_agent-1_mean: 3.36
    apples_agent-1_min: 0
    apples_agent-2_max: 54
    apples_agent-2_mean: 31.29
    apples_agent-2_min: 20
    apples_agent-3_max: 32
    apples_agent-3_mean: 9.39
    apples_agent-3_min: 3
    apples_agent-4_max: 25
    apples_agent-4_mean: 9.94
    apples_agent-4_min: 3
    apples_agent-5_max: 13
    apples_agent-5_mean: 5.22
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 25
    cleaning_beam_agent-0_mean: 2.45
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 244
    cleaning_beam_agent-1_mean: 219.56
    cleaning_beam_agent-1_min: 193
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 9.91
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 12.71
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 14
    cleaning_beam_agent-4_mean: 2.71
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 19
    cleaning_beam_agent-5_mean: 4.98
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-12-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 184.0
  episode_reward_mean: 134.87
  episode_reward_min: 101.0
  episodes_this_iter: 96
  episodes_total: 91296
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11978.155
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30889591574668884
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011424827389419079
        model: {}
        policy_loss: -0.002007776405662298
        total_loss: -0.0024260026402771473
        vf_explained_var: -0.0005538314580917358
        vf_loss: 1.254320502281189
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20102615654468536
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007356195710599422
        model: {}
        policy_loss: -0.0013196086511015892
        total_loss: -0.0016206242144107819
        vf_explained_var: 0.08645680546760559
        vf_loss: 0.5278831720352173
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44125884771347046
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012472000671550632
        model: {}
        policy_loss: -0.0018631056882441044
        total_loss: -0.0022420836612582207
        vf_explained_var: 0.017316102981567383
        vf_loss: 3.9763879776000977
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4234329164028168
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010359378065913916
        model: {}
        policy_loss: -0.0014915801584720612
        total_loss: -0.0020501897670328617
        vf_explained_var: -0.001779302954673767
        vf_loss: 1.866292953491211
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38361039757728577
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005912765627726912
        model: {}
        policy_loss: -0.0012336364015936852
        total_loss: -0.001820594072341919
        vf_explained_var: 0.005127310752868652
        vf_loss: 0.8819485306739807
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.547511100769043
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016143600223585963
        model: {}
        policy_loss: -0.0017400174401700497
        total_loss: -0.002575370017439127
        vf_explained_var: 0.019156187772750854
        vf_loss: 1.2826789617538452
    load_time_ms: 14178.472
    num_steps_sampled: 91296000
    num_steps_trained: 91296000
    sample_time_ms: 118916.907
    update_time_ms: 15.898
  iterations_since_restore: 301
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.763636363636365
    ram_util_percent: 12.373205741626794
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 18.0
    agent-2: 76.0
    agent-3: 44.0
    agent-4: 24.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 17.72
    agent-1: 9.14
    agent-2: 48.35
    agent-3: 26.55
    agent-4: 14.19
    agent-5: 18.92
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 32.0
    agent-3: 13.0
    agent-4: 7.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.20667657147922
    mean_inference_ms: 15.052788887870298
    mean_processing_ms: 73.27534139862337
  time_since_restore: 43725.64428758621
  time_this_iter_s: 144.66088032722473
  time_total_s: 132662.32241725922
  timestamp: 1637406767
  timesteps_since_restore: 28896000
  timesteps_this_iter: 96000
  timesteps_total: 91296000
  training_iteration: 951
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    951 |           132662 | 91296000 |   134.87 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 7.27
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 3.54
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 30.51
    apples_agent-2_min: 6
    apples_agent-3_max: 25
    apples_agent-3_mean: 9.62
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 9.55
    apples_agent-4_min: 1
    apples_agent-5_max: 26
    apples_agent-5_mean: 5.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 18
    cleaning_beam_agent-0_mean: 2.46
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 253
    cleaning_beam_agent-1_mean: 217.7
    cleaning_beam_agent-1_min: 36
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 9.07
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 10.12
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 16
    cleaning_beam_agent-4_mean: 3.12
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 4.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-15-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 188.0
  episode_reward_mean: 135.45
  episode_reward_min: 24.0
  episodes_this_iter: 96
  episodes_total: 91392
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11971.139
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3136627674102783
        entropy_coeff: 0.0017600000137463212
        kl: 0.001188388210721314
        model: {}
        policy_loss: -0.001969971228390932
        total_loss: -0.002410583198070526
        vf_explained_var: 0.013291463255882263
        vf_loss: 1.114353060722351
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19728875160217285
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007642714190296829
        model: {}
        policy_loss: -0.001400234643369913
        total_loss: -0.0016885469667613506
        vf_explained_var: 0.08871889114379883
        vf_loss: 0.5891829133033752
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43985116481781006
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014100228436291218
        model: {}
        policy_loss: -0.002016150625422597
        total_loss: -0.0023948675952851772
        vf_explained_var: 0.027519792318344116
        vf_loss: 3.9541964530944824
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4244384169578552
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013864516513422132
        model: {}
        policy_loss: -0.0014948903117328882
        total_loss: -0.0020468472503125668
        vf_explained_var: 0.003796130418777466
        vf_loss: 1.950562834739685
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38322556018829346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005118565168231726
        model: {}
        policy_loss: -0.0012480390723794699
        total_loss: -0.0018206669483333826
        vf_explained_var: 0.006680846214294434
        vf_loss: 1.018485426902771
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5541607737541199
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018148202216252685
        model: {}
        policy_loss: -0.0016539979260414839
        total_loss: -0.0024834254290908575
        vf_explained_var: 0.010317996144294739
        vf_loss: 1.4589147567749023
    load_time_ms: 14179.366
    num_steps_sampled: 91392000
    num_steps_trained: 91392000
    sample_time_ms: 118837.918
    update_time_ms: 15.868
  iterations_since_restore: 302
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.07766990291262
    ram_util_percent: 12.458252427184464
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 20.0
    agent-2: 70.0
    agent-3: 41.0
    agent-4: 25.0
    agent-5: 39.0
  policy_reward_mean:
    agent-0: 17.35
    agent-1: 9.55
    agent-2: 47.29
    agent-3: 27.49
    agent-4: 14.56
    agent-5: 19.21
  policy_reward_min:
    agent-0: 4.0
    agent-1: 1.0
    agent-2: 6.0
    agent-3: 7.0
    agent-4: 3.0
    agent-5: 1.0
  sampler_perf:
    mean_env_wait_ms: 28.20704109818192
    mean_inference_ms: 15.052608622659662
    mean_processing_ms: 73.27367240484706
  time_since_restore: 43870.166596889496
  time_this_iter_s: 144.5223093032837
  time_total_s: 132806.8447265625
  timestamp: 1637406912
  timesteps_since_restore: 28992000
  timesteps_this_iter: 96000
  timesteps_total: 91392000
  training_iteration: 952
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    952 |           132807 | 91392000 |   135.45 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 7.33
    apples_agent-0_min: 1
    apples_agent-1_max: 7
    apples_agent-1_mean: 3.35
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 32.04
    apples_agent-2_min: 15
    apples_agent-3_max: 26
    apples_agent-3_mean: 9.57
    apples_agent-3_min: 2
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.48
    apples_agent-4_min: 4
    apples_agent-5_max: 20
    apples_agent-5_mean: 5.05
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 2.0
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 251
    cleaning_beam_agent-1_mean: 223.61
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 19
    cleaning_beam_agent-2_mean: 8.58
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 38
    cleaning_beam_agent-3_mean: 9.51
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.15
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.26
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-17-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 192.0
  episode_reward_mean: 138.33
  episode_reward_min: 62.0
  episodes_this_iter: 96
  episodes_total: 91488
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11914.85
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3106938302516937
        entropy_coeff: 0.0017600000137463212
        kl: 0.000616237404756248
        model: {}
        policy_loss: -0.0015998657327145338
        total_loss: -0.0020237776916474104
        vf_explained_var: 0.005131900310516357
        vf_loss: 1.229080319404602
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2012498676776886
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007668030448257923
        model: {}
        policy_loss: -0.0014165854081511497
        total_loss: -0.0017144130542874336
        vf_explained_var: 0.08598783612251282
        vf_loss: 0.5637261271476746
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43037086725234985
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009704140829853714
        model: {}
        policy_loss: -0.001780414953827858
        total_loss: -0.0021009170450270176
        vf_explained_var: 0.02028888463973999
        vf_loss: 4.369518280029297
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4166518449783325
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011686378857120872
        model: {}
        policy_loss: -0.0013482295908033848
        total_loss: -0.0018693420570343733
        vf_explained_var: 0.003647252917289734
        vf_loss: 2.1219065189361572
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3859204351902008
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009955073473975062
        model: {}
        policy_loss: -0.00141944270581007
        total_loss: -0.0019960766658186913
        vf_explained_var: 0.022647053003311157
        vf_loss: 1.0258762836456299
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5490607023239136
        entropy_coeff: 0.0017600000137463212
        kl: 0.001803307211957872
        model: {}
        policy_loss: -0.001874598441645503
        total_loss: -0.0027100304141640663
        vf_explained_var: 0.016466423869132996
        vf_loss: 1.3091535568237305
    load_time_ms: 14195.142
    num_steps_sampled: 91488000
    num_steps_trained: 91488000
    sample_time_ms: 118902.749
    update_time_ms: 15.786
  iterations_since_restore: 303
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.857971014492755
    ram_util_percent: 12.378260869565219
  pid: 27065
  policy_reward_max:
    agent-0: 35.0
    agent-1: 19.0
    agent-2: 70.0
    agent-3: 44.0
    agent-4: 27.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 18.02
    agent-1: 9.33
    agent-2: 49.1
    agent-3: 27.45
    agent-4: 15.42
    agent-5: 19.01
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 20.0
    agent-3: 13.0
    agent-4: 7.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.207480442977317
    mean_inference_ms: 15.052251542307486
    mean_processing_ms: 73.27236798084193
  time_since_restore: 44015.572040081024
  time_this_iter_s: 145.40544319152832
  time_total_s: 132952.25016975403
  timestamp: 1637407057
  timesteps_since_restore: 29088000
  timesteps_this_iter: 96000
  timesteps_total: 91488000
  training_iteration: 953
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    953 |           132952 | 91488000 |   138.33 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 7.48
    apples_agent-0_min: 0
    apples_agent-1_max: 16
    apples_agent-1_mean: 3.54
    apples_agent-1_min: 0
    apples_agent-2_max: 55
    apples_agent-2_mean: 31.95
    apples_agent-2_min: 5
    apples_agent-3_max: 26
    apples_agent-3_mean: 9.66
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 9.93
    apples_agent-4_min: 1
    apples_agent-5_max: 22
    apples_agent-5_mean: 5.2
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 1.82
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 251
    cleaning_beam_agent-1_mean: 221.22
    cleaning_beam_agent-1_min: 62
    cleaning_beam_agent-2_max: 25
    cleaning_beam_agent-2_mean: 8.3
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 9.99
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.2
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 4.55
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-20-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 210.0
  episode_reward_mean: 137.74
  episode_reward_min: 19.0
  episodes_this_iter: 96
  episodes_total: 91584
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11910.05
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31765830516815186
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012173943687230349
        model: {}
        policy_loss: -0.001764805056154728
        total_loss: -0.0022027911618351936
        vf_explained_var: 0.008238643407821655
        vf_loss: 1.2109384536743164
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20067650079727173
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011917094234377146
        model: {}
        policy_loss: -0.001792702591046691
        total_loss: -0.0020898068323731422
        vf_explained_var: 0.10317975282669067
        vf_loss: 0.5608614087104797
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.430345356464386
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015953424153849483
        model: {}
        policy_loss: -0.0018170527182519436
        total_loss: -0.0021427394822239876
        vf_explained_var: 0.017027676105499268
        vf_loss: 4.317216873168945
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4232473373413086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009766892762854695
        model: {}
        policy_loss: -0.0012614591978490353
        total_loss: -0.0017824159003794193
        vf_explained_var: 0.011252790689468384
        vf_loss: 2.239576578140259
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37705177068710327
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012854839442297816
        model: {}
        policy_loss: -0.0013194512575864792
        total_loss: -0.00188547745347023
        vf_explained_var: 0.022881969809532166
        vf_loss: 0.9758253693580627
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5596774220466614
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012638303451240063
        model: {}
        policy_loss: -0.0015252245357260108
        total_loss: -0.002369662979617715
        vf_explained_var: 0.015034392476081848
        vf_loss: 1.4059580564498901
    load_time_ms: 14189.914
    num_steps_sampled: 91584000
    num_steps_trained: 91584000
    sample_time_ms: 118700.033
    update_time_ms: 15.965
  iterations_since_restore: 304
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.00242718446602
    ram_util_percent: 12.30097087378641
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 20.0
    agent-2: 75.0
    agent-3: 44.0
    agent-4: 23.0
    agent-5: 38.0
  policy_reward_mean:
    agent-0: 18.14
    agent-1: 9.62
    agent-2: 47.94
    agent-3: 27.54
    agent-4: 15.61
    agent-5: 18.89
  policy_reward_min:
    agent-0: 2.0
    agent-1: 1.0
    agent-2: 8.0
    agent-3: 4.0
    agent-4: 1.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.20774587357063
    mean_inference_ms: 15.051982060956146
    mean_processing_ms: 73.27235821399563
  time_since_restore: 44160.12851834297
  time_this_iter_s: 144.55647826194763
  time_total_s: 133096.80664801598
  timestamp: 1637407202
  timesteps_since_restore: 29184000
  timesteps_this_iter: 96000
  timesteps_total: 91584000
  training_iteration: 954
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    954 |           133097 | 91584000 |   137.74 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 7.19
    apples_agent-0_min: 1
    apples_agent-1_max: 97
    apples_agent-1_mean: 4.76
    apples_agent-1_min: 0
    apples_agent-2_max: 57
    apples_agent-2_mean: 31.69
    apples_agent-2_min: 17
    apples_agent-3_max: 23
    apples_agent-3_mean: 9.66
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.72
    apples_agent-4_min: 3
    apples_agent-5_max: 16
    apples_agent-5_mean: 5.36
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 5
    cleaning_beam_agent-0_mean: 1.27
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 303
    cleaning_beam_agent-1_mean: 222.51
    cleaning_beam_agent-1_min: 153
    cleaning_beam_agent-2_max: 18
    cleaning_beam_agent-2_mean: 7.26
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 10.64
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 15
    cleaning_beam_agent-4_mean: 3.33
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 4.74
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-22-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 185.0
  episode_reward_mean: 136.99
  episode_reward_min: 77.0
  episodes_this_iter: 96
  episodes_total: 91680
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11913.952
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31440794467926025
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010500665521249175
        model: {}
        policy_loss: -0.0018759528174996376
        total_loss: -0.0023167026229202747
        vf_explained_var: 0.009851574897766113
        vf_loss: 1.1261074542999268
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20126254856586456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011244295164942741
        model: {}
        policy_loss: -0.0016283411532640457
        total_loss: -0.001919623464345932
        vf_explained_var: 0.08413417637348175
        vf_loss: 0.6293997764587402
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4266851246356964
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015845968155190349
        model: {}
        policy_loss: -0.0019187764264643192
        total_loss: -0.002253517508506775
        vf_explained_var: 0.013164922595024109
        vf_loss: 4.162275314331055
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42503756284713745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015579768223688006
        model: {}
        policy_loss: -0.001715430524200201
        total_loss: -0.0022603021934628487
        vf_explained_var: 0.006014585494995117
        vf_loss: 2.031938314437866
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3762229382991791
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004987494321539998
        model: {}
        policy_loss: -0.0012058094143867493
        total_loss: -0.001768775749951601
        vf_explained_var: 0.013048335909843445
        vf_loss: 0.9918724894523621
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5623037219047546
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017143478617072105
        model: {}
        policy_loss: -0.0016780097503215075
        total_loss: -0.0025238399393856525
        vf_explained_var: 0.012155324220657349
        vf_loss: 1.4382296800613403
    load_time_ms: 14203.049
    num_steps_sampled: 91680000
    num_steps_trained: 91680000
    sample_time_ms: 118828.846
    update_time_ms: 15.742
  iterations_since_restore: 305
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.94615384615385
    ram_util_percent: 12.461538461538456
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 22.0
    agent-2: 82.0
    agent-3: 42.0
    agent-4: 24.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 17.57
    agent-1: 10.02
    agent-2: 47.92
    agent-3: 27.74
    agent-4: 14.65
    agent-5: 19.09
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 28.0
    agent-3: 12.0
    agent-4: 6.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.20829726674474
    mean_inference_ms: 15.052176433376124
    mean_processing_ms: 73.27274694492219
  time_since_restore: 44306.12535786629
  time_this_iter_s: 145.99683952331543
  time_total_s: 133242.8034875393
  timestamp: 1637407348
  timesteps_since_restore: 29280000
  timesteps_this_iter: 96000
  timesteps_total: 91680000
  training_iteration: 955
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    955 |           133243 | 91680000 |   136.99 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 7.21
    apples_agent-0_min: 1
    apples_agent-1_max: 11
    apples_agent-1_mean: 3.58
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 30.36
    apples_agent-2_min: 9
    apples_agent-3_max: 36
    apples_agent-3_mean: 9.77
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.81
    apples_agent-4_min: 2
    apples_agent-5_max: 16
    apples_agent-5_mean: 5.57
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.93
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 278
    cleaning_beam_agent-1_mean: 219.51
    cleaning_beam_agent-1_min: 70
    cleaning_beam_agent-2_max: 23
    cleaning_beam_agent-2_mean: 8.7
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 11.23
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 14
    cleaning_beam_agent-4_mean: 3.34
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 4.64
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-24-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 184.0
  episode_reward_mean: 134.8
  episode_reward_min: 51.0
  episodes_this_iter: 96
  episodes_total: 91776
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11890.139
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3107670545578003
        entropy_coeff: 0.0017600000137463212
        kl: 0.001066896365955472
        model: {}
        policy_loss: -0.0018027708865702152
        total_loss: -0.0022219885140657425
        vf_explained_var: 0.011411681771278381
        vf_loss: 1.2773284912109375
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19832220673561096
        entropy_coeff: 0.0017600000137463212
        kl: 0.001288115861825645
        model: {}
        policy_loss: -0.0012628883123397827
        total_loss: -0.0015521179884672165
        vf_explained_var: 0.09923027455806732
        vf_loss: 0.598175048828125
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4276477098464966
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012706423876807094
        model: {}
        policy_loss: -0.0017359633930027485
        total_loss: -0.0020742607302963734
        vf_explained_var: 0.027024775743484497
        vf_loss: 4.143649101257324
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42510220408439636
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019267877796664834
        model: {}
        policy_loss: -0.0019127558916807175
        total_loss: -0.0024554331321269274
        vf_explained_var: -0.002688974142074585
        vf_loss: 2.0550332069396973
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3808669447898865
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012301274109631777
        model: {}
        policy_loss: -0.0014583170413970947
        total_loss: -0.002026063622906804
        vf_explained_var: 0.026054158806800842
        vf_loss: 1.025821328163147
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.570006251335144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019231687765568495
        model: {}
        policy_loss: -0.001748223789036274
        total_loss: -0.00261229882016778
        vf_explained_var: 0.014273256063461304
        vf_loss: 1.3913630247116089
    load_time_ms: 14179.372
    num_steps_sampled: 91776000
    num_steps_trained: 91776000
    sample_time_ms: 118817.722
    update_time_ms: 15.922
  iterations_since_restore: 306
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.92548076923077
    ram_util_percent: 12.379807692307692
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 17.0
    agent-2: 66.0
    agent-3: 43.0
    agent-4: 25.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 17.61
    agent-1: 9.48
    agent-2: 46.63
    agent-3: 26.96
    agent-4: 14.56
    agent-5: 19.56
  policy_reward_min:
    agent-0: 9.0
    agent-1: 0.0
    agent-2: 20.0
    agent-3: 8.0
    agent-4: 6.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.209060578499784
    mean_inference_ms: 15.052214527548916
    mean_processing_ms: 73.2745236856126
  time_since_restore: 44451.78753519058
  time_this_iter_s: 145.66217732429504
  time_total_s: 133388.4656648636
  timestamp: 1637407494
  timesteps_since_restore: 29376000
  timesteps_this_iter: 96000
  timesteps_total: 91776000
  training_iteration: 956
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    956 |           133388 | 91776000 |    134.8 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 7.0
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 3.21
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 29.95
    apples_agent-2_min: 16
    apples_agent-3_max: 35
    apples_agent-3_mean: 9.77
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.3
    apples_agent-4_min: 3
    apples_agent-5_max: 75
    apples_agent-5_mean: 5.46
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 17
    cleaning_beam_agent-0_mean: 1.96
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 262
    cleaning_beam_agent-1_mean: 217.11
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 27
    cleaning_beam_agent-2_mean: 9.57
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 31
    cleaning_beam_agent-3_mean: 10.33
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 15
    cleaning_beam_agent-4_mean: 3.24
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.46
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-27-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 179.0
  episode_reward_mean: 132.03
  episode_reward_min: 58.0
  episodes_this_iter: 96
  episodes_total: 91872
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11820.441
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31381142139434814
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009420244605280459
        model: {}
        policy_loss: -0.001614550594240427
        total_loss: -0.0020578072872012854
        vf_explained_var: 0.007737502455711365
        vf_loss: 1.0905158519744873
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20216459035873413
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007010491099208593
        model: {}
        policy_loss: -0.0012662464287132025
        total_loss: -0.0015707723796367645
        vf_explained_var: 0.08368715643882751
        vf_loss: 0.5128163695335388
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43032586574554443
        entropy_coeff: 0.0017600000137463212
        kl: 0.001027698046527803
        model: {}
        policy_loss: -0.0017884401604533195
        total_loss: -0.0021772501058876514
        vf_explained_var: 0.019638270139694214
        vf_loss: 3.685643196105957
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42022672295570374
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012058583088219166
        model: {}
        policy_loss: -0.001497599296271801
        total_loss: -0.0020365575328469276
        vf_explained_var: -0.001092076301574707
        vf_loss: 2.0063958168029785
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3775556981563568
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006281360401771963
        model: {}
        policy_loss: -0.0012405943125486374
        total_loss: -0.001808688510209322
        vf_explained_var: 0.005731120705604553
        vf_loss: 0.9640560150146484
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5736550092697144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020449936855584383
        model: {}
        policy_loss: -0.001898627495393157
        total_loss: -0.0027747598942369223
        vf_explained_var: 0.00875188410282135
        vf_loss: 1.334961175918579
    load_time_ms: 14181.41
    num_steps_sampled: 91872000
    num_steps_trained: 91872000
    sample_time_ms: 118796.913
    update_time_ms: 15.736
  iterations_since_restore: 307
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.950970873786407
    ram_util_percent: 12.39271844660194
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 16.0
    agent-2: 66.0
    agent-3: 39.0
    agent-4: 27.0
    agent-5: 39.0
  policy_reward_mean:
    agent-0: 17.19
    agent-1: 8.7
    agent-2: 46.18
    agent-3: 26.89
    agent-4: 14.17
    agent-5: 18.9
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 23.0
    agent-3: 14.0
    agent-4: 6.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.20931015072646
    mean_inference_ms: 15.051942461869704
    mean_processing_ms: 73.27349387049473
  time_since_restore: 44596.26772236824
  time_this_iter_s: 144.48018717765808
  time_total_s: 133532.94585204124
  timestamp: 1637407639
  timesteps_since_restore: 29472000
  timesteps_this_iter: 96000
  timesteps_total: 91872000
  training_iteration: 957
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    957 |           133533 | 91872000 |   132.03 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 7.28
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.36
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 30.21
    apples_agent-2_min: 19
    apples_agent-3_max: 18
    apples_agent-3_mean: 9.2
    apples_agent-3_min: 1
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.23
    apples_agent-4_min: 3
    apples_agent-5_max: 23
    apples_agent-5_mean: 5.09
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 26
    cleaning_beam_agent-0_mean: 2.16
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 246
    cleaning_beam_agent-1_mean: 220.58
    cleaning_beam_agent-1_min: 183
    cleaning_beam_agent-2_max: 26
    cleaning_beam_agent-2_mean: 9.87
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 9.79
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.86
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 4.68
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-29-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 168.0
  episode_reward_mean: 135.15
  episode_reward_min: 104.0
  episodes_this_iter: 96
  episodes_total: 91968
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11813.285
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31848767399787903
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011080639669671655
        model: {}
        policy_loss: -0.0017880462110042572
        total_loss: -0.002238999120891094
        vf_explained_var: 0.01150427758693695
        vf_loss: 1.0958480834960938
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2026958465576172
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010671894997358322
        model: {}
        policy_loss: -0.0016639931127429008
        total_loss: -0.0019644470885396004
        vf_explained_var: 0.07913318276405334
        vf_loss: 0.5629432797431946
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43186819553375244
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011792099103331566
        model: {}
        policy_loss: -0.001867823302745819
        total_loss: -0.002258345950394869
        vf_explained_var: 0.004558756947517395
        vf_loss: 3.695664405822754
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42202749848365784
        entropy_coeff: 0.0017600000137463212
        kl: 0.001291264547035098
        model: {}
        policy_loss: -0.0013830706011503935
        total_loss: -0.001928077545017004
        vf_explained_var: 0.000817716121673584
        vf_loss: 1.9776264429092407
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37735748291015625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009264950640499592
        model: {}
        policy_loss: -0.0012991465628147125
        total_loss: -0.0018547158688306808
        vf_explained_var: 0.010621190071105957
        vf_loss: 1.085784912109375
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5771427750587463
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013971116859465837
        model: {}
        policy_loss: -0.0017118938267230988
        total_loss: -0.002598963910713792
        vf_explained_var: 0.015524536371231079
        vf_loss: 1.2870287895202637
    load_time_ms: 14166.01
    num_steps_sampled: 91968000
    num_steps_trained: 91968000
    sample_time_ms: 118842.437
    update_time_ms: 16.007
  iterations_since_restore: 308
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.928502415458937
    ram_util_percent: 12.489371980676324
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 17.0
    agent-2: 62.0
    agent-3: 40.0
    agent-4: 26.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 17.6
    agent-1: 9.76
    agent-2: 46.63
    agent-3: 27.77
    agent-4: 15.07
    agent-5: 18.32
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 25.0
    agent-3: 14.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.20964394223372
    mean_inference_ms: 15.051851360748426
    mean_processing_ms: 73.27274109790334
  time_since_restore: 44741.78135514259
  time_this_iter_s: 145.51363277435303
  time_total_s: 133678.4594848156
  timestamp: 1637407784
  timesteps_since_restore: 29568000
  timesteps_this_iter: 96000
  timesteps_total: 91968000
  training_iteration: 958
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    958 |           133678 | 91968000 |   135.15 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 7.6
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.17
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 29.92
    apples_agent-2_min: 15
    apples_agent-3_max: 29
    apples_agent-3_mean: 8.7
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.92
    apples_agent-4_min: 1
    apples_agent-5_max: 31
    apples_agent-5_mean: 5.4
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 15
    cleaning_beam_agent-0_mean: 1.72
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 278
    cleaning_beam_agent-1_mean: 217.72
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 32
    cleaning_beam_agent-2_mean: 9.82
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 10.74
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 3.26
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.19
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-32-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 202.0
  episode_reward_mean: 131.84
  episode_reward_min: 70.0
  episodes_this_iter: 96
  episodes_total: 92064
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11810.424
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3226737380027771
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015375297516584396
        model: {}
        policy_loss: -0.002042971784248948
        total_loss: -0.0025012462865561247
        vf_explained_var: 0.0061015784740448
        vf_loss: 1.0963239669799805
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19710615277290344
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010560128139331937
        model: {}
        policy_loss: -0.0015693285968154669
        total_loss: -0.0018621261697262526
        vf_explained_var: 0.09033016860485077
        vf_loss: 0.5410783290863037
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4326515197753906
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014478304656222463
        model: {}
        policy_loss: -0.001736343838274479
        total_loss: -0.0021160440519452095
        vf_explained_var: 0.026505380868911743
        vf_loss: 3.8176324367523193
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42252153158187866
        entropy_coeff: 0.0017600000137463212
        kl: 0.00151696871034801
        model: {}
        policy_loss: -0.0014610157813876867
        total_loss: -0.0020009141881018877
        vf_explained_var: 0.004629045724868774
        vf_loss: 2.0373990535736084
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3741736114025116
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006196280010044575
        model: {}
        policy_loss: -0.0013737375847995281
        total_loss: -0.0019363602623343468
        vf_explained_var: 0.019662097096443176
        vf_loss: 0.9592281579971313
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5731675028800964
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014338602777570486
        model: {}
        policy_loss: -0.0016902824863791466
        total_loss: -0.0025704719591885805
        vf_explained_var: 0.015508949756622314
        vf_loss: 1.2858656644821167
    load_time_ms: 14161.679
    num_steps_sampled: 92064000
    num_steps_trained: 92064000
    sample_time_ms: 118926.627
    update_time_ms: 15.867
  iterations_since_restore: 309
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.203398058252425
    ram_util_percent: 12.473300970873785
  pid: 27065
  policy_reward_max:
    agent-0: 26.0
    agent-1: 18.0
    agent-2: 79.0
    agent-3: 42.0
    agent-4: 29.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 16.47
    agent-1: 9.41
    agent-2: 45.47
    agent-3: 27.78
    agent-4: 14.48
    agent-5: 18.23
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 17.0
    agent-3: 17.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.21028382242069
    mean_inference_ms: 15.051857076333977
    mean_processing_ms: 73.2731878743172
  time_since_restore: 44886.75921034813
  time_this_iter_s: 144.9778552055359
  time_total_s: 133823.43734002113
  timestamp: 1637407929
  timesteps_since_restore: 29664000
  timesteps_this_iter: 96000
  timesteps_total: 92064000
  training_iteration: 959
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    959 |           133823 | 92064000 |   131.84 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 7.8
    apples_agent-0_min: 0
    apples_agent-1_max: 18
    apples_agent-1_mean: 3.65
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 30.31
    apples_agent-2_min: 19
    apples_agent-3_max: 30
    apples_agent-3_mean: 9.17
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.57
    apples_agent-4_min: 2
    apples_agent-5_max: 16
    apples_agent-5_mean: 5.0
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 2.03
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 249
    cleaning_beam_agent-1_mean: 215.2
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 22
    cleaning_beam_agent-2_mean: 7.94
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 10.17
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 17
    cleaning_beam_agent-4_mean: 3.82
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 4.06
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-34-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 176.0
  episode_reward_mean: 134.07
  episode_reward_min: 101.0
  episodes_this_iter: 96
  episodes_total: 92160
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11809.955
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32122984528541565
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013959452044218779
        model: {}
        policy_loss: -0.0019244453869760036
        total_loss: -0.0023695025593042374
        vf_explained_var: 0.011984243988990784
        vf_loss: 1.2030810117721558
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19906552135944366
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007993117906153202
        model: {}
        policy_loss: -0.001422565197572112
        total_loss: -0.0017216047272086143
        vf_explained_var: 0.09256619215011597
        vf_loss: 0.5131663084030151
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4126559793949127
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018018370028585196
        model: {}
        policy_loss: -0.0022845643106848
        total_loss: -0.0026124734431505203
        vf_explained_var: 0.009049341082572937
        vf_loss: 3.9836432933807373
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4288988709449768
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012960464227944613
        model: {}
        policy_loss: -0.0016079600900411606
        total_loss: -0.002165272831916809
        vf_explained_var: -0.002575784921646118
        vf_loss: 1.9754979610443115
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.377380907535553
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015619779005646706
        model: {}
        policy_loss: -0.0015362396370619535
        total_loss: -0.0020997687242925167
        vf_explained_var: 0.014306634664535522
        vf_loss: 1.0066436529159546
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5747250914573669
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020206747576594353
        model: {}
        policy_loss: -0.0018651240970939398
        total_loss: -0.0027516609989106655
        vf_explained_var: 0.01935736835002899
        vf_loss: 1.2497845888137817
    load_time_ms: 14153.787
    num_steps_sampled: 92160000
    num_steps_trained: 92160000
    sample_time_ms: 119004.367
    update_time_ms: 15.928
  iterations_since_restore: 310
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.860869565217396
    ram_util_percent: 12.466183574879222
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 63.0
    agent-3: 42.0
    agent-4: 32.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 17.65
    agent-1: 9.58
    agent-2: 47.1
    agent-3: 26.2
    agent-4: 14.46
    agent-5: 19.08
  policy_reward_min:
    agent-0: 9.0
    agent-1: 3.0
    agent-2: 30.0
    agent-3: 14.0
    agent-4: 5.0
    agent-5: 10.0
  sampler_perf:
    mean_env_wait_ms: 28.210349534390243
    mean_inference_ms: 15.051699398522596
    mean_processing_ms: 73.27264077151733
  time_since_restore: 45031.74017429352
  time_this_iter_s: 144.9809639453888
  time_total_s: 133968.41830396652
  timestamp: 1637408075
  timesteps_since_restore: 29760000
  timesteps_this_iter: 96000
  timesteps_total: 92160000
  training_iteration: 960
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    960 |           133968 | 92160000 |   134.07 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 7.12
    apples_agent-0_min: 2
    apples_agent-1_max: 14
    apples_agent-1_mean: 3.42
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 29.83
    apples_agent-2_min: 16
    apples_agent-3_max: 30
    apples_agent-3_mean: 9.5
    apples_agent-3_min: 2
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.01
    apples_agent-4_min: 2
    apples_agent-5_max: 16
    apples_agent-5_mean: 4.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.66
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 258
    cleaning_beam_agent-1_mean: 212.69
    cleaning_beam_agent-1_min: 157
    cleaning_beam_agent-2_max: 26
    cleaning_beam_agent-2_mean: 8.28
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 12.16
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 2.82
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-37-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 195.0
  episode_reward_mean: 133.01
  episode_reward_min: 86.0
  episodes_this_iter: 96
  episodes_total: 92256
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11813.078
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3166672885417938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009660241194069386
        model: {}
        policy_loss: -0.001789556466974318
        total_loss: -0.0022330153733491898
        vf_explained_var: 0.011140033602714539
        vf_loss: 1.1387295722961426
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19751477241516113
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007352697430178523
        model: {}
        policy_loss: -0.0013907491229474545
        total_loss: -0.0016773322131484747
        vf_explained_var: 0.08310957252979279
        vf_loss: 0.6104066967964172
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4195798933506012
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013488784898072481
        model: {}
        policy_loss: -0.001762271742336452
        total_loss: -0.00212366902269423
        vf_explained_var: 0.012944117188453674
        vf_loss: 3.7706665992736816
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42930352687835693
        entropy_coeff: 0.0017600000137463212
        kl: 0.001868357416242361
        model: {}
        policy_loss: -0.0017783400835469365
        total_loss: -0.002350269816815853
        vf_explained_var: 0.013836026191711426
        vf_loss: 1.8364546298980713
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38422685861587524
        entropy_coeff: 0.0017600000137463212
        kl: 0.001016044057905674
        model: {}
        policy_loss: -0.001340385410003364
        total_loss: -0.0019183505792170763
        vf_explained_var: 0.01144041121006012
        vf_loss: 0.9827505350112915
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5695873498916626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010231612250208855
        model: {}
        policy_loss: -0.0015595844015479088
        total_loss: -0.0024193930439651012
        vf_explained_var: 0.015773087739944458
        vf_loss: 1.4266455173492432
    load_time_ms: 14137.77
    num_steps_sampled: 92256000
    num_steps_trained: 92256000
    sample_time_ms: 119019.311
    update_time_ms: 15.854
  iterations_since_restore: 311
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.542380952380952
    ram_util_percent: 12.388095238095238
  pid: 27065
  policy_reward_max:
    agent-0: 33.0
    agent-1: 19.0
    agent-2: 65.0
    agent-3: 42.0
    agent-4: 26.0
    agent-5: 37.0
  policy_reward_mean:
    agent-0: 16.91
    agent-1: 9.92
    agent-2: 46.15
    agent-3: 26.03
    agent-4: 14.99
    agent-5: 19.01
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 30.0
    agent-3: 12.0
    agent-4: 6.0
    agent-5: 10.0
  sampler_perf:
    mean_env_wait_ms: 28.21012321569399
    mean_inference_ms: 15.051292868774322
    mean_processing_ms: 73.27106191806408
  time_since_restore: 45176.3705599308
  time_this_iter_s: 144.63038563728333
  time_total_s: 134113.0486896038
  timestamp: 1637408222
  timesteps_since_restore: 29856000
  timesteps_this_iter: 96000
  timesteps_total: 92256000
  training_iteration: 961
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    961 |           134113 | 92256000 |   133.01 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 6.86
    apples_agent-0_min: 2
    apples_agent-1_max: 17
    apples_agent-1_mean: 3.77
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 30.33
    apples_agent-2_min: 19
    apples_agent-3_max: 26
    apples_agent-3_mean: 9.67
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 9.97
    apples_agent-4_min: 3
    apples_agent-5_max: 19
    apples_agent-5_mean: 4.68
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 1.43
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 258
    cleaning_beam_agent-1_mean: 214.79
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 21
    cleaning_beam_agent-2_mean: 7.96
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 10.0
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 2.81
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.0
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-39-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 195.0
  episode_reward_mean: 134.34
  episode_reward_min: 91.0
  episodes_this_iter: 96
  episodes_total: 92352
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11830.781
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3124152719974518
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011002845130860806
        model: {}
        policy_loss: -0.0016213259659707546
        total_loss: -0.002070888876914978
        vf_explained_var: 0.0006824880838394165
        vf_loss: 1.0028728246688843
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1993030160665512
        entropy_coeff: 0.0017600000137463212
        kl: 0.001072694780305028
        model: {}
        policy_loss: -0.0015217405743896961
        total_loss: -0.0018106135539710522
        vf_explained_var: 0.08995124697685242
        vf_loss: 0.619015634059906
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41823190450668335
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011800049105659127
        model: {}
        policy_loss: -0.0018797367811203003
        total_loss: -0.002244458068162203
        vf_explained_var: 0.02312946319580078
        vf_loss: 3.713672161102295
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4254201650619507
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009651253931224346
        model: {}
        policy_loss: -0.0014173805247992277
        total_loss: -0.001961902715265751
        vf_explained_var: 0.007951989769935608
        vf_loss: 2.0421838760375977
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39085155725479126
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005309236003085971
        model: {}
        policy_loss: -0.0012586060911417007
        total_loss: -0.0018443744629621506
        vf_explained_var: 0.017086803913116455
        vf_loss: 1.0212986469268799
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5680261850357056
        entropy_coeff: 0.0017600000137463212
        kl: 0.001513732480816543
        model: {}
        policy_loss: -0.0019225297728553414
        total_loss: -0.0027990059461444616
        vf_explained_var: 0.013143494725227356
        vf_loss: 1.2324926853179932
    load_time_ms: 14150.226
    num_steps_sampled: 92352000
    num_steps_trained: 92352000
    sample_time_ms: 119068.944
    update_time_ms: 15.603
  iterations_since_restore: 312
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.79951690821256
    ram_util_percent: 12.472463768115938
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 65.0
    agent-3: 44.0
    agent-4: 29.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 16.38
    agent-1: 10.13
    agent-2: 46.98
    agent-3: 27.51
    agent-4: 14.72
    agent-5: 18.62
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 31.0
    agent-3: 13.0
    agent-4: 5.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.210244968266274
    mean_inference_ms: 15.051014784110036
    mean_processing_ms: 73.26891110575268
  time_since_restore: 45321.78137087822
  time_this_iter_s: 145.4108109474182
  time_total_s: 134258.45950055122
  timestamp: 1637408367
  timesteps_since_restore: 29952000
  timesteps_this_iter: 96000
  timesteps_total: 92352000
  training_iteration: 962
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    962 |           134258 | 92352000 |   134.34 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 7.6
    apples_agent-0_min: 1
    apples_agent-1_max: 17
    apples_agent-1_mean: 3.7
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 30.49
    apples_agent-2_min: 14
    apples_agent-3_max: 30
    apples_agent-3_mean: 9.22
    apples_agent-3_min: 3
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.78
    apples_agent-4_min: 3
    apples_agent-5_max: 20
    apples_agent-5_mean: 4.39
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 1.76
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 242
    cleaning_beam_agent-1_mean: 214.26
    cleaning_beam_agent-1_min: 179
    cleaning_beam_agent-2_max: 40
    cleaning_beam_agent-2_mean: 9.75
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 64
    cleaning_beam_agent-3_mean: 11.57
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 14
    cleaning_beam_agent-4_mean: 3.17
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 4.51
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-41-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 184.0
  episode_reward_mean: 132.21
  episode_reward_min: 90.0
  episodes_this_iter: 96
  episodes_total: 92448
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11815.144
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30278122425079346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010953336022794247
        model: {}
        policy_loss: -0.0018011846113950014
        total_loss: -0.002219271846115589
        vf_explained_var: 0.02320227026939392
        vf_loss: 1.148087739944458
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19898919761180878
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014448041329160333
        model: {}
        policy_loss: -0.0016601970419287682
        total_loss: -0.0019501140341162682
        vf_explained_var: 0.08928991854190826
        vf_loss: 0.6030304431915283
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4228705167770386
        entropy_coeff: 0.0017600000137463212
        kl: 0.001131323748268187
        model: {}
        policy_loss: -0.0017409783322364092
        total_loss: -0.0021027117036283016
        vf_explained_var: 0.010336160659790039
        vf_loss: 3.8251702785491943
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41871368885040283
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015277243219316006
        model: {}
        policy_loss: -0.0017444146797060966
        total_loss: -0.0022702841088175774
        vf_explained_var: 0.001917526125907898
        vf_loss: 2.110656261444092
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39712753891944885
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008508618921041489
        model: {}
        policy_loss: -0.001261039637029171
        total_loss: -0.0018605608493089676
        vf_explained_var: 0.022998914122581482
        vf_loss: 0.9942410588264465
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5653906464576721
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018365357536822557
        model: {}
        policy_loss: -0.0016828461084514856
        total_loss: -0.002548978431150317
        vf_explained_var: 0.015213757753372192
        vf_loss: 1.2895556688308716
    load_time_ms: 14139.567
    num_steps_sampled: 92448000
    num_steps_trained: 92448000
    sample_time_ms: 119004.585
    update_time_ms: 15.56
  iterations_since_restore: 313
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.926213592233005
    ram_util_percent: 12.470873786407765
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 22.0
    agent-2: 65.0
    agent-3: 43.0
    agent-4: 29.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 16.62
    agent-1: 9.67
    agent-2: 46.13
    agent-3: 27.19
    agent-4: 14.69
    agent-5: 17.91
  policy_reward_min:
    agent-0: 8.0
    agent-1: 2.0
    agent-2: 23.0
    agent-3: 10.0
    agent-4: 7.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.21036447710664
    mean_inference_ms: 15.05056547946062
    mean_processing_ms: 73.2677971202885
  time_since_restore: 45466.226007938385
  time_this_iter_s: 144.4446370601654
  time_total_s: 134402.9041376114
  timestamp: 1637408512
  timesteps_since_restore: 30048000
  timesteps_this_iter: 96000
  timesteps_total: 92448000
  training_iteration: 963
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    963 |           134403 | 92448000 |   132.21 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 7.4
    apples_agent-0_min: 1
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.47
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 29.84
    apples_agent-2_min: 17
    apples_agent-3_max: 20
    apples_agent-3_mean: 8.87
    apples_agent-3_min: 2
    apples_agent-4_max: 19
    apples_agent-4_mean: 10.1
    apples_agent-4_min: 1
    apples_agent-5_max: 29
    apples_agent-5_mean: 5.35
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 9
    cleaning_beam_agent-0_mean: 1.49
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 244
    cleaning_beam_agent-1_mean: 219.03
    cleaning_beam_agent-1_min: 165
    cleaning_beam_agent-2_max: 42
    cleaning_beam_agent-2_mean: 10.39
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 9.65
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.13
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.98
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-44-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 171.0
  episode_reward_mean: 135.26
  episode_reward_min: 95.0
  episodes_this_iter: 96
  episodes_total: 92544
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11805.322
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30244502425193787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008955830708146095
        model: {}
        policy_loss: -0.0015577804297208786
        total_loss: -0.0019651800394058228
        vf_explained_var: 0.020555749535560608
        vf_loss: 1.249021291732788
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2015204280614853
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008932508644647896
        model: {}
        policy_loss: -0.0013704278971999884
        total_loss: -0.0016654286300763488
        vf_explained_var: 0.08359405398368835
        vf_loss: 0.5967296361923218
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4290933310985565
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014524753205478191
        model: {}
        policy_loss: -0.0018000192940235138
        total_loss: -0.002172977663576603
        vf_explained_var: 0.02513335645198822
        vf_loss: 3.8224339485168457
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41580405831336975
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014810131397098303
        model: {}
        policy_loss: -0.0015021945582702756
        total_loss: -0.0020344783551990986
        vf_explained_var: -0.0016551017761230469
        vf_loss: 1.995313048362732
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40746402740478516
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007152543403208256
        model: {}
        policy_loss: -0.0013858951861038804
        total_loss: -0.0019973204471170902
        vf_explained_var: 0.008084028959274292
        vf_loss: 1.0570935010910034
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5577952861785889
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012098586885258555
        model: {}
        policy_loss: -0.0016459509497508407
        total_loss: -0.002493134466931224
        vf_explained_var: 0.022028177976608276
        vf_loss: 1.3453481197357178
    load_time_ms: 14131.252
    num_steps_sampled: 92544000
    num_steps_trained: 92544000
    sample_time_ms: 119070.696
    update_time_ms: 15.658
  iterations_since_restore: 314
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.974396135265696
    ram_util_percent: 12.468599033816421
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 64.0
    agent-3: 40.0
    agent-4: 28.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 17.44
    agent-1: 9.5
    agent-2: 46.71
    agent-3: 27.0
    agent-4: 14.99
    agent-5: 19.62
  policy_reward_min:
    agent-0: 9.0
    agent-1: 3.0
    agent-2: 27.0
    agent-3: 15.0
    agent-4: 6.0
    agent-5: 10.0
  sampler_perf:
    mean_env_wait_ms: 28.21075113432565
    mean_inference_ms: 15.050693236888515
    mean_processing_ms: 73.26792685804135
  time_since_restore: 45611.2813539505
  time_this_iter_s: 145.05534601211548
  time_total_s: 134547.9594836235
  timestamp: 1637408657
  timesteps_since_restore: 30144000
  timesteps_this_iter: 96000
  timesteps_total: 92544000
  training_iteration: 964
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    964 |           134548 | 92544000 |   135.26 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 8.05
    apples_agent-0_min: 2
    apples_agent-1_max: 25
    apples_agent-1_mean: 3.24
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 31.51
    apples_agent-2_min: 14
    apples_agent-3_max: 35
    apples_agent-3_mean: 10.5
    apples_agent-3_min: 2
    apples_agent-4_max: 19
    apples_agent-4_mean: 9.9
    apples_agent-4_min: 3
    apples_agent-5_max: 23
    apples_agent-5_mean: 5.23
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.45
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 248
    cleaning_beam_agent-1_mean: 218.33
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 42
    cleaning_beam_agent-2_mean: 10.71
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 9.64
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 2.99
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 3.65
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-46-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 177.0
  episode_reward_mean: 137.75
  episode_reward_min: 70.0
  episodes_this_iter: 96
  episodes_total: 92640
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11805.536
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3068796396255493
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012478335993364453
        model: {}
        policy_loss: -0.0017118004616349936
        total_loss: -0.002141619799658656
        vf_explained_var: 0.012606769800186157
        vf_loss: 1.1029092073440552
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20119750499725342
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010565349366515875
        model: {}
        policy_loss: -0.001475183991715312
        total_loss: -0.0017775247106328607
        vf_explained_var: 0.081600621342659
        vf_loss: 0.5176606178283691
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42309218645095825
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012430023634806275
        model: {}
        policy_loss: -0.0016428956296294928
        total_loss: -0.0019705079030245543
        vf_explained_var: 0.026521489024162292
        vf_loss: 4.170304298400879
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4106435775756836
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018036025576293468
        model: {}
        policy_loss: -0.0016467594541609287
        total_loss: -0.002149278298020363
        vf_explained_var: 0.004699975252151489
        vf_loss: 2.2021846771240234
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41125836968421936
        entropy_coeff: 0.0017600000137463212
        kl: 0.001249960856512189
        model: {}
        policy_loss: -0.0016041614580899477
        total_loss: -0.002228223253041506
        vf_explained_var: 0.015121132135391235
        vf_loss: 0.997516393661499
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5640072822570801
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019162537064403296
        model: {}
        policy_loss: -0.0017051059985533357
        total_loss: -0.0025560634676367044
        vf_explained_var: 0.021357685327529907
        vf_loss: 1.4169646501541138
    load_time_ms: 14106.209
    num_steps_sampled: 92640000
    num_steps_trained: 92640000
    sample_time_ms: 119027.309
    update_time_ms: 15.759
  iterations_since_restore: 315
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.898067632850243
    ram_util_percent: 12.468115942028982
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 17.0
    agent-2: 76.0
    agent-3: 44.0
    agent-4: 30.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 17.61
    agent-1: 8.79
    agent-2: 48.36
    agent-3: 28.46
    agent-4: 14.97
    agent-5: 19.56
  policy_reward_min:
    agent-0: 8.0
    agent-1: 2.0
    agent-2: 27.0
    agent-3: 16.0
    agent-4: 3.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.21104265993963
    mean_inference_ms: 15.050589396541355
    mean_processing_ms: 73.26735688666764
  time_since_restore: 45756.626730680466
  time_this_iter_s: 145.3453767299652
  time_total_s: 134693.30486035347
  timestamp: 1637408802
  timesteps_since_restore: 30240000
  timesteps_this_iter: 96000
  timesteps_total: 92640000
  training_iteration: 965
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    965 |           134693 | 92640000 |   137.75 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 7.22
    apples_agent-0_min: 0
    apples_agent-1_max: 25
    apples_agent-1_mean: 3.88
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 30.72
    apples_agent-2_min: 4
    apples_agent-3_max: 31
    apples_agent-3_mean: 8.85
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 9.79
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 4.48
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 1.34
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 248
    cleaning_beam_agent-1_mean: 211.75
    cleaning_beam_agent-1_min: 42
    cleaning_beam_agent-2_max: 24
    cleaning_beam_agent-2_mean: 10.72
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 8.68
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.2
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 4.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-49-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 176.0
  episode_reward_mean: 133.74
  episode_reward_min: 19.0
  episodes_this_iter: 96
  episodes_total: 92736
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11803.25
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3041073977947235
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011651295935735106
        model: {}
        policy_loss: -0.001613097032532096
        total_loss: -0.002038532868027687
        vf_explained_var: 0.018966659903526306
        vf_loss: 1.097930669784546
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19731378555297852
        entropy_coeff: 0.0017600000137463212
        kl: 0.000915344397071749
        model: {}
        policy_loss: -0.001466875895857811
        total_loss: -0.001760678831487894
        vf_explained_var: 0.1058996319770813
        vf_loss: 0.5347305536270142
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4280930459499359
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010549018625169992
        model: {}
        policy_loss: -0.0018422175198793411
        total_loss: -0.002220609225332737
        vf_explained_var: 0.017077431082725525
        vf_loss: 3.7504806518554688
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4086136519908905
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015500978333875537
        model: {}
        policy_loss: -0.0016727689653635025
        total_loss: -0.002172426786273718
        vf_explained_var: 0.0012989193201065063
        vf_loss: 2.1950278282165527
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4123184084892273
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008103332365863025
        model: {}
        policy_loss: -0.0015664295060560107
        total_loss: -0.0021854820661246777
        vf_explained_var: 0.01480121910572052
        vf_loss: 1.0663015842437744
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5710864067077637
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010470259003341198
        model: {}
        policy_loss: -0.0015115609858185053
        total_loss: -0.002372138435021043
        vf_explained_var: 0.017400741577148438
        vf_loss: 1.4453647136688232
    load_time_ms: 14106.226
    num_steps_sampled: 92736000
    num_steps_trained: 92736000
    sample_time_ms: 118998.342
    update_time_ms: 15.904
  iterations_since_restore: 316
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.89758454106281
    ram_util_percent: 12.464251207729466
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 19.0
    agent-2: 63.0
    agent-3: 44.0
    agent-4: 28.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 16.32
    agent-1: 9.49
    agent-2: 46.24
    agent-3: 27.38
    agent-4: 14.63
    agent-5: 19.68
  policy_reward_min:
    agent-0: 3.0
    agent-1: 0.0
    agent-2: 7.0
    agent-3: 5.0
    agent-4: 1.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.211202490253914
    mean_inference_ms: 15.050673928145775
    mean_processing_ms: 73.2666461499525
  time_since_restore: 45901.976378679276
  time_this_iter_s: 145.34964799880981
  time_total_s: 134838.65450835228
  timestamp: 1637408948
  timesteps_since_restore: 30336000
  timesteps_this_iter: 96000
  timesteps_total: 92736000
  training_iteration: 966
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    966 |           134839 | 92736000 |   133.74 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 6.84
    apples_agent-0_min: 2
    apples_agent-1_max: 26
    apples_agent-1_mean: 3.35
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 29.37
    apples_agent-2_min: 16
    apples_agent-3_max: 42
    apples_agent-3_mean: 9.55
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.13
    apples_agent-4_min: 2
    apples_agent-5_max: 15
    apples_agent-5_mean: 4.53
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.56
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 268
    cleaning_beam_agent-1_mean: 215.63
    cleaning_beam_agent-1_min: 140
    cleaning_beam_agent-2_max: 43
    cleaning_beam_agent-2_mean: 11.12
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 26
    cleaning_beam_agent-3_mean: 9.65
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 16
    cleaning_beam_agent-4_mean: 3.36
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-51-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 177.0
  episode_reward_mean: 131.64
  episode_reward_min: 77.0
  episodes_this_iter: 96
  episodes_total: 92832
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11796.386
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3069153428077698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007336079725064337
        model: {}
        policy_loss: -0.0017283265478909016
        total_loss: -0.0021581414621323347
        vf_explained_var: 0.014150038361549377
        vf_loss: 1.1035772562026978
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2014562040567398
        entropy_coeff: 0.0017600000137463212
        kl: 0.001612513791769743
        model: {}
        policy_loss: -0.001700840424746275
        total_loss: -0.0019991828594356775
        vf_explained_var: 0.08175723254680634
        vf_loss: 0.5622016191482544
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4288581609725952
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009633817826397717
        model: {}
        policy_loss: -0.0015719039365649223
        total_loss: -0.0019273047801107168
        vf_explained_var: 0.006540089845657349
        vf_loss: 3.99391770362854
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4102614223957062
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013986948179081082
        model: {}
        policy_loss: -0.0014237303985282779
        total_loss: -0.001957664964720607
        vf_explained_var: 0.014595240354537964
        vf_loss: 1.881248950958252
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4247020184993744
        entropy_coeff: 0.0017600000137463212
        kl: 0.001031873980537057
        model: {}
        policy_loss: -0.001248459331691265
        total_loss: -0.0019049299880862236
        vf_explained_var: 0.015542834997177124
        vf_loss: 0.9100757241249084
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5806299448013306
        entropy_coeff: 0.0017600000137463212
        kl: 0.001392799778841436
        model: {}
        policy_loss: -0.0014903983101248741
        total_loss: -0.002382485195994377
        vf_explained_var: 0.01134561002254486
        vf_loss: 1.2982465028762817
    load_time_ms: 14092.723
    num_steps_sampled: 92832000
    num_steps_trained: 92832000
    sample_time_ms: 119080.181
    update_time_ms: 15.854
  iterations_since_restore: 317
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.096135265700482
    ram_util_percent: 12.457004830917871
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 16.0
    agent-2: 68.0
    agent-3: 38.0
    agent-4: 24.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 17.34
    agent-1: 9.2
    agent-2: 46.09
    agent-3: 27.15
    agent-4: 13.69
    agent-5: 18.17
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 28.0
    agent-3: 14.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.211525167641597
    mean_inference_ms: 15.051178300595657
    mean_processing_ms: 73.26813933115926
  time_since_restore: 46047.08408522606
  time_this_iter_s: 145.10770654678345
  time_total_s: 134983.76221489906
  timestamp: 1637409093
  timesteps_since_restore: 30432000
  timesteps_this_iter: 96000
  timesteps_total: 92832000
  training_iteration: 967
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    967 |           134984 | 92832000 |   131.64 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 7.74
    apples_agent-0_min: 3
    apples_agent-1_max: 13
    apples_agent-1_mean: 3.03
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 31.57
    apples_agent-2_min: 18
    apples_agent-3_max: 25
    apples_agent-3_mean: 9.55
    apples_agent-3_min: 3
    apples_agent-4_max: 18
    apples_agent-4_mean: 10.18
    apples_agent-4_min: 4
    apples_agent-5_max: 21
    apples_agent-5_mean: 5.16
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.39
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 248
    cleaning_beam_agent-1_mean: 221.9
    cleaning_beam_agent-1_min: 164
    cleaning_beam_agent-2_max: 26
    cleaning_beam_agent-2_mean: 11.58
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 33
    cleaning_beam_agent-3_mean: 8.45
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 3.04
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.75
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-54-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 181.0
  episode_reward_mean: 141.27
  episode_reward_min: 98.0
  episodes_this_iter: 96
  episodes_total: 92928
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11802.731
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.305137574672699
        entropy_coeff: 0.0017600000137463212
        kl: 0.000976386247202754
        model: {}
        policy_loss: -0.0016857059672474861
        total_loss: -0.0020987289026379585
        vf_explained_var: 0.014337271451950073
        vf_loss: 1.2401561737060547
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20036281645298004
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010967892594635487
        model: {}
        policy_loss: -0.0016103247180581093
        total_loss: -0.0019035045988857746
        vf_explained_var: 0.07788389921188354
        vf_loss: 0.5945998430252075
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4229303002357483
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014105606824159622
        model: {}
        policy_loss: -0.0018277494236826897
        total_loss: -0.002144867554306984
        vf_explained_var: 0.026447802782058716
        vf_loss: 4.27241325378418
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41007575392723083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015926345949992537
        model: {}
        policy_loss: -0.0014420668594539165
        total_loss: -0.0019510774873197079
        vf_explained_var: 0.015767723321914673
        vf_loss: 2.1272192001342773
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4273003041744232
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013422940392047167
        model: {}
        policy_loss: -0.0016092965379357338
        total_loss: -0.0022537652403116226
        vf_explained_var: 0.015678822994232178
        vf_loss: 1.075807809829712
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5727448463439941
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019690757617354393
        model: {}
        policy_loss: -0.0015901618171483278
        total_loss: -0.0024521336890757084
        vf_explained_var: 0.015388458967208862
        vf_loss: 1.4605861902236938
    load_time_ms: 14099.185
    num_steps_sampled: 92928000
    num_steps_trained: 92928000
    sample_time_ms: 119150.853
    update_time_ms: 15.905
  iterations_since_restore: 318
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.94326923076923
    ram_util_percent: 12.470673076923076
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 17.0
    agent-2: 66.0
    agent-3: 47.0
    agent-4: 27.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 18.48
    agent-1: 9.55
    agent-2: 49.02
    agent-3: 28.67
    agent-4: 15.1
    agent-5: 20.45
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 32.0
    agent-3: 18.0
    agent-4: 5.0
    agent-5: 10.0
  sampler_perf:
    mean_env_wait_ms: 28.2121099097234
    mean_inference_ms: 15.051339662837536
    mean_processing_ms: 73.26908062191299
  time_since_restore: 46193.46166062355
  time_this_iter_s: 146.37757539749146
  time_total_s: 135130.13979029655
  timestamp: 1637409240
  timesteps_since_restore: 30528000
  timesteps_this_iter: 96000
  timesteps_total: 92928000
  training_iteration: 968
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    968 |           135130 | 92928000 |   141.27 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 8.51
    apples_agent-0_min: 1
    apples_agent-1_max: 22
    apples_agent-1_mean: 3.94
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 31.65
    apples_agent-2_min: 12
    apples_agent-3_max: 26
    apples_agent-3_mean: 9.96
    apples_agent-3_min: 3
    apples_agent-4_max: 27
    apples_agent-4_mean: 10.58
    apples_agent-4_min: 4
    apples_agent-5_max: 19
    apples_agent-5_mean: 5.05
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 19
    cleaning_beam_agent-0_mean: 1.52
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 258
    cleaning_beam_agent-1_mean: 222.85
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 40
    cleaning_beam_agent-2_mean: 11.64
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 9.34
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.2
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 4.58
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-56-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 180.0
  episode_reward_mean: 139.23
  episode_reward_min: 84.0
  episodes_this_iter: 96
  episodes_total: 93024
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11817.14
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3108581304550171
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011506525333970785
        model: {}
        policy_loss: -0.0018085310002788901
        total_loss: -0.002236112952232361
        vf_explained_var: 0.009770482778549194
        vf_loss: 1.195298671722412
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20212817192077637
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010682649444788694
        model: {}
        policy_loss: -0.001646827906370163
        total_loss: -0.0019425831269472837
        vf_explained_var: 0.07959742844104767
        vf_loss: 0.5999224185943604
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42384469509124756
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009364744764752686
        model: {}
        policy_loss: -0.001812795177102089
        total_loss: -0.0021607731468975544
        vf_explained_var: 0.012463182210922241
        vf_loss: 3.9798836708068848
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4130144417285919
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013244572328403592
        model: {}
        policy_loss: -0.0015472592785954475
        total_loss: -0.002076514530926943
        vf_explained_var: 0.012615159153938293
        vf_loss: 1.9764974117279053
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4251400828361511
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009167945827357471
        model: {}
        policy_loss: -0.0014605233445763588
        total_loss: -0.002103359904140234
        vf_explained_var: 0.009894877672195435
        vf_loss: 1.0541127920150757
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5704566836357117
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021018863189965487
        model: {}
        policy_loss: -0.0016011334955692291
        total_loss: -0.0024566890206187963
        vf_explained_var: 0.011293396353721619
        vf_loss: 1.4844635725021362
    load_time_ms: 14099.431
    num_steps_sampled: 93024000
    num_steps_trained: 93024000
    sample_time_ms: 119184.153
    update_time_ms: 15.84
  iterations_since_restore: 319
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.92019230769231
    ram_util_percent: 12.539423076923075
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 21.0
    agent-2: 67.0
    agent-3: 42.0
    agent-4: 28.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 17.93
    agent-1: 9.54
    agent-2: 49.05
    agent-3: 27.76
    agent-4: 15.14
    agent-5: 19.81
  policy_reward_min:
    agent-0: 7.0
    agent-1: 3.0
    agent-2: 27.0
    agent-3: 16.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.212405630425152
    mean_inference_ms: 15.0515097825548
    mean_processing_ms: 73.26967331331575
  time_since_restore: 46338.92052435875
  time_this_iter_s: 145.45886373519897
  time_total_s: 135275.59865403175
  timestamp: 1637409385
  timesteps_since_restore: 30624000
  timesteps_this_iter: 96000
  timesteps_total: 93024000
  training_iteration: 969
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    969 |           135276 | 93024000 |   139.23 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 7.75
    apples_agent-0_min: 0
    apples_agent-1_max: 34
    apples_agent-1_mean: 3.82
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 31.45
    apples_agent-2_min: 4
    apples_agent-3_max: 48
    apples_agent-3_mean: 10.35
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 9.44
    apples_agent-4_min: 0
    apples_agent-5_max: 19
    apples_agent-5_mean: 4.73
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.6
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 258
    cleaning_beam_agent-1_mean: 223.19
    cleaning_beam_agent-1_min: 25
    cleaning_beam_agent-2_max: 30
    cleaning_beam_agent-2_mean: 10.57
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 10.7
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.07
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.39
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_06-58-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 187.0
  episode_reward_mean: 139.74
  episode_reward_min: 13.0
  episodes_this_iter: 96
  episodes_total: 93120
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11836.159
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31590190529823303
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008858740911819041
        model: {}
        policy_loss: -0.0017424598336219788
        total_loss: -0.002174992114305496
        vf_explained_var: 0.012002423405647278
        vf_loss: 1.23455810546875
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1997714340686798
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007806194480508566
        model: {}
        policy_loss: -0.001439628191292286
        total_loss: -0.0017312373965978622
        vf_explained_var: 0.10148054361343384
        vf_loss: 0.5998901724815369
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4182882010936737
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014011281309649348
        model: {}
        policy_loss: -0.0018103866605088115
        total_loss: -0.0021035121753811836
        vf_explained_var: 0.023332267999649048
        vf_loss: 4.430614471435547
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40774059295654297
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016255429945886135
        model: {}
        policy_loss: -0.0015759419184178114
        total_loss: -0.0020711508113890886
        vf_explained_var: 0.005802243947982788
        vf_loss: 2.2241592407226562
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4148489534854889
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011677377624437213
        model: {}
        policy_loss: -0.0014402330853044987
        total_loss: -0.0020772197749465704
        vf_explained_var: 0.018646493554115295
        vf_loss: 0.9314947724342346
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5586886405944824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0027505899779498577
        model: {}
        policy_loss: -0.0018677907064557076
        total_loss: -0.0027045526076108217
        vf_explained_var: 0.016121163964271545
        vf_loss: 1.465285301208496
    load_time_ms: 14117.009
    num_steps_sampled: 93120000
    num_steps_trained: 93120000
    sample_time_ms: 119157.987
    update_time_ms: 16.188
  iterations_since_restore: 320
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.845410628019323
    ram_util_percent: 12.470531400966182
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 19.0
    agent-2: 71.0
    agent-3: 43.0
    agent-4: 25.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 18.32
    agent-1: 10.11
    agent-2: 49.29
    agent-3: 28.32
    agent-4: 14.28
    agent-5: 19.42
  policy_reward_min:
    agent-0: 2.0
    agent-1: 0.0
    agent-2: 6.0
    agent-3: 2.0
    agent-4: 0.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.212574018668125
    mean_inference_ms: 15.051410793272145
    mean_processing_ms: 73.26815515080459
  time_since_restore: 46484.09274458885
  time_this_iter_s: 145.17222023010254
  time_total_s: 135420.77087426186
  timestamp: 1637409530
  timesteps_since_restore: 30720000
  timesteps_this_iter: 96000
  timesteps_total: 93120000
  training_iteration: 970
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    970 |           135421 | 93120000 |   139.74 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 7.57
    apples_agent-0_min: 0
    apples_agent-1_max: 27
    apples_agent-1_mean: 3.66
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 31.89
    apples_agent-2_min: 19
    apples_agent-3_max: 37
    apples_agent-3_mean: 9.74
    apples_agent-3_min: 2
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.23
    apples_agent-4_min: 1
    apples_agent-5_max: 17
    apples_agent-5_mean: 4.6
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.56
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 264
    cleaning_beam_agent-1_mean: 226.88
    cleaning_beam_agent-1_min: 193
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 10.49
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 27
    cleaning_beam_agent-3_mean: 9.04
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.97
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.32
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-01-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 187.0
  episode_reward_mean: 140.64
  episode_reward_min: 89.0
  episodes_this_iter: 96
  episodes_total: 93216
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11829.272
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32139497995376587
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009592509595677257
        model: {}
        policy_loss: -0.001909920945763588
        total_loss: -0.0023640934377908707
        vf_explained_var: 0.008479118347167969
        vf_loss: 1.1148223876953125
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2043679654598236
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009662505472078919
        model: {}
        policy_loss: -0.0015080412849783897
        total_loss: -0.0018106978386640549
        vf_explained_var: 0.07605147361755371
        vf_loss: 0.5703283548355103
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4191063940525055
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014259766321629286
        model: {}
        policy_loss: -0.0019249512115493417
        total_loss: -0.0022450238466262817
        vf_explained_var: 0.01181735098361969
        vf_loss: 4.175551891326904
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4004705846309662
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016159157967194915
        model: {}
        policy_loss: -0.001441368367522955
        total_loss: -0.001924054929986596
        vf_explained_var: -0.004650339484214783
        vf_loss: 2.221411943435669
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42058372497558594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010470885317772627
        model: {}
        policy_loss: -0.0015381776029244065
        total_loss: -0.0021774086635559797
        vf_explained_var: 0.01444178819656372
        vf_loss: 1.00997793674469
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5511191487312317
        entropy_coeff: 0.0017600000137463212
        kl: 0.001262102508917451
        model: {}
        policy_loss: -0.001503878622315824
        total_loss: -0.002339618280529976
        vf_explained_var: 0.00870707631111145
        vf_loss: 1.3423011302947998
    load_time_ms: 14118.804
    num_steps_sampled: 93216000
    num_steps_trained: 93216000
    sample_time_ms: 119106.541
    update_time_ms: 16.238
  iterations_since_restore: 321
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.99663461538461
    ram_util_percent: 12.48221153846154
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 21.0
    agent-2: 69.0
    agent-3: 49.0
    agent-4: 28.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 17.85
    agent-1: 9.24
    agent-2: 49.94
    agent-3: 29.11
    agent-4: 14.84
    agent-5: 19.66
  policy_reward_min:
    agent-0: 8.0
    agent-1: 2.0
    agent-2: 34.0
    agent-3: 16.0
    agent-4: 4.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.212718376831525
    mean_inference_ms: 15.051555380936755
    mean_processing_ms: 73.26681030339954
  time_since_restore: 46628.14945888519
  time_this_iter_s: 144.05671429634094
  time_total_s: 135564.8275885582
  timestamp: 1637409677
  timesteps_since_restore: 30816000
  timesteps_this_iter: 96000
  timesteps_total: 93216000
  training_iteration: 971
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    971 |           135565 | 93216000 |   140.64 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 6.97
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.37
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 31.54
    apples_agent-2_min: 13
    apples_agent-3_max: 25
    apples_agent-3_mean: 10.1
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.66
    apples_agent-4_min: 3
    apples_agent-5_max: 28
    apples_agent-5_mean: 5.63
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 1.62
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 268
    cleaning_beam_agent-1_mean: 228.56
    cleaning_beam_agent-1_min: 174
    cleaning_beam_agent-2_max: 55
    cleaning_beam_agent-2_mean: 11.27
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 10.25
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 2.84
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.48
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-03-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 197.0
  episode_reward_mean: 139.64
  episode_reward_min: 61.0
  episodes_this_iter: 96
  episodes_total: 93312
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11822.07
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32191020250320435
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007668905891478062
        model: {}
        policy_loss: -0.0016494430601596832
        total_loss: -0.002094944939017296
        vf_explained_var: 0.015165328979492188
        vf_loss: 1.2106231451034546
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20327861607074738
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008620210574008524
        model: {}
        policy_loss: -0.0012915171682834625
        total_loss: -0.0015906693879514933
        vf_explained_var: 0.07942403852939606
        vf_loss: 0.5861499309539795
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42311838269233704
        entropy_coeff: 0.0017600000137463212
        kl: 0.001945704803802073
        model: {}
        policy_loss: -0.002089402638375759
        total_loss: -0.002409517765045166
        vf_explained_var: 0.018556341528892517
        vf_loss: 4.245708465576172
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40056344866752625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017907739384099841
        model: {}
        policy_loss: -0.0015798539388924837
        total_loss: -0.0020537395030260086
        vf_explained_var: 0.006158113479614258
        vf_loss: 2.3110625743865967
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40392839908599854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016401340253651142
        model: {}
        policy_loss: -0.0015017655678093433
        total_loss: -0.0021097175776958466
        vf_explained_var: 0.016193583607673645
        vf_loss: 1.0296432971954346
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5529643893241882
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015891855582594872
        model: {}
        policy_loss: -0.0015340783866122365
        total_loss: -0.002360692247748375
        vf_explained_var: 0.015618011355400085
        vf_loss: 1.4660553932189941
    load_time_ms: 14103.027
    num_steps_sampled: 93312000
    num_steps_trained: 93312000
    sample_time_ms: 119074.37
    update_time_ms: 16.285
  iterations_since_restore: 322
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.88932038834951
    ram_util_percent: 12.468446601941743
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 17.0
    agent-2: 78.0
    agent-3: 46.0
    agent-4: 25.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 17.67
    agent-1: 9.37
    agent-2: 48.66
    agent-3: 28.57
    agent-4: 15.17
    agent-5: 20.2
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 24.0
    agent-3: 8.0
    agent-4: 5.0
    agent-5: 10.0
  sampler_perf:
    mean_env_wait_ms: 28.212838401535386
    mean_inference_ms: 15.051395385929569
    mean_processing_ms: 73.26444506586098
  time_since_restore: 46772.923505067825
  time_this_iter_s: 144.77404618263245
  time_total_s: 135709.60163474083
  timestamp: 1637409822
  timesteps_since_restore: 30912000
  timesteps_this_iter: 96000
  timesteps_total: 93312000
  training_iteration: 972
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    972 |           135710 | 93312000 |   139.64 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 7.31
    apples_agent-0_min: 1
    apples_agent-1_max: 17
    apples_agent-1_mean: 3.7
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 30.16
    apples_agent-2_min: 13
    apples_agent-3_max: 21
    apples_agent-3_mean: 9.44
    apples_agent-3_min: 3
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.15
    apples_agent-4_min: 3
    apples_agent-5_max: 27
    apples_agent-5_mean: 5.78
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 18
    cleaning_beam_agent-0_mean: 1.86
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 268
    cleaning_beam_agent-1_mean: 226.48
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 41
    cleaning_beam_agent-2_mean: 13.84
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 10.45
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.46
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 4.75
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-06-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 179.0
  episode_reward_mean: 137.9
  episode_reward_min: 51.0
  episodes_this_iter: 96
  episodes_total: 93408
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11828.029
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3199741840362549
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011416664347052574
        model: {}
        policy_loss: -0.0017769711557775736
        total_loss: -0.0022089616395533085
        vf_explained_var: 0.012614309787750244
        vf_loss: 1.311659812927246
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20587730407714844
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009800128173083067
        model: {}
        policy_loss: -0.001368807628750801
        total_loss: -0.001665137242525816
        vf_explained_var: 0.09366738796234131
        vf_loss: 0.6601344347000122
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42761677503585815
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013156638015061617
        model: {}
        policy_loss: -0.001910022459924221
        total_loss: -0.002263410948216915
        vf_explained_var: 0.010609135031700134
        vf_loss: 3.992164134979248
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40347060561180115
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011688773520290852
        model: {}
        policy_loss: -0.0014149812050163746
        total_loss: -0.001914369873702526
        vf_explained_var: 0.012434959411621094
        vf_loss: 2.1071949005126953
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40233397483825684
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008418788202106953
        model: {}
        policy_loss: -0.001308902632445097
        total_loss: -0.0019050533883273602
        vf_explained_var: 0.01309405267238617
        vf_loss: 1.1195732355117798
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.554751992225647
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009712294558994472
        model: {}
        policy_loss: -0.0013204831629991531
        total_loss: -0.0021497546695172787
        vf_explained_var: 0.012365177273750305
        vf_loss: 1.4709296226501465
    load_time_ms: 14101.17
    num_steps_sampled: 93408000
    num_steps_trained: 93408000
    sample_time_ms: 119150.999
    update_time_ms: 16.41
  iterations_since_restore: 323
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.974396135265703
    ram_util_percent: 12.467149758454104
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 21.0
    agent-2: 69.0
    agent-3: 43.0
    agent-4: 28.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 18.02
    agent-1: 10.06
    agent-2: 47.69
    agent-3: 27.88
    agent-4: 14.76
    agent-5: 19.49
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 18.0
    agent-3: 11.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.213459646327728
    mean_inference_ms: 15.051339610865433
    mean_processing_ms: 73.26452339075442
  time_since_restore: 46918.223871707916
  time_this_iter_s: 145.30036664009094
  time_total_s: 135854.90200138092
  timestamp: 1637409967
  timesteps_since_restore: 31008000
  timesteps_this_iter: 96000
  timesteps_total: 93408000
  training_iteration: 973
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    973 |           135855 | 93408000 |    137.9 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 6.72
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.15
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 30.88
    apples_agent-2_min: 17
    apples_agent-3_max: 27
    apples_agent-3_mean: 10.55
    apples_agent-3_min: 2
    apples_agent-4_max: 27
    apples_agent-4_mean: 10.68
    apples_agent-4_min: 4
    apples_agent-5_max: 22
    apples_agent-5_mean: 5.45
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 18
    cleaning_beam_agent-0_mean: 2.12
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 271
    cleaning_beam_agent-1_mean: 228.9
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 32
    cleaning_beam_agent-2_mean: 13.31
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 9.73
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 3.12
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.54
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-08-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 186.0
  episode_reward_mean: 139.74
  episode_reward_min: 64.0
  episodes_this_iter: 96
  episodes_total: 93504
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11843.297
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3126303553581238
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009676687186583877
        model: {}
        policy_loss: -0.0017572036013007164
        total_loss: -0.0021893344819545746
        vf_explained_var: 0.016730934381484985
        vf_loss: 1.180997371673584
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2018352746963501
        entropy_coeff: 0.0017600000137463212
        kl: 0.001402554684318602
        model: {}
        policy_loss: -0.0017661089077591896
        total_loss: -0.002061277162283659
        vf_explained_var: 0.06868338584899902
        vf_loss: 0.6006168127059937
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.422406405210495
        entropy_coeff: 0.0017600000137463212
        kl: 0.001475171884521842
        model: {}
        policy_loss: -0.0018864083103835583
        total_loss: -0.00223298417404294
        vf_explained_var: 0.02355186641216278
        vf_loss: 3.9686012268066406
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3997466266155243
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011675754794850945
        model: {}
        policy_loss: -0.0016872512642294168
        total_loss: -0.0021881945431232452
        vf_explained_var: 0.006696432828903198
        vf_loss: 2.0261168479919434
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4008297026157379
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008560662972740829
        model: {}
        policy_loss: -0.0013683194993063807
        total_loss: -0.001967916265130043
        vf_explained_var: 0.018647372722625732
        vf_loss: 1.058647871017456
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5622997283935547
        entropy_coeff: 0.0017600000137463212
        kl: 0.002719344338402152
        model: {}
        policy_loss: -0.0017495170468464494
        total_loss: -0.002603054279461503
        vf_explained_var: 0.011685624718666077
        vf_loss: 1.3610986471176147
    load_time_ms: 14098.785
    num_steps_sampled: 93504000
    num_steps_trained: 93504000
    sample_time_ms: 119248.195
    update_time_ms: 16.34
  iterations_since_restore: 324
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.999043062200958
    ram_util_percent: 12.54354066985646
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 19.0
    agent-2: 66.0
    agent-3: 44.0
    agent-4: 27.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 17.24
    agent-1: 9.37
    agent-2: 49.6
    agent-3: 28.48
    agent-4: 15.2
    agent-5: 19.85
  policy_reward_min:
    agent-0: 3.0
    agent-1: 3.0
    agent-2: 25.0
    agent-3: 14.0
    agent-4: 5.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.21449357379699
    mean_inference_ms: 15.051650993953304
    mean_processing_ms: 73.2670253454841
  time_since_restore: 47064.35668301582
  time_this_iter_s: 146.1328113079071
  time_total_s: 136001.03481268883
  timestamp: 1637410113
  timesteps_since_restore: 31104000
  timesteps_this_iter: 96000
  timesteps_total: 93504000
  training_iteration: 974
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    974 |           136001 | 93504000 |   139.74 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 7.31
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.46
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 31.93
    apples_agent-2_min: 11
    apples_agent-3_max: 34
    apples_agent-3_mean: 10.26
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.5
    apples_agent-4_min: 1
    apples_agent-5_max: 24
    apples_agent-5_mean: 5.56
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 18
    cleaning_beam_agent-0_mean: 1.66
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 273
    cleaning_beam_agent-1_mean: 232.56
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 28
    cleaning_beam_agent-2_mean: 10.95
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 9.1
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 16
    cleaning_beam_agent-4_mean: 3.75
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.78
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-10-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 204.0
  episode_reward_mean: 140.27
  episode_reward_min: 46.0
  episodes_this_iter: 96
  episodes_total: 93600
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11843.467
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30661192536354065
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018382184207439423
        model: {}
        policy_loss: -0.001816388452425599
        total_loss: -0.0022286665625870228
        vf_explained_var: 0.013225868344306946
        vf_loss: 1.273604154586792
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20506364107131958
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010610371828079224
        model: {}
        policy_loss: -0.001566986320540309
        total_loss: -0.0018657243344932795
        vf_explained_var: 0.08962585031986237
        vf_loss: 0.6216955184936523
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41713979840278625
        entropy_coeff: 0.0017600000137463212
        kl: 0.001198729034513235
        model: {}
        policy_loss: -0.0017128442414104939
        total_loss: -0.0020214556716382504
        vf_explained_var: 0.02039623260498047
        vf_loss: 4.25554084777832
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3978879451751709
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014202045276761055
        model: {}
        policy_loss: -0.0014808448031544685
        total_loss: -0.001973275560885668
        vf_explained_var: 0.013423070311546326
        vf_loss: 2.0785140991210938
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3911157250404358
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009639409836381674
        model: {}
        policy_loss: -0.0014192775124683976
        total_loss: -0.002005013870075345
        vf_explained_var: 0.006763175129890442
        vf_loss: 1.0262922048568726
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5697411298751831
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012790285982191563
        model: {}
        policy_loss: -0.0014628847129642963
        total_loss: -0.002319730818271637
        vf_explained_var: 0.01697954535484314
        vf_loss: 1.4589776992797852
    load_time_ms: 14107.069
    num_steps_sampled: 93600000
    num_steps_trained: 93600000
    sample_time_ms: 119216.917
    update_time_ms: 16.524
  iterations_since_restore: 325
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.027669902912624
    ram_util_percent: 12.484466019417477
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 24.0
    agent-2: 66.0
    agent-3: 43.0
    agent-4: 26.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 17.95
    agent-1: 9.79
    agent-2: 49.22
    agent-3: 28.29
    agent-4: 15.09
    agent-5: 19.93
  policy_reward_min:
    agent-0: 5.0
    agent-1: 3.0
    agent-2: 20.0
    agent-3: 12.0
    agent-4: 1.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.215197101969917
    mean_inference_ms: 15.05189133984908
    mean_processing_ms: 73.26651456367603
  time_since_restore: 47209.47307229042
  time_this_iter_s: 145.11638927459717
  time_total_s: 136146.15120196342
  timestamp: 1637410259
  timesteps_since_restore: 31200000
  timesteps_this_iter: 96000
  timesteps_total: 93600000
  training_iteration: 975
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    975 |           136146 | 93600000 |   140.27 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 7.32
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 3.7
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 30.55
    apples_agent-2_min: 16
    apples_agent-3_max: 32
    apples_agent-3_mean: 9.35
    apples_agent-3_min: 4
    apples_agent-4_max: 17
    apples_agent-4_mean: 9.31
    apples_agent-4_min: 3
    apples_agent-5_max: 29
    apples_agent-5_mean: 5.31
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 1.45
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 266
    cleaning_beam_agent-1_mean: 229.96
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 34
    cleaning_beam_agent-2_mean: 14.25
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 25
    cleaning_beam_agent-3_mean: 8.36
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.79
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 4.37
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-13-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 185.0
  episode_reward_mean: 138.78
  episode_reward_min: 69.0
  episodes_this_iter: 96
  episodes_total: 93696
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11838.859
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30071431398391724
        entropy_coeff: 0.0017600000137463212
        kl: 0.001243475591763854
        model: {}
        policy_loss: -0.0017834212630987167
        total_loss: -0.002202687319368124
        vf_explained_var: 0.010189101099967957
        vf_loss: 1.099892020225525
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2050701379776001
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013318054843693972
        model: {}
        policy_loss: -0.001692386344075203
        total_loss: -0.001994131598621607
        vf_explained_var: 0.08081500232219696
        vf_loss: 0.5917891263961792
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4250907599925995
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015165804652497172
        model: {}
        policy_loss: -0.001935562351718545
        total_loss: -0.002275151666253805
        vf_explained_var: 0.014694750308990479
        vf_loss: 4.0857157707214355
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4026448428630829
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010606811847537756
        model: {}
        policy_loss: -0.001270859967917204
        total_loss: -0.0017676358111202717
        vf_explained_var: 0.009946480393409729
        vf_loss: 2.1188032627105713
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38811585307121277
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014066393487155437
        model: {}
        policy_loss: -0.0015808993484824896
        total_loss: -0.00216244556941092
        vf_explained_var: 0.010498538613319397
        vf_loss: 1.015381932258606
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5695144534111023
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024992018006742
        model: {}
        policy_loss: -0.0017289509996771812
        total_loss: -0.0025902758352458477
        vf_explained_var: 0.02377977967262268
        vf_loss: 1.4101998805999756
    load_time_ms: 14109.333
    num_steps_sampled: 93696000
    num_steps_trained: 93696000
    sample_time_ms: 119139.622
    update_time_ms: 16.176
  iterations_since_restore: 326
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.994660194174756
    ram_util_percent: 12.385436893203881
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 18.0
    agent-2: 66.0
    agent-3: 42.0
    agent-4: 32.0
    agent-5: 37.0
  policy_reward_mean:
    agent-0: 17.69
    agent-1: 9.7
    agent-2: 48.77
    agent-3: 28.68
    agent-4: 14.72
    agent-5: 19.22
  policy_reward_min:
    agent-0: 8.0
    agent-1: 2.0
    agent-2: 30.0
    agent-3: 18.0
    agent-4: 4.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.2161428274847
    mean_inference_ms: 15.051615881465604
    mean_processing_ms: 73.26565423738558
  time_since_restore: 47354.029252290726
  time_this_iter_s: 144.55618000030518
  time_total_s: 136290.70738196373
  timestamp: 1637410403
  timesteps_since_restore: 31296000
  timesteps_this_iter: 96000
  timesteps_total: 93696000
  training_iteration: 976
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    976 |           136291 | 93696000 |   138.78 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 7.05
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.35
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 31.39
    apples_agent-2_min: 12
    apples_agent-3_max: 30
    apples_agent-3_mean: 9.14
    apples_agent-3_min: 2
    apples_agent-4_max: 32
    apples_agent-4_mean: 11.3
    apples_agent-4_min: 2
    apples_agent-5_max: 15
    apples_agent-5_mean: 5.2
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 1.49
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 268
    cleaning_beam_agent-1_mean: 229.08
    cleaning_beam_agent-1_min: 201
    cleaning_beam_agent-2_max: 34
    cleaning_beam_agent-2_mean: 13.53
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 30
    cleaning_beam_agent-3_mean: 9.78
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 16
    cleaning_beam_agent-4_mean: 3.98
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.31
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-15-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 195.0
  episode_reward_mean: 139.39
  episode_reward_min: 96.0
  episodes_this_iter: 96
  episodes_total: 93792
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11855.148
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30056965351104736
        entropy_coeff: 0.0017600000137463212
        kl: 0.001849604770541191
        model: {}
        policy_loss: -0.001781540922820568
        total_loss: -0.0021767346188426018
        vf_explained_var: 0.013038769364356995
        vf_loss: 1.338056206703186
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20352701842784882
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009733448969200253
        model: {}
        policy_loss: -0.0016991343582049012
        total_loss: -0.002005215035751462
        vf_explained_var: 0.10379375517368317
        vf_loss: 0.5212525129318237
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4205665588378906
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014045883435755968
        model: {}
        policy_loss: -0.0019485503435134888
        total_loss: -0.002272719517350197
        vf_explained_var: 0.022066831588745117
        vf_loss: 4.160279273986816
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3963874280452728
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012984946370124817
        model: {}
        policy_loss: -0.0013798102736473083
        total_loss: -0.0018617603927850723
        vf_explained_var: 0.004601597785949707
        vf_loss: 2.156879425048828
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3843127191066742
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006799944094382226
        model: {}
        policy_loss: -0.0012825177982449532
        total_loss: -0.0018416973762214184
        vf_explained_var: 0.009514644742012024
        vf_loss: 1.1721274852752686
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5682156085968018
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020544426515698433
        model: {}
        policy_loss: -0.001815712545067072
        total_loss: -0.0026733330450952053
        vf_explained_var: 0.016718819737434387
        vf_loss: 1.4243656396865845
    load_time_ms: 14131.068
    num_steps_sampled: 93792000
    num_steps_trained: 93792000
    sample_time_ms: 119066.381
    update_time_ms: 16.392
  iterations_since_restore: 327
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.95265700483092
    ram_util_percent: 12.46183574879227
  pid: 27065
  policy_reward_max:
    agent-0: 33.0
    agent-1: 17.0
    agent-2: 68.0
    agent-3: 39.0
    agent-4: 30.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 18.86
    agent-1: 9.16
    agent-2: 48.31
    agent-3: 27.47
    agent-4: 16.26
    agent-5: 19.33
  policy_reward_min:
    agent-0: 7.0
    agent-1: 3.0
    agent-2: 29.0
    agent-3: 8.0
    agent-4: 7.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.216659279224896
    mean_inference_ms: 15.051441335583412
    mean_processing_ms: 73.26521218440561
  time_since_restore: 47498.775297403336
  time_this_iter_s: 144.74604511260986
  time_total_s: 136435.45342707634
  timestamp: 1637410548
  timesteps_since_restore: 31392000
  timesteps_this_iter: 96000
  timesteps_total: 93792000
  training_iteration: 977
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    977 |           136435 | 93792000 |   139.39 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 7.99
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.26
    apples_agent-1_min: 0
    apples_agent-2_max: 57
    apples_agent-2_mean: 30.72
    apples_agent-2_min: 11
    apples_agent-3_max: 22
    apples_agent-3_mean: 8.43
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.43
    apples_agent-4_min: 2
    apples_agent-5_max: 18
    apples_agent-5_mean: 5.08
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.81
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 264
    cleaning_beam_agent-1_mean: 226.03
    cleaning_beam_agent-1_min: 66
    cleaning_beam_agent-2_max: 36
    cleaning_beam_agent-2_mean: 13.99
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 8.54
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.86
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.87
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-18-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 173.0
  episode_reward_mean: 139.54
  episode_reward_min: 43.0
  episodes_this_iter: 96
  episodes_total: 93888
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11853.163
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3034600615501404
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011978185502812266
        model: {}
        policy_loss: -0.0017464538104832172
        total_loss: -0.002146494574844837
        vf_explained_var: 0.013469964265823364
        vf_loss: 1.3405133485794067
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20254679024219513
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011678165756165981
        model: {}
        policy_loss: -0.0016343221068382263
        total_loss: -0.0019351472146809101
        vf_explained_var: 0.09621357917785645
        vf_loss: 0.5565659403800964
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42927759885787964
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013009551912546158
        model: {}
        policy_loss: -0.0017964006401598454
        total_loss: -0.0021314620971679688
        vf_explained_var: 0.015655383467674255
        vf_loss: 4.204656600952148
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4038672149181366
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016263762954622507
        model: {}
        policy_loss: -0.0011898311786353588
        total_loss: -0.0016979349311441183
        vf_explained_var: 0.0062759071588516235
        vf_loss: 2.027029037475586
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3808557391166687
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008529081824235618
        model: {}
        policy_loss: -0.0014467258006334305
        total_loss: -0.002005527727305889
        vf_explained_var: 0.002243325114250183
        vf_loss: 1.1150074005126953
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.559402585029602
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020358613692224026
        model: {}
        policy_loss: -0.0017824480310082436
        total_loss: -0.002616223180666566
        vf_explained_var: 0.008024290204048157
        vf_loss: 1.5077651739120483
    load_time_ms: 14115.626
    num_steps_sampled: 93888000
    num_steps_trained: 93888000
    sample_time_ms: 119136.877
    update_time_ms: 16.377
  iterations_since_restore: 328
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.763157894736842
    ram_util_percent: 12.468899521531098
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 25.0
    agent-2: 67.0
    agent-3: 39.0
    agent-4: 27.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 18.97
    agent-1: 9.34
    agent-2: 48.34
    agent-3: 27.3
    agent-4: 15.74
    agent-5: 19.85
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 13.0
    agent-3: 6.0
    agent-4: 2.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.217177982824914
    mean_inference_ms: 15.051682000365913
    mean_processing_ms: 73.26463089714206
  time_since_restore: 47645.70913529396
  time_this_iter_s: 146.933837890625
  time_total_s: 136582.38726496696
  timestamp: 1637410695
  timesteps_since_restore: 31488000
  timesteps_this_iter: 96000
  timesteps_total: 93888000
  training_iteration: 978
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    978 |           136582 | 93888000 |   139.54 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 7.57
    apples_agent-0_min: 0
    apples_agent-1_max: 63
    apples_agent-1_mean: 4.2
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 31.49
    apples_agent-2_min: 15
    apples_agent-3_max: 28
    apples_agent-3_mean: 9.65
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.8
    apples_agent-4_min: 3
    apples_agent-5_max: 17
    apples_agent-5_mean: 5.69
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 5
    cleaning_beam_agent-0_mean: 1.54
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 283
    cleaning_beam_agent-1_mean: 231.13
    cleaning_beam_agent-1_min: 200
    cleaning_beam_agent-2_max: 35
    cleaning_beam_agent-2_mean: 14.81
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 8.94
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 4.07
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.85
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-20-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 183.0
  episode_reward_mean: 139.3
  episode_reward_min: 81.0
  episodes_this_iter: 96
  episodes_total: 93984
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11853.266
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3120845854282379
        entropy_coeff: 0.0017600000137463212
        kl: 0.000887641916051507
        model: {}
        policy_loss: -0.0017113350331783295
        total_loss: -0.0021468200720846653
        vf_explained_var: 0.014850243926048279
        vf_loss: 1.1378381252288818
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20701949298381805
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013517048209905624
        model: {}
        policy_loss: -0.0019407584331929684
        total_loss: -0.0022439430467784405
        vf_explained_var: 0.08870202302932739
        vf_loss: 0.6116883754730225
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.430105060338974
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012915986590087414
        model: {}
        policy_loss: -0.001893409644253552
        total_loss: -0.0022434769198298454
        vf_explained_var: 0.010380089282989502
        vf_loss: 4.069135665893555
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40877896547317505
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007441031048074365
        model: {}
        policy_loss: -0.00131305493414402
        total_loss: -0.0018356879008933902
        vf_explained_var: 0.006534785032272339
        vf_loss: 1.9681650400161743
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38362979888916016
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008949530893005431
        model: {}
        policy_loss: -0.0014798300107941031
        total_loss: -0.002047225832939148
        vf_explained_var: 0.01832956075668335
        vf_loss: 1.0779114961624146
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5663215517997742
        entropy_coeff: 0.0017600000137463212
        kl: 0.00272856536321342
        model: {}
        policy_loss: -0.0020517671946436167
        total_loss: -0.0029066780116409063
        vf_explained_var: 0.010697245597839355
        vf_loss: 1.4181506633758545
    load_time_ms: 14117.525
    num_steps_sampled: 93984000
    num_steps_trained: 93984000
    sample_time_ms: 119047.02
    update_time_ms: 16.482
  iterations_since_restore: 329
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.96359223300971
    ram_util_percent: 12.494660194174756
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 16.0
    agent-2: 70.0
    agent-3: 45.0
    agent-4: 27.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 17.55
    agent-1: 9.69
    agent-2: 49.56
    agent-3: 27.04
    agent-4: 15.59
    agent-5: 19.87
  policy_reward_min:
    agent-0: 5.0
    agent-1: 3.0
    agent-2: 35.0
    agent-3: 16.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.217828979993165
    mean_inference_ms: 15.05200114152128
    mean_processing_ms: 73.26406903388457
  time_since_restore: 47790.28869843483
  time_this_iter_s: 144.57956314086914
  time_total_s: 136726.96682810783
  timestamp: 1637410840
  timesteps_since_restore: 31584000
  timesteps_this_iter: 96000
  timesteps_total: 93984000
  training_iteration: 979
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    979 |           136727 | 93984000 |    139.3 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 7.61
    apples_agent-0_min: 2
    apples_agent-1_max: 13
    apples_agent-1_mean: 3.99
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 31.61
    apples_agent-2_min: 19
    apples_agent-3_max: 32
    apples_agent-3_mean: 10.33
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.76
    apples_agent-4_min: 3
    apples_agent-5_max: 13
    apples_agent-5_mean: 5.27
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.37
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 273
    cleaning_beam_agent-1_mean: 235.19
    cleaning_beam_agent-1_min: 185
    cleaning_beam_agent-2_max: 64
    cleaning_beam_agent-2_mean: 14.18
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 9.91
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 3.63
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.86
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-23-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 208.0
  episode_reward_mean: 145.56
  episode_reward_min: 101.0
  episodes_this_iter: 96
  episodes_total: 94080
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11846.892
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3151949644088745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012694428442046046
        model: {}
        policy_loss: -0.001921369694173336
        total_loss: -0.0023594172671437263
        vf_explained_var: 0.005392521619796753
        vf_loss: 1.1669721603393555
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2051786631345749
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011288654059171677
        model: {}
        policy_loss: -0.0015566423535346985
        total_loss: -0.0018520508892834187
        vf_explained_var: 0.08952479064464569
        vf_loss: 0.6570453643798828
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42243844270706177
        entropy_coeff: 0.0017600000137463212
        kl: 0.001090389909222722
        model: {}
        policy_loss: -0.0016519371420145035
        total_loss: -0.001973138190805912
        vf_explained_var: 0.020133987069129944
        vf_loss: 4.222919464111328
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40610364079475403
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013666481245309114
        model: {}
        policy_loss: -0.0014448706060647964
        total_loss: -0.0019455468282103539
        vf_explained_var: 0.016104862093925476
        vf_loss: 2.14064621925354
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3736567199230194
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008774549351073802
        model: {}
        policy_loss: -0.0014433907344937325
        total_loss: -0.0019805515184998512
        vf_explained_var: 0.010853707790374756
        vf_loss: 1.204754114151001
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5616515278816223
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017887115245684981
        model: {}
        policy_loss: -0.0016829371452331543
        total_loss: -0.0025199903175234795
        vf_explained_var: 0.013879850506782532
        vf_loss: 1.5145082473754883
    load_time_ms: 14110.08
    num_steps_sampled: 94080000
    num_steps_trained: 94080000
    sample_time_ms: 119077.771
    update_time_ms: 16.196
  iterations_since_restore: 330
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.95748792270532
    ram_util_percent: 12.468599033816421
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 19.0
    agent-2: 72.0
    agent-3: 42.0
    agent-4: 32.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 18.61
    agent-1: 10.45
    agent-2: 50.61
    agent-3: 29.15
    agent-4: 15.79
    agent-5: 20.95
  policy_reward_min:
    agent-0: 9.0
    agent-1: 3.0
    agent-2: 35.0
    agent-3: 14.0
    agent-4: 5.0
    agent-5: 10.0
  sampler_perf:
    mean_env_wait_ms: 28.218525095301178
    mean_inference_ms: 15.052213413208447
    mean_processing_ms: 73.26329871570216
  time_since_restore: 47935.56397342682
  time_this_iter_s: 145.27527499198914
  time_total_s: 136872.24210309982
  timestamp: 1637410985
  timesteps_since_restore: 31680000
  timesteps_this_iter: 96000
  timesteps_total: 94080000
  training_iteration: 980
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    980 |           136872 | 94080000 |   145.56 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 7.92
    apples_agent-0_min: 2
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.67
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 32.8
    apples_agent-2_min: 20
    apples_agent-3_max: 24
    apples_agent-3_mean: 9.26
    apples_agent-3_min: 3
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.56
    apples_agent-4_min: 3
    apples_agent-5_max: 15
    apples_agent-5_mean: 5.53
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 20
    cleaning_beam_agent-0_mean: 1.8
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 270
    cleaning_beam_agent-1_mean: 229.99
    cleaning_beam_agent-1_min: 193
    cleaning_beam_agent-2_max: 38
    cleaning_beam_agent-2_mean: 16.23
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 8.61
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.03
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.64
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-25-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 178.0
  episode_reward_mean: 143.09
  episode_reward_min: 85.0
  episodes_this_iter: 96
  episodes_total: 94176
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11853.94
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30807656049728394
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010134584736078978
        model: {}
        policy_loss: -0.0016742004081606865
        total_loss: -0.0020974045619368553
        vf_explained_var: 0.011257648468017578
        vf_loss: 1.19013512134552
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20028498768806458
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013011433184146881
        model: {}
        policy_loss: -0.001584359211847186
        total_loss: -0.0018807128071784973
        vf_explained_var: 0.086090087890625
        vf_loss: 0.5614885687828064
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41675153374671936
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017986019374802709
        model: {}
        policy_loss: -0.0020288187079131603
        total_loss: -0.002368259709328413
        vf_explained_var: 0.010032206773757935
        vf_loss: 3.9404349327087402
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39295485615730286
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022109798155725002
        model: {}
        policy_loss: -0.0018758808728307486
        total_loss: -0.0023636086843907833
        vf_explained_var: 0.002458050847053528
        vf_loss: 2.038722515106201
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3721151351928711
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007991605089046061
        model: {}
        policy_loss: -0.0013753118691965938
        total_loss: -0.0019252151250839233
        vf_explained_var: 0.018228590488433838
        vf_loss: 1.0501984357833862
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5517182946205139
        entropy_coeff: 0.0017600000137463212
        kl: 0.002011749427765608
        model: {}
        policy_loss: -0.0018486399203538895
        total_loss: -0.0026829615235328674
        vf_explained_var: 0.018173545598983765
        vf_loss: 1.3670119047164917
    load_time_ms: 14109.299
    num_steps_sampled: 94176000
    num_steps_trained: 94176000
    sample_time_ms: 119168.449
    update_time_ms: 16.062
  iterations_since_restore: 331
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.66047619047619
    ram_util_percent: 12.38857142857143
  pid: 27065
  policy_reward_max:
    agent-0: 33.0
    agent-1: 19.0
    agent-2: 67.0
    agent-3: 43.0
    agent-4: 28.0
    agent-5: 37.0
  policy_reward_mean:
    agent-0: 18.75
    agent-1: 9.92
    agent-2: 49.67
    agent-3: 28.43
    agent-4: 16.22
    agent-5: 20.1
  policy_reward_min:
    agent-0: 8.0
    agent-1: 3.0
    agent-2: 33.0
    agent-3: 14.0
    agent-4: 5.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.21881766035514
    mean_inference_ms: 15.051982468445283
    mean_processing_ms: 73.26254745637496
  time_since_restore: 48080.59306836128
  time_this_iter_s: 145.0290949344635
  time_total_s: 137017.2711980343
  timestamp: 1637411133
  timesteps_since_restore: 31776000
  timesteps_this_iter: 96000
  timesteps_total: 94176000
  training_iteration: 981
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    981 |           137017 | 94176000 |   143.09 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 7.48
    apples_agent-0_min: 1
    apples_agent-1_max: 30
    apples_agent-1_mean: 4.2
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 31.33
    apples_agent-2_min: 12
    apples_agent-3_max: 27
    apples_agent-3_mean: 9.57
    apples_agent-3_min: 2
    apples_agent-4_max: 30
    apples_agent-4_mean: 10.78
    apples_agent-4_min: 3
    apples_agent-5_max: 18
    apples_agent-5_mean: 5.69
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 17
    cleaning_beam_agent-0_mean: 1.73
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 257
    cleaning_beam_agent-1_mean: 220.95
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 32
    cleaning_beam_agent-2_mean: 14.21
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 8.21
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 2.94
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 4.2
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-27-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 182.0
  episode_reward_mean: 138.32
  episode_reward_min: 68.0
  episodes_this_iter: 96
  episodes_total: 94272
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11862.735
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3034442067146301
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012219060445204377
        model: {}
        policy_loss: -0.0018945666961371899
        total_loss: -0.0023150532506406307
        vf_explained_var: 0.010146498680114746
        vf_loss: 1.1357520818710327
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20092232525348663
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010618960950523615
        model: {}
        policy_loss: -0.001553068868815899
        total_loss: -0.0018449751660227776
        vf_explained_var: 0.08237089216709137
        vf_loss: 0.6171896457672119
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41821712255477905
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010675983503460884
        model: {}
        policy_loss: -0.0019171219319105148
        total_loss: -0.002216844819486141
        vf_explained_var: 0.01246151328086853
        vf_loss: 4.363398551940918
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4038536250591278
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010643312707543373
        model: {}
        policy_loss: -0.0012795701622962952
        total_loss: -0.0017726337537169456
        vf_explained_var: -0.004685908555984497
        vf_loss: 2.1771867275238037
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3667202889919281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007035457529127598
        model: {}
        policy_loss: -0.0012344466522336006
        total_loss: -0.0017771064303815365
        vf_explained_var: 0.012144088745117188
        vf_loss: 1.027708649635315
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5558509230613708
        entropy_coeff: 0.0017600000137463212
        kl: 0.001716000260785222
        model: {}
        policy_loss: -0.0017588688060641289
        total_loss: -0.002593378070741892
        vf_explained_var: 0.010823637247085571
        vf_loss: 1.437901258468628
    load_time_ms: 14131.725
    num_steps_sampled: 94272000
    num_steps_trained: 94272000
    sample_time_ms: 119131.163
    update_time_ms: 16.063
  iterations_since_restore: 332
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.988349514563108
    ram_util_percent: 12.46116504854369
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 71.0
    agent-3: 45.0
    agent-4: 26.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 17.49
    agent-1: 9.65
    agent-2: 48.77
    agent-3: 27.95
    agent-4: 15.2
    agent-5: 19.26
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 27.0
    agent-3: 8.0
    agent-4: 3.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.219152535686263
    mean_inference_ms: 15.051961139238017
    mean_processing_ms: 73.26171439587927
  time_since_restore: 48225.37618660927
  time_this_iter_s: 144.78311824798584
  time_total_s: 137162.05431628227
  timestamp: 1637411278
  timesteps_since_restore: 31872000
  timesteps_this_iter: 96000
  timesteps_total: 94272000
  training_iteration: 982
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    982 |           137162 | 94272000 |   138.32 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 7.59
    apples_agent-0_min: 1
    apples_agent-1_max: 16
    apples_agent-1_mean: 4.05
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 30.54
    apples_agent-2_min: 17
    apples_agent-3_max: 20
    apples_agent-3_mean: 9.19
    apples_agent-3_min: 3
    apples_agent-4_max: 18
    apples_agent-4_mean: 9.83
    apples_agent-4_min: 2
    apples_agent-5_max: 22
    apples_agent-5_mean: 5.51
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 17
    cleaning_beam_agent-0_mean: 1.75
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 262
    cleaning_beam_agent-1_mean: 227.44
    cleaning_beam_agent-1_min: 154
    cleaning_beam_agent-2_max: 39
    cleaning_beam_agent-2_mean: 15.82
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 8.96
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.74
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.98
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-30-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 175.0
  episode_reward_mean: 137.5
  episode_reward_min: 86.0
  episodes_this_iter: 96
  episodes_total: 94368
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11854.823
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30269747972488403
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010240215342491865
        model: {}
        policy_loss: -0.0016226519364863634
        total_loss: -0.0020478153601288795
        vf_explained_var: 0.005963623523712158
        vf_loss: 1.07582426071167
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2047426551580429
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014014433836564422
        model: {}
        policy_loss: -0.0017414377070963383
        total_loss: -0.0020401272922754288
        vf_explained_var: 0.07648879289627075
        vf_loss: 0.6165749430656433
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41732466220855713
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017343891086056828
        model: {}
        policy_loss: -0.0019635651260614395
        total_loss: -0.002300330437719822
        vf_explained_var: 0.01878124475479126
        vf_loss: 3.9772655963897705
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40092387795448303
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016779728466644883
        model: {}
        policy_loss: -0.0014987578615546227
        total_loss: -0.0019879546016454697
        vf_explained_var: -3.167986869812012e-05
        vf_loss: 2.1642918586730957
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3649120330810547
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007503918604925275
        model: {}
        policy_loss: -0.0013232538476586342
        total_loss: -0.0018604826182126999
        vf_explained_var: 0.017418116331100464
        vf_loss: 1.0501664876937866
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5501481890678406
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014016757486388087
        model: {}
        policy_loss: -0.0016508906846866012
        total_loss: -0.0024712425656616688
        vf_explained_var: 0.022779449820518494
        vf_loss: 1.4791030883789062
    load_time_ms: 14142.745
    num_steps_sampled: 94368000
    num_steps_trained: 94368000
    sample_time_ms: 119085.726
    update_time_ms: 15.958
  iterations_since_restore: 333
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.180676328502415
    ram_util_percent: 12.385507246376813
  pid: 27065
  policy_reward_max:
    agent-0: 23.0
    agent-1: 19.0
    agent-2: 71.0
    agent-3: 44.0
    agent-4: 27.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 16.35
    agent-1: 9.95
    agent-2: 47.63
    agent-3: 27.89
    agent-4: 15.16
    agent-5: 20.52
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 28.0
    agent-3: 12.0
    agent-4: 3.0
    agent-5: 10.0
  sampler_perf:
    mean_env_wait_ms: 28.219991372209556
    mean_inference_ms: 15.051923889745614
    mean_processing_ms: 73.26137709436522
  time_since_restore: 48370.20592713356
  time_this_iter_s: 144.829740524292
  time_total_s: 137306.88405680656
  timestamp: 1637411422
  timesteps_since_restore: 31968000
  timesteps_this_iter: 96000
  timesteps_total: 94368000
  training_iteration: 983
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    983 |           137307 | 94368000 |    137.5 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 7.0
    apples_agent-0_min: 2
    apples_agent-1_max: 8
    apples_agent-1_mean: 2.97
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 29.84
    apples_agent-2_min: 17
    apples_agent-3_max: 36
    apples_agent-3_mean: 8.37
    apples_agent-3_min: 1
    apples_agent-4_max: 37
    apples_agent-4_mean: 10.63
    apples_agent-4_min: 3
    apples_agent-5_max: 25
    apples_agent-5_mean: 4.67
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.82
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 269
    cleaning_beam_agent-1_mean: 226.85
    cleaning_beam_agent-1_min: 158
    cleaning_beam_agent-2_max: 48
    cleaning_beam_agent-2_mean: 17.31
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 9.19
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.75
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 4.01
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-32-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 173.0
  episode_reward_mean: 133.09
  episode_reward_min: 94.0
  episodes_this_iter: 96
  episodes_total: 94464
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11847.869
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30356964468955994
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011778951156884432
        model: {}
        policy_loss: -0.0019998187199234962
        total_loss: -0.002424050122499466
        vf_explained_var: 0.015519306063652039
        vf_loss: 1.1005568504333496
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20204171538352966
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015923840692266822
        model: {}
        policy_loss: -0.0016526849940419197
        total_loss: -0.0019564731046557426
        vf_explained_var: 0.07760773599147797
        vf_loss: 0.5180377960205078
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42603445053100586
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013872886775061488
        model: {}
        policy_loss: -0.0019415300339460373
        total_loss: -0.002299119718372822
        vf_explained_var: 0.015214696526527405
        vf_loss: 3.922301769256592
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4012709856033325
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011367489350959659
        model: {}
        policy_loss: -0.0014139804989099503
        total_loss: -0.0019323863089084625
        vf_explained_var: 0.013323664665222168
        vf_loss: 1.8783012628555298
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36093342304229736
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005755905876867473
        model: {}
        policy_loss: -0.0012954927515238523
        total_loss: -0.001817386131733656
        vf_explained_var: 0.013576656579971313
        vf_loss: 1.1334741115570068
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5495061278343201
        entropy_coeff: 0.0017600000137463212
        kl: 0.002048914320766926
        model: {}
        policy_loss: -0.0016479651676490903
        total_loss: -0.002491640392690897
        vf_explained_var: 0.018399983644485474
        vf_loss: 1.2345362901687622
    load_time_ms: 14138.467
    num_steps_sampled: 94464000
    num_steps_trained: 94464000
    sample_time_ms: 119068.033
    update_time_ms: 15.958
  iterations_since_restore: 334
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.946859903381643
    ram_util_percent: 12.366666666666667
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 71.0
    agent-3: 39.0
    agent-4: 32.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 16.93
    agent-1: 8.48
    agent-2: 47.52
    agent-3: 26.04
    agent-4: 15.64
    agent-5: 18.48
  policy_reward_min:
    agent-0: 4.0
    agent-1: 2.0
    agent-2: 31.0
    agent-3: 16.0
    agent-4: 7.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.220619851518844
    mean_inference_ms: 15.05187019816878
    mean_processing_ms: 73.26122817913738
  time_since_restore: 48516.05121564865
  time_this_iter_s: 145.84528851509094
  time_total_s: 137452.72934532166
  timestamp: 1637411568
  timesteps_since_restore: 32064000
  timesteps_this_iter: 96000
  timesteps_total: 94464000
  training_iteration: 984
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    984 |           137453 | 94464000 |   133.09 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 7.88
    apples_agent-0_min: 2
    apples_agent-1_max: 28
    apples_agent-1_mean: 3.46
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 30.39
    apples_agent-2_min: 21
    apples_agent-3_max: 20
    apples_agent-3_mean: 9.1
    apples_agent-3_min: 3
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.55
    apples_agent-4_min: 3
    apples_agent-5_max: 33
    apples_agent-5_mean: 4.88
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.21
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 266
    cleaning_beam_agent-1_mean: 225.1
    cleaning_beam_agent-1_min: 163
    cleaning_beam_agent-2_max: 47
    cleaning_beam_agent-2_mean: 15.91
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 26
    cleaning_beam_agent-3_mean: 8.43
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 3.1
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 4.61
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-35-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 181.0
  episode_reward_mean: 136.92
  episode_reward_min: 96.0
  episodes_this_iter: 96
  episodes_total: 94560
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11857.282
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3140111565589905
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012912845704704523
        model: {}
        policy_loss: -0.0016570081934332848
        total_loss: -0.0020936853252351284
        vf_explained_var: 0.01430317759513855
        vf_loss: 1.159818172454834
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20281259715557098
        entropy_coeff: 0.0017600000137463212
        kl: 0.000978492433205247
        model: {}
        policy_loss: -0.0017546212766319513
        total_loss: -0.0020605018362402916
        vf_explained_var: 0.08385846018791199
        vf_loss: 0.5107055902481079
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42260420322418213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011553687509149313
        model: {}
        policy_loss: -0.0018718382343649864
        total_loss: -0.002232604194432497
        vf_explained_var: 0.01976136863231659
        vf_loss: 3.830181837081909
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3977731466293335
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011424212716519833
        model: {}
        policy_loss: -0.0016436195001006126
        total_loss: -0.0021507292985916138
        vf_explained_var: 0.0004788637161254883
        vf_loss: 1.9296960830688477
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3619755804538727
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008215566631406546
        model: {}
        policy_loss: -0.0013003931380808353
        total_loss: -0.0018322690157219768
        vf_explained_var: 0.0175178200006485
        vf_loss: 1.0520023107528687
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.558488130569458
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026173442602157593
        model: {}
        policy_loss: -0.0018086652271449566
        total_loss: -0.0026636826805770397
        vf_explained_var: 0.013409212231636047
        vf_loss: 1.2792071104049683
    load_time_ms: 14142.82
    num_steps_sampled: 94560000
    num_steps_trained: 94560000
    sample_time_ms: 119081.456
    update_time_ms: 16.054
  iterations_since_restore: 335
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.914423076923075
    ram_util_percent: 12.36778846153846
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 17.0
    agent-2: 71.0
    agent-3: 44.0
    agent-4: 26.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 17.84
    agent-1: 8.94
    agent-2: 48.31
    agent-3: 26.9
    agent-4: 15.86
    agent-5: 19.07
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 31.0
    agent-3: 12.0
    agent-4: 5.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.221196503947848
    mean_inference_ms: 15.051985891189458
    mean_processing_ms: 73.26098913040822
  time_since_restore: 48661.418202638626
  time_this_iter_s: 145.36698698997498
  time_total_s: 137598.09633231163
  timestamp: 1637411714
  timesteps_since_restore: 32160000
  timesteps_this_iter: 96000
  timesteps_total: 94560000
  training_iteration: 985
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    985 |           137598 | 94560000 |   136.92 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 7.04
    apples_agent-0_min: 2
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.58
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 29.34
    apples_agent-2_min: 14
    apples_agent-3_max: 31
    apples_agent-3_mean: 9.07
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.8
    apples_agent-4_min: 4
    apples_agent-5_max: 20
    apples_agent-5_mean: 5.12
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 1.35
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 249
    cleaning_beam_agent-1_mean: 216.27
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 51
    cleaning_beam_agent-2_mean: 16.72
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 26
    cleaning_beam_agent-3_mean: 7.91
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.45
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.37
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-37-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 169.0
  episode_reward_mean: 134.34
  episode_reward_min: 72.0
  episodes_this_iter: 96
  episodes_total: 94656
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11860.801
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3055643141269684
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009704740950837731
        model: {}
        policy_loss: -0.0016995454207062721
        total_loss: -0.0021218191832304
        vf_explained_var: 0.025974437594413757
        vf_loss: 1.155205249786377
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19581948220729828
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008995294920168817
        model: {}
        policy_loss: -0.001428924035280943
        total_loss: -0.001714284997433424
        vf_explained_var: 0.07783277332782745
        vf_loss: 0.5928237438201904
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4178517460823059
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012238419149070978
        model: {}
        policy_loss: -0.0017628339119255543
        total_loss: -0.0021046113688498735
        vf_explained_var: 0.006295397877693176
        vf_loss: 3.9364230632781982
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4034345746040344
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016957499319687486
        model: {}
        policy_loss: -0.0015212104190140963
        total_loss: -0.0020253679249435663
        vf_explained_var: 0.013529971241950989
        vf_loss: 2.0588762760162354
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36323440074920654
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006798000540584326
        model: {}
        policy_loss: -0.0011891964823007584
        total_loss: -0.0017295146826654673
        vf_explained_var: 0.0191744863986969
        vf_loss: 0.989754855632782
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5568450689315796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009505442576482892
        model: {}
        policy_loss: -0.0016462909989058971
        total_loss: -0.002499697497114539
        vf_explained_var: 0.004814639687538147
        vf_loss: 1.2664132118225098
    load_time_ms: 14136.849
    num_steps_sampled: 94656000
    num_steps_trained: 94656000
    sample_time_ms: 118994.304
    update_time_ms: 15.97
  iterations_since_restore: 336
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.038235294117648
    ram_util_percent: 12.449509803921568
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 19.0
    agent-2: 64.0
    agent-3: 40.0
    agent-4: 27.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 17.92
    agent-1: 9.46
    agent-2: 47.19
    agent-3: 26.48
    agent-4: 14.83
    agent-5: 18.46
  policy_reward_min:
    agent-0: 9.0
    agent-1: 3.0
    agent-2: 25.0
    agent-3: 13.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.22118842999332
    mean_inference_ms: 15.051680750352602
    mean_processing_ms: 73.25940572042501
  time_since_restore: 48805.07546019554
  time_this_iter_s: 143.65725755691528
  time_total_s: 137741.75358986855
  timestamp: 1637411858
  timesteps_since_restore: 32256000
  timesteps_this_iter: 96000
  timesteps_total: 94656000
  training_iteration: 986
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    986 |           137742 | 94656000 |   134.34 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 7.36
    apples_agent-0_min: 2
    apples_agent-1_max: 17
    apples_agent-1_mean: 3.7
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 30.24
    apples_agent-2_min: 19
    apples_agent-3_max: 42
    apples_agent-3_mean: 9.21
    apples_agent-3_min: 2
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.16
    apples_agent-4_min: 1
    apples_agent-5_max: 27
    apples_agent-5_mean: 5.39
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.34
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 285
    cleaning_beam_agent-1_mean: 221.25
    cleaning_beam_agent-1_min: 181
    cleaning_beam_agent-2_max: 37
    cleaning_beam_agent-2_mean: 16.08
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 31
    cleaning_beam_agent-3_mean: 7.16
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 2.93
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 14
    cleaning_beam_agent-5_mean: 4.57
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-40-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 180.0
  episode_reward_mean: 136.12
  episode_reward_min: 76.0
  episodes_this_iter: 96
  episodes_total: 94752
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11850.676
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3052724003791809
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010124781401827931
        model: {}
        policy_loss: -0.001615932211279869
        total_loss: -0.0020288657397031784
        vf_explained_var: 0.005081981420516968
        vf_loss: 1.243471384048462
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20022565126419067
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009546063956804574
        model: {}
        policy_loss: -0.001180580584332347
        total_loss: -0.0014708016533404589
        vf_explained_var: 0.06989046931266785
        vf_loss: 0.6217594742774963
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4179699718952179
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011014125775545835
        model: {}
        policy_loss: -0.0018098854925483465
        total_loss: -0.0021935643162578344
        vf_explained_var: 0.011271804571151733
        vf_loss: 3.519467353820801
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39689233899116516
        entropy_coeff: 0.0017600000137463212
        kl: 0.001221834565512836
        model: {}
        policy_loss: -0.0015554369892925024
        total_loss: -0.002044924534857273
        vf_explained_var: 0.004788458347320557
        vf_loss: 2.0904037952423096
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36999428272247314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014818704221397638
        model: {}
        policy_loss: -0.0015971041284501553
        total_loss: -0.002140096854418516
        vf_explained_var: 0.014446660876274109
        vf_loss: 1.0819839239120483
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5450155138969421
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014218888245522976
        model: {}
        policy_loss: -0.0016392276156693697
        total_loss: -0.002457003341987729
        vf_explained_var: 0.014027252793312073
        vf_loss: 1.414517879486084
    load_time_ms: 14139.096
    num_steps_sampled: 94752000
    num_steps_trained: 94752000
    sample_time_ms: 119125.787
    update_time_ms: 15.923
  iterations_since_restore: 337
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.863942307692312
    ram_util_percent: 12.440384615384612
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 19.0
    agent-2: 66.0
    agent-3: 41.0
    agent-4: 27.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 17.29
    agent-1: 9.93
    agent-2: 47.03
    agent-3: 27.54
    agent-4: 15.53
    agent-5: 18.8
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 30.0
    agent-3: 15.0
    agent-4: 3.0
    agent-5: -32.0
  sampler_perf:
    mean_env_wait_ms: 28.221487519670003
    mean_inference_ms: 15.051591306355704
    mean_processing_ms: 73.25891251551639
  time_since_restore: 48951.05631566048
  time_this_iter_s: 145.9808554649353
  time_total_s: 137887.73444533348
  timestamp: 1637412004
  timesteps_since_restore: 32352000
  timesteps_this_iter: 96000
  timesteps_total: 94752000
  training_iteration: 987
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    987 |           137888 | 94752000 |   136.12 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 7.29
    apples_agent-0_min: 1
    apples_agent-1_max: 58
    apples_agent-1_mean: 4.33
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 30.56
    apples_agent-2_min: 11
    apples_agent-3_max: 17
    apples_agent-3_mean: 9.01
    apples_agent-3_min: 2
    apples_agent-4_max: 18
    apples_agent-4_mean: 10.14
    apples_agent-4_min: 2
    apples_agent-5_max: 15
    apples_agent-5_mean: 5.12
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 16
    cleaning_beam_agent-0_mean: 1.43
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 220.04
    cleaning_beam_agent-1_min: 58
    cleaning_beam_agent-2_max: 44
    cleaning_beam_agent-2_mean: 17.07
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 27
    cleaning_beam_agent-3_mean: 7.61
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 2.81
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.75
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-42-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 178.0
  episode_reward_mean: 138.25
  episode_reward_min: 50.0
  episodes_this_iter: 96
  episodes_total: 94848
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11842.404
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30673813819885254
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014435929479077458
        model: {}
        policy_loss: -0.001657969318330288
        total_loss: -0.0020728260278701782
        vf_explained_var: 0.01753467321395874
        vf_loss: 1.2500264644622803
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19772538542747498
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011909372406080365
        model: {}
        policy_loss: -0.001399278873577714
        total_loss: -0.0016827364452183247
        vf_explained_var: 0.09480859339237213
        vf_loss: 0.6453992128372192
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41333436965942383
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011439576046541333
        model: {}
        policy_loss: -0.0017931628972291946
        total_loss: -0.0021160696633160114
        vf_explained_var: 0.019668489694595337
        vf_loss: 4.045628547668457
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39958614110946655
        entropy_coeff: 0.0017600000137463212
        kl: 0.001715726568363607
        model: {}
        policy_loss: -0.0015790751203894615
        total_loss: -0.002075556665658951
        vf_explained_var: 0.003803253173828125
        vf_loss: 2.0678939819335938
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3724170923233032
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005234950222074986
        model: {}
        policy_loss: -0.0011315313167870045
        total_loss: -0.0016699018888175488
        vf_explained_var: 0.011463373899459839
        vf_loss: 1.1708297729492188
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5407703518867493
        entropy_coeff: 0.0017600000137463212
        kl: 0.001769165857695043
        model: {}
        policy_loss: -0.0016313712112605572
        total_loss: -0.0024427827447652817
        vf_explained_var: 0.010550156235694885
        vf_loss: 1.403438925743103
    load_time_ms: 14152.569
    num_steps_sampled: 94848000
    num_steps_trained: 94848000
    sample_time_ms: 118930.552
    update_time_ms: 15.8
  iterations_since_restore: 338
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.01787439613527
    ram_util_percent: 12.456038647342993
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 23.0
    agent-2: 66.0
    agent-3: 49.0
    agent-4: 31.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 17.76
    agent-1: 9.51
    agent-2: 47.49
    agent-3: 27.72
    agent-4: 16.07
    agent-5: 19.7
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 20.0
    agent-3: 10.0
    agent-4: 2.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.221987289251686
    mean_inference_ms: 15.051665314606039
    mean_processing_ms: 73.25878652178181
  time_since_restore: 49096.08824634552
  time_this_iter_s: 145.03193068504333
  time_total_s: 138032.76637601852
  timestamp: 1637412149
  timesteps_since_restore: 32448000
  timesteps_this_iter: 96000
  timesteps_total: 94848000
  training_iteration: 988
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    988 |           138033 | 94848000 |   138.25 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 7.45
    apples_agent-0_min: 2
    apples_agent-1_max: 35
    apples_agent-1_mean: 4.05
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 30.05
    apples_agent-2_min: 15
    apples_agent-3_max: 40
    apples_agent-3_mean: 9.15
    apples_agent-3_min: 2
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.14
    apples_agent-4_min: 3
    apples_agent-5_max: 22
    apples_agent-5_mean: 5.24
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 1.3
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 222.73
    cleaning_beam_agent-1_min: 147
    cleaning_beam_agent-2_max: 45
    cleaning_beam_agent-2_mean: 18.52
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 22
    cleaning_beam_agent-3_mean: 7.14
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.94
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.67
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-44-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 172.0
  episode_reward_mean: 133.56
  episode_reward_min: 77.0
  episodes_this_iter: 96
  episodes_total: 94944
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11858.0
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30898815393447876
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010762553429231048
        model: {}
        policy_loss: -0.00177313934545964
        total_loss: -0.0022130790166556835
        vf_explained_var: 0.013912245631217957
        vf_loss: 1.0387729406356812
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2008739858865738
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007417960441671312
        model: {}
        policy_loss: -0.001437814673408866
        total_loss: -0.0017364569939672947
        vf_explained_var: 0.09842722117900848
        vf_loss: 0.5489724278450012
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.424113392829895
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013779952423647046
        model: {}
        policy_loss: -0.001903484109789133
        total_loss: -0.0022597280330955982
        vf_explained_var: 0.020649418234825134
        vf_loss: 3.901959180831909
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39376071095466614
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017755351727828383
        model: {}
        policy_loss: -0.0018859179690480232
        total_loss: -0.0023803841322660446
        vf_explained_var: 0.002738863229751587
        vf_loss: 1.985529899597168
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37084686756134033
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009323735721409321
        model: {}
        policy_loss: -0.001452314667403698
        total_loss: -0.002011893317103386
        vf_explained_var: 0.009762704372406006
        vf_loss: 0.9311108589172363
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5379498600959778
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013102273223921657
        model: {}
        policy_loss: -0.0017329631373286247
        total_loss: -0.0025496017187833786
        vf_explained_var: 0.011516451835632324
        vf_loss: 1.3015211820602417
    load_time_ms: 14164.576
    num_steps_sampled: 94944000
    num_steps_trained: 94944000
    sample_time_ms: 119019.054
    update_time_ms: 15.686
  iterations_since_restore: 339
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.94567307692308
    ram_util_percent: 12.377884615384614
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 19.0
    agent-2: 65.0
    agent-3: 41.0
    agent-4: 24.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 17.19
    agent-1: 9.26
    agent-2: 47.14
    agent-3: 26.47
    agent-4: 14.58
    agent-5: 18.92
  policy_reward_min:
    agent-0: 8.0
    agent-1: 1.0
    agent-2: 20.0
    agent-3: 11.0
    agent-4: 7.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.222579622027244
    mean_inference_ms: 15.051712766939108
    mean_processing_ms: 73.25945299612592
  time_since_restore: 49241.83078742027
  time_this_iter_s: 145.7425410747528
  time_total_s: 138178.50891709328
  timestamp: 1637412295
  timesteps_since_restore: 32544000
  timesteps_this_iter: 96000
  timesteps_total: 94944000
  training_iteration: 989
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    989 |           138179 | 94944000 |   133.56 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 7.02
    apples_agent-0_min: 2
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.08
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 30.62
    apples_agent-2_min: 16
    apples_agent-3_max: 16
    apples_agent-3_mean: 8.46
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.54
    apples_agent-4_min: 3
    apples_agent-5_max: 25
    apples_agent-5_mean: 5.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 17
    cleaning_beam_agent-0_mean: 1.62
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 270
    cleaning_beam_agent-1_mean: 222.42
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 43
    cleaning_beam_agent-2_mean: 19.01
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 6.92
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 7
    cleaning_beam_agent-4_mean: 2.85
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 12
    cleaning_beam_agent-5_mean: 4.25
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-47-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 174.0
  episode_reward_mean: 135.5
  episode_reward_min: 68.0
  episodes_this_iter: 96
  episodes_total: 95040
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11862.624
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3150331377983093
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009976401925086975
        model: {}
        policy_loss: -0.0017543795984238386
        total_loss: -0.002198144793510437
        vf_explained_var: 0.008737459778785706
        vf_loss: 1.1069203615188599
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20228932797908783
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013706735335290432
        model: {}
        policy_loss: -0.0016435477882623672
        total_loss: -0.001950505655258894
        vf_explained_var: 0.0757947564125061
        vf_loss: 0.49069392681121826
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42196857929229736
        entropy_coeff: 0.0017600000137463212
        kl: 0.001140989363193512
        model: {}
        policy_loss: -0.0018913759849965572
        total_loss: -0.0022545475512742996
        vf_explained_var: 0.013111397624015808
        vf_loss: 3.7949390411376953
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39472997188568115
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013887217501178384
        model: {}
        policy_loss: -0.0014100763946771622
        total_loss: -0.0018910695798695087
        vf_explained_var: 0.0045667290687561035
        vf_loss: 2.1373252868652344
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3665989637374878
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007847052766010165
        model: {}
        policy_loss: -0.0012320270761847496
        total_loss: -0.0017823204398155212
        vf_explained_var: 0.012039154767990112
        vf_loss: 0.949183464050293
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5355625152587891
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013341589365154505
        model: {}
        policy_loss: -0.0013501994544640183
        total_loss: -0.0021504145115613937
        vf_explained_var: 0.01056903600692749
        vf_loss: 1.42372465133667
    load_time_ms: 14161.798
    num_steps_sampled: 95040000
    num_steps_trained: 95040000
    sample_time_ms: 119045.303
    update_time_ms: 16.042
  iterations_since_restore: 340
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.049758454106275
    ram_util_percent: 12.454106280193237
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 18.0
    agent-2: 66.0
    agent-3: 49.0
    agent-4: 24.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 17.11
    agent-1: 8.74
    agent-2: 47.91
    agent-3: 27.49
    agent-4: 15.05
    agent-5: 19.2
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 30.0
    agent-3: 11.0
    agent-4: 1.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.22326972486202
    mean_inference_ms: 15.052091574755417
    mean_processing_ms: 73.2601353430167
  time_since_restore: 49387.392318964005
  time_this_iter_s: 145.5615315437317
  time_total_s: 138324.070448637
  timestamp: 1637412441
  timesteps_since_restore: 32640000
  timesteps_this_iter: 96000
  timesteps_total: 95040000
  training_iteration: 990
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    990 |           138324 | 95040000 |    135.5 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 7.38
    apples_agent-0_min: 1
    apples_agent-1_max: 37
    apples_agent-1_mean: 3.55
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 30.48
    apples_agent-2_min: 15
    apples_agent-3_max: 27
    apples_agent-3_mean: 9.44
    apples_agent-3_min: 2
    apples_agent-4_max: 18
    apples_agent-4_mean: 9.82
    apples_agent-4_min: 1
    apples_agent-5_max: 26
    apples_agent-5_mean: 4.79
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 5
    cleaning_beam_agent-0_mean: 1.2
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 268
    cleaning_beam_agent-1_mean: 214.97
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 49
    cleaning_beam_agent-2_mean: 19.17
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 8.41
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 2.68
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.45
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-49-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 194.0
  episode_reward_mean: 135.1
  episode_reward_min: 75.0
  episodes_this_iter: 96
  episodes_total: 95136
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11850.785
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3121585249900818
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016126666450873017
        model: {}
        policy_loss: -0.0017581628635525703
        total_loss: -0.0021953824907541275
        vf_explained_var: 0.01923653483390808
        vf_loss: 1.1217999458312988
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1985415667295456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009717575740069151
        model: {}
        policy_loss: -0.0015232961159199476
        total_loss: -0.0018196904566138983
        vf_explained_var: 0.0713045597076416
        vf_loss: 0.5304137468338013
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41779714822769165
        entropy_coeff: 0.0017600000137463212
        kl: 0.00166971271391958
        model: {}
        policy_loss: -0.001886061392724514
        total_loss: -0.002214491367340088
        vf_explained_var: 0.016339927911758423
        vf_loss: 4.068924427032471
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39192941784858704
        entropy_coeff: 0.0017600000137463212
        kl: 0.00134453980717808
        model: {}
        policy_loss: -0.0014571738429367542
        total_loss: -0.0019439123570919037
        vf_explained_var: 0.013078033924102783
        vf_loss: 2.030555486679077
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37370410561561584
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007688601035624743
        model: {}
        policy_loss: -0.0013817884027957916
        total_loss: -0.0019451379776000977
        vf_explained_var: 0.020635604858398438
        vf_loss: 0.9437351822853088
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5332733988761902
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013446060474961996
        model: {}
        policy_loss: -0.001653033308684826
        total_loss: -0.0024574892595410347
        vf_explained_var: 0.021229594945907593
        vf_loss: 1.341051459312439
    load_time_ms: 14163.559
    num_steps_sampled: 95136000
    num_steps_trained: 95136000
    sample_time_ms: 119027.91
    update_time_ms: 16.203
  iterations_since_restore: 341
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.68857142857143
    ram_util_percent: 12.46047619047619
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 17.0
    agent-2: 64.0
    agent-3: 42.0
    agent-4: 27.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 17.83
    agent-1: 8.68
    agent-2: 47.76
    agent-3: 26.37
    agent-4: 15.17
    agent-5: 19.29
  policy_reward_min:
    agent-0: 9.0
    agent-1: 2.0
    agent-2: 28.0
    agent-3: 14.0
    agent-4: 4.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.223433892801346
    mean_inference_ms: 15.052192960976377
    mean_processing_ms: 73.260112546028
  time_since_restore: 49532.14609217644
  time_this_iter_s: 144.75377321243286
  time_total_s: 138468.82422184944
  timestamp: 1637412588
  timesteps_since_restore: 32736000
  timesteps_this_iter: 96000
  timesteps_total: 95136000
  training_iteration: 991
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    991 |           138469 | 95136000 |    135.1 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 7.96
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.22
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 30.56
    apples_agent-2_min: 17
    apples_agent-3_max: 31
    apples_agent-3_mean: 8.79
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.9
    apples_agent-4_min: 4
    apples_agent-5_max: 20
    apples_agent-5_mean: 5.22
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 1.33
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 247
    cleaning_beam_agent-1_mean: 215.84
    cleaning_beam_agent-1_min: 166
    cleaning_beam_agent-2_max: 41
    cleaning_beam_agent-2_mean: 18.4
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 27
    cleaning_beam_agent-3_mean: 7.14
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 18
    cleaning_beam_agent-4_mean: 3.37
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.68
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-52-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 194.0
  episode_reward_mean: 138.08
  episode_reward_min: 104.0
  episodes_this_iter: 96
  episodes_total: 95232
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11856.571
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.308943510055542
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008721511112526059
        model: {}
        policy_loss: -0.0016984911635518074
        total_loss: -0.002125465776771307
        vf_explained_var: 0.00905543565750122
        vf_loss: 1.1676833629608154
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19959452748298645
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009641944780014455
        model: {}
        policy_loss: -0.0016032656421884894
        total_loss: -0.0019001588225364685
        vf_explained_var: 0.07408474385738373
        vf_loss: 0.5439556837081909
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4040796756744385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013715100940316916
        model: {}
        policy_loss: -0.0017610008362680674
        total_loss: -0.0020774027798324823
        vf_explained_var: 0.01703794300556183
        vf_loss: 3.9477779865264893
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3891840875148773
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015145557699725032
        model: {}
        policy_loss: -0.00167683488689363
        total_loss: -0.002153250388801098
        vf_explained_var: 0.013950929045677185
        vf_loss: 2.085451126098633
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37832212448120117
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008045816794037819
        model: {}
        policy_loss: -0.0012279674410820007
        total_loss: -0.0017806971445679665
        vf_explained_var: 0.015596464276313782
        vf_loss: 1.1311501264572144
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5204557180404663
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013718698173761368
        model: {}
        policy_loss: -0.0017455299384891987
        total_loss: -0.002519388450309634
        vf_explained_var: 0.024065598845481873
        vf_loss: 1.4214441776275635
    load_time_ms: 14165.471
    num_steps_sampled: 95232000
    num_steps_trained: 95232000
    sample_time_ms: 119119.827
    update_time_ms: 16.478
  iterations_since_restore: 342
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.792307692307695
    ram_util_percent: 12.456249999999999
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 17.0
    agent-2: 70.0
    agent-3: 44.0
    agent-4: 28.0
    agent-5: 38.0
  policy_reward_mean:
    agent-0: 18.04
    agent-1: 9.09
    agent-2: 47.78
    agent-3: 27.15
    agent-4: 16.11
    agent-5: 19.91
  policy_reward_min:
    agent-0: 7.0
    agent-1: 3.0
    agent-2: 32.0
    agent-3: 14.0
    agent-4: 5.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.22341310267313
    mean_inference_ms: 15.052072377757245
    mean_processing_ms: 73.25928265888604
  time_since_restore: 49677.9588329792
  time_this_iter_s: 145.8127408027649
  time_total_s: 138614.6369626522
  timestamp: 1637412734
  timesteps_since_restore: 32832000
  timesteps_this_iter: 96000
  timesteps_total: 95232000
  training_iteration: 992
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    992 |           138615 | 95232000 |   138.08 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 6.98
    apples_agent-0_min: 1
    apples_agent-1_max: 18
    apples_agent-1_mean: 3.81
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 29.42
    apples_agent-2_min: 17
    apples_agent-3_max: 16
    apples_agent-3_mean: 8.1
    apples_agent-3_min: 3
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.1
    apples_agent-4_min: 3
    apples_agent-5_max: 37
    apples_agent-5_mean: 4.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 23
    cleaning_beam_agent-0_mean: 1.44
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 247
    cleaning_beam_agent-1_mean: 210.29
    cleaning_beam_agent-1_min: 181
    cleaning_beam_agent-2_max: 39
    cleaning_beam_agent-2_mean: 20.16
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 22
    cleaning_beam_agent-3_mean: 7.33
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 2.95
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.24
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-54-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 175.0
  episode_reward_mean: 132.85
  episode_reward_min: 95.0
  episodes_this_iter: 96
  episodes_total: 95328
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11865.208
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3101339042186737
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012944929767400026
        model: {}
        policy_loss: -0.0018150843679904938
        total_loss: -0.0022425279021263123
        vf_explained_var: 0.009538829326629639
        vf_loss: 1.1839368343353271
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19622273743152618
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008142352453432977
        model: {}
        policy_loss: -0.0015312018804252148
        total_loss: -0.0018228164408355951
        vf_explained_var: 0.09657555818557739
        vf_loss: 0.5373479723930359
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40742558240890503
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012753696646541357
        model: {}
        policy_loss: -0.001690322533249855
        total_loss: -0.00202705105766654
        vf_explained_var: 0.020544826984405518
        vf_loss: 3.8034048080444336
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3863925337791443
        entropy_coeff: 0.0017600000137463212
        kl: 0.00163902691565454
        model: {}
        policy_loss: -0.001593549968674779
        total_loss: -0.0020855946931988
        vf_explained_var: 0.001496434211730957
        vf_loss: 1.8800666332244873
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37822985649108887
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006409634952433407
        model: {}
        policy_loss: -0.0012419749982655048
        total_loss: -0.0018104859627783298
        vf_explained_var: 0.019114181399345398
        vf_loss: 0.9717562794685364
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5270179510116577
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023637707345187664
        model: {}
        policy_loss: -0.002047956455498934
        total_loss: -0.002850889228284359
        vf_explained_var: 0.016635209321975708
        vf_loss: 1.2461693286895752
    load_time_ms: 14164.109
    num_steps_sampled: 95328000
    num_steps_trained: 95328000
    sample_time_ms: 119220.212
    update_time_ms: 16.756
  iterations_since_restore: 343
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.896135265700483
    ram_util_percent: 12.388405797101447
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 19.0
    agent-2: 71.0
    agent-3: 44.0
    agent-4: 25.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 17.5
    agent-1: 9.14
    agent-2: 47.11
    agent-3: 25.85
    agent-4: 14.77
    agent-5: 18.48
  policy_reward_min:
    agent-0: 6.0
    agent-1: 1.0
    agent-2: 27.0
    agent-3: 11.0
    agent-4: 5.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.223696808614612
    mean_inference_ms: 15.052126271620123
    mean_processing_ms: 73.25912333121808
  time_since_restore: 49823.86670303345
  time_this_iter_s: 145.907870054245
  time_total_s: 138760.54483270645
  timestamp: 1637412880
  timesteps_since_restore: 32928000
  timesteps_this_iter: 96000
  timesteps_total: 95328000
  training_iteration: 993
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    993 |           138761 | 95328000 |   132.85 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 6.89
    apples_agent-0_min: 1
    apples_agent-1_max: 18
    apples_agent-1_mean: 3.45
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 28.53
    apples_agent-2_min: 14
    apples_agent-3_max: 45
    apples_agent-3_mean: 8.76
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.9
    apples_agent-4_min: 1
    apples_agent-5_max: 34
    apples_agent-5_mean: 5.4
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.48
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 240
    cleaning_beam_agent-1_mean: 210.38
    cleaning_beam_agent-1_min: 163
    cleaning_beam_agent-2_max: 42
    cleaning_beam_agent-2_mean: 19.68
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 31
    cleaning_beam_agent-3_mean: 7.74
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 2.98
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.69
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-57-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 174.0
  episode_reward_mean: 131.18
  episode_reward_min: 83.0
  episodes_this_iter: 96
  episodes_total: 95424
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11859.428
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30599841475486755
        entropy_coeff: 0.0017600000137463212
        kl: 0.000538900843821466
        model: {}
        policy_loss: -0.001580961630679667
        total_loss: -0.002004214096814394
        vf_explained_var: 0.013201162219047546
        vf_loss: 1.1530309915542603
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1986597180366516
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008969600312411785
        model: {}
        policy_loss: -0.0015903431922197342
        total_loss: -0.0018822047859430313
        vf_explained_var: 0.09621484577655792
        vf_loss: 0.5777713656425476
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40448880195617676
        entropy_coeff: 0.0017600000137463212
        kl: 0.001132326084189117
        model: {}
        policy_loss: -0.001757926307618618
        total_loss: -0.0020727263763546944
        vf_explained_var: 0.018552705645561218
        vf_loss: 3.970989227294922
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3871641755104065
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018277132185176015
        model: {}
        policy_loss: -0.0016220873221755028
        total_loss: -0.002129278378561139
        vf_explained_var: 0.015546470880508423
        vf_loss: 1.742148518562317
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3781113624572754
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006262896931730211
        model: {}
        policy_loss: -0.0013501179637387395
        total_loss: -0.001905989134684205
        vf_explained_var: 0.013316154479980469
        vf_loss: 1.0960755348205566
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5371478796005249
        entropy_coeff: 0.0017600000137463212
        kl: 0.001965942792594433
        model: {}
        policy_loss: -0.0016513457521796227
        total_loss: -0.0024586289655417204
        vf_explained_var: 0.017095118761062622
        vf_loss: 1.3809475898742676
    load_time_ms: 14170.212
    num_steps_sampled: 95424000
    num_steps_trained: 95424000
    sample_time_ms: 119093.477
    update_time_ms: 16.566
  iterations_since_restore: 344
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.882608695652173
    ram_util_percent: 12.447826086956521
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 20.0
    agent-2: 60.0
    agent-3: 37.0
    agent-4: 29.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 17.07
    agent-1: 9.03
    agent-2: 45.75
    agent-3: 25.82
    agent-4: 15.17
    agent-5: 18.34
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 26.0
    agent-3: 10.0
    agent-4: 5.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.223691773702708
    mean_inference_ms: 15.051996599179128
    mean_processing_ms: 73.2579458792066
  time_since_restore: 49968.44989347458
  time_this_iter_s: 144.5831904411316
  time_total_s: 138905.12802314758
  timestamp: 1637413024
  timesteps_since_restore: 33024000
  timesteps_this_iter: 96000
  timesteps_total: 95424000
  training_iteration: 994
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    994 |           138905 | 95424000 |   131.18 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 6.95
    apples_agent-0_min: 0
    apples_agent-1_max: 15
    apples_agent-1_mean: 3.72
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 30.72
    apples_agent-2_min: 16
    apples_agent-3_max: 30
    apples_agent-3_mean: 9.33
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 10.08
    apples_agent-4_min: 1
    apples_agent-5_max: 30
    apples_agent-5_mean: 5.44
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 23
    cleaning_beam_agent-0_mean: 1.35
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 255
    cleaning_beam_agent-1_mean: 213.55
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 49
    cleaning_beam_agent-2_mean: 19.18
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 27
    cleaning_beam_agent-3_mean: 6.55
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.16
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 3.28
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_07-59-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 178.0
  episode_reward_mean: 134.98
  episode_reward_min: 75.0
  episodes_this_iter: 96
  episodes_total: 95520
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11843.318
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3125503659248352
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008359662024304271
        model: {}
        policy_loss: -0.0016709361225366592
        total_loss: -0.002104382961988449
        vf_explained_var: 0.014946803450584412
        vf_loss: 1.1664044857025146
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19847220182418823
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012262072414159775
        model: {}
        policy_loss: -0.0016602855175733566
        total_loss: -0.00195356085896492
        vf_explained_var: 0.07685580849647522
        vf_loss: 0.5603758692741394
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40166693925857544
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012630471028387547
        model: {}
        policy_loss: -0.0017313463613390923
        total_loss: -0.002047270070761442
        vf_explained_var: 0.023416325449943542
        vf_loss: 3.910144329071045
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3789910078048706
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016381987370550632
        model: {}
        policy_loss: -0.0016738492995500565
        total_loss: -0.0021354593336582184
        vf_explained_var: 0.0037025511264801025
        vf_loss: 2.054154634475708
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3725718855857849
        entropy_coeff: 0.0017600000137463212
        kl: 0.001484178239479661
        model: {}
        policy_loss: -0.0014949049800634384
        total_loss: -0.0020460542291402817
        vf_explained_var: 0.010467350482940674
        vf_loss: 1.045779824256897
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5335910320281982
        entropy_coeff: 0.0017600000137463212
        kl: 0.001157109159976244
        model: {}
        policy_loss: -0.001623182324692607
        total_loss: -0.0024247453548014164
        vf_explained_var: 0.00967097282409668
        vf_loss: 1.3755847215652466
    load_time_ms: 14155.406
    num_steps_sampled: 95520000
    num_steps_trained: 95520000
    sample_time_ms: 119121.943
    update_time_ms: 16.276
  iterations_since_restore: 345
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.929468599033815
    ram_util_percent: 12.45652173913043
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 23.0
    agent-2: 67.0
    agent-3: 42.0
    agent-4: 29.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 17.91
    agent-1: 9.07
    agent-2: 47.36
    agent-3: 26.52
    agent-4: 15.04
    agent-5: 19.08
  policy_reward_min:
    agent-0: 8.0
    agent-1: 3.0
    agent-2: 26.0
    agent-3: 7.0
    agent-4: 6.0
    agent-5: 10.0
  sampler_perf:
    mean_env_wait_ms: 28.223904764910518
    mean_inference_ms: 15.051969673838693
    mean_processing_ms: 73.25793109487451
  time_since_restore: 50113.77975964546
  time_this_iter_s: 145.32986617088318
  time_total_s: 139050.45788931847
  timestamp: 1637413170
  timesteps_since_restore: 33120000
  timesteps_this_iter: 96000
  timesteps_total: 95520000
  training_iteration: 995
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    995 |           139050 | 95520000 |   134.98 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 8.02
    apples_agent-0_min: 1
    apples_agent-1_max: 12
    apples_agent-1_mean: 3.18
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 29.87
    apples_agent-2_min: 11
    apples_agent-3_max: 29
    apples_agent-3_mean: 8.78
    apples_agent-3_min: 2
    apples_agent-4_max: 30
    apples_agent-4_mean: 11.08
    apples_agent-4_min: 3
    apples_agent-5_max: 24
    apples_agent-5_mean: 6.04
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 17
    cleaning_beam_agent-0_mean: 1.68
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 214.94
    cleaning_beam_agent-1_min: 176
    cleaning_beam_agent-2_max: 106
    cleaning_beam_agent-2_mean: 19.04
    cleaning_beam_agent-2_min: 8
    cleaning_beam_agent-3_max: 28
    cleaning_beam_agent-3_mean: 6.78
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.12
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.56
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-01-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 175.0
  episode_reward_mean: 136.38
  episode_reward_min: 67.0
  episodes_this_iter: 96
  episodes_total: 95616
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11849.935
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3041268587112427
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012915630359202623
        model: {}
        policy_loss: -0.0019230407197028399
        total_loss: -0.002340844599530101
        vf_explained_var: 0.016329869627952576
        vf_loss: 1.1746046543121338
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1988939791917801
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010107372654601932
        model: {}
        policy_loss: -0.0015394689980894327
        total_loss: -0.001835977891460061
        vf_explained_var: 0.08992858231067657
        vf_loss: 0.5354577302932739
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3994506597518921
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014848627615720034
        model: {}
        policy_loss: -0.001826401799917221
        total_loss: -0.0021615910809487104
        vf_explained_var: 0.021609216928482056
        vf_loss: 3.678441047668457
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3865881860256195
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013470492558553815
        model: {}
        policy_loss: -0.0014346661046147346
        total_loss: -0.0019323648884892464
        vf_explained_var: 0.008693203330039978
        vf_loss: 1.826981782913208
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3715992569923401
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010546351550146937
        model: {}
        policy_loss: -0.0015587153611704707
        total_loss: -0.002102986676618457
        vf_explained_var: 0.013879507780075073
        vf_loss: 1.0973931550979614
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5323641896247864
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020329379476606846
        model: {}
        policy_loss: -0.0018301147501915693
        total_loss: -0.0026143307331949472
        vf_explained_var: 0.004118144512176514
        vf_loss: 1.5274690389633179
    load_time_ms: 14156.247
    num_steps_sampled: 95616000
    num_steps_trained: 95616000
    sample_time_ms: 119170.595
    update_time_ms: 16.55
  iterations_since_restore: 346
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.099024390243905
    ram_util_percent: 12.480487804878045
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 19.0
    agent-2: 63.0
    agent-3: 41.0
    agent-4: 29.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 18.43
    agent-1: 8.45
    agent-2: 47.02
    agent-3: 26.89
    agent-4: 15.79
    agent-5: 19.8
  policy_reward_min:
    agent-0: 8.0
    agent-1: 2.0
    agent-2: 27.0
    agent-3: 13.0
    agent-4: 5.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.223869981820553
    mean_inference_ms: 15.051662073242541
    mean_processing_ms: 73.25611109893654
  time_since_restore: 50258.00068902969
  time_this_iter_s: 144.22092938423157
  time_total_s: 139194.6788187027
  timestamp: 1637413314
  timesteps_since_restore: 33216000
  timesteps_this_iter: 96000
  timesteps_total: 95616000
  training_iteration: 996
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    996 |           139195 | 95616000 |   136.38 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 7.17
    apples_agent-0_min: 1
    apples_agent-1_max: 32
    apples_agent-1_mean: 4.23
    apples_agent-1_min: 0
    apples_agent-2_max: 57
    apples_agent-2_mean: 29.48
    apples_agent-2_min: 17
    apples_agent-3_max: 32
    apples_agent-3_mean: 8.93
    apples_agent-3_min: 1
    apples_agent-4_max: 27
    apples_agent-4_mean: 11.05
    apples_agent-4_min: 2
    apples_agent-5_max: 19
    apples_agent-5_mean: 5.39
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 1.35
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 249
    cleaning_beam_agent-1_mean: 216.42
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 49
    cleaning_beam_agent-2_mean: 16.04
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 28
    cleaning_beam_agent-3_mean: 6.28
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 3.73
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 3.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-04-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 183.0
  episode_reward_mean: 132.84
  episode_reward_min: 63.0
  episodes_this_iter: 96
  episodes_total: 95712
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11867.794
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30358579754829407
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010252028005197644
        model: {}
        policy_loss: -0.0018541552126407623
        total_loss: -0.0022807642817497253
        vf_explained_var: 0.01781006157398224
        vf_loss: 1.077020525932312
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19790995121002197
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013823425397276878
        model: {}
        policy_loss: -0.0016247531166300178
        total_loss: -0.0019156610360369086
        vf_explained_var: 0.07491974532604218
        vf_loss: 0.5741491317749023
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.400478720664978
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014354503946378827
        model: {}
        policy_loss: -0.001743740402162075
        total_loss: -0.0020698022563010454
        vf_explained_var: 0.004035323858261108
        vf_loss: 3.7878079414367676
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37933266162872314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017255735583603382
        model: {}
        policy_loss: -0.0017203395254909992
        total_loss: -0.0021836664527654648
        vf_explained_var: 0.014127761125564575
        vf_loss: 2.0429790019989014
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38016748428344727
        entropy_coeff: 0.0017600000137463212
        kl: 0.001314696273766458
        model: {}
        policy_loss: -0.0015056640841066837
        total_loss: -0.0020665850024670362
        vf_explained_var: 0.013939991593360901
        vf_loss: 1.0817317962646484
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5312411785125732
        entropy_coeff: 0.0017600000137463212
        kl: 0.001179028069600463
        model: {}
        policy_loss: -0.0014868290163576603
        total_loss: -0.002281470922753215
        vf_explained_var: 0.009878352284431458
        vf_loss: 1.4034252166748047
    load_time_ms: 14175.196
    num_steps_sampled: 95712000
    num_steps_trained: 95712000
    sample_time_ms: 119045.379
    update_time_ms: 16.52
  iterations_since_restore: 347
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.954589371980678
    ram_util_percent: 12.475845410628017
  pid: 27065
  policy_reward_max:
    agent-0: 27.0
    agent-1: 22.0
    agent-2: 66.0
    agent-3: 41.0
    agent-4: 27.0
    agent-5: 41.0
  policy_reward_mean:
    agent-0: 16.94
    agent-1: 9.16
    agent-2: 46.32
    agent-3: 26.03
    agent-4: 15.2
    agent-5: 19.19
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 26.0
    agent-3: 10.0
    agent-4: -39.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.22407148452182
    mean_inference_ms: 15.051582842862453
    mean_processing_ms: 73.25547881157183
  time_since_restore: 50403.13312458992
  time_this_iter_s: 145.13243556022644
  time_total_s: 139339.81125426292
  timestamp: 1637413459
  timesteps_since_restore: 33312000
  timesteps_this_iter: 96000
  timesteps_total: 95712000
  training_iteration: 997
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    997 |           139340 | 95712000 |   132.84 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 7.12
    apples_agent-0_min: 1
    apples_agent-1_max: 32
    apples_agent-1_mean: 3.42
    apples_agent-1_min: 0
    apples_agent-2_max: 94
    apples_agent-2_mean: 29.88
    apples_agent-2_min: 9
    apples_agent-3_max: 30
    apples_agent-3_mean: 8.94
    apples_agent-3_min: 3
    apples_agent-4_max: 31
    apples_agent-4_mean: 9.77
    apples_agent-4_min: 2
    apples_agent-5_max: 33
    apples_agent-5_mean: 5.77
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.3
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 254
    cleaning_beam_agent-1_mean: 215.42
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 54
    cleaning_beam_agent-2_mean: 18.11
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 23
    cleaning_beam_agent-3_mean: 6.79
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 3.37
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-06-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 179.0
  episode_reward_mean: 133.29
  episode_reward_min: 49.0
  episodes_this_iter: 96
  episodes_total: 95808
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11871.281
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3010290861129761
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011135025415569544
        model: {}
        policy_loss: -0.0016142763197422028
        total_loss: -0.0020311574917286634
        vf_explained_var: 0.007796630263328552
        vf_loss: 1.1292805671691895
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19648697972297668
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012840013951063156
        model: {}
        policy_loss: -0.0016906987875699997
        total_loss: -0.0019831452518701553
        vf_explained_var: 0.08470804989337921
        vf_loss: 0.5336834788322449
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40398284792900085
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014522230485454202
        model: {}
        policy_loss: -0.0019114327151328325
        total_loss: -0.0022026547230780125
        vf_explained_var: 0.01787427067756653
        vf_loss: 4.19788932800293
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3818911910057068
        entropy_coeff: 0.0017600000137463212
        kl: 0.001301301526837051
        model: {}
        policy_loss: -0.0015338010853156447
        total_loss: -0.0020039810333400965
        vf_explained_var: 0.018672913312911987
        vf_loss: 2.0194716453552246
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3752095699310303
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012341707479208708
        model: {}
        policy_loss: -0.0012970417737960815
        total_loss: -0.0018441295251250267
        vf_explained_var: 0.013880029320716858
        vf_loss: 1.1327650547027588
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5316731929779053
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019325782777741551
        model: {}
        policy_loss: -0.001965901115909219
        total_loss: -0.0027608058881014585
        vf_explained_var: 0.0180140882730484
        vf_loss: 1.4084181785583496
    load_time_ms: 14158.614
    num_steps_sampled: 95808000
    num_steps_trained: 95808000
    sample_time_ms: 119082.502
    update_time_ms: 16.547
  iterations_since_restore: 348
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.003864734299512
    ram_util_percent: 12.466666666666663
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 17.0
    agent-2: 68.0
    agent-3: 40.0
    agent-4: 30.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 17.25
    agent-1: 9.19
    agent-2: 46.35
    agent-3: 25.95
    agent-4: 14.81
    agent-5: 19.74
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 18.0
    agent-3: 9.0
    agent-4: 5.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.22459630103335
    mean_inference_ms: 15.051746150588196
    mean_processing_ms: 73.25612137096915
  time_since_restore: 50548.3175342083
  time_this_iter_s: 145.18440961837769
  time_total_s: 139484.9956638813
  timestamp: 1637413605
  timesteps_since_restore: 33408000
  timesteps_this_iter: 96000
  timesteps_total: 95808000
  training_iteration: 998
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    998 |           139485 | 95808000 |   133.29 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 7.13
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.25
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 30.55
    apples_agent-2_min: 16
    apples_agent-3_max: 33
    apples_agent-3_mean: 8.69
    apples_agent-3_min: 2
    apples_agent-4_max: 18
    apples_agent-4_mean: 10.14
    apples_agent-4_min: 2
    apples_agent-5_max: 21
    apples_agent-5_mean: 5.86
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 1.34
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 247
    cleaning_beam_agent-1_mean: 215.23
    cleaning_beam_agent-1_min: 169
    cleaning_beam_agent-2_max: 36
    cleaning_beam_agent-2_mean: 16.88
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 6.77
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.56
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.66
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-09-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 184.0
  episode_reward_mean: 135.49
  episode_reward_min: 94.0
  episodes_this_iter: 96
  episodes_total: 95904
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11851.318
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3000330328941345
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011925625149160624
        model: {}
        policy_loss: -0.0014184198807924986
        total_loss: -0.0018262136727571487
        vf_explained_var: 0.007477283477783203
        vf_loss: 1.2026249170303345
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1971796303987503
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010260951239615679
        model: {}
        policy_loss: -0.001697541680186987
        total_loss: -0.001984845381230116
        vf_explained_var: 0.07896760106086731
        vf_loss: 0.5973281860351562
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39972811937332153
        entropy_coeff: 0.0017600000137463212
        kl: 0.001732167904265225
        model: {}
        policy_loss: -0.0019248626194894314
        total_loss: -0.0022511803545057774
        vf_explained_var: 0.01663142442703247
        vf_loss: 3.7720160484313965
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3779992163181305
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017661501187831163
        model: {}
        policy_loss: -0.0015994288260117173
        total_loss: -0.0020543134305626154
        vf_explained_var: -0.0034895241260528564
        vf_loss: 2.103945732116699
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37868639826774597
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007684876909479499
        model: {}
        policy_loss: -0.0014347555115818977
        total_loss: -0.0019976142793893814
        vf_explained_var: 0.011331215500831604
        vf_loss: 1.0362387895584106
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5280876755714417
        entropy_coeff: 0.0017600000137463212
        kl: 0.001791793038137257
        model: {}
        policy_loss: -0.001802497310563922
        total_loss: -0.002592963632196188
        vf_explained_var: 0.006780147552490234
        vf_loss: 1.3896548748016357
    load_time_ms: 14151.229
    num_steps_sampled: 95904000
    num_steps_trained: 95904000
    sample_time_ms: 119056.779
    update_time_ms: 16.819
  iterations_since_restore: 349
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.041545893719807
    ram_util_percent: 12.452173913043476
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 18.0
    agent-2: 65.0
    agent-3: 40.0
    agent-4: 28.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 17.92
    agent-1: 8.85
    agent-2: 48.12
    agent-3: 26.08
    agent-4: 15.25
    agent-5: 19.27
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 30.0
    agent-3: 14.0
    agent-4: 4.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.225156346310595
    mean_inference_ms: 15.051863122001345
    mean_processing_ms: 73.25708734704669
  time_since_restore: 50693.53120756149
  time_this_iter_s: 145.2136733531952
  time_total_s: 139630.2093372345
  timestamp: 1637413750
  timesteps_since_restore: 33504000
  timesteps_this_iter: 96000
  timesteps_total: 95904000
  training_iteration: 999
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |    999 |           139630 | 95904000 |   135.49 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 7.63
    apples_agent-0_min: 2
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.09
    apples_agent-1_min: 0
    apples_agent-2_max: 63
    apples_agent-2_mean: 29.99
    apples_agent-2_min: 18
    apples_agent-3_max: 29
    apples_agent-3_mean: 8.22
    apples_agent-3_min: 2
    apples_agent-4_max: 32
    apples_agent-4_mean: 11.34
    apples_agent-4_min: 3
    apples_agent-5_max: 14
    apples_agent-5_mean: 5.2
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 1.62
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 250
    cleaning_beam_agent-1_mean: 217.94
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 36
    cleaning_beam_agent-2_mean: 19.03
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 20
    cleaning_beam_agent-3_mean: 7.08
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.14
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 3.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-11-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 171.0
  episode_reward_mean: 133.97
  episode_reward_min: 70.0
  episodes_this_iter: 96
  episodes_total: 96000
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11844.795
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.301852822303772
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014830470317974687
        model: {}
        policy_loss: -0.0019022701308131218
        total_loss: -0.0023166174069046974
        vf_explained_var: 0.01942257583141327
        vf_loss: 1.1691296100616455
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.2003341168165207
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012716237688437104
        model: {}
        policy_loss: -0.0014986188616603613
        total_loss: -0.0017979731783270836
        vf_explained_var: 0.07041510939598083
        vf_loss: 0.5323144197463989
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3967454433441162
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006852741353213787
        model: {}
        policy_loss: -0.0015044929459691048
        total_loss: -0.0018273922614753246
        vf_explained_var: 0.022900938987731934
        vf_loss: 3.7537617683410645
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3892471492290497
        entropy_coeff: 0.0017600000137463212
        kl: 0.001047600875608623
        model: {}
        policy_loss: -0.0013787578791379929
        total_loss: -0.0018749209120869637
        vf_explained_var: 0.0016039460897445679
        vf_loss: 1.8891013860702515
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37466752529144287
        entropy_coeff: 0.0017600000137463212
        kl: 0.000695994938723743
        model: {}
        policy_loss: -0.0011039922246709466
        total_loss: -0.0016513722948729992
        vf_explained_var: 0.012800261378288269
        vf_loss: 1.120347499847412
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.521284818649292
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020164530724287033
        model: {}
        policy_loss: -0.0016747105401009321
        total_loss: -0.0024487983901053667
        vf_explained_var: 0.015999838709831238
        vf_loss: 1.4337376356124878
    load_time_ms: 14158.398
    num_steps_sampled: 96000000
    num_steps_trained: 96000000
    sample_time_ms: 118939.065
    update_time_ms: 16.559
  iterations_since_restore: 350
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.93658536585366
    ram_util_percent: 12.4590243902439
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 16.0
    agent-2: 61.0
    agent-3: 38.0
    agent-4: 33.0
    agent-5: 36.0
  policy_reward_mean:
    agent-0: 17.73
    agent-1: 8.84
    agent-2: 46.46
    agent-3: 25.63
    agent-4: 15.57
    agent-5: 19.74
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 27.0
    agent-3: 13.0
    agent-4: 4.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.22495471706298
    mean_inference_ms: 15.051455141079343
    mean_processing_ms: 73.25484925615285
  time_since_restore: 50837.90235829353
  time_this_iter_s: 144.3711507320404
  time_total_s: 139774.58048796654
  timestamp: 1637413895
  timesteps_since_restore: 33600000
  timesteps_this_iter: 96000
  timesteps_total: 96000000
  training_iteration: 1000
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1000 |           139775 | 96000000 |   133.97 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 8.17
    apples_agent-0_min: 2
    apples_agent-1_max: 27
    apples_agent-1_mean: 3.78
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 30.1
    apples_agent-2_min: 15
    apples_agent-3_max: 27
    apples_agent-3_mean: 9.16
    apples_agent-3_min: 2
    apples_agent-4_max: 25
    apples_agent-4_mean: 11.12
    apples_agent-4_min: 3
    apples_agent-5_max: 39
    apples_agent-5_mean: 6.14
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 1.56
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 215.97
    cleaning_beam_agent-1_min: 152
    cleaning_beam_agent-2_max: 39
    cleaning_beam_agent-2_mean: 17.56
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 20
    cleaning_beam_agent-3_mean: 7.15
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.24
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.72
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-14-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 193.0
  episode_reward_mean: 136.64
  episode_reward_min: 99.0
  episodes_this_iter: 96
  episodes_total: 96096
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11837.487
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3024478554725647
        entropy_coeff: 0.0017600000137463212
        kl: 0.001226763240993023
        model: {}
        policy_loss: -0.0015862262807786465
        total_loss: -0.0019979793578386307
        vf_explained_var: 0.0160122811794281
        vf_loss: 1.2055459022521973
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.20240022242069244
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007942591328173876
        model: {}
        policy_loss: -0.0013140966184437275
        total_loss: -0.0016106069087982178
        vf_explained_var: 0.07828454673290253
        vf_loss: 0.5971552133560181
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3940078616142273
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012142239138484001
        model: {}
        policy_loss: -0.001731343101710081
        total_loss: -0.0020403084345161915
        vf_explained_var: 0.01717473566532135
        vf_loss: 3.8448572158813477
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3817830979824066
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017586024478077888
        model: {}
        policy_loss: -0.0018196548335254192
        total_loss: -0.0023000561632215977
        vf_explained_var: -0.00016069412231445312
        vf_loss: 1.9153858423233032
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3669918179512024
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011547948233783245
        model: {}
        policy_loss: -0.0013204850256443024
        total_loss: -0.001853044144809246
        vf_explained_var: 0.016019925475120544
        vf_loss: 1.1334452629089355
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5176720023155212
        entropy_coeff: 0.0017600000137463212
        kl: 0.00216107489541173
        model: {}
        policy_loss: -0.001934908563271165
        total_loss: -0.0027081517037004232
        vf_explained_var: 0.006538182497024536
        vf_loss: 1.3785982131958008
    load_time_ms: 14145.812
    num_steps_sampled: 96096000
    num_steps_trained: 96096000
    sample_time_ms: 118937.801
    update_time_ms: 16.471
  iterations_since_restore: 351
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.60334928229665
    ram_util_percent: 12.439234449760763
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 74.0
    agent-3: 43.0
    agent-4: 28.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 18.07
    agent-1: 9.14
    agent-2: 46.84
    agent-3: 27.15
    agent-4: 16.31
    agent-5: 19.13
  policy_reward_min:
    agent-0: 8.0
    agent-1: 3.0
    agent-2: 31.0
    agent-3: 16.0
    agent-4: 8.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.224653400720825
    mean_inference_ms: 15.051107722496022
    mean_processing_ms: 73.25289823313214
  time_since_restore: 50982.450263261795
  time_this_iter_s: 144.54790496826172
  time_total_s: 139919.1283929348
  timestamp: 1637414042
  timesteps_since_restore: 33696000
  timesteps_this_iter: 96000
  timesteps_total: 96096000
  training_iteration: 1001
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1001 |           139919 | 96096000 |   136.64 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 6.95
    apples_agent-0_min: 1
    apples_agent-1_max: 12
    apples_agent-1_mean: 3.55
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 30.33
    apples_agent-2_min: 14
    apples_agent-3_max: 29
    apples_agent-3_mean: 9.43
    apples_agent-3_min: 3
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.91
    apples_agent-4_min: 4
    apples_agent-5_max: 32
    apples_agent-5_mean: 5.97
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 15
    cleaning_beam_agent-0_mean: 1.82
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 239
    cleaning_beam_agent-1_mean: 210.62
    cleaning_beam_agent-1_min: 157
    cleaning_beam_agent-2_max: 44
    cleaning_beam_agent-2_mean: 17.62
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 21
    cleaning_beam_agent-3_mean: 6.97
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.42
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.39
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-16-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 170.0
  episode_reward_mean: 133.68
  episode_reward_min: 89.0
  episodes_this_iter: 96
  episodes_total: 96192
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11826.995
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3098531663417816
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015029265778139234
        model: {}
        policy_loss: -0.0018184806685894728
        total_loss: -0.0022506695240736008
        vf_explained_var: 0.006001755595207214
        vf_loss: 1.1315464973449707
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19841623306274414
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011134428204968572
        model: {}
        policy_loss: -0.0014985122252255678
        total_loss: -0.0017917340155690908
        vf_explained_var: 0.09562882781028748
        vf_loss: 0.5599325895309448
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.396644651889801
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013332937378436327
        model: {}
        policy_loss: -0.001761781983077526
        total_loss: -0.0020705696661025286
        vf_explained_var: 0.02034999430179596
        vf_loss: 3.8930654525756836
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3832054138183594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015197678003460169
        model: {}
        policy_loss: -0.0016929656267166138
        total_loss: -0.0021808501332998276
        vf_explained_var: 0.018474340438842773
        vf_loss: 1.8655686378479004
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36660969257354736
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006286377902142704
        model: {}
        policy_loss: -0.001222888007760048
        total_loss: -0.0017479927046224475
        vf_explained_var: 0.013006314635276794
        vf_loss: 1.2012712955474854
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5171300768852234
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010638937819749117
        model: {}
        policy_loss: -0.0014994327211752534
        total_loss: -0.002273712307214737
        vf_explained_var: 0.012600362300872803
        vf_loss: 1.3586779832839966
    load_time_ms: 14119.296
    num_steps_sampled: 96192000
    num_steps_trained: 96192000
    sample_time_ms: 118974.183
    update_time_ms: 16.435
  iterations_since_restore: 352
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.78846153846154
    ram_util_percent: 12.465384615384615
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 19.0
    agent-2: 65.0
    agent-3: 39.0
    agent-4: 25.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 16.77
    agent-1: 9.16
    agent-2: 46.64
    agent-3: 26.39
    agent-4: 15.89
    agent-5: 18.83
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 26.0
    agent-3: 14.0
    agent-4: 6.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.22471232071011
    mean_inference_ms: 15.051149458971823
    mean_processing_ms: 73.25208690365992
  time_since_restore: 51128.21803545952
  time_this_iter_s: 145.7677721977234
  time_total_s: 140064.89616513252
  timestamp: 1637414188
  timesteps_since_restore: 33792000
  timesteps_this_iter: 96000
  timesteps_total: 96192000
  training_iteration: 1002
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1002 |           140065 | 96192000 |   133.68 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 8.01
    apples_agent-0_min: 2
    apples_agent-1_max: 31
    apples_agent-1_mean: 3.86
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 30.02
    apples_agent-2_min: 17
    apples_agent-3_max: 36
    apples_agent-3_mean: 9.78
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.54
    apples_agent-4_min: 2
    apples_agent-5_max: 24
    apples_agent-5_mean: 5.79
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 20
    cleaning_beam_agent-0_mean: 1.55
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 238
    cleaning_beam_agent-1_mean: 209.56
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 35
    cleaning_beam_agent-2_mean: 14.99
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 7.0
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 9
    cleaning_beam_agent-4_mean: 3.03
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.39
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-18-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 186.0
  episode_reward_mean: 135.67
  episode_reward_min: 69.0
  episodes_this_iter: 96
  episodes_total: 96288
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11825.842
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3106776773929596
        entropy_coeff: 0.0017600000137463212
        kl: 0.00088596431305632
        model: {}
        policy_loss: -0.0019423698540776968
        total_loss: -0.0023611311335116625
        vf_explained_var: 0.0029661208391189575
        vf_loss: 1.2803051471710205
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19266459345817566
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008220963645726442
        model: {}
        policy_loss: -0.0012641632929444313
        total_loss: -0.0015456229448318481
        vf_explained_var: 0.10020580887794495
        vf_loss: 0.5762759447097778
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3911087214946747
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007859867764636874
        model: {}
        policy_loss: -0.001656899694353342
        total_loss: -0.0019504544325172901
        vf_explained_var: 0.0068131983280181885
        vf_loss: 3.9479806423187256
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38058924674987793
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017067000735551119
        model: {}
        policy_loss: -0.001763148233294487
        total_loss: -0.002226971322670579
        vf_explained_var: 0.0071751028299331665
        vf_loss: 2.06010365486145
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36396926641464233
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007974956533871591
        model: {}
        policy_loss: -0.0014797758776694536
        total_loss: -0.0020123629365116358
        vf_explained_var: 0.014444008469581604
        vf_loss: 1.0800114870071411
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5186976194381714
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015955627895891666
        model: {}
        policy_loss: -0.0016990690492093563
        total_loss: -0.002469941508024931
        vf_explained_var: 0.019204795360565186
        vf_loss: 1.420332431793213
    load_time_ms: 14106.651
    num_steps_sampled: 96288000
    num_steps_trained: 96288000
    sample_time_ms: 118870.124
    update_time_ms: 16.084
  iterations_since_restore: 353
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.77718446601942
    ram_util_percent: 12.46359223300971
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 18.0
    agent-2: 67.0
    agent-3: 46.0
    agent-4: 26.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 17.81
    agent-1: 9.51
    agent-2: 46.61
    agent-3: 26.38
    agent-4: 15.52
    agent-5: 19.84
  policy_reward_min:
    agent-0: 8.0
    agent-1: 2.0
    agent-2: 26.0
    agent-3: 9.0
    agent-4: 6.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.224410518785838
    mean_inference_ms: 15.050847838789759
    mean_processing_ms: 73.25022047575399
  time_since_restore: 51272.94335961342
  time_this_iter_s: 144.72532415390015
  time_total_s: 140209.62148928642
  timestamp: 1637414332
  timesteps_since_restore: 33888000
  timesteps_this_iter: 96000
  timesteps_total: 96288000
  training_iteration: 1003
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1003 |           140210 | 96288000 |   135.67 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 7.8
    apples_agent-0_min: 0
    apples_agent-1_max: 51
    apples_agent-1_mean: 3.83
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 30.85
    apples_agent-2_min: 17
    apples_agent-3_max: 40
    apples_agent-3_mean: 10.42
    apples_agent-3_min: 2
    apples_agent-4_max: 18
    apples_agent-4_mean: 10.31
    apples_agent-4_min: 3
    apples_agent-5_max: 16
    apples_agent-5_mean: 5.39
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 22
    cleaning_beam_agent-0_mean: 1.53
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 240
    cleaning_beam_agent-1_mean: 215.18
    cleaning_beam_agent-1_min: 192
    cleaning_beam_agent-2_max: 29
    cleaning_beam_agent-2_mean: 16.21
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 20
    cleaning_beam_agent-3_mean: 6.8
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 8
    cleaning_beam_agent-4_mean: 3.49
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.96
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-21-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 177.0
  episode_reward_mean: 136.42
  episode_reward_min: 24.0
  episodes_this_iter: 96
  episodes_total: 96384
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11849.199
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30781108140945435
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011117069516330957
        model: {}
        policy_loss: -0.0018623461946845055
        total_loss: -0.00229330500587821
        vf_explained_var: 0.022947043180465698
        vf_loss: 1.107865810394287
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19539517164230347
        entropy_coeff: 0.0017600000137463212
        kl: 0.001351522165350616
        model: {}
        policy_loss: -0.001561167649924755
        total_loss: -0.0018532522954046726
        vf_explained_var: 0.07242102921009064
        vf_loss: 0.5180772542953491
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3891959488391876
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015264579560607672
        model: {}
        policy_loss: -0.0017929032910615206
        total_loss: -0.002095260890200734
        vf_explained_var: 0.01812557876110077
        vf_loss: 3.826265335083008
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3837563693523407
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012948141666129231
        model: {}
        policy_loss: -0.0013023867504671216
        total_loss: -0.0017717514419928193
        vf_explained_var: 0.01060950756072998
        vf_loss: 2.0604517459869385
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35987618565559387
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008895802893675864
        model: {}
        policy_loss: -0.0015573145356029272
        total_loss: -0.002088173758238554
        vf_explained_var: 0.021885201334953308
        vf_loss: 1.0252506732940674
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5325919389724731
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010189611930400133
        model: {}
        policy_loss: -0.0016307253390550613
        total_loss: -0.0024315062910318375
        vf_explained_var: 0.009585469961166382
        vf_loss: 1.3658368587493896
    load_time_ms: 14130.873
    num_steps_sampled: 96384000
    num_steps_trained: 96384000
    sample_time_ms: 118840.877
    update_time_ms: 16.106
  iterations_since_restore: 354
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.82173913043478
    ram_util_percent: 12.476811594202896
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 20.0
    agent-2: 68.0
    agent-3: 45.0
    agent-4: 26.0
    agent-5: 37.0
  policy_reward_mean:
    agent-0: 18.13
    agent-1: 8.79
    agent-2: 48.74
    agent-3: 26.76
    agent-4: 14.83
    agent-5: 19.17
  policy_reward_min:
    agent-0: 8.0
    agent-1: 2.0
    agent-2: 33.0
    agent-3: -28.0
    agent-4: -29.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.224293208398922
    mean_inference_ms: 15.050470180505428
    mean_processing_ms: 73.24865770312
  time_since_restore: 51417.766015052795
  time_this_iter_s: 144.82265543937683
  time_total_s: 140354.4441447258
  timestamp: 1637414477
  timesteps_since_restore: 33984000
  timesteps_this_iter: 96000
  timesteps_total: 96384000
  training_iteration: 1004
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1004 |           140354 | 96384000 |   136.42 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 7.43
    apples_agent-0_min: 1
    apples_agent-1_max: 37
    apples_agent-1_mean: 3.92
    apples_agent-1_min: 0
    apples_agent-2_max: 62
    apples_agent-2_mean: 30.49
    apples_agent-2_min: 11
    apples_agent-3_max: 23
    apples_agent-3_mean: 8.32
    apples_agent-3_min: 2
    apples_agent-4_max: 25
    apples_agent-4_mean: 9.62
    apples_agent-4_min: 2
    apples_agent-5_max: 28
    apples_agent-5_mean: 5.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 26
    cleaning_beam_agent-0_mean: 1.39
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 237
    cleaning_beam_agent-1_mean: 207.39
    cleaning_beam_agent-1_min: 168
    cleaning_beam_agent-2_max: 34
    cleaning_beam_agent-2_mean: 15.77
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 27
    cleaning_beam_agent-3_mean: 7.17
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 3.06
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.57
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-23-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 166.0
  episode_reward_mean: 134.11
  episode_reward_min: 94.0
  episodes_this_iter: 96
  episodes_total: 96480
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11860.597
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3127436637878418
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013533998280763626
        model: {}
        policy_loss: -0.0019178134389221668
        total_loss: -0.002350353170186281
        vf_explained_var: 0.008080795407295227
        vf_loss: 1.1788971424102783
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19266754388809204
        entropy_coeff: 0.0017600000137463212
        kl: 0.001193358446471393
        model: {}
        policy_loss: -0.0015230108983814716
        total_loss: -0.0018123339395970106
        vf_explained_var: 0.08869613707065582
        vf_loss: 0.49774467945098877
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39757785201072693
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016197082586586475
        model: {}
        policy_loss: -0.0020377240143716335
        total_loss: -0.002362934872508049
        vf_explained_var: 0.01858082413673401
        vf_loss: 3.745252847671509
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38681668043136597
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018564172787591815
        model: {}
        policy_loss: -0.0016711163334548473
        total_loss: -0.002160770818591118
        vf_explained_var: -0.0025954842567443848
        vf_loss: 1.911443829536438
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3613448143005371
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006025599432177842
        model: {}
        policy_loss: -0.001428512274287641
        total_loss: -0.001956491032615304
        vf_explained_var: 0.017522171139717102
        vf_loss: 1.0798873901367188
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5309909582138062
        entropy_coeff: 0.0017600000137463212
        kl: 0.001464939210563898
        model: {}
        policy_loss: -0.0015587280504405499
        total_loss: -0.0023490837775170803
        vf_explained_var: 0.004835769534111023
        vf_loss: 1.4418553113937378
    load_time_ms: 14136.994
    num_steps_sampled: 96480000
    num_steps_trained: 96480000
    sample_time_ms: 118789.849
    update_time_ms: 16.29
  iterations_since_restore: 355
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.891787439613527
    ram_util_percent: 12.455072463768115
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 16.0
    agent-2: 60.0
    agent-3: 38.0
    agent-4: 24.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 18.02
    agent-1: 8.49
    agent-2: 46.45
    agent-3: 26.62
    agent-4: 15.21
    agent-5: 19.32
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 33.0
    agent-3: 14.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.224107438819498
    mean_inference_ms: 15.050215267155268
    mean_processing_ms: 73.24790642897962
  time_since_restore: 51562.70214009285
  time_this_iter_s: 144.93612504005432
  time_total_s: 140499.38026976585
  timestamp: 1637414622
  timesteps_since_restore: 34080000
  timesteps_this_iter: 96000
  timesteps_total: 96480000
  training_iteration: 1005
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1005 |           140499 | 96480000 |   134.11 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 7.21
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 2.63
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 28.3
    apples_agent-2_min: 12
    apples_agent-3_max: 19
    apples_agent-3_mean: 7.87
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.05
    apples_agent-4_min: 0
    apples_agent-5_max: 19
    apples_agent-5_mean: 4.89
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.32
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 239
    cleaning_beam_agent-1_mean: 204.45
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 44
    cleaning_beam_agent-2_mean: 16.48
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 7.27
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 17
    cleaning_beam_agent-4_mean: 3.29
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.7
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-26-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 173.0
  episode_reward_mean: 128.25
  episode_reward_min: 61.0
  episodes_this_iter: 96
  episodes_total: 96576
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11884.509
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3208596408367157
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014355214079841971
        model: {}
        policy_loss: -0.002039147773757577
        total_loss: -0.002490134909749031
        vf_explained_var: -0.0037592649459838867
        vf_loss: 1.1372711658477783
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1937730759382248
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010175505885854363
        model: {}
        policy_loss: -0.0014201067388057709
        total_loss: -0.001714129000902176
        vf_explained_var: 0.09378054738044739
        vf_loss: 0.4701630771160126
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4032990336418152
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010803600307554007
        model: {}
        policy_loss: -0.0017603813903406262
        total_loss: -0.0020930604077875614
        vf_explained_var: 0.0010612457990646362
        vf_loss: 3.771270751953125
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39202845096588135
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014400073559954762
        model: {}
        policy_loss: -0.0014287088997662067
        total_loss: -0.0019301646389067173
        vf_explained_var: 0.015366524457931519
        vf_loss: 1.8851556777954102
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3568526804447174
        entropy_coeff: 0.0017600000137463212
        kl: 0.000907410925719887
        model: {}
        policy_loss: -0.0013873036950826645
        total_loss: -0.0019101365469396114
        vf_explained_var: 0.015941813588142395
        vf_loss: 1.0522851943969727
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5268879532814026
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026084501296281815
        model: {}
        policy_loss: -0.0020833066664636135
        total_loss: -0.002878866158425808
        vf_explained_var: 0.023003578186035156
        vf_loss: 1.3176230192184448
    load_time_ms: 14132.927
    num_steps_sampled: 96576000
    num_steps_trained: 96576000
    sample_time_ms: 118875.777
    update_time_ms: 16.062
  iterations_since_restore: 356
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.902898550724636
    ram_util_percent: 12.473913043478259
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 16.0
    agent-2: 65.0
    agent-3: 38.0
    agent-4: 26.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 16.4
    agent-1: 8.24
    agent-2: 45.23
    agent-3: 25.46
    agent-4: 14.9
    agent-5: 18.02
  policy_reward_min:
    agent-0: 8.0
    agent-1: 2.0
    agent-2: 18.0
    agent-3: 13.0
    agent-4: 4.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.22409315459695
    mean_inference_ms: 15.050209538260601
    mean_processing_ms: 73.24793687972763
  time_since_restore: 51707.98134303093
  time_this_iter_s: 145.27920293807983
  time_total_s: 140644.65947270393
  timestamp: 1637414768
  timesteps_since_restore: 34176000
  timesteps_this_iter: 96000
  timesteps_total: 96576000
  training_iteration: 1006
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1006 |           140645 | 96576000 |   128.25 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 7.26
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.31
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 28.63
    apples_agent-2_min: 11
    apples_agent-3_max: 31
    apples_agent-3_mean: 8.38
    apples_agent-3_min: 1
    apples_agent-4_max: 29
    apples_agent-4_mean: 10.18
    apples_agent-4_min: 1
    apples_agent-5_max: 31
    apples_agent-5_mean: 5.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 8
    cleaning_beam_agent-0_mean: 1.18
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 241
    cleaning_beam_agent-1_mean: 207.53
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 37
    cleaning_beam_agent-2_mean: 15.87
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 28
    cleaning_beam_agent-3_mean: 7.48
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 3.59
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 4.23
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-28-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 183.0
  episode_reward_mean: 132.42
  episode_reward_min: 60.0
  episodes_this_iter: 96
  episodes_total: 96672
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11861.274
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.33468520641326904
        entropy_coeff: 0.0017600000137463212
        kl: 0.001473610638640821
        model: {}
        policy_loss: -0.0018967237556353211
        total_loss: -0.0023623346351087093
        vf_explained_var: 0.015331923961639404
        vf_loss: 1.2343355417251587
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19032372534275055
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008682467741891742
        model: {}
        policy_loss: -0.0014798685442656279
        total_loss: -0.0017599391285330057
        vf_explained_var: 0.10271935164928436
        vf_loss: 0.5489907264709473
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4023456573486328
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014359813649207354
        model: {}
        policy_loss: -0.0018190257251262665
        total_loss: -0.0021389639005064964
        vf_explained_var: 0.01662038266658783
        vf_loss: 3.8818933963775635
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3874167501926422
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013491252902895212
        model: {}
        policy_loss: -0.001360100694000721
        total_loss: -0.0018547959625720978
        vf_explained_var: 0.023687928915023804
        vf_loss: 1.8715497255325317
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36697423458099365
        entropy_coeff: 0.0017600000137463212
        kl: 0.00043565899250097573
        model: {}
        policy_loss: -0.001086362637579441
        total_loss: -0.001621435396373272
        vf_explained_var: 0.013953536748886108
        vf_loss: 1.1080223321914673
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5305197834968567
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010759069118648767
        model: {}
        policy_loss: -0.0017495411448180676
        total_loss: -0.0025529819540679455
        vf_explained_var: 0.011740803718566895
        vf_loss: 1.302721381187439
    load_time_ms: 14108.091
    num_steps_sampled: 96672000
    num_steps_trained: 96672000
    sample_time_ms: 118850.699
    update_time_ms: 15.949
  iterations_since_restore: 357
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.035121951219516
    ram_util_percent: 12.466341463414631
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 20.0
    agent-2: 63.0
    agent-3: 43.0
    agent-4: 26.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 17.03
    agent-1: 9.1
    agent-2: 45.72
    agent-3: 26.47
    agent-4: 15.52
    agent-5: 18.58
  policy_reward_min:
    agent-0: 5.0
    agent-1: 4.0
    agent-2: 20.0
    agent-3: 14.0
    agent-4: 5.0
    agent-5: 10.0
  sampler_perf:
    mean_env_wait_ms: 28.223785863004963
    mean_inference_ms: 15.050019469425983
    mean_processing_ms: 73.24778981563493
  time_since_restore: 51852.3812353611
  time_this_iter_s: 144.39989233016968
  time_total_s: 140789.0593650341
  timestamp: 1637414912
  timesteps_since_restore: 34272000
  timesteps_this_iter: 96000
  timesteps_total: 96672000
  training_iteration: 1007
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1007 |           140789 | 96672000 |   132.42 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 7.34
    apples_agent-0_min: 3
    apples_agent-1_max: 44
    apples_agent-1_mean: 3.86
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 30.08
    apples_agent-2_min: 15
    apples_agent-3_max: 22
    apples_agent-3_mean: 8.78
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.58
    apples_agent-4_min: 3
    apples_agent-5_max: 31
    apples_agent-5_mean: 5.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.16
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 236
    cleaning_beam_agent-1_mean: 207.86
    cleaning_beam_agent-1_min: 179
    cleaning_beam_agent-2_max: 55
    cleaning_beam_agent-2_mean: 17.34
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 23
    cleaning_beam_agent-3_mean: 7.71
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.41
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 15
    cleaning_beam_agent-5_mean: 4.34
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-30-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 175.0
  episode_reward_mean: 133.64
  episode_reward_min: 98.0
  episodes_this_iter: 96
  episodes_total: 96768
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11870.053
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3328936696052551
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009057042771019042
        model: {}
        policy_loss: -0.0019058845937252045
        total_loss: -0.002376188524067402
        vf_explained_var: 0.00418873131275177
        vf_loss: 1.1558799743652344
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1927119493484497
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010714626405388117
        model: {}
        policy_loss: -0.0016450483817607164
        total_loss: -0.001931703183799982
        vf_explained_var: 0.07913768291473389
        vf_loss: 0.5251865983009338
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4043230414390564
        entropy_coeff: 0.0017600000137463212
        kl: 0.001354406587779522
        model: {}
        policy_loss: -0.001723291352391243
        total_loss: -0.0020498980302363634
        vf_explained_var: 0.030946463346481323
        vf_loss: 3.8500523567199707
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38984087109565735
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011741140624508262
        model: {}
        policy_loss: -0.0014996100217103958
        total_loss: -0.001994045451283455
        vf_explained_var: 0.006195515394210815
        vf_loss: 1.916851282119751
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36370187997817993
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009835464879870415
        model: {}
        policy_loss: -0.0013095482718199492
        total_loss: -0.0018463705200701952
        vf_explained_var: 0.014305651187896729
        vf_loss: 1.0329349040985107
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5324909687042236
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016058863839134574
        model: {}
        policy_loss: -0.00172793073579669
        total_loss: -0.002528749406337738
        vf_explained_var: 0.009327813982963562
        vf_loss: 1.363650918006897
    load_time_ms: 14119.112
    num_steps_sampled: 96768000
    num_steps_trained: 96768000
    sample_time_ms: 118898.793
    update_time_ms: 16.094
  iterations_since_restore: 358
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.96298076923077
    ram_util_percent: 12.47403846153846
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 19.0
    agent-2: 63.0
    agent-3: 42.0
    agent-4: 32.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 17.83
    agent-1: 8.73
    agent-2: 46.27
    agent-3: 26.46
    agent-4: 15.08
    agent-5: 19.27
  policy_reward_min:
    agent-0: 9.0
    agent-1: 2.0
    agent-2: 29.0
    agent-3: 15.0
    agent-4: 8.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.223761599716013
    mean_inference_ms: 15.049763322569211
    mean_processing_ms: 73.24744044036511
  time_since_restore: 51998.251281023026
  time_this_iter_s: 145.87004566192627
  time_total_s: 140934.92941069603
  timestamp: 1637415058
  timesteps_since_restore: 34368000
  timesteps_this_iter: 96000
  timesteps_total: 96768000
  training_iteration: 1008
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1008 |           140935 | 96768000 |   133.64 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 6.8
    apples_agent-0_min: 0
    apples_agent-1_max: 16
    apples_agent-1_mean: 3.23
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 29.53
    apples_agent-2_min: 7
    apples_agent-3_max: 23
    apples_agent-3_mean: 8.16
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.11
    apples_agent-4_min: 2
    apples_agent-5_max: 26
    apples_agent-5_mean: 5.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.04
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 244
    cleaning_beam_agent-1_mean: 204.9
    cleaning_beam_agent-1_min: 43
    cleaning_beam_agent-2_max: 43
    cleaning_beam_agent-2_mean: 18.23
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 7.35
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 24
    cleaning_beam_agent-4_mean: 4.06
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.94
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-33-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 187.0
  episode_reward_mean: 131.59
  episode_reward_min: 27.0
  episodes_this_iter: 96
  episodes_total: 96864
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11879.267
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3316449522972107
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010443811770528555
        model: {}
        policy_loss: -0.0018076901324093342
        total_loss: -0.0022703739814460278
        vf_explained_var: -0.001999408006668091
        vf_loss: 1.210123062133789
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19028937816619873
        entropy_coeff: 0.0017600000137463212
        kl: 0.001161097316071391
        model: {}
        policy_loss: -0.001487697591073811
        total_loss: -0.0017670124070718884
        vf_explained_var: 0.07164342701435089
        vf_loss: 0.5559419989585876
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41156303882598877
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011427064891904593
        model: {}
        policy_loss: -0.0015682731755077839
        total_loss: -0.0019235950894653797
        vf_explained_var: 0.029538482427597046
        vf_loss: 3.690281391143799
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3892013132572174
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015887339832261205
        model: {}
        policy_loss: -0.0014137709513306618
        total_loss: -0.0019107991829514503
        vf_explained_var: -0.002093076705932617
        vf_loss: 1.8796414136886597
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36484938859939575
        entropy_coeff: 0.0017600000137463212
        kl: 0.000711992965079844
        model: {}
        policy_loss: -0.0013417033478617668
        total_loss: -0.0018803812563419342
        vf_explained_var: 0.01387140154838562
        vf_loss: 1.0345849990844727
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.533141553401947
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012906542979180813
        model: {}
        policy_loss: -0.0014458941295742989
        total_loss: -0.002250166144222021
        vf_explained_var: 0.018454328179359436
        vf_loss: 1.3405545949935913
    load_time_ms: 14118.5
    num_steps_sampled: 96864000
    num_steps_trained: 96864000
    sample_time_ms: 118954.151
    update_time_ms: 15.646
  iterations_since_restore: 359
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.91394230769231
    ram_util_percent: 12.465865384615382
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 19.0
    agent-2: 66.0
    agent-3: 44.0
    agent-4: 26.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 17.52
    agent-1: 8.37
    agent-2: 46.49
    agent-3: 25.74
    agent-4: 14.78
    agent-5: 18.69
  policy_reward_min:
    agent-0: 2.0
    agent-1: 1.0
    agent-2: 10.0
    agent-3: 4.0
    agent-4: 5.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.22389720572608
    mean_inference_ms: 15.049887119827526
    mean_processing_ms: 73.2481246703686
  time_since_restore: 52144.16858959198
  time_this_iter_s: 145.91730856895447
  time_total_s: 141080.84671926498
  timestamp: 1637415204
  timesteps_since_restore: 34464000
  timesteps_this_iter: 96000
  timesteps_total: 96864000
  training_iteration: 1009
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1009 |           141081 | 96864000 |   131.59 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 8.26
    apples_agent-0_min: 2
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.31
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 30.96
    apples_agent-2_min: 17
    apples_agent-3_max: 28
    apples_agent-3_mean: 8.99
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.14
    apples_agent-4_min: 3
    apples_agent-5_max: 18
    apples_agent-5_mean: 5.69
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.05
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 240
    cleaning_beam_agent-1_mean: 206.52
    cleaning_beam_agent-1_min: 155
    cleaning_beam_agent-2_max: 54
    cleaning_beam_agent-2_mean: 18.47
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 9.11
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 14
    cleaning_beam_agent-4_mean: 4.31
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.97
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-35-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 176.0
  episode_reward_mean: 133.95
  episode_reward_min: 87.0
  episodes_this_iter: 96
  episodes_total: 96960
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11868.059
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3215824067592621
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011803717352449894
        model: {}
        policy_loss: -0.0018624326912686229
        total_loss: -0.0023143677972257137
        vf_explained_var: 0.0012817084789276123
        vf_loss: 1.1404800415039062
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19251298904418945
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009326941217295825
        model: {}
        policy_loss: -0.0014115645317360759
        total_loss: -0.0016985577531158924
        vf_explained_var: 0.08423386514186859
        vf_loss: 0.5183159708976746
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41018354892730713
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013902503997087479
        model: {}
        policy_loss: -0.001560916076414287
        total_loss: -0.0018700914224609733
        vf_explained_var: 0.024465754628181458
        vf_loss: 4.1274495124816895
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4017648994922638
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015356362564489245
        model: {}
        policy_loss: -0.00148323317989707
        total_loss: -0.0019980832003057003
        vf_explained_var: 0.00554889440536499
        vf_loss: 1.922565221786499
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36870142817497253
        entropy_coeff: 0.0017600000137463212
        kl: 0.001013732049614191
        model: {}
        policy_loss: -0.0013831127434968948
        total_loss: -0.0019302521832287312
        vf_explained_var: 0.019188448786735535
        vf_loss: 1.0177544355392456
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5344536900520325
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016482628416270018
        model: {}
        policy_loss: -0.0018705474212765694
        total_loss: -0.0026847003027796745
        vf_explained_var: 0.02467799186706543
        vf_loss: 1.2648283243179321
    load_time_ms: 14101.658
    num_steps_sampled: 96960000
    num_steps_trained: 96960000
    sample_time_ms: 119052.306
    update_time_ms: 15.654
  iterations_since_restore: 360
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.986407766990293
    ram_util_percent: 12.413106796116505
  pid: 27065
  policy_reward_max:
    agent-0: 33.0
    agent-1: 19.0
    agent-2: 67.0
    agent-3: 44.0
    agent-4: 26.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 17.71
    agent-1: 9.04
    agent-2: 47.46
    agent-3: 26.36
    agent-4: 15.0
    agent-5: 18.38
  policy_reward_min:
    agent-0: 9.0
    agent-1: 3.0
    agent-2: 30.0
    agent-3: 10.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.22400873302699
    mean_inference_ms: 15.049923652637409
    mean_processing_ms: 73.24815223318886
  time_since_restore: 52289.193568468094
  time_this_iter_s: 145.0249788761139
  time_total_s: 141225.8716981411
  timestamp: 1637415350
  timesteps_since_restore: 34560000
  timesteps_this_iter: 96000
  timesteps_total: 96960000
  training_iteration: 1010
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1010 |           141226 | 96960000 |   133.95 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 7.55
    apples_agent-0_min: 1
    apples_agent-1_max: 20
    apples_agent-1_mean: 3.8
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 29.63
    apples_agent-2_min: 16
    apples_agent-3_max: 23
    apples_agent-3_mean: 8.85
    apples_agent-3_min: 2
    apples_agent-4_max: 40
    apples_agent-4_mean: 10.34
    apples_agent-4_min: 4
    apples_agent-5_max: 38
    apples_agent-5_mean: 6.16
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 1.18
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 241
    cleaning_beam_agent-1_mean: 207.61
    cleaning_beam_agent-1_min: 171
    cleaning_beam_agent-2_max: 44
    cleaning_beam_agent-2_mean: 18.99
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 8.8
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.41
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.3
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-38-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 181.0
  episode_reward_mean: 134.73
  episode_reward_min: 93.0
  episodes_this_iter: 96
  episodes_total: 97056
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11888.518
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3283822238445282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011625137412920594
        model: {}
        policy_loss: -0.0017774553271010518
        total_loss: -0.0022329974453896284
        vf_explained_var: 0.01706203818321228
        vf_loss: 1.2240827083587646
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19402511417865753
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011353016598150134
        model: {}
        policy_loss: -0.0013600115198642015
        total_loss: -0.0016493326984345913
        vf_explained_var: 0.09938788414001465
        vf_loss: 0.521629810333252
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40356796979904175
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011398791102692485
        model: {}
        policy_loss: -0.0015246719121932983
        total_loss: -0.001865476369857788
        vf_explained_var: 0.02277320623397827
        vf_loss: 3.6947407722473145
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4018215537071228
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013980463845655322
        model: {}
        policy_loss: -0.0016856265719980001
        total_loss: -0.0021811227779835463
        vf_explained_var: 0.003191426396369934
        vf_loss: 2.11711049079895
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36430037021636963
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006805931334383786
        model: {}
        policy_loss: -0.0013006057124584913
        total_loss: -0.0018347383011132479
        vf_explained_var: 0.02333526313304901
        vf_loss: 1.0703569650650024
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5304229855537415
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012726783752441406
        model: {}
        policy_loss: -0.0016854009591042995
        total_loss: -0.0024882815778255463
        vf_explained_var: 0.014998868107795715
        vf_loss: 1.3066332340240479
    load_time_ms: 14113.406
    num_steps_sampled: 97056000
    num_steps_trained: 97056000
    sample_time_ms: 119045.367
    update_time_ms: 15.653
  iterations_since_restore: 361
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.50761904761905
    ram_util_percent: 12.371428571428572
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 16.0
    agent-2: 70.0
    agent-3: 46.0
    agent-4: 33.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 18.34
    agent-1: 9.22
    agent-2: 46.56
    agent-3: 26.85
    agent-4: 15.26
    agent-5: 18.5
  policy_reward_min:
    agent-0: 4.0
    agent-1: 3.0
    agent-2: 26.0
    agent-3: 14.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.223338796317027
    mean_inference_ms: 15.04938175998424
    mean_processing_ms: 73.24579183467547
  time_since_restore: 52433.99428844452
  time_this_iter_s: 144.80071997642517
  time_total_s: 141370.67241811752
  timestamp: 1637415497
  timesteps_since_restore: 34656000
  timesteps_this_iter: 96000
  timesteps_total: 97056000
  training_iteration: 1011
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1011 |           141371 | 97056000 |   134.73 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 7.56
    apples_agent-0_min: 2
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.36
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 29.96
    apples_agent-2_min: 18
    apples_agent-3_max: 30
    apples_agent-3_mean: 8.42
    apples_agent-3_min: 1
    apples_agent-4_max: 25
    apples_agent-4_mean: 10.01
    apples_agent-4_min: 3
    apples_agent-5_max: 23
    apples_agent-5_mean: 5.76
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.13
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 247
    cleaning_beam_agent-1_mean: 212.08
    cleaning_beam_agent-1_min: 175
    cleaning_beam_agent-2_max: 47
    cleaning_beam_agent-2_mean: 17.19
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 27
    cleaning_beam_agent-3_mean: 7.56
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 17
    cleaning_beam_agent-4_mean: 3.55
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.2
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-40-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 173.0
  episode_reward_mean: 134.31
  episode_reward_min: 79.0
  episodes_this_iter: 96
  episodes_total: 97152
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11904.248
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3327330946922302
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010770752560347319
        model: {}
        policy_loss: -0.0013278634287416935
        total_loss: -0.001647383556701243
        vf_explained_var: -0.002349376678466797
        vf_loss: 2.660869836807251
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1948925256729126
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005960207781754434
        model: {}
        policy_loss: -0.0012096127029508352
        total_loss: -0.0015019923448562622
        vf_explained_var: 0.08634766936302185
        vf_loss: 0.5063105821609497
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40901100635528564
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015303882537409663
        model: {}
        policy_loss: -0.0019743568263947964
        total_loss: -0.0023385933600366116
        vf_explained_var: 0.011239156126976013
        vf_loss: 3.556222677230835
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3940446078777313
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015894268872216344
        model: {}
        policy_loss: -0.001362415961921215
        total_loss: -0.0018577254377305508
        vf_explained_var: 0.01689445972442627
        vf_loss: 1.9820880889892578
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3683047890663147
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008501750999130309
        model: {}
        policy_loss: -0.0013663587160408497
        total_loss: -0.0019107195548713207
        vf_explained_var: 0.01100185513496399
        vf_loss: 1.0385664701461792
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5365136861801147
        entropy_coeff: 0.0017600000137463212
        kl: 0.001843305304646492
        model: {}
        policy_loss: -0.0016701966524124146
        total_loss: -0.0024817585945129395
        vf_explained_var: 0.01779836416244507
        vf_loss: 1.3270148038864136
    load_time_ms: 14142.978
    num_steps_sampled: 97152000
    num_steps_trained: 97152000
    sample_time_ms: 118911.931
    update_time_ms: 15.585
  iterations_since_restore: 362
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.89033816425121
    ram_util_percent: 12.454106280193232
  pid: 27065
  policy_reward_max:
    agent-0: 34.0
    agent-1: 21.0
    agent-2: 62.0
    agent-3: 44.0
    agent-4: 28.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 17.51
    agent-1: 8.63
    agent-2: 46.83
    agent-3: 26.75
    agent-4: 14.91
    agent-5: 19.68
  policy_reward_min:
    agent-0: -37.0
    agent-1: 3.0
    agent-2: 32.0
    agent-3: 16.0
    agent-4: 6.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.22315550415973
    mean_inference_ms: 15.049146463737543
    mean_processing_ms: 73.24493456593586
  time_since_restore: 52578.92361617088
  time_this_iter_s: 144.92932772636414
  time_total_s: 141515.6017458439
  timestamp: 1637415642
  timesteps_since_restore: 34752000
  timesteps_this_iter: 96000
  timesteps_total: 97152000
  training_iteration: 1012
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1012 |           141516 | 97152000 |   134.31 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 6.22
    apples_agent-0_min: 2
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.31
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 30.37
    apples_agent-2_min: 18
    apples_agent-3_max: 19
    apples_agent-3_mean: 8.11
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.35
    apples_agent-4_min: 3
    apples_agent-5_max: 26
    apples_agent-5_mean: 6.01
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 5
    cleaning_beam_agent-0_mean: 1.0
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 245
    cleaning_beam_agent-1_mean: 214.26
    cleaning_beam_agent-1_min: 183
    cleaning_beam_agent-2_max: 43
    cleaning_beam_agent-2_mean: 18.66
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 24
    cleaning_beam_agent-3_mean: 8.14
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 46
    cleaning_beam_agent-4_mean: 3.7
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-43-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 170.0
  episode_reward_mean: 133.32
  episode_reward_min: 101.0
  episodes_this_iter: 96
  episodes_total: 97248
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11899.5
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32876837253570557
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013724444434046745
        model: {}
        policy_loss: -0.0020518742967396975
        total_loss: -0.002513473154976964
        vf_explained_var: 0.014279842376708984
        vf_loss: 1.1703689098358154
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1969519704580307
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010508710984140635
        model: {}
        policy_loss: -0.001482116524130106
        total_loss: -0.0017724927747622132
        vf_explained_var: 0.08674000203609467
        vf_loss: 0.5626133680343628
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39977920055389404
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014275494031608105
        model: {}
        policy_loss: -0.0019292315701022744
        total_loss: -0.0022733183577656746
        vf_explained_var: 0.011446014046669006
        vf_loss: 3.5952391624450684
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39523327350616455
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017489900346845388
        model: {}
        policy_loss: -0.001428721472620964
        total_loss: -0.001932936953380704
        vf_explained_var: 0.002164125442504883
        vf_loss: 1.9139630794525146
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3634386658668518
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009253609459847212
        model: {}
        policy_loss: -0.0013097808696329594
        total_loss: -0.0018544596387073398
        vf_explained_var: 0.014151260256767273
        vf_loss: 0.9497315883636475
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5316476821899414
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017411609878763556
        model: {}
        policy_loss: -0.0016369898803532124
        total_loss: -0.002441852819174528
        vf_explained_var: 0.021338358521461487
        vf_loss: 1.3083624839782715
    load_time_ms: 14140.425
    num_steps_sampled: 97248000
    num_steps_trained: 97248000
    sample_time_ms: 118994.085
    update_time_ms: 15.712
  iterations_since_restore: 363
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.969082125603872
    ram_util_percent: 12.378260869565219
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 60.0
    agent-3: 43.0
    agent-4: 25.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 17.34
    agent-1: 9.08
    agent-2: 46.71
    agent-3: 26.16
    agent-4: 14.81
    agent-5: 19.22
  policy_reward_min:
    agent-0: 8.0
    agent-1: 2.0
    agent-2: 30.0
    agent-3: 14.0
    agent-4: 6.0
    agent-5: 10.0
  sampler_perf:
    mean_env_wait_ms: 28.22314958902782
    mean_inference_ms: 15.049158001387406
    mean_processing_ms: 73.24546162437444
  time_since_restore: 52724.39815378189
  time_this_iter_s: 145.4745376110077
  time_total_s: 141661.0762834549
  timestamp: 1637415787
  timesteps_since_restore: 34848000
  timesteps_this_iter: 96000
  timesteps_total: 97248000
  training_iteration: 1013
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1013 |           141661 | 97248000 |   133.32 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 7.04
    apples_agent-0_min: 2
    apples_agent-1_max: 12
    apples_agent-1_mean: 3.5
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 29.52
    apples_agent-2_min: 10
    apples_agent-3_max: 17
    apples_agent-3_mean: 8.61
    apples_agent-3_min: 1
    apples_agent-4_max: 18
    apples_agent-4_mean: 9.82
    apples_agent-4_min: 3
    apples_agent-5_max: 26
    apples_agent-5_mean: 5.22
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 5
    cleaning_beam_agent-0_mean: 0.94
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 275
    cleaning_beam_agent-1_mean: 210.75
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 39
    cleaning_beam_agent-2_mean: 16.09
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 9.28
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.69
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.09
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-45-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 172.0
  episode_reward_mean: 133.34
  episode_reward_min: 55.0
  episodes_this_iter: 96
  episodes_total: 97344
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11880.83
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32240939140319824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011057290248572826
        model: {}
        policy_loss: -0.001611297600902617
        total_loss: -0.002058721147477627
        vf_explained_var: 0.013081952929496765
        vf_loss: 1.2001417875289917
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19347673654556274
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008086112793534994
        model: {}
        policy_loss: -0.0013788938522338867
        total_loss: -0.001670006662607193
        vf_explained_var: 0.07303859293460846
        vf_loss: 0.49408775568008423
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39051491022109985
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011192813981324434
        model: {}
        policy_loss: -0.0017935610376298428
        total_loss: -0.0020966962911188602
        vf_explained_var: 0.005830138921737671
        vf_loss: 3.8417396545410156
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39804500341415405
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013230431359261274
        model: {}
        policy_loss: -0.0015460643917322159
        total_loss: -0.002044231630861759
        vf_explained_var: 0.016173407435417175
        vf_loss: 2.0239243507385254
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35820040106773376
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009747849544510245
        model: {}
        policy_loss: -0.0015586251392960548
        total_loss: -0.0020990315824747086
        vf_explained_var: 0.015044569969177246
        vf_loss: 0.9002724289894104
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5328454971313477
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017013093456625938
        model: {}
        policy_loss: -0.0015172542771324515
        total_loss: -0.002319428138434887
        vf_explained_var: 0.021326541900634766
        vf_loss: 1.3563288450241089
    load_time_ms: 14128.736
    num_steps_sampled: 97344000
    num_steps_trained: 97344000
    sample_time_ms: 119161.376
    update_time_ms: 15.685
  iterations_since_restore: 364
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.95673076923077
    ram_util_percent: 12.449519230769226
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 16.0
    agent-2: 64.0
    agent-3: 45.0
    agent-4: 25.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 18.12
    agent-1: 8.55
    agent-2: 46.77
    agent-3: 26.49
    agent-4: 14.5
    agent-5: 18.91
  policy_reward_min:
    agent-0: 4.0
    agent-1: 3.0
    agent-2: 16.0
    agent-3: 13.0
    agent-4: 4.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.223421243857224
    mean_inference_ms: 15.04935554893726
    mean_processing_ms: 73.24681154085985
  time_since_restore: 52870.531223773956
  time_this_iter_s: 146.13306999206543
  time_total_s: 141807.20935344696
  timestamp: 1637415934
  timesteps_since_restore: 34944000
  timesteps_this_iter: 96000
  timesteps_total: 97344000
  training_iteration: 1014
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1014 |           141807 | 97344000 |   133.34 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 7.82
    apples_agent-0_min: 2
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.61
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 29.12
    apples_agent-2_min: 5
    apples_agent-3_max: 19
    apples_agent-3_mean: 8.27
    apples_agent-3_min: 3
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.73
    apples_agent-4_min: 3
    apples_agent-5_max: 27
    apples_agent-5_mean: 5.56
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 9
    cleaning_beam_agent-0_mean: 1.11
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 253
    cleaning_beam_agent-1_mean: 210.82
    cleaning_beam_agent-1_min: 63
    cleaning_beam_agent-2_max: 81
    cleaning_beam_agent-2_mean: 16.32
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 29
    cleaning_beam_agent-3_mean: 7.55
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.98
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.25
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-48-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 175.0
  episode_reward_mean: 130.68
  episode_reward_min: 36.0
  episodes_this_iter: 96
  episodes_total: 97440
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11875.862
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31947749853134155
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013729555066674948
        model: {}
        policy_loss: -0.0019993195310235023
        total_loss: -0.0024491623044013977
        vf_explained_var: 0.010572925209999084
        vf_loss: 1.1243793964385986
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19198036193847656
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011106808669865131
        model: {}
        policy_loss: -0.0016087237745523453
        total_loss: -0.0018961848691105843
        vf_explained_var: 0.08496677875518799
        vf_loss: 0.5041912198066711
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39513304829597473
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015807218151167035
        model: {}
        policy_loss: -0.0018182354979217052
        total_loss: -0.0021361401304602623
        vf_explained_var: 0.005500048398971558
        vf_loss: 3.7753028869628906
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3953855633735657
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014040145324543118
        model: {}
        policy_loss: -0.0015859780833125114
        total_loss: -0.0020960872061550617
        vf_explained_var: 0.0012719184160232544
        vf_loss: 1.8577065467834473
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3581780791282654
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007582014659419656
        model: {}
        policy_loss: -0.0012552961707115173
        total_loss: -0.0017922339029610157
        vf_explained_var: 0.01486864686012268
        vf_loss: 0.9345804452896118
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5362805128097534
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028035056311637163
        model: {}
        policy_loss: -0.0018206266686320305
        total_loss: -0.002637216355651617
        vf_explained_var: 0.011711731553077698
        vf_loss: 1.2726620435714722
    load_time_ms: 14160.642
    num_steps_sampled: 97440000
    num_steps_trained: 97440000
    sample_time_ms: 119215.213
    update_time_ms: 15.568
  iterations_since_restore: 365
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.861538461538462
    ram_util_percent: 12.461057692307689
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 18.0
    agent-2: 71.0
    agent-3: 40.0
    agent-4: 25.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 17.58
    agent-1: 8.7
    agent-2: 46.27
    agent-3: 25.81
    agent-4: 13.97
    agent-5: 18.35
  policy_reward_min:
    agent-0: 5.0
    agent-1: 1.0
    agent-2: 14.0
    agent-3: 6.0
    agent-4: 6.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.223569263006087
    mean_inference_ms: 15.049394769355958
    mean_processing_ms: 73.24678318285169
  time_since_restore: 53016.37734436989
  time_this_iter_s: 145.846120595932
  time_total_s: 141953.0554740429
  timestamp: 1637416080
  timesteps_since_restore: 35040000
  timesteps_this_iter: 96000
  timesteps_total: 97440000
  training_iteration: 1015
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1015 |           141953 | 97440000 |   130.68 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 6.88
    apples_agent-0_min: 1
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.44
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 29.56
    apples_agent-2_min: 18
    apples_agent-3_max: 28
    apples_agent-3_mean: 8.76
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.81
    apples_agent-4_min: 3
    apples_agent-5_max: 13
    apples_agent-5_mean: 4.66
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 5
    cleaning_beam_agent-0_mean: 1.09
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 250
    cleaning_beam_agent-1_mean: 211.57
    cleaning_beam_agent-1_min: 161
    cleaning_beam_agent-2_max: 40
    cleaning_beam_agent-2_mean: 15.52
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 9.3
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 15
    cleaning_beam_agent-4_mean: 3.99
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 3.21
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-50-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 181.0
  episode_reward_mean: 131.06
  episode_reward_min: 90.0
  episodes_this_iter: 96
  episodes_total: 97536
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11844.369
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30923303961753845
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009456114494241774
        model: {}
        policy_loss: -0.001714066369459033
        total_loss: -0.0021501719020307064
        vf_explained_var: 0.014781653881072998
        vf_loss: 1.0814484357833862
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19299376010894775
        entropy_coeff: 0.0017600000137463212
        kl: 0.001563627040013671
        model: {}
        policy_loss: -0.00167172122746706
        total_loss: -0.0019593280740082264
        vf_explained_var: 0.07384298741817474
        vf_loss: 0.5206041932106018
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39174360036849976
        entropy_coeff: 0.0017600000137463212
        kl: 0.00148398382589221
        model: {}
        policy_loss: -0.001621421193704009
        total_loss: -0.001958605367690325
        vf_explained_var: 0.023776575922966003
        vf_loss: 3.5228309631347656
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.401639461517334
        entropy_coeff: 0.0017600000137463212
        kl: 0.00141684350091964
        model: {}
        policy_loss: -0.0015813224017620087
        total_loss: -0.002095733769237995
        vf_explained_var: 0.021424055099487305
        vf_loss: 1.9247832298278809
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3556613028049469
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009331452311016619
        model: {}
        policy_loss: -0.0016090278513729572
        total_loss: -0.0021345093846321106
        vf_explained_var: 0.010359793901443481
        vf_loss: 1.0048202276229858
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5310890674591064
        entropy_coeff: 0.0017600000137463212
        kl: 0.001956651918590069
        model: {}
        policy_loss: -0.0017071603797376156
        total_loss: -0.0025095464661717415
        vf_explained_var: 0.013781413435935974
        vf_loss: 1.323312759399414
    load_time_ms: 14173.198
    num_steps_sampled: 97536000
    num_steps_trained: 97536000
    sample_time_ms: 119212.722
    update_time_ms: 15.707
  iterations_since_restore: 366
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.860869565217392
    ram_util_percent: 12.453623188405796
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 18.0
    agent-2: 62.0
    agent-3: 38.0
    agent-4: 29.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 17.03
    agent-1: 9.02
    agent-2: 45.67
    agent-3: 26.4
    agent-4: 14.68
    agent-5: 18.26
  policy_reward_min:
    agent-0: 8.0
    agent-1: 3.0
    agent-2: 34.0
    agent-3: 13.0
    agent-4: 5.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.22367322535833
    mean_inference_ms: 15.049184755245417
    mean_processing_ms: 73.24604867527546
  time_since_restore: 53161.43923306465
  time_this_iter_s: 145.06188869476318
  time_total_s: 142098.11736273766
  timestamp: 1637416225
  timesteps_since_restore: 35136000
  timesteps_this_iter: 96000
  timesteps_total: 97536000
  training_iteration: 1016
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1016 |           142098 | 97536000 |   131.06 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 7.54
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.07
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 29.12
    apples_agent-2_min: 12
    apples_agent-3_max: 27
    apples_agent-3_mean: 9.5
    apples_agent-3_min: 2
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.87
    apples_agent-4_min: 3
    apples_agent-5_max: 26
    apples_agent-5_mean: 5.79
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 1.28
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 249
    cleaning_beam_agent-1_mean: 207.69
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 40
    cleaning_beam_agent-2_mean: 15.3
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 8.73
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 22
    cleaning_beam_agent-4_mean: 4.35
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 13
    cleaning_beam_agent-5_mean: 3.59
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-52-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 176.0
  episode_reward_mean: 132.85
  episode_reward_min: 89.0
  episodes_this_iter: 96
  episodes_total: 97632
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11849.935
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30658531188964844
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009295950294472277
        model: {}
        policy_loss: -0.0017202049493789673
        total_loss: -0.002141182078048587
        vf_explained_var: 0.019931048154830933
        vf_loss: 1.186149001121521
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19176284968852997
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011920726392418146
        model: {}
        policy_loss: -0.001458232756704092
        total_loss: -0.0017434835899621248
        vf_explained_var: 0.07655782997608185
        vf_loss: 0.5225528478622437
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39503249526023865
        entropy_coeff: 0.0017600000137463212
        kl: 0.000941416306886822
        model: {}
        policy_loss: -0.0018015539972111583
        total_loss: -0.002161267213523388
        vf_explained_var: 0.027603864669799805
        vf_loss: 3.3554587364196777
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3930591344833374
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013471153797581792
        model: {}
        policy_loss: -0.0016452306881546974
        total_loss: -0.002137809991836548
        vf_explained_var: 0.021324574947357178
        vf_loss: 1.9920730590820312
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3524952530860901
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006492889951914549
        model: {}
        policy_loss: -0.0013512559235095978
        total_loss: -0.0018782899715006351
        vf_explained_var: 0.01336611807346344
        vf_loss: 0.933601975440979
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.525741457939148
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022219279780983925
        model: {}
        policy_loss: -0.0016268575564026833
        total_loss: -0.00240797339938581
        vf_explained_var: 0.011926248669624329
        vf_loss: 1.4419041872024536
    load_time_ms: 14189.802
    num_steps_sampled: 97632000
    num_steps_trained: 97632000
    sample_time_ms: 119280.216
    update_time_ms: 15.822
  iterations_since_restore: 367
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.841545893719807
    ram_util_percent: 12.467149758454104
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 16.0
    agent-2: 61.0
    agent-3: 42.0
    agent-4: 27.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 18.32
    agent-1: 8.71
    agent-2: 45.13
    agent-3: 27.0
    agent-4: 14.69
    agent-5: 19.0
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 27.0
    agent-3: 14.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.223796569831293
    mean_inference_ms: 15.048908105785186
    mean_processing_ms: 73.2448943581373
  time_since_restore: 53306.77845740318
  time_this_iter_s: 145.3392243385315
  time_total_s: 142243.4565870762
  timestamp: 1637416370
  timesteps_since_restore: 35232000
  timesteps_this_iter: 96000
  timesteps_total: 97632000
  training_iteration: 1017
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1017 |           142243 | 97632000 |   132.85 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 6.59
    apples_agent-0_min: 1
    apples_agent-1_max: 8
    apples_agent-1_mean: 3.08
    apples_agent-1_min: 0
    apples_agent-2_max: 77
    apples_agent-2_mean: 30.01
    apples_agent-2_min: 15
    apples_agent-3_max: 18
    apples_agent-3_mean: 8.18
    apples_agent-3_min: 2
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.24
    apples_agent-4_min: 1
    apples_agent-5_max: 17
    apples_agent-5_mean: 5.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 13
    cleaning_beam_agent-0_mean: 1.41
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 256
    cleaning_beam_agent-1_mean: 210.36
    cleaning_beam_agent-1_min: 164
    cleaning_beam_agent-2_max: 51
    cleaning_beam_agent-2_mean: 15.58
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 9.23
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 15
    cleaning_beam_agent-4_mean: 3.82
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.18
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-55-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 165.0
  episode_reward_mean: 130.22
  episode_reward_min: 88.0
  episodes_this_iter: 96
  episodes_total: 97728
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11841.284
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3041897118091583
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013005593791604042
        model: {}
        policy_loss: -0.0018018987029790878
        total_loss: -0.0022318921983242035
        vf_explained_var: 0.014901325106620789
        vf_loss: 1.0538088083267212
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19580325484275818
        entropy_coeff: 0.0017600000137463212
        kl: 0.00101373135112226
        model: {}
        policy_loss: -0.0016107885167002678
        total_loss: -0.001908507663756609
        vf_explained_var: 0.0869961678981781
        vf_loss: 0.46893343329429626
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3917100727558136
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014816909097135067
        model: {}
        policy_loss: -0.0018720111111178994
        total_loss: -0.00219110120087862
        vf_explained_var: 0.02101139724254608
        vf_loss: 3.703176259994507
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4012022316455841
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015685891266912222
        model: {}
        policy_loss: -0.0014707185328006744
        total_loss: -0.0020025838166475296
        vf_explained_var: 0.005718782544136047
        vf_loss: 1.7425028085708618
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3519955575466156
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008569852216169238
        model: {}
        policy_loss: -0.0014787493273615837
        total_loss: -0.0020087631419301033
        vf_explained_var: 0.014785528182983398
        vf_loss: 0.8949722647666931
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5321173667907715
        entropy_coeff: 0.0017600000137463212
        kl: 0.00137002719566226
        model: {}
        policy_loss: -0.001416439888998866
        total_loss: -0.002230799524113536
        vf_explained_var: 0.01210072636604309
        vf_loss: 1.2216483354568481
    load_time_ms: 14174.297
    num_steps_sampled: 97728000
    num_steps_trained: 97728000
    sample_time_ms: 119194.734
    update_time_ms: 15.72
  iterations_since_restore: 368
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.92233009708738
    ram_util_percent: 12.45388349514563
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 17.0
    agent-2: 71.0
    agent-3: 38.0
    agent-4: 24.0
    agent-5: 30.0
  policy_reward_mean:
    agent-0: 17.0
    agent-1: 8.57
    agent-2: 46.24
    agent-3: 25.02
    agent-4: 14.64
    agent-5: 18.75
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 30.0
    agent-3: 9.0
    agent-4: 7.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.223632936785688
    mean_inference_ms: 15.0487096905045
    mean_processing_ms: 73.24425757316146
  time_since_restore: 53451.545417547226
  time_this_iter_s: 144.76696014404297
  time_total_s: 142388.22354722023
  timestamp: 1637416515
  timesteps_since_restore: 35328000
  timesteps_this_iter: 96000
  timesteps_total: 97728000
  training_iteration: 1018
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1018 |           142388 | 97728000 |   130.22 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 7.48
    apples_agent-0_min: 1
    apples_agent-1_max: 11
    apples_agent-1_mean: 3.12
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 29.05
    apples_agent-2_min: 16
    apples_agent-3_max: 21
    apples_agent-3_mean: 8.43
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.22
    apples_agent-4_min: 3
    apples_agent-5_max: 23
    apples_agent-5_mean: 5.51
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.11
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 242
    cleaning_beam_agent-1_mean: 205.0
    cleaning_beam_agent-1_min: 50
    cleaning_beam_agent-2_max: 32
    cleaning_beam_agent-2_mean: 14.0
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 10.1
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 4.03
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.3
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_08-57-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 170.0
  episode_reward_mean: 130.96
  episode_reward_min: 42.0
  episodes_this_iter: 96
  episodes_total: 97824
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11835.473
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3152530789375305
        entropy_coeff: 0.0017600000137463212
        kl: 0.001883319579064846
        model: {}
        policy_loss: -0.001920737442560494
        total_loss: -0.002352197654545307
        vf_explained_var: 0.009278550744056702
        vf_loss: 1.2338221073150635
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19039617478847504
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010583784896880388
        model: {}
        policy_loss: -0.0016466788947582245
        total_loss: -0.0019356170669198036
        vf_explained_var: 0.10201852023601532
        vf_loss: 0.46158432960510254
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38518667221069336
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011898427037522197
        model: {}
        policy_loss: -0.0017867069691419601
        total_loss: -0.0020904438570141792
        vf_explained_var: 0.024017900228500366
        vf_loss: 3.741933822631836
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3984081447124481
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008534100488759577
        model: {}
        policy_loss: -0.0013264426961541176
        total_loss: -0.0018415618687868118
        vf_explained_var: 0.013007089495658875
        vf_loss: 1.8607687950134277
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35879284143447876
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011603685561567545
        model: {}
        policy_loss: -0.001377586741000414
        total_loss: -0.0019068983383476734
        vf_explained_var: 0.01824118196964264
        vf_loss: 1.0216286182403564
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5330647230148315
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016990606673061848
        model: {}
        policy_loss: -0.0016947395633906126
        total_loss: -0.0025052179116755724
        vf_explained_var: 0.017276301980018616
        vf_loss: 1.2771717309951782
    load_time_ms: 14207.077
    num_steps_sampled: 97824000
    num_steps_trained: 97824000
    sample_time_ms: 119145.439
    update_time_ms: 15.932
  iterations_since_restore: 369
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.803365384615383
    ram_util_percent: 12.470673076923076
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 18.0
    agent-2: 62.0
    agent-3: 41.0
    agent-4: 28.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 18.23
    agent-1: 8.31
    agent-2: 46.43
    agent-3: 25.31
    agent-4: 14.24
    agent-5: 18.44
  policy_reward_min:
    agent-0: 7.0
    agent-1: 1.0
    agent-2: 17.0
    agent-3: 2.0
    agent-4: 5.0
    agent-5: 5.0
  sampler_perf:
    mean_env_wait_ms: 28.223816542132777
    mean_inference_ms: 15.048472782668242
    mean_processing_ms: 73.24347537916645
  time_since_restore: 53597.254232645035
  time_this_iter_s: 145.70881509780884
  time_total_s: 142533.93236231804
  timestamp: 1637416661
  timesteps_since_restore: 35424000
  timesteps_this_iter: 96000
  timesteps_total: 97824000
  training_iteration: 1019
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1019 |           142534 | 97824000 |   130.96 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 7.14
    apples_agent-0_min: 1
    apples_agent-1_max: 16
    apples_agent-1_mean: 3.51
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 29.42
    apples_agent-2_min: 11
    apples_agent-3_max: 23
    apples_agent-3_mean: 8.46
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 9.85
    apples_agent-4_min: 3
    apples_agent-5_max: 25
    apples_agent-5_mean: 5.65
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 12
    cleaning_beam_agent-0_mean: 1.66
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 237
    cleaning_beam_agent-1_mean: 208.5
    cleaning_beam_agent-1_min: 168
    cleaning_beam_agent-2_max: 49
    cleaning_beam_agent-2_mean: 14.2
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 33
    cleaning_beam_agent-3_mean: 9.61
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 14
    cleaning_beam_agent-4_mean: 4.24
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.12
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-00-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 164.0
  episode_reward_mean: 132.38
  episode_reward_min: 79.0
  episodes_this_iter: 96
  episodes_total: 97920
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11864.032
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3141118586063385
        entropy_coeff: 0.0017600000137463212
        kl: 0.001356076798401773
        model: {}
        policy_loss: -0.0018103709444403648
        total_loss: -0.0022505552042275667
        vf_explained_var: 0.013717427849769592
        vf_loss: 1.1265162229537964
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.191499263048172
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008199575822800398
        model: {}
        policy_loss: -0.0014259156305342913
        total_loss: -0.0017107052262872458
        vf_explained_var: 0.08423280715942383
        vf_loss: 0.5224706530570984
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3810596466064453
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011471209581941366
        model: {}
        policy_loss: -0.0016606143908575177
        total_loss: -0.0019316342659294605
        vf_explained_var: 0.01241891086101532
        vf_loss: 3.9964256286621094
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39339444041252136
        entropy_coeff: 0.0017600000137463212
        kl: 0.001540999161079526
        model: {}
        policy_loss: -0.0014383201487362385
        total_loss: -0.0019362769089639187
        vf_explained_var: 0.012552455067634583
        vf_loss: 1.9441584348678589
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3529846966266632
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006555297295562923
        model: {}
        policy_loss: -0.0014137018006294966
        total_loss: -0.0019378266297280788
        vf_explained_var: 0.013374581933021545
        vf_loss: 0.971308708190918
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5266737341880798
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009370987536385655
        model: {}
        policy_loss: -0.0016055036103352904
        total_loss: -0.0024052481167018414
        vf_explained_var: 0.0034510642290115356
        vf_loss: 1.2720282077789307
    load_time_ms: 14217.627
    num_steps_sampled: 97920000
    num_steps_trained: 97920000
    sample_time_ms: 119070.861
    update_time_ms: 16.061
  iterations_since_restore: 370
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.990776699029126
    ram_util_percent: 12.4752427184466
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 18.0
    agent-2: 61.0
    agent-3: 40.0
    agent-4: 23.0
    agent-5: 43.0
  policy_reward_mean:
    agent-0: 17.57
    agent-1: 8.7
    agent-2: 47.07
    agent-3: 25.79
    agent-4: 14.38
    agent-5: 18.87
  policy_reward_min:
    agent-0: 9.0
    agent-1: 2.0
    agent-2: 26.0
    agent-3: 0.0
    agent-4: -32.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.22340330011524
    mean_inference_ms: 15.048290926484293
    mean_processing_ms: 73.24147750758375
  time_since_restore: 53741.92920470238
  time_this_iter_s: 144.67497205734253
  time_total_s: 142678.60733437538
  timestamp: 1637416806
  timesteps_since_restore: 35520000
  timesteps_this_iter: 96000
  timesteps_total: 97920000
  training_iteration: 1020
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1020 |           142679 | 97920000 |   132.38 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 7.3
    apples_agent-0_min: 1
    apples_agent-1_max: 19
    apples_agent-1_mean: 3.36
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 28.37
    apples_agent-2_min: 12
    apples_agent-3_max: 18
    apples_agent-3_mean: 8.39
    apples_agent-3_min: 2
    apples_agent-4_max: 27
    apples_agent-4_mean: 10.05
    apples_agent-4_min: 2
    apples_agent-5_max: 16
    apples_agent-5_mean: 5.71
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 1.5
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 250
    cleaning_beam_agent-1_mean: 209.81
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 40
    cleaning_beam_agent-2_mean: 15.57
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 34
    cleaning_beam_agent-3_mean: 9.54
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 16
    cleaning_beam_agent-4_mean: 3.96
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.87
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-02-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 171.0
  episode_reward_mean: 131.35
  episode_reward_min: 82.0
  episodes_this_iter: 96
  episodes_total: 98016
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11866.164
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.314267635345459
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011030005989596248
        model: {}
        policy_loss: -0.0017767185345292091
        total_loss: -0.0022102873772382736
        vf_explained_var: 0.011992588639259338
        vf_loss: 1.1954032182693481
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19089734554290771
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010483580408617854
        model: {}
        policy_loss: -0.0016145296394824982
        total_loss: -0.0019002370536327362
        vf_explained_var: 0.09354093670845032
        vf_loss: 0.5027040839195251
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38735413551330566
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013143881224095821
        model: {}
        policy_loss: -0.0015550439711660147
        total_loss: -0.0018731860909610987
        vf_explained_var: 0.015205353498458862
        vf_loss: 3.6359987258911133
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39372488856315613
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012928044889122248
        model: {}
        policy_loss: -0.001667209668084979
        total_loss: -0.0021685755345970392
        vf_explained_var: 0.009434714913368225
        vf_loss: 1.9159140586853027
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3572966754436493
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009477781131863594
        model: {}
        policy_loss: -0.0012405135203152895
        total_loss: -0.0017729378305375576
        vf_explained_var: 0.011608138680458069
        vf_loss: 0.964174211025238
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5284512639045715
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014119520783424377
        model: {}
        policy_loss: -0.0016801375895738602
        total_loss: -0.0024709575809538364
        vf_explained_var: 0.014203980565071106
        vf_loss: 1.3925594091415405
    load_time_ms: 14229.738
    num_steps_sampled: 98016000
    num_steps_trained: 98016000
    sample_time_ms: 119071.057
    update_time_ms: 15.983
  iterations_since_restore: 371
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.771428571428572
    ram_util_percent: 12.551428571428572
  pid: 27065
  policy_reward_max:
    agent-0: 28.0
    agent-1: 16.0
    agent-2: 67.0
    agent-3: 43.0
    agent-4: 24.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 17.39
    agent-1: 8.74
    agent-2: 45.41
    agent-3: 25.8
    agent-4: 14.75
    agent-5: 19.26
  policy_reward_min:
    agent-0: 4.0
    agent-1: 1.0
    agent-2: 23.0
    agent-3: 13.0
    agent-4: 3.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.223874421784377
    mean_inference_ms: 15.04808189000314
    mean_processing_ms: 73.2417324221428
  time_since_restore: 53886.91320347786
  time_this_iter_s: 144.98399877548218
  time_total_s: 142823.59133315086
  timestamp: 1637416953
  timesteps_since_restore: 35616000
  timesteps_this_iter: 96000
  timesteps_total: 98016000
  training_iteration: 1021
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1021 |           142824 | 98016000 |   131.35 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 7.38
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 3.43
    apples_agent-1_min: 0
    apples_agent-2_max: 54
    apples_agent-2_mean: 30.13
    apples_agent-2_min: 16
    apples_agent-3_max: 24
    apples_agent-3_mean: 8.87
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 9.63
    apples_agent-4_min: 2
    apples_agent-5_max: 16
    apples_agent-5_mean: 4.96
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 5
    cleaning_beam_agent-0_mean: 0.99
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 247
    cleaning_beam_agent-1_mean: 211.88
    cleaning_beam_agent-1_min: 55
    cleaning_beam_agent-2_max: 46
    cleaning_beam_agent-2_mean: 14.01
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 10.12
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 15
    cleaning_beam_agent-4_mean: 4.21
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.43
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-04-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 178.0
  episode_reward_mean: 131.44
  episode_reward_min: 80.0
  episodes_this_iter: 96
  episodes_total: 98112
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11857.546
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31658127903938293
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007655047811567783
        model: {}
        policy_loss: -0.001385123934596777
        total_loss: -0.0018243789672851562
        vf_explained_var: 0.01117686927318573
        vf_loss: 1.1792799234390259
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.18840385973453522
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011095110094174743
        model: {}
        policy_loss: -0.0014353918377310038
        total_loss: -0.0017189416103065014
        vf_explained_var: 0.09545990824699402
        vf_loss: 0.4804060459136963
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3837542235851288
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013914721785113215
        model: {}
        policy_loss: -0.0018120561726391315
        total_loss: -0.002098121214658022
        vf_explained_var: 0.02710890769958496
        vf_loss: 3.893416404724121
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3903077244758606
        entropy_coeff: 0.0017600000137463212
        kl: 0.001188898691907525
        model: {}
        policy_loss: -0.0013837222941219807
        total_loss: -0.0018621496856212616
        vf_explained_var: 0.016615331172943115
        vf_loss: 2.0851216316223145
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3596153259277344
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008250120445154607
        model: {}
        policy_loss: -0.001462162472307682
        total_loss: -0.0019964762032032013
        vf_explained_var: 0.010598614811897278
        vf_loss: 0.9861191511154175
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5301558971405029
        entropy_coeff: 0.0017600000137463212
        kl: 0.002181670628488064
        model: {}
        policy_loss: -0.0017337999306619167
        total_loss: -0.002524279523640871
        vf_explained_var: 0.020984038710594177
        vf_loss: 1.4259382486343384
    load_time_ms: 14202.279
    num_steps_sampled: 98112000
    num_steps_trained: 98112000
    sample_time_ms: 119079.492
    update_time_ms: 15.905
  iterations_since_restore: 372
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.929126213592237
    ram_util_percent: 12.467475728155335
  pid: 27065
  policy_reward_max:
    agent-0: 33.0
    agent-1: 16.0
    agent-2: 66.0
    agent-3: 41.0
    agent-4: 25.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 17.29
    agent-1: 8.4
    agent-2: 47.31
    agent-3: 26.15
    agent-4: 13.92
    agent-5: 18.37
  policy_reward_min:
    agent-0: 7.0
    agent-1: 3.0
    agent-2: 24.0
    agent-3: 13.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.223957638691488
    mean_inference_ms: 15.047822818529815
    mean_processing_ms: 73.2408508190554
  time_since_restore: 54031.45370078087
  time_this_iter_s: 144.54049730300903
  time_total_s: 142968.13183045387
  timestamp: 1637417098
  timesteps_since_restore: 35712000
  timesteps_this_iter: 96000
  timesteps_total: 98112000
  training_iteration: 1022
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1022 |           142968 | 98112000 |   131.44 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 7.15
    apples_agent-0_min: 2
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.31
    apples_agent-1_min: 0
    apples_agent-2_max: 60
    apples_agent-2_mean: 30.69
    apples_agent-2_min: 5
    apples_agent-3_max: 23
    apples_agent-3_mean: 8.78
    apples_agent-3_min: 1
    apples_agent-4_max: 31
    apples_agent-4_mean: 9.82
    apples_agent-4_min: 1
    apples_agent-5_max: 28
    apples_agent-5_mean: 5.61
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 9
    cleaning_beam_agent-0_mean: 0.87
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 250
    cleaning_beam_agent-1_mean: 215.09
    cleaning_beam_agent-1_min: 55
    cleaning_beam_agent-2_max: 47
    cleaning_beam_agent-2_mean: 17.35
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 39
    cleaning_beam_agent-3_mean: 10.09
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 11
    cleaning_beam_agent-4_mean: 3.76
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.98
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-07-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 183.0
  episode_reward_mean: 134.17
  episode_reward_min: 22.0
  episodes_this_iter: 96
  episodes_total: 98208
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11848.39
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.30667126178741455
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008992680814117193
        model: {}
        policy_loss: -0.0017342667561024427
        total_loss: -0.0021427548490464687
        vf_explained_var: 0.01189453899860382
        vf_loss: 1.3125497102737427
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19073863327503204
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012193812290206552
        model: {}
        policy_loss: -0.001456032507121563
        total_loss: -0.0017396220937371254
        vf_explained_var: 0.09525351226329803
        vf_loss: 0.521125078201294
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38640230894088745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014681133907288313
        model: {}
        policy_loss: -0.0017946383450180292
        total_loss: -0.0020720302127301693
        vf_explained_var: 0.015140160918235779
        vf_loss: 4.026734352111816
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3863694965839386
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012691474985331297
        model: {}
        policy_loss: -0.0014257977018132806
        total_loss: -0.0018926627235487103
        vf_explained_var: 0.015227928757667542
        vf_loss: 2.131502151489258
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36058372259140015
        entropy_coeff: 0.0017600000137463212
        kl: 0.000803896167781204
        model: {}
        policy_loss: -0.0013306960463523865
        total_loss: -0.001869620755314827
        vf_explained_var: 0.005957543849945068
        vf_loss: 0.956995964050293
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5216924548149109
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023999931290745735
        model: {}
        policy_loss: -0.0018167495727539062
        total_loss: -0.0026002731174230576
        vf_explained_var: -0.00041159987449645996
        vf_loss: 1.3465594053268433
    load_time_ms: 14227.018
    num_steps_sampled: 98208000
    num_steps_trained: 98208000
    sample_time_ms: 119073.936
    update_time_ms: 15.85
  iterations_since_restore: 373
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.926442307692312
    ram_util_percent: 12.473557692307692
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 16.0
    agent-2: 65.0
    agent-3: 42.0
    agent-4: 27.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 18.23
    agent-1: 8.88
    agent-2: 47.51
    agent-3: 26.1
    agent-4: 14.93
    agent-5: 18.52
  policy_reward_min:
    agent-0: 4.0
    agent-1: 1.0
    agent-2: 10.0
    agent-3: 3.0
    agent-4: 2.0
    agent-5: 1.0
  sampler_perf:
    mean_env_wait_ms: 28.224201259148632
    mean_inference_ms: 15.047828499581476
    mean_processing_ms: 73.24152063901484
  time_since_restore: 54177.09998726845
  time_this_iter_s: 145.64628648757935
  time_total_s: 143113.77811694145
  timestamp: 1637417244
  timesteps_since_restore: 35808000
  timesteps_this_iter: 96000
  timesteps_total: 98208000
  training_iteration: 1023
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1023 |           143114 | 98208000 |   134.17 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 7.53
    apples_agent-0_min: 1
    apples_agent-1_max: 12
    apples_agent-1_mean: 3.02
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 29.74
    apples_agent-2_min: 13
    apples_agent-3_max: 32
    apples_agent-3_mean: 8.45
    apples_agent-3_min: 2
    apples_agent-4_max: 27
    apples_agent-4_mean: 9.58
    apples_agent-4_min: 2
    apples_agent-5_max: 21
    apples_agent-5_mean: 5.33
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 0.79
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 241
    cleaning_beam_agent-1_mean: 210.84
    cleaning_beam_agent-1_min: 154
    cleaning_beam_agent-2_max: 42
    cleaning_beam_agent-2_mean: 14.64
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 10.87
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 3.9
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 6
    cleaning_beam_agent-5_mean: 2.55
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-09-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 178.0
  episode_reward_mean: 132.27
  episode_reward_min: 94.0
  episodes_this_iter: 96
  episodes_total: 98304
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11837.467
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32323867082595825
        entropy_coeff: 0.0017600000137463212
        kl: 0.001937491470016539
        model: {}
        policy_loss: -0.002029205672442913
        total_loss: -0.0024833758361637592
        vf_explained_var: 0.017001211643218994
        vf_loss: 1.1473134756088257
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1895265281200409
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008508296450600028
        model: {}
        policy_loss: -0.0013318604324012995
        total_loss: -0.0016172153409570456
        vf_explained_var: 0.07920844852924347
        vf_loss: 0.48211532831192017
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3849162757396698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012614341685548425
        model: {}
        policy_loss: -0.0016245716251432896
        total_loss: -0.0019339313730597496
        vf_explained_var: 0.021685317158699036
        vf_loss: 3.6809298992156982
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38551002740859985
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011533830547705293
        model: {}
        policy_loss: -0.001707213930785656
        total_loss: -0.0021943675819784403
        vf_explained_var: 0.0032555609941482544
        vf_loss: 1.9134269952774048
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36149299144744873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013544157845899463
        model: {}
        policy_loss: -0.001592843560501933
        total_loss: -0.0021310928277671337
        vf_explained_var: 0.01834152638912201
        vf_loss: 0.9797660112380981
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5119712948799133
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009589315159246325
        model: {}
        policy_loss: -0.0014091571792960167
        total_loss: -0.0021832175552845
        vf_explained_var: 0.013536736369132996
        vf_loss: 1.2700614929199219
    load_time_ms: 14232.66
    num_steps_sampled: 98304000
    num_steps_trained: 98304000
    sample_time_ms: 119046.839
    update_time_ms: 15.977
  iterations_since_restore: 374
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.866826923076925
    ram_util_percent: 12.462499999999999
  pid: 27065
  policy_reward_max:
    agent-0: 35.0
    agent-1: 17.0
    agent-2: 65.0
    agent-3: 40.0
    agent-4: 27.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 17.57
    agent-1: 8.75
    agent-2: 45.74
    agent-3: 26.18
    agent-4: 14.9
    agent-5: 19.13
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 28.0
    agent-3: 14.0
    agent-4: 6.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.22445248387605
    mean_inference_ms: 15.047822102227723
    mean_processing_ms: 73.24163189133169
  time_since_restore: 54322.916897535324
  time_this_iter_s: 145.81691026687622
  time_total_s: 143259.59502720833
  timestamp: 1637417389
  timesteps_since_restore: 35904000
  timesteps_this_iter: 96000
  timesteps_total: 98304000
  training_iteration: 1024
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1024 |           143260 | 98304000 |   132.27 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 7.24
    apples_agent-0_min: 1
    apples_agent-1_max: 24
    apples_agent-1_mean: 3.84
    apples_agent-1_min: 0
    apples_agent-2_max: 54
    apples_agent-2_mean: 31.38
    apples_agent-2_min: 11
    apples_agent-3_max: 18
    apples_agent-3_mean: 8.46
    apples_agent-3_min: 2
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.56
    apples_agent-4_min: 1
    apples_agent-5_max: 22
    apples_agent-5_mean: 5.99
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 5
    cleaning_beam_agent-0_mean: 0.83
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 242
    cleaning_beam_agent-1_mean: 209.94
    cleaning_beam_agent-1_min: 50
    cleaning_beam_agent-2_max: 49
    cleaning_beam_agent-2_mean: 16.2
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 35
    cleaning_beam_agent-3_mean: 9.2
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 19
    cleaning_beam_agent-4_mean: 4.64
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.83
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-12-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 194.0
  episode_reward_mean: 136.23
  episode_reward_min: 58.0
  episodes_this_iter: 96
  episodes_total: 98400
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11834.431
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3150259554386139
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010951380245387554
        model: {}
        policy_loss: -0.0017327852547168732
        total_loss: -0.002164115197956562
        vf_explained_var: 0.007143110036849976
        vf_loss: 1.2311407327651978
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1875215470790863
        entropy_coeff: 0.0017600000137463212
        kl: 0.001523794373497367
        model: {}
        policy_loss: -0.00156043516471982
        total_loss: -0.0018381057307124138
        vf_explained_var: 0.09719309210777283
        vf_loss: 0.5237042903900146
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37652164697647095
        entropy_coeff: 0.0017600000137463212
        kl: 0.001191763672977686
        model: {}
        policy_loss: -0.001507194945588708
        total_loss: -0.0017505160067230463
        vf_explained_var: 0.03969733417034149
        vf_loss: 4.193576812744141
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3853367567062378
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019400820601731539
        model: {}
        policy_loss: -0.0014752401039004326
        total_loss: -0.0019706934690475464
        vf_explained_var: 0.024817034602165222
        vf_loss: 1.827357530593872
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36462682485580444
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006941230967640877
        model: {}
        policy_loss: -0.0013774312101304531
        total_loss: -0.001915616448968649
        vf_explained_var: 0.014177829027175903
        vf_loss: 1.0356085300445557
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5117234587669373
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017997074173763394
        model: {}
        policy_loss: -0.0015559950843453407
        total_loss: -0.0023200265131890774
        vf_explained_var: 0.019952788949012756
        vf_loss: 1.3660016059875488
    load_time_ms: 14203.826
    num_steps_sampled: 98400000
    num_steps_trained: 98400000
    sample_time_ms: 119000.589
    update_time_ms: 16.031
  iterations_since_restore: 375
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.099514563106798
    ram_util_percent: 12.549514563106795
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 19.0
    agent-2: 73.0
    agent-3: 45.0
    agent-4: 29.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 18.04
    agent-1: 9.01
    agent-2: 48.68
    agent-3: 25.95
    agent-4: 15.37
    agent-5: 19.18
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 17.0
    agent-3: 12.0
    agent-4: 4.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.22472813867765
    mean_inference_ms: 15.047868228876554
    mean_processing_ms: 73.24251231661918
  time_since_restore: 54467.9310028553
  time_this_iter_s: 145.0141053199768
  time_total_s: 143404.6091325283
  timestamp: 1637417535
  timesteps_since_restore: 36000000
  timesteps_this_iter: 96000
  timesteps_total: 98400000
  training_iteration: 1025
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1025 |           143405 | 98400000 |   136.23 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 8.01
    apples_agent-0_min: 1
    apples_agent-1_max: 31
    apples_agent-1_mean: 3.41
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 29.96
    apples_agent-2_min: 11
    apples_agent-3_max: 34
    apples_agent-3_mean: 9.51
    apples_agent-3_min: 2
    apples_agent-4_max: 25
    apples_agent-4_mean: 10.72
    apples_agent-4_min: 3
    apples_agent-5_max: 18
    apples_agent-5_mean: 4.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 6
    cleaning_beam_agent-0_mean: 0.91
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 255
    cleaning_beam_agent-1_mean: 217.26
    cleaning_beam_agent-1_min: 182
    cleaning_beam_agent-2_max: 36
    cleaning_beam_agent-2_mean: 16.83
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 27
    cleaning_beam_agent-3_mean: 8.67
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 4.44
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.09
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-14-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 179.0
  episode_reward_mean: 134.55
  episode_reward_min: 83.0
  episodes_this_iter: 96
  episodes_total: 98496
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11840.668
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3171098828315735
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011474561179056764
        model: {}
        policy_loss: -0.0017489832825958729
        total_loss: -0.002206017728894949
        vf_explained_var: 0.019774332642555237
        vf_loss: 1.0107967853546143
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19149664044380188
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011343861697241664
        model: {}
        policy_loss: -0.0014447225257754326
        total_loss: -0.0017258794978260994
        vf_explained_var: 0.07911355793476105
        vf_loss: 0.5587267279624939
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3915449380874634
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017884195549413562
        model: {}
        policy_loss: -0.0017310571856796741
        total_loss: -0.0020623458549380302
        vf_explained_var: 0.018011555075645447
        vf_loss: 3.5783379077911377
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3927359879016876
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012718053767457604
        model: {}
        policy_loss: -0.001590218162164092
        total_loss: -0.0020869611762464046
        vf_explained_var: 0.001125916838645935
        vf_loss: 1.9447238445281982
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36505648493766785
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012349954340606928
        model: {}
        policy_loss: -0.001619596965610981
        total_loss: -0.002155243419110775
        vf_explained_var: 0.01174435019493103
        vf_loss: 1.0685235261917114
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5041006207466125
        entropy_coeff: 0.0017600000137463212
        kl: 0.002068797592073679
        model: {}
        policy_loss: -0.0016500074416399002
        total_loss: -0.0024145469069480896
        vf_explained_var: 0.013538450002670288
        vf_loss: 1.226773977279663
    load_time_ms: 14204.998
    num_steps_sampled: 98496000
    num_steps_trained: 98496000
    sample_time_ms: 119032.556
    update_time_ms: 16.075
  iterations_since_restore: 376
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.942028985507246
    ram_util_percent: 12.465217391304344
  pid: 27065
  policy_reward_max:
    agent-0: 31.0
    agent-1: 22.0
    agent-2: 65.0
    agent-3: 49.0
    agent-4: 27.0
    agent-5: 31.0
  policy_reward_mean:
    agent-0: 17.82
    agent-1: 9.06
    agent-2: 46.77
    agent-3: 26.34
    agent-4: 15.74
    agent-5: 18.82
  policy_reward_min:
    agent-0: 8.0
    agent-1: 3.0
    agent-2: 25.0
    agent-3: 13.0
    agent-4: 5.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.224850129806512
    mean_inference_ms: 15.047775372791142
    mean_processing_ms: 73.24328025431998
  time_since_restore: 54613.386770009995
  time_this_iter_s: 145.4557671546936
  time_total_s: 143550.064899683
  timestamp: 1637417680
  timesteps_since_restore: 36096000
  timesteps_this_iter: 96000
  timesteps_total: 98496000
  training_iteration: 1026
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1026 |           143550 | 98496000 |   134.55 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 8.35
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 3.24
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 30.24
    apples_agent-2_min: 9
    apples_agent-3_max: 24
    apples_agent-3_mean: 8.68
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.29
    apples_agent-4_min: 2
    apples_agent-5_max: 30
    apples_agent-5_mean: 5.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 5
    cleaning_beam_agent-0_mean: 1.03
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 257
    cleaning_beam_agent-1_mean: 217.06
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 72
    cleaning_beam_agent-2_mean: 19.47
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 10.8
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 16
    cleaning_beam_agent-4_mean: 4.39
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.76
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-17-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 177.0
  episode_reward_mean: 136.63
  episode_reward_min: 38.0
  episodes_this_iter: 96
  episodes_total: 98592
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11853.187
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3164617717266083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014597661793231964
        model: {}
        policy_loss: -0.0020167315378785133
        total_loss: -0.0024488153867423534
        vf_explained_var: 0.01204615831375122
        vf_loss: 1.2488927841186523
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.190544992685318
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013207381125539541
        model: {}
        policy_loss: -0.0015121279284358025
        total_loss: -0.001789601519703865
        vf_explained_var: 0.07394289970397949
        vf_loss: 0.5788410902023315
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39329129457473755
        entropy_coeff: 0.0017600000137463212
        kl: 0.00138657761272043
        model: {}
        policy_loss: -0.00198798137716949
        total_loss: -0.0022585077676922083
        vf_explained_var: 0.0196906179189682
        vf_loss: 4.216690540313721
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3893357515335083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016398655716329813
        model: {}
        policy_loss: -0.0017343066865578294
        total_loss: -0.002203503856435418
        vf_explained_var: 0.021823301911354065
        vf_loss: 2.160350799560547
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3734533190727234
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015666161198168993
        model: {}
        policy_loss: -0.0015169787220656872
        total_loss: -0.0020630392245948315
        vf_explained_var: 0.009457364678382874
        vf_loss: 1.1121960878372192
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4979543089866638
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014080987311899662
        model: {}
        policy_loss: -0.001459999126382172
        total_loss: -0.002192537300288677
        vf_explained_var: 0.017070695757865906
        vf_loss: 1.4385960102081299
    load_time_ms: 14205.738
    num_steps_sampled: 98592000
    num_steps_trained: 98592000
    sample_time_ms: 119063.878
    update_time_ms: 16.387
  iterations_since_restore: 377
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.977884615384614
    ram_util_percent: 12.46057692307692
  pid: 27065
  policy_reward_max:
    agent-0: 34.0
    agent-1: 18.0
    agent-2: 63.0
    agent-3: 44.0
    agent-4: 27.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 18.39
    agent-1: 8.67
    agent-2: 47.6
    agent-3: 26.79
    agent-4: 15.6
    agent-5: 19.58
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 14.0
    agent-3: 5.0
    agent-4: 4.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.225066670983896
    mean_inference_ms: 15.047884334578775
    mean_processing_ms: 73.24439844625249
  time_since_restore: 54759.14917969704
  time_this_iter_s: 145.76240968704224
  time_total_s: 143695.82730937004
  timestamp: 1637417826
  timesteps_since_restore: 36192000
  timesteps_this_iter: 96000
  timesteps_total: 98592000
  training_iteration: 1027
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1027 |           143696 | 98592000 |   136.63 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 7.13
    apples_agent-0_min: 2
    apples_agent-1_max: 10
    apples_agent-1_mean: 3.55
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 31.06
    apples_agent-2_min: 15
    apples_agent-3_max: 25
    apples_agent-3_mean: 8.75
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 9.73
    apples_agent-4_min: 2
    apples_agent-5_max: 20
    apples_agent-5_mean: 4.45
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 17
    cleaning_beam_agent-0_mean: 1.13
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 272
    cleaning_beam_agent-1_mean: 218.54
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 72
    cleaning_beam_agent-2_mean: 18.97
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 67
    cleaning_beam_agent-3_mean: 9.35
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 15
    cleaning_beam_agent-4_mean: 4.05
    cleaning_beam_agent-4_min: 1
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.89
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-19-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 187.0
  episode_reward_mean: 136.06
  episode_reward_min: 56.0
  episodes_this_iter: 96
  episodes_total: 98688
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11848.68
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3196995258331299
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011684669880196452
        model: {}
        policy_loss: -0.0017549102194607258
        total_loss: -0.0021979920566082
        vf_explained_var: 0.011235043406486511
        vf_loss: 1.1958955526351929
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19160610437393188
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010302421869710088
        model: {}
        policy_loss: -0.0013970909640192986
        total_loss: -0.0016758814454078674
        vf_explained_var: 0.08206351101398468
        vf_loss: 0.584366500377655
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39566290378570557
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015439935959875584
        model: {}
        policy_loss: -0.0016382449539378285
        total_loss: -0.00195415411144495
        vf_explained_var: 0.01791819930076599
        vf_loss: 3.804568290710449
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39204463362693787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015868536429479718
        model: {}
        policy_loss: -0.0015304089756682515
        total_loss: -0.002038342412561178
        vf_explained_var: 0.022597894072532654
        vf_loss: 1.8206285238265991
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3656141757965088
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011911758920177817
        model: {}
        policy_loss: -0.001390987541526556
        total_loss: -0.0019340100698173046
        vf_explained_var: 0.008565783500671387
        vf_loss: 1.0045465230941772
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49283647537231445
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011819107457995415
        model: {}
        policy_loss: -0.0014882257673889399
        total_loss: -0.0022183386608958244
        vf_explained_var: 0.006296411156654358
        vf_loss: 1.37281334400177
    load_time_ms: 14213.285
    num_steps_sampled: 98688000
    num_steps_trained: 98688000
    sample_time_ms: 119126.977
    update_time_ms: 16.302
  iterations_since_restore: 378
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.762500000000003
    ram_util_percent: 12.462019230769227
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 19.0
    agent-2: 68.0
    agent-3: 36.0
    agent-4: 30.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 17.9
    agent-1: 9.25
    agent-2: 48.0
    agent-3: 25.82
    agent-4: 15.43
    agent-5: 19.66
  policy_reward_min:
    agent-0: 6.0
    agent-1: 2.0
    agent-2: 22.0
    agent-3: 11.0
    agent-4: 6.0
    agent-5: 4.0
  sampler_perf:
    mean_env_wait_ms: 28.22538784333456
    mean_inference_ms: 15.047761410740835
    mean_processing_ms: 73.24347302972848
  time_since_restore: 54904.58094954491
  time_this_iter_s: 145.43176984786987
  time_total_s: 143841.2590792179
  timestamp: 1637417972
  timesteps_since_restore: 36288000
  timesteps_this_iter: 96000
  timesteps_total: 98688000
  training_iteration: 1028
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1028 |           143841 | 98688000 |   136.06 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 7.46
    apples_agent-0_min: 1
    apples_agent-1_max: 23
    apples_agent-1_mean: 3.68
    apples_agent-1_min: 0
    apples_agent-2_max: 49
    apples_agent-2_mean: 31.19
    apples_agent-2_min: 5
    apples_agent-3_max: 21
    apples_agent-3_mean: 9.14
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 9.79
    apples_agent-4_min: 1
    apples_agent-5_max: 17
    apples_agent-5_mean: 4.89
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 5
    cleaning_beam_agent-0_mean: 0.98
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 266
    cleaning_beam_agent-1_mean: 217.13
    cleaning_beam_agent-1_min: 15
    cleaning_beam_agent-2_max: 51
    cleaning_beam_agent-2_mean: 19.7
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 31
    cleaning_beam_agent-3_mean: 9.8
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 14
    cleaning_beam_agent-4_mean: 3.62
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.36
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-21-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 198.0
  episode_reward_mean: 136.83
  episode_reward_min: 14.0
  episodes_this_iter: 96
  episodes_total: 98784
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11862.977
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3229296803474426
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011896125506609678
        model: {}
        policy_loss: -0.0017591838259249926
        total_loss: -0.0022027974482625723
        vf_explained_var: 0.017157524824142456
        vf_loss: 1.2474234104156494
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.18937455117702484
        entropy_coeff: 0.0017600000137463212
        kl: 0.000749734987039119
        model: {}
        policy_loss: -0.0012376445811241865
        total_loss: -0.0015115307178348303
        vf_explained_var: 0.0812712013721466
        vf_loss: 0.5941558480262756
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3942998945713043
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016273767687380314
        model: {}
        policy_loss: -0.0017903083935379982
        total_loss: -0.002053157426416874
        vf_explained_var: 0.023810535669326782
        vf_loss: 4.311201095581055
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3857610821723938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006763460696674883
        model: {}
        policy_loss: -0.0013625621795654297
        total_loss: -0.0018229596316814423
        vf_explained_var: 0.008823588490486145
        vf_loss: 2.185394048690796
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3704518675804138
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007131517631933093
        model: {}
        policy_loss: -0.0012297257781028748
        total_loss: -0.0017708763480186462
        vf_explained_var: 0.007598176598548889
        vf_loss: 1.1084741353988647
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4958682060241699
        entropy_coeff: 0.0017600000137463212
        kl: 0.002083448925986886
        model: {}
        policy_loss: -0.0018851696513593197
        total_loss: -0.0026183980517089367
        vf_explained_var: 0.01315230131149292
        vf_loss: 1.394985318183899
    load_time_ms: 14181.737
    num_steps_sampled: 98784000
    num_steps_trained: 98784000
    sample_time_ms: 119187.234
    update_time_ms: 16.404
  iterations_since_restore: 379
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.892788461538462
    ram_util_percent: 12.549519230769231
  pid: 27065
  policy_reward_max:
    agent-0: 35.0
    agent-1: 19.0
    agent-2: 71.0
    agent-3: 51.0
    agent-4: 28.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 17.86
    agent-1: 9.48
    agent-2: 48.57
    agent-3: 25.91
    agent-4: 15.7
    agent-5: 19.31
  policy_reward_min:
    agent-0: 0.0
    agent-1: 2.0
    agent-2: 8.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: 1.0
  sampler_perf:
    mean_env_wait_ms: 28.225884004667087
    mean_inference_ms: 15.047759990472045
    mean_processing_ms: 73.2435322796226
  time_since_restore: 55050.637271642685
  time_this_iter_s: 146.05632209777832
  time_total_s: 143987.3154013157
  timestamp: 1637418118
  timesteps_since_restore: 36384000
  timesteps_this_iter: 96000
  timesteps_total: 98784000
  training_iteration: 1029
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1029 |           143987 | 98784000 |   136.83 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 8.37
    apples_agent-0_min: 2
    apples_agent-1_max: 14
    apples_agent-1_mean: 3.7
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 30.16
    apples_agent-2_min: 16
    apples_agent-3_max: 42
    apples_agent-3_mean: 8.86
    apples_agent-3_min: 1
    apples_agent-4_max: 21
    apples_agent-4_mean: 10.27
    apples_agent-4_min: 2
    apples_agent-5_max: 20
    apples_agent-5_mean: 5.05
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.05
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 259
    cleaning_beam_agent-1_mean: 221.48
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 40
    cleaning_beam_agent-2_mean: 15.74
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 26
    cleaning_beam_agent-3_mean: 8.28
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 14
    cleaning_beam_agent-4_mean: 3.99
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 2.87
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-24-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 216.0
  episode_reward_mean: 138.31
  episode_reward_min: 64.0
  episodes_this_iter: 96
  episodes_total: 98880
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11852.025
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3320978283882141
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010247204918414354
        model: {}
        policy_loss: -0.0018398689571768045
        total_loss: -0.0023016079794615507
        vf_explained_var: 0.004669517278671265
        vf_loss: 1.227522850036621
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1906888484954834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007547476561740041
        model: {}
        policy_loss: -0.001430470496416092
        total_loss: -0.0017115948721766472
        vf_explained_var: 0.10486172139644623
        vf_loss: 0.5448951125144958
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3879348039627075
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016427314840257168
        model: {}
        policy_loss: -0.0017246962524950504
        total_loss: -0.001996095757931471
        vf_explained_var: 0.01641009747982025
        vf_loss: 4.113661766052246
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3795226812362671
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014965595910325646
        model: {}
        policy_loss: -0.0014685988426208496
        total_loss: -0.0019379444420337677
        vf_explained_var: 0.008837476372718811
        vf_loss: 1.9861525297164917
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3619977533817291
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011035504285246134
        model: {}
        policy_loss: -0.0013285987079143524
        total_loss: -0.0018550073727965355
        vf_explained_var: 0.015333101153373718
        vf_loss: 1.1070793867111206
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4889373183250427
        entropy_coeff: 0.0017600000137463212
        kl: 0.001453083474189043
        model: {}
        policy_loss: -0.0014304560609161854
        total_loss: -0.002146763727068901
        vf_explained_var: 0.007472515106201172
        vf_loss: 1.442195177078247
    load_time_ms: 14184.152
    num_steps_sampled: 98880000
    num_steps_trained: 98880000
    sample_time_ms: 119304.254
    update_time_ms: 16.21
  iterations_since_restore: 380
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.012500000000003
    ram_util_percent: 12.474519230769229
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 19.0
    agent-2: 74.0
    agent-3: 43.0
    agent-4: 30.0
    agent-5: 35.0
  policy_reward_mean:
    agent-0: 18.46
    agent-1: 9.68
    agent-2: 48.01
    agent-3: 26.97
    agent-4: 16.25
    agent-5: 18.94
  policy_reward_min:
    agent-0: 8.0
    agent-1: 3.0
    agent-2: 25.0
    agent-3: 8.0
    agent-4: 5.0
    agent-5: 3.0
  sampler_perf:
    mean_env_wait_ms: 28.22666513268631
    mean_inference_ms: 15.047976741877148
    mean_processing_ms: 73.24500387351254
  time_since_restore: 55196.43555665016
  time_this_iter_s: 145.7982850074768
  time_total_s: 144133.11368632317
  timestamp: 1637418264
  timesteps_since_restore: 36480000
  timesteps_this_iter: 96000
  timesteps_total: 98880000
  training_iteration: 1030
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1030 |           144133 | 98880000 |   138.31 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 7.38
    apples_agent-0_min: 1
    apples_agent-1_max: 27
    apples_agent-1_mean: 3.51
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 29.95
    apples_agent-2_min: 19
    apples_agent-3_max: 27
    apples_agent-3_mean: 8.85
    apples_agent-3_min: 3
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.91
    apples_agent-4_min: 1
    apples_agent-5_max: 31
    apples_agent-5_mean: 5.69
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 0.94
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 257
    cleaning_beam_agent-1_mean: 217.79
    cleaning_beam_agent-1_min: 151
    cleaning_beam_agent-2_max: 37
    cleaning_beam_agent-2_mean: 14.51
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 42
    cleaning_beam_agent-3_mean: 10.29
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 26
    cleaning_beam_agent-4_mean: 3.82
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 3.05
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-26-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 174.0
  episode_reward_mean: 139.43
  episode_reward_min: 78.0
  episodes_this_iter: 96
  episodes_total: 98976
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11838.937
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32845261693000793
        entropy_coeff: 0.0017600000137463212
        kl: 0.00175494235008955
        model: {}
        policy_loss: -0.0020800032652914524
        total_loss: -0.002537030726671219
        vf_explained_var: 0.02675749361515045
        vf_loss: 1.2105087041854858
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19001437723636627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009590100962668657
        model: {}
        policy_loss: -0.0015194611623883247
        total_loss: -0.001799291931092739
        vf_explained_var: 0.08884105086326599
        vf_loss: 0.5459176898002625
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38314396142959595
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014944627182558179
        model: {}
        policy_loss: -0.0019201738759875298
        total_loss: -0.002197139896452427
        vf_explained_var: 0.020667970180511475
        vf_loss: 3.9736642837524414
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3848320543766022
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006755786016583443
        model: {}
        policy_loss: -0.0013818852603435516
        total_loss: -0.0018494254909455776
        vf_explained_var: 0.02511228621006012
        vf_loss: 2.097626209259033
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36956483125686646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006010895594954491
        model: {}
        policy_loss: -0.001344786724075675
        total_loss: -0.0018683469388633966
        vf_explained_var: 0.008914336562156677
        vf_loss: 1.2687151432037354
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4917113184928894
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014498500386253
        model: {}
        policy_loss: -0.001580529846251011
        total_loss: -0.0022984957322478294
        vf_explained_var: 0.008360534906387329
        vf_loss: 1.474446177482605
    load_time_ms: 14185.559
    num_steps_sampled: 98976000
    num_steps_trained: 98976000
    sample_time_ms: 119310.336
    update_time_ms: 16.177
  iterations_since_restore: 381
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.67904761904762
    ram_util_percent: 12.456190476190475
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 17.0
    agent-2: 65.0
    agent-3: 45.0
    agent-4: 32.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 18.35
    agent-1: 9.0
    agent-2: 48.27
    agent-3: 27.11
    agent-4: 16.76
    agent-5: 19.94
  policy_reward_min:
    agent-0: 10.0
    agent-1: 2.0
    agent-2: 26.0
    agent-3: 13.0
    agent-4: 8.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.226828894406886
    mean_inference_ms: 15.047908073556208
    mean_processing_ms: 73.24488012544866
  time_since_restore: 55341.321937799454
  time_this_iter_s: 144.886381149292
  time_total_s: 144278.00006747246
  timestamp: 1637418411
  timesteps_since_restore: 36576000
  timesteps_this_iter: 96000
  timesteps_total: 98976000
  training_iteration: 1031
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1031 |           144278 | 98976000 |   139.43 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 7.56
    apples_agent-0_min: 2
    apples_agent-1_max: 15
    apples_agent-1_mean: 3.14
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 30.9
    apples_agent-2_min: 10
    apples_agent-3_max: 23
    apples_agent-3_mean: 8.71
    apples_agent-3_min: 3
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.73
    apples_agent-4_min: 3
    apples_agent-5_max: 23
    apples_agent-5_mean: 5.57
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 1.07
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 248
    cleaning_beam_agent-1_mean: 211.83
    cleaning_beam_agent-1_min: 59
    cleaning_beam_agent-2_max: 39
    cleaning_beam_agent-2_mean: 16.33
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 10.78
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 14
    cleaning_beam_agent-4_mean: 3.81
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 2.93
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-29-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 196.0
  episode_reward_mean: 135.43
  episode_reward_min: 53.0
  episodes_this_iter: 96
  episodes_total: 99072
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11839.322
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3294658958911896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009183247457258403
        model: {}
        policy_loss: -0.0017110737971961498
        total_loss: -0.0021700174547731876
        vf_explained_var: 0.010894715785980225
        vf_loss: 1.2091429233551025
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.18849624693393707
        entropy_coeff: 0.0017600000137463212
        kl: 0.001030638930387795
        model: {}
        policy_loss: -0.0016005029901862144
        total_loss: -0.00187781173735857
        vf_explained_var: 0.09020808339118958
        vf_loss: 0.5444425344467163
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3870979845523834
        entropy_coeff: 0.0017600000137463212
        kl: 0.001320596318691969
        model: {}
        policy_loss: -0.001686979434452951
        total_loss: -0.0019766571931540966
        vf_explained_var: 0.011555954813957214
        vf_loss: 3.9161715507507324
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3818310499191284
        entropy_coeff: 0.0017600000137463212
        kl: 0.001544672530144453
        model: {}
        policy_loss: -0.001753529068082571
        total_loss: -0.0022058102767914534
        vf_explained_var: 0.009347692131996155
        vf_loss: 2.197392225265503
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3687392473220825
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012127860682085156
        model: {}
        policy_loss: -0.0014299806207418442
        total_loss: -0.0019553452730178833
        vf_explained_var: 0.011273935437202454
        vf_loss: 1.2361464500427246
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4928302764892578
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023132057394832373
        model: {}
        policy_loss: -0.0018916111439466476
        total_loss: -0.0026188218034803867
        vf_explained_var: 0.018026262521743774
        vf_loss: 1.4017047882080078
    load_time_ms: 14204.587
    num_steps_sampled: 99072000
    num_steps_trained: 99072000
    sample_time_ms: 119395.108
    update_time_ms: 15.919
  iterations_since_restore: 382
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.891304347826086
    ram_util_percent: 12.470531400966182
  pid: 27065
  policy_reward_max:
    agent-0: 33.0
    agent-1: 18.0
    agent-2: 73.0
    agent-3: 52.0
    agent-4: 31.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 18.01
    agent-1: 8.66
    agent-2: 47.0
    agent-3: 26.53
    agent-4: 16.07
    agent-5: 19.16
  policy_reward_min:
    agent-0: 8.0
    agent-1: 2.0
    agent-2: 16.0
    agent-3: 8.0
    agent-4: 5.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.226885375370653
    mean_inference_ms: 15.047949625523568
    mean_processing_ms: 73.24442786663465
  time_since_restore: 55486.9765689373
  time_this_iter_s: 145.6546311378479
  time_total_s: 144423.6546986103
  timestamp: 1637418557
  timesteps_since_restore: 36672000
  timesteps_this_iter: 96000
  timesteps_total: 99072000
  training_iteration: 1032
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1032 |           144424 | 99072000 |   135.43 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 7.64
    apples_agent-0_min: 1
    apples_agent-1_max: 21
    apples_agent-1_mean: 3.09
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 30.78
    apples_agent-2_min: 15
    apples_agent-3_max: 46
    apples_agent-3_mean: 9.53
    apples_agent-3_min: 2
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.1
    apples_agent-4_min: 3
    apples_agent-5_max: 20
    apples_agent-5_mean: 4.98
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 0.83
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 216.85
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 47
    cleaning_beam_agent-2_mean: 17.27
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 38
    cleaning_beam_agent-3_mean: 10.38
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.93
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.61
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-31-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 188.0
  episode_reward_mean: 138.5
  episode_reward_min: 63.0
  episodes_this_iter: 96
  episodes_total: 99168
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11844.385
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3254668414592743
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009975915309041739
        model: {}
        policy_loss: -0.001671814126893878
        total_loss: -0.0021286006085574627
        vf_explained_var: 0.01839756965637207
        vf_loss: 1.160346508026123
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1884710192680359
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007042817305773497
        model: {}
        policy_loss: -0.0012582712806761265
        total_loss: -0.0015309515874832869
        vf_explained_var: 0.08823980391025543
        vf_loss: 0.590286374092102
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39024272561073303
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011718840105459094
        model: {}
        policy_loss: -0.0017169085331261158
        total_loss: -0.00197172025218606
        vf_explained_var: 0.015945345163345337
        vf_loss: 4.320180892944336
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37775173783302307
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012151048285886645
        model: {}
        policy_loss: -0.0016355856787413359
        total_loss: -0.0021027312614023685
        vf_explained_var: 0.022562891244888306
        vf_loss: 1.9769656658172607
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36696919798851013
        entropy_coeff: 0.0017600000137463212
        kl: 0.001294859335757792
        model: {}
        policy_loss: -0.0016009495593607426
        total_loss: -0.002142275683581829
        vf_explained_var: 0.009711161255836487
        vf_loss: 1.045393466949463
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49858805537223816
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011757997563108802
        model: {}
        policy_loss: -0.0014222507597878575
        total_loss: -0.0021643689833581448
        vf_explained_var: 0.009248197078704834
        vf_loss: 1.3539636135101318
    load_time_ms: 14191.15
    num_steps_sampled: 99168000
    num_steps_trained: 99168000
    sample_time_ms: 119328.212
    update_time_ms: 15.855
  iterations_since_restore: 383
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.12669902912621
    ram_util_percent: 12.503883495145628
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 18.0
    agent-2: 72.0
    agent-3: 42.0
    agent-4: 29.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 17.76
    agent-1: 9.29
    agent-2: 48.67
    agent-3: 27.73
    agent-4: 15.3
    agent-5: 19.75
  policy_reward_min:
    agent-0: 3.0
    agent-1: 2.0
    agent-2: 28.0
    agent-3: 14.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.227027782410065
    mean_inference_ms: 15.047770256990221
    mean_processing_ms: 73.24357717753311
  time_since_restore: 55631.80144524574
  time_this_iter_s: 144.82487630844116
  time_total_s: 144568.47957491875
  timestamp: 1637418702
  timesteps_since_restore: 36768000
  timesteps_this_iter: 96000
  timesteps_total: 99168000
  training_iteration: 1033
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1033 |           144568 | 99168000 |    138.5 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 7.75
    apples_agent-0_min: 1
    apples_agent-1_max: 15
    apples_agent-1_mean: 3.58
    apples_agent-1_min: 0
    apples_agent-2_max: 67
    apples_agent-2_mean: 31.49
    apples_agent-2_min: 15
    apples_agent-3_max: 46
    apples_agent-3_mean: 9.32
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.62
    apples_agent-4_min: 2
    apples_agent-5_max: 23
    apples_agent-5_mean: 5.85
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 1.01
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 267
    cleaning_beam_agent-1_mean: 213.36
    cleaning_beam_agent-1_min: 166
    cleaning_beam_agent-2_max: 34
    cleaning_beam_agent-2_mean: 15.25
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 31
    cleaning_beam_agent-3_mean: 10.01
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 3.53
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 10
    cleaning_beam_agent-5_mean: 3.51
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-34-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 201.0
  episode_reward_mean: 138.18
  episode_reward_min: 100.0
  episodes_this_iter: 96
  episodes_total: 99264
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11864.518
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32008665800094604
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013826328795403242
        model: {}
        policy_loss: -0.0019985935650765896
        total_loss: -0.002438219264149666
        vf_explained_var: 0.010650888085365295
        vf_loss: 1.2372456789016724
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.18857766687870026
        entropy_coeff: 0.0017600000137463212
        kl: 0.001438311068341136
        model: {}
        policy_loss: -0.001573779620230198
        total_loss: -0.0018462864682078362
        vf_explained_var: 0.08673503994941711
        vf_loss: 0.5938793420791626
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38522768020629883
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013692264910787344
        model: {}
        policy_loss: -0.0018231114372611046
        total_loss: -0.0020901081152260303
        vf_explained_var: 0.011469736695289612
        vf_loss: 4.110041618347168
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37759193778038025
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015016963006928563
        model: {}
        policy_loss: -0.0013925433158874512
        total_loss: -0.0018712589517235756
        vf_explained_var: 0.016440317034721375
        vf_loss: 1.8584811687469482
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3732178211212158
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007547024870291352
        model: {}
        policy_loss: -0.0013957871124148369
        total_loss: -0.0019402503967285156
        vf_explained_var: 0.013597950339317322
        vf_loss: 1.1239514350891113
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4984435737133026
        entropy_coeff: 0.0017600000137463212
        kl: 0.002097203629091382
        model: {}
        policy_loss: -0.00168188801035285
        total_loss: -0.0024216817691922188
        vf_explained_var: 0.012151554226875305
        vf_loss: 1.3746623992919922
    load_time_ms: 14177.383
    num_steps_sampled: 99264000
    num_steps_trained: 99264000
    sample_time_ms: 119241.582
    update_time_ms: 16.045
  iterations_since_restore: 384
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.895169082125605
    ram_util_percent: 12.467149758454104
  pid: 27065
  policy_reward_max:
    agent-0: 30.0
    agent-1: 21.0
    agent-2: 68.0
    agent-3: 40.0
    agent-4: 28.0
    agent-5: 29.0
  policy_reward_mean:
    agent-0: 17.87
    agent-1: 9.21
    agent-2: 48.43
    agent-3: 26.63
    agent-4: 16.08
    agent-5: 19.96
  policy_reward_min:
    agent-0: 9.0
    agent-1: 1.0
    agent-2: 29.0
    agent-3: 14.0
    agent-4: 5.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.22708092488785
    mean_inference_ms: 15.047745096044626
    mean_processing_ms: 73.24344151747114
  time_since_restore: 55776.81209230423
  time_this_iter_s: 145.01064705848694
  time_total_s: 144713.49022197723
  timestamp: 1637418847
  timesteps_since_restore: 36864000
  timesteps_this_iter: 96000
  timesteps_total: 99264000
  training_iteration: 1034
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1034 |           144713 | 99264000 |   138.18 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 7.65
    apples_agent-0_min: 1
    apples_agent-1_max: 11
    apples_agent-1_mean: 3.06
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 31.12
    apples_agent-2_min: 14
    apples_agent-3_max: 32
    apples_agent-3_mean: 9.03
    apples_agent-3_min: 1
    apples_agent-4_max: 23
    apples_agent-4_mean: 10.65
    apples_agent-4_min: 3
    apples_agent-5_max: 32
    apples_agent-5_mean: 5.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 5
    cleaning_beam_agent-0_mean: 0.85
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 257
    cleaning_beam_agent-1_mean: 214.33
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 34
    cleaning_beam_agent-2_mean: 15.8
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 36
    cleaning_beam_agent-3_mean: 8.19
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 3.98
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.5
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-36-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 181.0
  episode_reward_mean: 137.95
  episode_reward_min: 61.0
  episodes_this_iter: 96
  episodes_total: 99360
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11876.504
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32161834836006165
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013739780988544226
        model: {}
        policy_loss: -0.002095382194966078
        total_loss: -0.0025239649694412947
        vf_explained_var: -0.00013841688632965088
        vf_loss: 1.3746713399887085
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.18599754571914673
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006510347011499107
        model: {}
        policy_loss: -0.001391533063724637
        total_loss: -0.0016678695101290941
        vf_explained_var: 0.09283170104026794
        vf_loss: 0.5102077722549438
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37746462225914
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011387134436517954
        model: {}
        policy_loss: -0.001720345113426447
        total_loss: -0.001965597737580538
        vf_explained_var: 0.028339341282844543
        vf_loss: 4.190855979919434
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3725869059562683
        entropy_coeff: 0.0017600000137463212
        kl: 0.001457221806049347
        model: {}
        policy_loss: -0.0014018220826983452
        total_loss: -0.001854808535426855
        vf_explained_var: 0.01747630536556244
        vf_loss: 2.027641773223877
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37708500027656555
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007712049409747124
        model: {}
        policy_loss: -0.001213099341839552
        total_loss: -0.0017648772336542606
        vf_explained_var: 0.006372466683387756
        vf_loss: 1.118934154510498
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49014827609062195
        entropy_coeff: 0.0017600000137463212
        kl: 0.001946579897776246
        model: {}
        policy_loss: -0.001727524446323514
        total_loss: -0.002455516019836068
        vf_explained_var: 0.010296225547790527
        vf_loss: 1.3466969728469849
    load_time_ms: 14190.988
    num_steps_sampled: 99360000
    num_steps_trained: 99360000
    sample_time_ms: 119219.92
    update_time_ms: 15.943
  iterations_since_restore: 385
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.92512077294686
    ram_util_percent: 12.465217391304348
  pid: 27065
  policy_reward_max:
    agent-0: 35.0
    agent-1: 20.0
    agent-2: 66.0
    agent-3: 40.0
    agent-4: 28.0
    agent-5: 37.0
  policy_reward_mean:
    agent-0: 18.39
    agent-1: 8.61
    agent-2: 48.67
    agent-3: 27.06
    agent-4: 15.57
    agent-5: 19.65
  policy_reward_min:
    agent-0: 8.0
    agent-1: 0.0
    agent-2: 22.0
    agent-3: 12.0
    agent-4: 5.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.227326351103066
    mean_inference_ms: 15.047550292822912
    mean_processing_ms: 73.24340171513938
  time_since_restore: 55921.87637877464
  time_this_iter_s: 145.0642864704132
  time_total_s: 144858.55450844765
  timestamp: 1637418992
  timesteps_since_restore: 36960000
  timesteps_this_iter: 96000
  timesteps_total: 99360000
  training_iteration: 1035
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1035 |           144859 | 99360000 |   137.95 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 6.85
    apples_agent-0_min: 1
    apples_agent-1_max: 9
    apples_agent-1_mean: 3.4
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 30.77
    apples_agent-2_min: 20
    apples_agent-3_max: 18
    apples_agent-3_mean: 8.4
    apples_agent-3_min: 1
    apples_agent-4_max: 22
    apples_agent-4_mean: 10.7
    apples_agent-4_min: 2
    apples_agent-5_max: 23
    apples_agent-5_mean: 5.64
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 7
    cleaning_beam_agent-0_mean: 0.91
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 258
    cleaning_beam_agent-1_mean: 215.1
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 41
    cleaning_beam_agent-2_mean: 14.74
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 32
    cleaning_beam_agent-3_mean: 8.74
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 13
    cleaning_beam_agent-4_mean: 4.09
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.33
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-38-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 187.0
  episode_reward_mean: 138.85
  episode_reward_min: 81.0
  episodes_this_iter: 96
  episodes_total: 99456
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11878.886
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.326663613319397
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015291138552129269
        model: {}
        policy_loss: -0.0018067224882543087
        total_loss: -0.0022522162180393934
        vf_explained_var: 0.013270720839500427
        vf_loss: 1.2943483591079712
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1887436807155609
        entropy_coeff: 0.0017600000137463212
        kl: 0.00118568679317832
        model: {}
        policy_loss: -0.0017917575314640999
        total_loss: -0.0020650941878557205
        vf_explained_var: 0.06795898079872131
        vf_loss: 0.5885211229324341
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3718382716178894
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011277706362307072
        model: {}
        policy_loss: -0.001898912014439702
        total_loss: -0.0021537705324590206
        vf_explained_var: 0.00885377824306488
        vf_loss: 3.995795726776123
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3827264904975891
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019447961822152138
        model: {}
        policy_loss: -0.0017469637095928192
        total_loss: -0.002208334393799305
        vf_explained_var: 0.012286350131034851
        vf_loss: 2.122224807739258
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3818496763706207
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016729261260479689
        model: {}
        policy_loss: -0.0015466944314539433
        total_loss: -0.0021102135069668293
        vf_explained_var: 0.0021272748708724976
        vf_loss: 1.0853837728500366
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4834723472595215
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019855666905641556
        model: {}
        policy_loss: -0.0017256804276257753
        total_loss: -0.0024339216761291027
        vf_explained_var: 0.0001182258129119873
        vf_loss: 1.4267005920410156
    load_time_ms: 14170.987
    num_steps_sampled: 99456000
    num_steps_trained: 99456000
    sample_time_ms: 119113.256
    update_time_ms: 15.849
  iterations_since_restore: 386
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.95609756097561
    ram_util_percent: 12.464878048780484
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 19.0
    agent-2: 67.0
    agent-3: 43.0
    agent-4: 28.0
    agent-5: 39.0
  policy_reward_mean:
    agent-0: 18.3
    agent-1: 9.02
    agent-2: 48.4
    agent-3: 26.77
    agent-4: 16.29
    agent-5: 20.07
  policy_reward_min:
    agent-0: 8.0
    agent-1: 1.0
    agent-2: 22.0
    agent-3: 4.0
    agent-4: 6.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.227308007179964
    mean_inference_ms: 15.047194837923216
    mean_processing_ms: 73.24261232853306
  time_since_restore: 56066.08889579773
  time_this_iter_s: 144.21251702308655
  time_total_s: 145002.76702547073
  timestamp: 1637419137
  timesteps_since_restore: 37056000
  timesteps_this_iter: 96000
  timesteps_total: 99456000
  training_iteration: 1036
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1036 |           145003 | 99456000 |   138.85 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 7.8
    apples_agent-0_min: 2
    apples_agent-1_max: 18
    apples_agent-1_mean: 3.85
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 30.59
    apples_agent-2_min: 14
    apples_agent-3_max: 40
    apples_agent-3_mean: 8.94
    apples_agent-3_min: 1
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.71
    apples_agent-4_min: 0
    apples_agent-5_max: 19
    apples_agent-5_mean: 5.12
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 14
    cleaning_beam_agent-0_mean: 1.18
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 252
    cleaning_beam_agent-1_mean: 213.04
    cleaning_beam_agent-1_min: 170
    cleaning_beam_agent-2_max: 37
    cleaning_beam_agent-2_mean: 15.51
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 9.28
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 12
    cleaning_beam_agent-4_mean: 3.57
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 3.33
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-41-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 188.0
  episode_reward_mean: 139.41
  episode_reward_min: 80.0
  episodes_this_iter: 96
  episodes_total: 99552
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11886.746
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3242871165275574
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011128393234685063
        model: {}
        policy_loss: -0.001781431958079338
        total_loss: -0.0022175712510943413
        vf_explained_var: 0.014160454273223877
        vf_loss: 1.3460748195648193
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1882908195257187
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008541467832401395
        model: {}
        policy_loss: -0.0014876129571348429
        total_loss: -0.0017655036645010114
        vf_explained_var: 0.10198026895523071
        vf_loss: 0.5350365042686462
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37891533970832825
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009816933888942003
        model: {}
        policy_loss: -0.0016797254793345928
        total_loss: -0.0019286684691905975
        vf_explained_var: 0.021448001265525818
        vf_loss: 4.179466247558594
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38497620820999146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008560721180401742
        model: {}
        policy_loss: -0.0011914854403585196
        total_loss: -0.0016821259632706642
        vf_explained_var: -0.00048425793647766113
        vf_loss: 1.869178056716919
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3876199424266815
        entropy_coeff: 0.0017600000137463212
        kl: 0.00048205541679635644
        model: {}
        policy_loss: -0.0012437133118510246
        total_loss: -0.0018166396766901016
        vf_explained_var: 0.013199776411056519
        vf_loss: 1.0928595066070557
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47830474376678467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019814041443169117
        model: {}
        policy_loss: -0.0017124875448644161
        total_loss: -0.0023910384625196457
        vf_explained_var: 0.008553951978683472
        vf_loss: 1.6326085329055786
    load_time_ms: 14169.65
    num_steps_sampled: 99552000
    num_steps_trained: 99552000
    sample_time_ms: 119038.913
    update_time_ms: 15.409
  iterations_since_restore: 387
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.87729468599034
    ram_util_percent: 12.552173913043479
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 22.0
    agent-2: 71.0
    agent-3: 39.0
    agent-4: 25.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 18.67
    agent-1: 9.21
    agent-2: 48.7
    agent-3: 26.5
    agent-4: 15.8
    agent-5: 20.53
  policy_reward_min:
    agent-0: 8.0
    agent-1: 3.0
    agent-2: 24.0
    agent-3: 13.0
    agent-4: 4.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.227413909044863
    mean_inference_ms: 15.047012986106179
    mean_processing_ms: 73.24204737173774
  time_since_restore: 56211.183680057526
  time_this_iter_s: 145.09478425979614
  time_total_s: 145147.86180973053
  timestamp: 1637419282
  timesteps_since_restore: 37152000
  timesteps_this_iter: 96000
  timesteps_total: 99552000
  training_iteration: 1037
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1037 |           145148 | 99552000 |   139.41 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 7.73
    apples_agent-0_min: 2
    apples_agent-1_max: 12
    apples_agent-1_mean: 3.18
    apples_agent-1_min: 0
    apples_agent-2_max: 246
    apples_agent-2_mean: 32.85
    apples_agent-2_min: 20
    apples_agent-3_max: 40
    apples_agent-3_mean: 9.14
    apples_agent-3_min: 1
    apples_agent-4_max: 30
    apples_agent-4_mean: 10.8
    apples_agent-4_min: 2
    apples_agent-5_max: 16
    apples_agent-5_mean: 4.88
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 17
    cleaning_beam_agent-0_mean: 0.99
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 262
    cleaning_beam_agent-1_mean: 211.91
    cleaning_beam_agent-1_min: 171
    cleaning_beam_agent-2_max: 32
    cleaning_beam_agent-2_mean: 16.86
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 45
    cleaning_beam_agent-3_mean: 9.2
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 15
    cleaning_beam_agent-4_mean: 3.42
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 9
    cleaning_beam_agent-5_mean: 3.44
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-43-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 194.0
  episode_reward_mean: 137.77
  episode_reward_min: 105.0
  episodes_this_iter: 96
  episodes_total: 99648
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11880.291
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32160457968711853
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013175783678889275
        model: {}
        policy_loss: -0.0019047553651034832
        total_loss: -0.0023486660793423653
        vf_explained_var: 0.01631033420562744
        vf_loss: 1.2211153507232666
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.18957427144050598
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009099494200199842
        model: {}
        policy_loss: -0.0013962574303150177
        total_loss: -0.0016793287359178066
        vf_explained_var: 0.09699025750160217
        vf_loss: 0.5058133602142334
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.38160771131515503
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009426861652173102
        model: {}
        policy_loss: -0.0016784127801656723
        total_loss: -0.0019594619516283274
        vf_explained_var: 0.014352396130561829
        vf_loss: 3.9057798385620117
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3770434558391571
        entropy_coeff: 0.0017600000137463212
        kl: 0.001995418220758438
        model: {}
        policy_loss: -0.0017960509285330772
        total_loss: -0.0022675003856420517
        vf_explained_var: 0.01727592945098877
        vf_loss: 1.9214885234832764
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37983351945877075
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006447758059948683
        model: {}
        policy_loss: -0.0011745616793632507
        total_loss: -0.0017258734442293644
        vf_explained_var: 0.008980467915534973
        vf_loss: 1.1719276905059814
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47337639331817627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010451370617374778
        model: {}
        policy_loss: -0.0013899505138397217
        total_loss: -0.0020837862975895405
        vf_explained_var: 0.004129156470298767
        vf_loss: 1.3930869102478027
    load_time_ms: 14167.486
    num_steps_sampled: 99648000
    num_steps_trained: 99648000
    sample_time_ms: 118932.183
    update_time_ms: 15.516
  iterations_since_restore: 388
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.967475728155335
    ram_util_percent: 12.470388349514561
  pid: 27065
  policy_reward_max:
    agent-0: 34.0
    agent-1: 18.0
    agent-2: 71.0
    agent-3: 41.0
    agent-4: 31.0
    agent-5: 33.0
  policy_reward_mean:
    agent-0: 17.75
    agent-1: 9.21
    agent-2: 47.62
    agent-3: 27.07
    agent-4: 16.55
    agent-5: 19.57
  policy_reward_min:
    agent-0: 6.0
    agent-1: 3.0
    agent-2: 33.0
    agent-3: 16.0
    agent-4: 6.0
    agent-5: 7.0
  sampler_perf:
    mean_env_wait_ms: 28.227294208487315
    mean_inference_ms: 15.046944869318873
    mean_processing_ms: 73.24161196111068
  time_since_restore: 56355.4595746994
  time_this_iter_s: 144.27589464187622
  time_total_s: 145292.1377043724
  timestamp: 1637419426
  timesteps_since_restore: 37248000
  timesteps_this_iter: 96000
  timesteps_total: 99648000
  training_iteration: 1038
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1038 |           145292 | 99648000 |   137.77 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 7.87
    apples_agent-0_min: 2
    apples_agent-1_max: 16
    apples_agent-1_mean: 3.19
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 31.3
    apples_agent-2_min: 17
    apples_agent-3_max: 31
    apples_agent-3_mean: 8.99
    apples_agent-3_min: 2
    apples_agent-4_max: 24
    apples_agent-4_mean: 10.84
    apples_agent-4_min: 5
    apples_agent-5_max: 29
    apples_agent-5_mean: 5.68
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 10
    cleaning_beam_agent-0_mean: 1.2
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 244
    cleaning_beam_agent-1_mean: 214.8
    cleaning_beam_agent-1_min: 173
    cleaning_beam_agent-2_max: 49
    cleaning_beam_agent-2_mean: 16.58
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 38
    cleaning_beam_agent-3_mean: 10.36
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.11
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.54
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-46-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 186.0
  episode_reward_mean: 139.83
  episode_reward_min: 105.0
  episodes_this_iter: 96
  episodes_total: 99744
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11854.504
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.32646000385284424
        entropy_coeff: 0.0017600000137463212
        kl: 0.001552889822050929
        model: {}
        policy_loss: -0.0021930914372205734
        total_loss: -0.0026346584782004356
        vf_explained_var: 0.015140622854232788
        vf_loss: 1.3300206661224365
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.18906745314598083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014839348150417209
        model: {}
        policy_loss: -0.0017154733650386333
        total_loss: -0.0019939211197197437
        vf_explained_var: 0.09498146176338196
        vf_loss: 0.5430830717086792
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3815825581550598
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014539998956024647
        model: {}
        policy_loss: -0.001738724298775196
        total_loss: -0.001999716740101576
        vf_explained_var: 0.02809824049472809
        vf_loss: 4.105976104736328
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3776542842388153
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012649649288505316
        model: {}
        policy_loss: -0.0013932964066043496
        total_loss: -0.0018542683683335781
        vf_explained_var: 0.008287504315376282
        vf_loss: 2.037015199661255
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37414729595184326
        entropy_coeff: 0.0017600000137463212
        kl: 0.000790682970546186
        model: {}
        policy_loss: -0.0012770090252161026
        total_loss: -0.001834433525800705
        vf_explained_var: 0.013174265623092651
        vf_loss: 1.01073157787323
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4754568040370941
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011457731015980244
        model: {}
        policy_loss: -0.0013346625491976738
        total_loss: -0.0020154155790805817
        vf_explained_var: 0.0036611706018447876
        vf_loss: 1.5605099201202393
    load_time_ms: 14175.355
    num_steps_sampled: 99744000
    num_steps_trained: 99744000
    sample_time_ms: 118950.185
    update_time_ms: 15.31
  iterations_since_restore: 389
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.89903846153846
    ram_util_percent: 12.468269230769229
  pid: 27065
  policy_reward_max:
    agent-0: 35.0
    agent-1: 17.0
    agent-2: 70.0
    agent-3: 44.0
    agent-4: 28.0
    agent-5: 38.0
  policy_reward_mean:
    agent-0: 19.14
    agent-1: 8.98
    agent-2: 48.69
    agent-3: 27.08
    agent-4: 15.48
    agent-5: 20.46
  policy_reward_min:
    agent-0: 9.0
    agent-1: 3.0
    agent-2: 31.0
    agent-3: 12.0
    agent-4: 7.0
    agent-5: 9.0
  sampler_perf:
    mean_env_wait_ms: 28.227890256112772
    mean_inference_ms: 15.046861589302134
    mean_processing_ms: 73.24198510485506
  time_since_restore: 56501.515876054764
  time_this_iter_s: 146.05630135536194
  time_total_s: 145438.19400572777
  timestamp: 1637419572
  timesteps_since_restore: 37344000
  timesteps_this_iter: 96000
  timesteps_total: 99744000
  training_iteration: 1039
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1039 |           145438 | 99744000 |   139.83 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 7.16
    apples_agent-0_min: 1
    apples_agent-1_max: 14
    apples_agent-1_mean: 3.24
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 29.99
    apples_agent-2_min: 18
    apples_agent-3_max: 33
    apples_agent-3_mean: 9.59
    apples_agent-3_min: 2
    apples_agent-4_max: 19
    apples_agent-4_mean: 10.07
    apples_agent-4_min: 3
    apples_agent-5_max: 27
    apples_agent-5_mean: 5.39
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 11
    cleaning_beam_agent-0_mean: 1.12
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 246
    cleaning_beam_agent-1_mean: 210.97
    cleaning_beam_agent-1_min: 176
    cleaning_beam_agent-2_max: 37
    cleaning_beam_agent-2_mean: 18.6
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 9.09
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 14
    cleaning_beam_agent-4_mean: 3.61
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.55
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-48-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 177.0
  episode_reward_mean: 133.81
  episode_reward_min: 89.0
  episodes_this_iter: 96
  episodes_total: 99840
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11851.101
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3264516294002533
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009268351132050157
        model: {}
        policy_loss: -0.0017950688488781452
        total_loss: -0.0022530541755259037
        vf_explained_var: 0.0181560218334198
        vf_loss: 1.165678858757019
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1922188401222229
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006754710921086371
        model: {}
        policy_loss: -0.001265510218217969
        total_loss: -0.0015564070781692863
        vf_explained_var: 0.07550808787345886
        vf_loss: 0.4740656316280365
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39449745416641235
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015267906710505486
        model: {}
        policy_loss: -0.00172494538128376
        total_loss: -0.002049538306891918
        vf_explained_var: 0.017985448241233826
        vf_loss: 3.69724702835083
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.37602484226226807
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012946652714163065
        model: {}
        policy_loss: -0.00160212442278862
        total_loss: -0.0020754635334014893
        vf_explained_var: 0.013370916247367859
        vf_loss: 1.8846584558486938
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3685738444328308
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008146802429109812
        model: {}
        policy_loss: -0.0013362257741391659
        total_loss: -0.0018853647634387016
        vf_explained_var: 0.0022361725568771362
        vf_loss: 0.995520830154419
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4753536880016327
        entropy_coeff: 0.0017600000137463212
        kl: 0.00114952411968261
        model: {}
        policy_loss: -0.0014579351991415024
        total_loss: -0.0021558806765824556
        vf_explained_var: 0.0073748379945755005
        vf_loss: 1.3867640495300293
    load_time_ms: 14164.276
    num_steps_sampled: 99840000
    num_steps_trained: 99840000
    sample_time_ms: 118932.123
    update_time_ms: 15.368
  iterations_since_restore: 390
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.99565217391304
    ram_util_percent: 12.475362318840578
  pid: 27065
  policy_reward_max:
    agent-0: 32.0
    agent-1: 17.0
    agent-2: 69.0
    agent-3: 39.0
    agent-4: 25.0
    agent-5: 32.0
  policy_reward_mean:
    agent-0: 17.65
    agent-1: 8.21
    agent-2: 46.91
    agent-3: 26.4
    agent-4: 14.86
    agent-5: 19.78
  policy_reward_min:
    agent-0: 7.0
    agent-1: 2.0
    agent-2: 26.0
    agent-3: 13.0
    agent-4: 6.0
    agent-5: 11.0
  sampler_perf:
    mean_env_wait_ms: 28.22830465538877
    mean_inference_ms: 15.046999607771939
    mean_processing_ms: 73.24208109226221
  time_since_restore: 56646.99197220802
  time_this_iter_s: 145.47609615325928
  time_total_s: 145583.67010188103
  timestamp: 1637419718
  timesteps_since_restore: 37440000
  timesteps_this_iter: 96000
  timesteps_total: 99840000
  training_iteration: 1040
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1040 |           145584 | 99840000 |   133.81 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 7.67
    apples_agent-0_min: 1
    apples_agent-1_max: 45
    apples_agent-1_mean: 3.96
    apples_agent-1_min: 0
    apples_agent-2_max: 62
    apples_agent-2_mean: 29.95
    apples_agent-2_min: 17
    apples_agent-3_max: 34
    apples_agent-3_mean: 9.28
    apples_agent-3_min: 2
    apples_agent-4_max: 26
    apples_agent-4_mean: 10.38
    apples_agent-4_min: 2
    apples_agent-5_max: 34
    apples_agent-5_mean: 6.5
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 9
    cleaning_beam_agent-0_mean: 0.97
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 238
    cleaning_beam_agent-1_mean: 212.84
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 42
    cleaning_beam_agent-2_mean: 21.81
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 9.64
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 10
    cleaning_beam_agent-4_mean: 3.61
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 11
    cleaning_beam_agent-5_mean: 3.53
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-51-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 179.0
  episode_reward_mean: 136.4
  episode_reward_min: 94.0
  episodes_this_iter: 96
  episodes_total: 99936
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11862.801
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3170264661312103
        entropy_coeff: 0.0017600000137463212
        kl: 0.001092300284653902
        model: {}
        policy_loss: -0.00182049791328609
        total_loss: -0.00225811661221087
        vf_explained_var: 0.010525360703468323
        vf_loss: 1.2035095691680908
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.19076943397521973
        entropy_coeff: 0.0017600000137463212
        kl: 0.001115290098823607
        model: {}
        policy_loss: -0.0015368026215583086
        total_loss: -0.0018243188969790936
        vf_explained_var: 0.09537960588932037
        vf_loss: 0.48239797353744507
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40174075961112976
        entropy_coeff: 0.0017600000137463212
        kl: 0.00149670266546309
        model: {}
        policy_loss: -0.0018703246023505926
        total_loss: -0.002223856979981065
        vf_explained_var: 0.010056987404823303
        vf_loss: 3.535306453704834
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3748413026332855
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014259596355259418
        model: {}
        policy_loss: -0.0015101819299161434
        total_loss: -0.001965775154531002
        vf_explained_var: 0.017230674624443054
        vf_loss: 2.04128098487854
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36746877431869507
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005117023829370737
        model: {}
        policy_loss: -0.001183176413178444
        total_loss: -0.0017275456339120865
        vf_explained_var: 0.00794951617717743
        vf_loss: 1.0237103700637817
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4781745970249176
        entropy_coeff: 0.0017600000137463212
        kl: 0.001377514679916203
        model: {}
        policy_loss: -0.0017180934082716703
        total_loss: -0.002424040110781789
        vf_explained_var: -0.0008780211210250854
        vf_loss: 1.3563697338104248
    load_time_ms: 14151.349
    num_steps_sampled: 99936000
    num_steps_trained: 99936000
    sample_time_ms: 118928.082
    update_time_ms: 15.401
  iterations_since_restore: 391
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.729999999999997
    ram_util_percent: 12.468095238095238
  pid: 27065
  policy_reward_max:
    agent-0: 29.0
    agent-1: 21.0
    agent-2: 69.0
    agent-3: 44.0
    agent-4: 32.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 18.4
    agent-1: 8.57
    agent-2: 46.98
    agent-3: 27.09
    agent-4: 15.59
    agent-5: 19.77
  policy_reward_min:
    agent-0: 8.0
    agent-1: 2.0
    agent-2: 29.0
    agent-3: 14.0
    agent-4: 9.0
    agent-5: 6.0
  sampler_perf:
    mean_env_wait_ms: 28.22867737047727
    mean_inference_ms: 15.047027495606388
    mean_processing_ms: 73.2417176784759
  time_since_restore: 56791.83068013191
  time_this_iter_s: 144.83870792388916
  time_total_s: 145728.50880980492
  timestamp: 1637419865
  timesteps_since_restore: 37536000
  timesteps_this_iter: 96000
  timesteps_total: 99936000
  training_iteration: 1041
  trial_id: '00000'
  
[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.4:27065 |   1041 |           145729 | 99936000 |    136.4 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=27065)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f9a164b15f8> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 8.54
    apples_agent-0_min: 2
    apples_agent-1_max: 36
    apples_agent-1_mean: 3.9
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 31.02
    apples_agent-2_min: 17
    apples_agent-3_max: 31
    apples_agent-3_mean: 9.39
    apples_agent-3_min: 3
    apples_agent-4_max: 20
    apples_agent-4_mean: 10.52
    apples_agent-4_min: 3
    apples_agent-5_max: 28
    apples_agent-5_mean: 6.23
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 15
    cleaning_beam_agent-0_mean: 1.32
    cleaning_beam_agent-0_min: 0
    cleaning_beam_agent-1_max: 238
    cleaning_beam_agent-1_mean: 211.94
    cleaning_beam_agent-1_min: 170
    cleaning_beam_agent-2_max: 48
    cleaning_beam_agent-2_mean: 22.98
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 37
    cleaning_beam_agent-3_mean: 9.93
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 14
    cleaning_beam_agent-4_mean: 3.64
    cleaning_beam_agent-4_min: 0
    cleaning_beam_agent-5_max: 8
    cleaning_beam_agent-5_mean: 3.32
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-20_09-53-30
  done: true
  episode_len_mean: 1000.0
  episode_reward_max: 183.0
  episode_reward_mean: 136.93
  episode_reward_min: 91.0
  episodes_this_iter: 96
  episodes_total: 100032
  experiment_id: 3c6b6aeed9554b029d33c73dbbd6e5a7
  experiment_tag: '0'
  hostname: gpu004
  info:
    grad_time_ms: 11861.613
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.31096982955932617
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017900706734508276
        model: {}
        policy_loss: -0.002405398990958929
        total_loss: -0.0028209590818732977
        vf_explained_var: 0.009220793843269348
        vf_loss: 1.317444086074829
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.1893119364976883
        entropy_coeff: 0.0017600000137463212
        kl: 0.001164869638159871
        model: {}
        policy_loss: -0.0014838587958365679
        total_loss: -0.0017613109666854143
        vf_explained_var: 0.0904177874326706
        vf_loss: 0.5573598146438599
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39954861998558044
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016385316848754883
        model: {}
        policy_loss: -0.0019368133507668972
        total_loss: -0.0022410505916923285
        vf_explained_var: 0.01522737741470337
        vf_loss: 3.989687442779541
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.36816853284835815
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014207714702934027
        model: {}
        policy_loss: -0.00144968181848526
        total_loss: -0.0019141368102282286
        vf_explained_var: 0.002110317349433899
        vf_loss: 1.8352203369140625
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3585541546344757
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009111163089983165
        model: {}
        policy_loss: -0.0011665928177535534
        total_loss: -0.001693435711786151
        vf_explained_var: 0.011795580387115479
        vf_loss: 1.0421139001846313
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4709335267543793
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023453061003237963
        model: {}
        policy_loss: -0.0016902422066777945
        total_loss: -0.0023619229905307293
        vf_explained_var: 0.01049824059009552
        vf_loss: 1.571621298789978
    load_time_ms: 14167.119
    num_steps_sampled: 100032000
    num_steps_trained: 100032000
    sample_time_ms: 118798.999
    update_time_ms: 15.628
  iterations_since_restore: 392
  node_ip: 172.17.8.4
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.00922330097088
    ram_util_percent: 12.469417475728152
  pid: 27065
  policy_reward_max:
    agent-0: 34.0
    agent-1: 23.0
    agent-2: 68.0
    agent-3: 39.0
    agent-4: 26.0
    agent-5: 34.0
  policy_reward_mean:
    agent-0: 18.43
    agent-1: 8.81
    agent-2: 47.94
    agent-3: 25.89
    agent-4: 15.95
    agent-5: 19.91
  policy_reward_min:
    agent-0: 5.0
    agent-1: 2.0
    agent-2: 27.0
    agent-3: 8.0
    agent-4: 2.0
    agent-5: 8.0
  sampler_perf:
    mean_env_wait_ms: 28.22872277977128
    mean_inference_ms: 15.046825518837808
    mean_processing_ms: 73.24069520435845
  time_since_restore: 56936.357962846756
  time_this_iter_s: 144.52728271484375
  time_total_s: 145873.03609251976
  timestamp: 1637420010
  timesteps_since_restore: 37632000
  timesteps_this_iter: 96000
  timesteps_total: 100032000
  training_iteration: 1042
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/32 CPUs, 0.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 TERMINATED)
+--------------------------------------+------------+-------+--------+------------------+-----------+----------+
| Trial name                           | status     | loc   |   iter |   total time (s) |        ts |   reward |
|--------------------------------------+------------+-------+--------+------------------+-----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | TERMINATED |       |   1042 |           145873 | 100032000 |   136.93 |
+--------------------------------------+------------+-------+--------+------------------+-----------+----------+


== Status ==
Memory usage on this node: 15.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/32 CPUs, 0.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.67 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 TERMINATED)
+--------------------------------------+------------+-------+--------+------------------+-----------+----------+
| Trial name                           | status     | loc   |   iter |   total time (s) |        ts |   reward |
|--------------------------------------+------------+-------+--------+------------------+-----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | TERMINATED |       |   1042 |           145873 | 100032000 |   136.93 |
+--------------------------------------+------------+-------+--------+------------------+-----------+----------+


