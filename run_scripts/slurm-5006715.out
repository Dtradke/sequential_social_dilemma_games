>>> RESTORING FROM:  /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-17_17-38-56m07ou1p0/checkpoint_20
== Status ==
Memory usage on this node: 12.1/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    160 |          22017.4 | 15360000 |    18.37 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m 2021-11-18 02:16:14,315	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=5668)[0m 2021-11-18 02:16:14,331	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=5668)[0m 2021-11-18 02:18:08,654	INFO trainable.py:180 -- _setup took 114.339 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=5668)[0m 2021-11-18 02:18:08,654	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=5668)[0m 2021-11-18 02:18:08,655	WARNING util.py:37 -- Install gputil for GPU system monitoring.
[2m[36m(pid=5668)[0m 2021-11-18 02:18:11,907	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=5668)[0m 2021-11-18 02:18:11,907	INFO trainable.py:423 -- Restored on 172.17.8.42 from checkpoint: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-17_17-39-29lhwwiwf8/tmpxcimkwurrestore_from_object/checkpoint-160
[2m[36m(pid=5668)[0m 2021-11-18 02:18:11,907	INFO trainable.py:430 -- Current state after restoring: {'_iteration': 160, '_timesteps_total': 15360000, '_time_total': 22510.283242464066, '_episodes_total': 15360}
== Status ==
Memory usage on this node: 17.7/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    160 |          22017.4 | 15360000 |    18.37 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 2.25
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 1.0
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 2.1458333333333335
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.5104166666666665
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.2083333333333333
    apples_agent-4_min: 0
    apples_agent-5_max: 51
    apples_agent-5_mean: 1.4895833333333333
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 196
    cleaning_beam_agent-0_mean: 103.125
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 497
    cleaning_beam_agent-1_mean: 283.0729166666667
    cleaning_beam_agent-1_min: 157
    cleaning_beam_agent-2_max: 121
    cleaning_beam_agent-2_mean: 26.0
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 244
    cleaning_beam_agent-3_mean: 93.80208333333333
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 295
    cleaning_beam_agent-4_mean: 99.58333333333333
    cleaning_beam_agent-4_min: 21
    cleaning_beam_agent-5_max: 69
    cleaning_beam_agent-5_mean: 18.927083333333332
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.020833333333333332
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.010416666666666666
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-20-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 50.0
  episode_reward_mean: 17.5
  episode_reward_min: -40.0
  episodes_this_iter: 96
  episodes_total: 15456
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 23017.76
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.6465216279029846
        entropy_coeff: 0.0017600000137463212
        kl: 0.008815992623567581
        model: {}
        policy_loss: -0.022206248715519905
        total_loss: -0.021560074761509895
        vf_explained_var: 0.010171651840209961
        vf_loss: 0.20850950479507446
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.5672916173934937
        entropy_coeff: 0.0017600000137463212
        kl: 0.006450649816542864
        model: {}
        policy_loss: -0.015671782195568085
        total_loss: -0.015360765159130096
        vf_explained_var: 0.03562548756599426
        vf_loss: 0.19322505593299866
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.5559475421905518
        entropy_coeff: 0.0017600000137463212
        kl: 0.007755273953080177
        model: {}
        policy_loss: -0.020177200436592102
        total_loss: -0.019558575004339218
        vf_explained_var: 0.010102033615112305
        vf_loss: 0.4603695571422577
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.6955819725990295
        entropy_coeff: 0.0017600000137463212
        kl: 0.007169209886342287
        model: {}
        policy_loss: -0.017108254134655
        total_loss: -0.016858357936143875
        vf_explained_var: 0.021303370594978333
        vf_loss: 0.4027576148509979
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.7245538234710693
        entropy_coeff: 0.0017600000137463212
        kl: 0.008628442883491516
        model: {}
        policy_loss: -0.02253558859229088
        total_loss: -0.02204611711204052
        vf_explained_var: 0.027554750442504883
        vf_loss: 0.39003270864486694
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.7991513013839722
        entropy_coeff: 0.0017600000137463212
        kl: 0.006832926534116268
        model: {}
        policy_loss: -0.01566632278263569
        total_loss: -0.01569574698805809
        vf_explained_var: 0.01850186288356781
        vf_loss: 0.10498017817735672
    load_time_ms: 17823.174
    num_steps_sampled: 15456000
    num_steps_trained: 15456000
    sample_time_ms: 103012.677
    update_time_ms: 3365.457
  iterations_since_restore: 1
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.020524017467253
    ram_util_percent: 11.92052401746725
  pid: 5668
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 15.0
    agent-4: 16.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.65625
    agent-1: 1.5833333333333333
    agent-2: 4.15625
    agent-3: 3.8645833333333335
    agent-4: 3.8020833333333335
    agent-5: 1.4375
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.09868794618112
    mean_inference_ms: 14.409842548313195
    mean_processing_ms: 58.24695560799572
  time_since_restore: 150.43997311592102
  time_this_iter_s: 150.43997311592102
  time_total_s: 22660.723215579987
  timestamp: 1637220049
  timesteps_since_restore: 96000
  timesteps_this_iter: 96000
  timesteps_total: 15456000
  training_iteration: 161
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    161 |          22660.7 | 15456000 |     17.5 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 2.01
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 0.87
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 2.59
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 2.58
    apples_agent-3_min: 0
    apples_agent-4_max: 51
    apples_agent-4_mean: 1.76
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 0.59
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 203
    cleaning_beam_agent-0_mean: 102.61
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 421
    cleaning_beam_agent-1_mean: 254.38
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 96
    cleaning_beam_agent-2_mean: 26.34
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 210
    cleaning_beam_agent-3_mean: 85.21
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 363
    cleaning_beam_agent-4_mean: 109.4
    cleaning_beam_agent-4_min: 35
    cleaning_beam_agent-5_max: 68
    cleaning_beam_agent-5_mean: 17.38
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-22-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 76.0
  episode_reward_mean: 17.65
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 15552
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 17716.752
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.641643762588501
        entropy_coeff: 0.0017600000137463212
        kl: 0.008793298155069351
        model: {}
        policy_loss: -0.022105827927589417
        total_loss: -0.021458089351654053
        vf_explained_var: 0.010293558239936829
        vf_loss: 0.18370237946510315
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.5620855093002319
        entropy_coeff: 0.0017600000137463212
        kl: 0.006393304094672203
        model: {}
        policy_loss: -0.014017990790307522
        total_loss: -0.013712207786738873
        vf_explained_var: 0.03131188452243805
        vf_loss: 0.16392256319522858
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.5542818903923035
        entropy_coeff: 0.0017600000137463212
        kl: 0.007423329167068005
        model: {}
        policy_loss: -0.019879799336194992
        total_loss: -0.019329989328980446
        vf_explained_var: 0.008917197585105896
        vf_loss: 0.40680956840515137
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.6803759932518005
        entropy_coeff: 0.0017600000137463212
        kl: 0.00673668971285224
        model: {}
        policy_loss: -0.015302409417927265
        total_loss: -0.015108122490346432
        vf_explained_var: 0.00851219892501831
        vf_loss: 0.44410836696624756
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.7341362237930298
        entropy_coeff: 0.0017600000137463212
        kl: 0.008296024054288864
        model: {}
        policy_loss: -0.02197059616446495
        total_loss: -0.021562065929174423
        vf_explained_var: 0.03179578483104706
        vf_loss: 0.4140625596046448
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.787087082862854
        entropy_coeff: 0.0017600000137463212
        kl: 0.006480633281171322
        model: {}
        policy_loss: -0.013945174403488636
        total_loss: -0.014026040211319923
        vf_explained_var: 0.023625969886779785
        vf_loss: 0.08279868215322495
    load_time_ms: 15866.48
    num_steps_sampled: 15552000
    num_steps_trained: 15552000
    sample_time_ms: 103219.014
    update_time_ms: 1691.108
  iterations_since_restore: 2
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.775675675675675
    ram_util_percent: 13.422162162162165
  pid: 5668
  policy_reward_max:
    agent-0: 11.0
    agent-1: 16.0
    agent-2: 16.0
    agent-3: 24.0
    agent-4: 16.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.72
    agent-1: 1.75
    agent-2: 4.07
    agent-3: 3.87
    agent-4: 4.05
    agent-5: 1.19
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.155657262599064
    mean_inference_ms: 13.8273145293046
    mean_processing_ms: 58.48414521894343
  time_since_restore: 280.31500458717346
  time_this_iter_s: 129.87503147125244
  time_total_s: 22790.59824705124
  timestamp: 1637220179
  timesteps_since_restore: 192000
  timesteps_this_iter: 96000
  timesteps_total: 15552000
  training_iteration: 162
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    162 |          22790.6 | 15552000 |    17.65 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 2.16
    apples_agent-0_min: 0
    apples_agent-1_max: 3
    apples_agent-1_mean: 0.76
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 1.73
    apples_agent-2_min: 0
    apples_agent-3_max: 8
    apples_agent-3_mean: 1.99
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 1.03
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 213
    cleaning_beam_agent-0_mean: 100.14
    cleaning_beam_agent-0_min: 41
    cleaning_beam_agent-1_max: 468
    cleaning_beam_agent-1_mean: 261.33
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 94
    cleaning_beam_agent-2_mean: 30.4
    cleaning_beam_agent-2_min: 7
    cleaning_beam_agent-3_max: 213
    cleaning_beam_agent-3_mean: 99.65
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 222
    cleaning_beam_agent-4_mean: 107.36
    cleaning_beam_agent-4_min: 25
    cleaning_beam_agent-5_max: 66
    cleaning_beam_agent-5_mean: 17.02
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-25-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 13.8
  episode_reward_min: -93.0
  episodes_this_iter: 96
  episodes_total: 15648
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 15968.549
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.6477349996566772
        entropy_coeff: 0.0017600000137463212
        kl: 0.005183707922697067
        model: {}
        policy_loss: -0.011277449317276478
        total_loss: -0.01122452411800623
        vf_explained_var: 0.005924329161643982
        vf_loss: 1.56198251247406
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.5542430877685547
        entropy_coeff: 0.0017600000137463212
        kl: 0.006228887476027012
        model: {}
        policy_loss: -0.015399469062685966
        total_loss: -0.015117011032998562
        vf_explained_var: 0.03139685094356537
        vf_loss: 0.12150201201438904
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.5567245483398438
        entropy_coeff: 0.0017600000137463212
        kl: 0.007325858809053898
        model: {}
        policy_loss: -0.01934349350631237
        total_loss: -0.018822187557816505
        vf_explained_var: 0.007662266492843628
        vf_loss: 0.3597155809402466
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.6976230144500732
        entropy_coeff: 0.0017600000137463212
        kl: 0.006007864139974117
        model: {}
        policy_loss: -0.009579069912433624
        total_loss: -0.008930074982345104
        vf_explained_var: 0.00450935959815979
        vf_loss: 6.752392768859863
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.7360411882400513
        entropy_coeff: 0.0017600000137463212
        kl: 0.008727913722395897
        model: {}
        policy_loss: -0.022804420441389084
        total_loss: -0.022322872653603554
        vf_explained_var: 0.03131631016731262
        vf_loss: 0.3139708638191223
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.7916306257247925
        entropy_coeff: 0.0017600000137463212
        kl: 0.005122792441397905
        model: {}
        policy_loss: -0.007009394932538271
        total_loss: -0.007253106217831373
        vf_explained_var: 0.010092347860336304
        vf_loss: 1.2499973773956299
    load_time_ms: 15199.351
    num_steps_sampled: 15648000
    num_steps_trained: 15648000
    sample_time_ms: 102351.872
    update_time_ms: 1132.972
  iterations_since_restore: 3
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 30.512707182320444
    ram_util_percent: 13.42817679558011
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 12.0
    agent-2: 17.0
    agent-3: 17.0
    agent-4: 11.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.12
    agent-1: 1.61
    agent-2: 3.77
    agent-3: 1.72
    agent-4: 3.71
    agent-5: 0.87
  policy_reward_min:
    agent-0: -46.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: -98.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 23.11847048450833
    mean_inference_ms: 13.485993780430006
    mean_processing_ms: 58.31238828028553
  time_since_restore: 407.39582419395447
  time_this_iter_s: 127.080819606781
  time_total_s: 22917.67906665802
  timestamp: 1637220306
  timesteps_since_restore: 288000
  timesteps_this_iter: 96000
  timesteps_total: 15648000
  training_iteration: 163
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    163 |          22917.7 | 15648000 |     13.8 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 1.79
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.82
    apples_agent-1_min: 0
    apples_agent-2_max: 8
    apples_agent-2_mean: 1.84
    apples_agent-2_min: 0
    apples_agent-3_max: 30
    apples_agent-3_mean: 3.26
    apples_agent-3_min: 0
    apples_agent-4_max: 32
    apples_agent-4_mean: 2.2
    apples_agent-4_min: 0
    apples_agent-5_max: 66
    apples_agent-5_mean: 1.59
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 196
    cleaning_beam_agent-0_mean: 92.64
    cleaning_beam_agent-0_min: 39
    cleaning_beam_agent-1_max: 474
    cleaning_beam_agent-1_mean: 269.94
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 91
    cleaning_beam_agent-2_mean: 27.91
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 209
    cleaning_beam_agent-3_mean: 92.15
    cleaning_beam_agent-3_min: 37
    cleaning_beam_agent-4_max: 234
    cleaning_beam_agent-4_mean: 100.41
    cleaning_beam_agent-4_min: 30
    cleaning_beam_agent-5_max: 64
    cleaning_beam_agent-5_mean: 19.59
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-27-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 16.29
  episode_reward_min: -93.0
  episodes_this_iter: 96
  episodes_total: 15744
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 15079.723
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.6428314447402954
        entropy_coeff: 0.0017600000137463212
        kl: 0.008144298568367958
        model: {}
        policy_loss: -0.021160196512937546
        total_loss: -0.020635124295949936
        vf_explained_var: 0.0035795867443084717
        vf_loss: 0.27600836753845215
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.5523616075515747
        entropy_coeff: 0.0017600000137463212
        kl: 0.003947591409087181
        model: {}
        policy_loss: -0.0073345196433365345
        total_loss: -0.007372541818767786
        vf_explained_var: 0.011724531650543213
        vf_loss: 1.4461543560028076
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.5682274103164673
        entropy_coeff: 0.0017600000137463212
        kl: 0.005829536821693182
        model: {}
        policy_loss: -0.012018809095025063
        total_loss: -0.011547209694981575
        vf_explained_var: 0.0172012597322464
        vf_loss: 3.0577239990234375
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.7023313641548157
        entropy_coeff: 0.0017600000137463212
        kl: 0.007189207710325718
        model: {}
        policy_loss: -0.017059866338968277
        total_loss: -0.016821635887026787
        vf_explained_var: -0.006815001368522644
        vf_loss: 0.36489635705947876
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.72686767578125
        entropy_coeff: 0.0017600000137463212
        kl: 0.006586406379938126
        model: {}
        policy_loss: -0.013336863368749619
        total_loss: -0.013136574998497963
        vf_explained_var: 0.02054046094417572
        vf_loss: 1.6229524612426758
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.8045470118522644
        entropy_coeff: 0.0017600000137463212
        kl: 0.007375996559858322
        model: {}
        policy_loss: -0.015452385880053043
        total_loss: -0.015379691496491432
        vf_explained_var: 0.023230507969856262
        vf_loss: 0.13499347865581512
    load_time_ms: 14862.294
    num_steps_sampled: 15744000
    num_steps_trained: 15744000
    sample_time_ms: 102007.322
    update_time_ms: 855.393
  iterations_since_restore: 4
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.78901098901099
    ram_util_percent: 13.251098901098905
  pid: 5668
  policy_reward_max:
    agent-0: 11.0
    agent-1: 10.0
    agent-2: 20.0
    agent-3: 12.0
    agent-4: 12.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.16
    agent-1: 1.31
    agent-2: 3.57
    agent-3: 2.97
    agent-4: 3.62
    agent-5: 1.66
  policy_reward_min:
    agent-0: -1.0
    agent-1: -46.0
    agent-2: -50.0
    agent-3: -98.0
    agent-4: -47.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.05907808701872
    mean_inference_ms: 13.316983488601124
    mean_processing_ms: 58.088694586335585
  time_since_restore: 534.789302110672
  time_this_iter_s: 127.39347791671753
  time_total_s: 23045.072544574738
  timestamp: 1637220433
  timesteps_since_restore: 384000
  timesteps_this_iter: 96000
  timesteps_total: 15744000
  training_iteration: 164
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    164 |          23045.1 | 15744000 |    16.29 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 1.66
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 1.14
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 1.82
    apples_agent-2_min: 0
    apples_agent-3_max: 8
    apples_agent-3_mean: 1.97
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.8
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 0.88
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 183
    cleaning_beam_agent-0_mean: 94.5
    cleaning_beam_agent-0_min: 31
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 249.75
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 80
    cleaning_beam_agent-2_mean: 27.62
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 177
    cleaning_beam_agent-3_mean: 92.77
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 284
    cleaning_beam_agent-4_mean: 99.86
    cleaning_beam_agent-4_min: 25
    cleaning_beam_agent-5_max: 70
    cleaning_beam_agent-5_mean: 18.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-29-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 40.0
  episode_reward_mean: 13.21
  episode_reward_min: -97.0
  episodes_this_iter: 96
  episodes_total: 15840
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 14538.874
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.6328266859054565
        entropy_coeff: 0.0017600000137463212
        kl: 0.008198540657758713
        model: {}
        policy_loss: -0.019589710980653763
        total_loss: -0.019042421132326126
        vf_explained_var: 0.006114020943641663
        vf_loss: 0.21353638172149658
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002775744069367647
        entropy: 0.5590484142303467
        entropy_coeff: 0.0017600000137463212
        kl: 0.00870364997535944
        model: {}
        policy_loss: -0.01014819834381342
        total_loss: -0.01008745189756155
        vf_explained_var: 0.010978445410728455
        vf_loss: 1.7430715560913086
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.5641458034515381
        entropy_coeff: 0.0017600000137463212
        kl: 0.00826382264494896
        model: {}
        policy_loss: -0.018668118864297867
        total_loss: -0.017968282103538513
        vf_explained_var: -0.007987424731254578
        vf_loss: 0.39970818161964417
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.7071582674980164
        entropy_coeff: 0.0017600000137463212
        kl: 0.0047949980944395065
        model: {}
        policy_loss: -0.00797714851796627
        total_loss: -0.008102718740701675
        vf_explained_var: 0.004703104496002197
        vf_loss: 1.6002881526947021
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.7365262508392334
        entropy_coeff: 0.0017600000137463212
        kl: 0.005196369253098965
        model: {}
        policy_loss: -0.01055264100432396
        total_loss: -0.010247312486171722
        vf_explained_var: 0.006104975938796997
        vf_loss: 5.623405456542969
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.796440601348877
        entropy_coeff: 0.0017600000137463212
        kl: 0.007191379554569721
        model: {}
        policy_loss: -0.014849751256406307
        total_loss: -0.014803461730480194
        vf_explained_var: -0.008310288190841675
        vf_loss: 0.09747716784477234
    load_time_ms: 14583.028
    num_steps_sampled: 15840000
    num_steps_trained: 15840000
    sample_time_ms: 101605.36
    update_time_ms: 687.656
  iterations_since_restore: 5
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.775555555555556
    ram_util_percent: 13.556666666666665
  pid: 5668
  policy_reward_max:
    agent-0: 9.0
    agent-1: 10.0
    agent-2: 14.0
    agent-3: 13.0
    agent-4: 13.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.65
    agent-1: 0.72
    agent-2: 3.62
    agent-3: 2.59
    agent-4: 2.33
    agent-5: 1.3
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: -50.0
    agent-4: -100.0
    agent-5: -2.0
  sampler_perf:
    mean_env_wait_ms: 23.037423479805273
    mean_inference_ms: 13.211771657626352
    mean_processing_ms: 57.98528454204688
  time_since_restore: 660.7724390029907
  time_this_iter_s: 125.98313689231873
  time_total_s: 23171.055681467056
  timestamp: 1637220560
  timesteps_since_restore: 480000
  timesteps_this_iter: 96000
  timesteps_total: 15840000
  training_iteration: 165
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    165 |          23171.1 | 15840000 |    13.21 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 1.78
    apples_agent-0_min: 0
    apples_agent-1_max: 3
    apples_agent-1_mean: 0.72
    apples_agent-1_min: 0
    apples_agent-2_max: 37
    apples_agent-2_mean: 1.97
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.34
    apples_agent-3_min: 0
    apples_agent-4_max: 40
    apples_agent-4_mean: 1.4
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 0.87
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 179
    cleaning_beam_agent-0_mean: 95.81
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 448
    cleaning_beam_agent-1_mean: 245.32
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 131
    cleaning_beam_agent-2_mean: 30.5
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 191
    cleaning_beam_agent-3_mean: 88.84
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 278
    cleaning_beam_agent-4_mean: 101.66
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 17.03
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-31-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 44.0
  episode_reward_mean: 16.55
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 15936
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 14197.524
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.6372929215431213
        entropy_coeff: 0.0017600000137463212
        kl: 0.008371601812541485
        model: {}
        policy_loss: -0.02135588228702545
        total_loss: -0.020782452076673508
        vf_explained_var: 0.006303876638412476
        vf_loss: 0.2074684500694275
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002715839946176857
        entropy: 0.5471562147140503
        entropy_coeff: 0.0017600000137463212
        kl: 0.0073237246833741665
        model: {}
        policy_loss: -0.014955483376979828
        total_loss: -0.01517290435731411
        vf_explained_var: 0.015470296144485474
        vf_loss: 0.1320016086101532
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.5617583394050598
        entropy_coeff: 0.0017600000137463212
        kl: 0.008539753034710884
        model: {}
        policy_loss: -0.02172379568219185
        total_loss: -0.020966995507478714
        vf_explained_var: -0.0030075907707214355
        vf_loss: 0.3754441738128662
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002715839946176857
        entropy: 0.6937850117683411
        entropy_coeff: 0.0017600000137463212
        kl: 0.00785474106669426
        model: {}
        policy_loss: -0.01587841473519802
        total_loss: -0.01628406159579754
        vf_explained_var: 0.0005767494440078735
        vf_loss: 0.29940423369407654
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.7264990210533142
        entropy_coeff: 0.0017600000137463212
        kl: 0.008840997703373432
        model: {}
        policy_loss: -0.022284071892499924
        total_loss: -0.021761763840913773
        vf_explained_var: 0.014129012823104858
        vf_loss: 0.3274780809879303
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.798801064491272
        entropy_coeff: 0.0017600000137463212
        kl: 0.007167177740484476
        model: {}
        policy_loss: -0.01465081050992012
        total_loss: -0.014614362269639969
        vf_explained_var: 0.01654568314552307
        vf_loss: 0.08902694284915924
    load_time_ms: 14440.209
    num_steps_sampled: 15936000
    num_steps_trained: 15936000
    sample_time_ms: 101464.643
    update_time_ms: 575.993
  iterations_since_restore: 6
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.74088397790055
    ram_util_percent: 13.524309392265193
  pid: 5668
  policy_reward_max:
    agent-0: 8.0
    agent-1: 10.0
    agent-2: 15.0
    agent-3: 15.0
    agent-4: 12.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.78
    agent-1: 1.71
    agent-2: 4.0
    agent-3: 3.23
    agent-4: 3.52
    agent-5: 1.31
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 23.042890655544774
    mean_inference_ms: 13.14343721239359
    mean_processing_ms: 57.967213546854595
  time_since_restore: 787.8805868625641
  time_this_iter_s: 127.10814785957336
  time_total_s: 23298.16382932663
  timestamp: 1637220687
  timesteps_since_restore: 576000
  timesteps_this_iter: 96000
  timesteps_total: 15936000
  training_iteration: 166
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    166 |          23298.2 | 15936000 |    16.55 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 2.02
    apples_agent-0_min: 0
    apples_agent-1_max: 23
    apples_agent-1_mean: 1.29
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 2.04
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.51
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 1.39
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.08
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 210
    cleaning_beam_agent-0_mean: 92.39
    cleaning_beam_agent-0_min: 34
    cleaning_beam_agent-1_max: 391
    cleaning_beam_agent-1_mean: 242.13
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 128
    cleaning_beam_agent-2_mean: 29.41
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 191
    cleaning_beam_agent-3_mean: 82.67
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 365
    cleaning_beam_agent-4_mean: 102.65
    cleaning_beam_agent-4_min: 19
    cleaning_beam_agent-5_max: 73
    cleaning_beam_agent-5_mean: 16.81
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 9
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-33-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 11.92
  episode_reward_min: -749.0
  episodes_this_iter: 96
  episodes_total: 16032
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 13938.572
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.6403571963310242
        entropy_coeff: 0.0017600000137463212
        kl: 0.003986577037721872
        model: {}
        policy_loss: -0.006977473385632038
        total_loss: 0.0013798493891954422
        vf_explained_var: 0.005558982491493225
        vf_loss: 86.870361328125
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002655936114024371
        entropy: 0.5312244892120361
        entropy_coeff: 0.0017600000137463212
        kl: 0.007321917451918125
        model: {}
        policy_loss: -0.013549542054533958
        total_loss: -0.013731489889323711
        vf_explained_var: 0.0172145813703537
        vf_loss: 0.20813164114952087
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.5447425842285156
        entropy_coeff: 0.0017600000137463212
        kl: 0.007346221245825291
        model: {}
        policy_loss: -0.01967756822705269
        total_loss: -0.01912068948149681
        vf_explained_var: 0.007541432976722717
        vf_loss: 0.4638088643550873
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002655936114024371
        entropy: 0.7066906094551086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0050480663776397705
        model: {}
        policy_loss: -0.005569095257669687
        total_loss: -0.0022436250001192093
        vf_explained_var: 3.427267074584961e-05
        vf_loss: 40.6444206237793
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.7343363761901855
        entropy_coeff: 0.0017600000137463212
        kl: 0.009310642257332802
        model: {}
        policy_loss: -0.022239932790398598
        total_loss: -0.02162785269320011
        vf_explained_var: 0.03063538670539856
        vf_loss: 0.42381763458251953
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.7815643548965454
        entropy_coeff: 0.0017600000137463212
        kl: 0.007893151603639126
        model: {}
        policy_loss: -0.014201752841472626
        total_loss: -0.013980403542518616
        vf_explained_var: 0.02281181514263153
        vf_loss: 0.18271316587924957
    load_time_ms: 14338.3
    num_steps_sampled: 16032000
    num_steps_trained: 16032000
    sample_time_ms: 101257.469
    update_time_ms: 496.057
  iterations_since_restore: 7
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.280555555555555
    ram_util_percent: 13.517222222222221
  pid: 5668
  policy_reward_max:
    agent-0: 18.0
    agent-1: 14.0
    agent-2: 16.0
    agent-3: 12.0
    agent-4: 12.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: -1.14
    agent-1: 1.98
    agent-2: 4.43
    agent-3: 0.68
    agent-4: 4.32
    agent-5: 1.65
  policy_reward_min:
    agent-0: -449.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -295.0
    agent-4: 0.0
    agent-5: -9.0
  sampler_perf:
    mean_env_wait_ms: 23.04607323888501
    mean_inference_ms: 13.096162380126714
    mean_processing_ms: 58.02528401879723
  time_since_restore: 914.1255195140839
  time_this_iter_s: 126.24493265151978
  time_total_s: 23424.40876197815
  timestamp: 1637220813
  timesteps_since_restore: 672000
  timesteps_this_iter: 96000
  timesteps_total: 16032000
  training_iteration: 167
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    167 |          23424.4 | 16032000 |    11.92 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 1.72
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.9
    apples_agent-1_min: 0
    apples_agent-2_max: 41
    apples_agent-2_mean: 2.21
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.53
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.88
    apples_agent-4_min: 0
    apples_agent-5_max: 40
    apples_agent-5_mean: 1.29
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 226
    cleaning_beam_agent-0_mean: 94.73
    cleaning_beam_agent-0_min: 43
    cleaning_beam_agent-1_max: 387
    cleaning_beam_agent-1_mean: 239.81
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 137
    cleaning_beam_agent-2_mean: 30.39
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 86.34
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 243
    cleaning_beam_agent-4_mean: 111.5
    cleaning_beam_agent-4_min: 28
    cleaning_beam_agent-5_max: 58
    cleaning_beam_agent-5_mean: 18.1
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-35-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 14.31
  episode_reward_min: -43.0
  episodes_this_iter: 96
  episodes_total: 16128
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 13746.644
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00025960319908335805
        entropy: 0.632191002368927
        entropy_coeff: 0.0017600000137463212
        kl: 0.010804057121276855
        model: {}
        policy_loss: -0.021337663754820824
        total_loss: -0.021346675232052803
        vf_explained_var: -0.1972033679485321
        vf_loss: 0.2324582189321518
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00025960319908335805
        entropy: 0.545357346534729
        entropy_coeff: 0.0017600000137463212
        kl: 0.00684861745685339
        model: {}
        policy_loss: -0.009288271889090538
        total_loss: -0.009290511719882488
        vf_explained_var: 0.008174821734428406
        vf_loss: 2.7272820472717285
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.553902268409729
        entropy_coeff: 0.0017600000137463212
        kl: 0.007760684937238693
        model: {}
        policy_loss: -0.021092357113957405
        total_loss: -0.020482083782553673
        vf_explained_var: 0.006213381886482239
        vf_loss: 0.33006197214126587
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00025960319908335805
        entropy: 0.7167434692382812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0073769586160779
        model: {}
        policy_loss: -0.009163794107735157
        total_loss: -0.00938793271780014
        vf_explained_var: -0.005058303475379944
        vf_loss: 2.9963388442993164
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.7199455499649048
        entropy_coeff: 0.0017600000137463212
        kl: 0.008533256128430367
        model: {}
        policy_loss: -0.021312270313501358
        total_loss: -0.020841442048549652
        vf_explained_var: 0.01650714874267578
        vf_loss: 0.3127625584602356
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.7928597927093506
        entropy_coeff: 0.0017600000137463212
        kl: 0.004802902229130268
        model: {}
        policy_loss: -0.00720446091145277
        total_loss: -0.007498056627810001
        vf_explained_var: 0.024718865752220154
        vf_loss: 1.4125572443008423
    load_time_ms: 14257.045
    num_steps_sampled: 16128000
    num_steps_trained: 16128000
    sample_time_ms: 101107.812
    update_time_ms: 436.268
  iterations_since_restore: 8
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.130726256983237
    ram_util_percent: 13.530167597765363
  pid: 5668
  policy_reward_max:
    agent-0: 10.0
    agent-1: 9.0
    agent-2: 10.0
    agent-3: 16.0
    agent-4: 14.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 2.49
    agent-1: 1.02
    agent-2: 3.9
    agent-3: 2.36
    agent-4: 3.4
    agent-5: 1.14
  policy_reward_min:
    agent-0: -1.0
    agent-1: -47.0
    agent-2: 0.0
    agent-3: -48.0
    agent-4: 0.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 23.01404169368374
    mean_inference_ms: 13.054827315107923
    mean_processing_ms: 57.970280281727064
  time_since_restore: 1040.4066588878632
  time_this_iter_s: 126.2811393737793
  time_total_s: 23550.68990135193
  timestamp: 1637220940
  timesteps_since_restore: 768000
  timesteps_this_iter: 96000
  timesteps_total: 16128000
  training_iteration: 168
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    168 |          23550.7 | 16128000 |    14.31 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 1.82
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 1.46
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 2.35
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.54
    apples_agent-3_min: 0
    apples_agent-4_max: 81
    apples_agent-4_mean: 2.17
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.06
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 212
    cleaning_beam_agent-0_mean: 96.34
    cleaning_beam_agent-0_min: 38
    cleaning_beam_agent-1_max: 400
    cleaning_beam_agent-1_mean: 243.74
    cleaning_beam_agent-1_min: 152
    cleaning_beam_agent-2_max: 161
    cleaning_beam_agent-2_mean: 34.17
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 196
    cleaning_beam_agent-3_mean: 96.39
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 249
    cleaning_beam_agent-4_mean: 107.32
    cleaning_beam_agent-4_min: 29
    cleaning_beam_agent-5_max: 45
    cleaning_beam_agent-5_mean: 15.69
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-37-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 16.26
  episode_reward_min: -78.0
  episodes_this_iter: 96
  episodes_total: 16224
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 13588.017
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000253612786764279
        entropy: 0.6384021639823914
        entropy_coeff: 0.0017600000137463212
        kl: 0.007204565219581127
        model: {}
        policy_loss: -0.011480187065899372
        total_loss: -0.011735305190086365
        vf_explained_var: -0.04544854164123535
        vf_loss: 1.4801160097122192
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000253612786764279
        entropy: 0.5436661839485168
        entropy_coeff: 0.0017600000137463212
        kl: 0.007664570584893227
        model: {}
        policy_loss: -0.015561388805508614
        total_loss: -0.015732984989881516
        vf_explained_var: 0.0038535743951797485
        vf_loss: 0.1880190223455429
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.5584167242050171
        entropy_coeff: 0.0017600000137463212
        kl: 0.006797785870730877
        model: {}
        policy_loss: -0.016893619671463966
        total_loss: -0.016461452469229698
        vf_explained_var: 0.021662414073944092
        vf_loss: 0.5542768239974976
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000253612786764279
        entropy: 0.7050715684890747
        entropy_coeff: 0.0017600000137463212
        kl: 0.008043225854635239
        model: {}
        policy_loss: -0.015486132353544235
        total_loss: -0.015891049057245255
        vf_explained_var: -0.013404369354248047
        vf_loss: 0.31688880920410156
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.726102352142334
        entropy_coeff: 0.0017600000137463212
        kl: 0.00846921093761921
        model: {}
        policy_loss: -0.020842717960476875
        total_loss: -0.020394062623381615
        vf_explained_var: 0.030135110020637512
        vf_loss: 0.3275405466556549
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000253612786764279
        entropy: 0.7754297256469727
        entropy_coeff: 0.0017600000137463212
        kl: 0.007739679422229528
        model: {}
        policy_loss: -0.012963701039552689
        total_loss: -0.013536801561713219
        vf_explained_var: 0.009059369564056396
        vf_loss: 0.17687667906284332
    load_time_ms: 14169.437
    num_steps_sampled: 16224000
    num_steps_trained: 16224000
    sample_time_ms: 100987.184
    update_time_ms: 389.625
  iterations_since_restore: 9
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.743333333333336
    ram_util_percent: 13.520555555555555
  pid: 5668
  policy_reward_max:
    agent-0: 10.0
    agent-1: 9.0
    agent-2: 27.0
    agent-3: 16.0
    agent-4: 12.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 2.0
    agent-1: 2.12
    agent-2: 3.57
    agent-3: 3.38
    agent-4: 3.83
    agent-5: 1.36
  policy_reward_min:
    agent-0: -41.0
    agent-1: 0.0
    agent-2: -42.0
    agent-3: 0.0
    agent-4: -1.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 22.99103149103418
    mean_inference_ms: 13.02544994284482
    mean_processing_ms: 57.891705652830254
  time_since_restore: 1166.3509860038757
  time_this_iter_s: 125.94432711601257
  time_total_s: 23676.63422846794
  timestamp: 1637221066
  timesteps_since_restore: 864000
  timesteps_this_iter: 96000
  timesteps_total: 16224000
  training_iteration: 169
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    169 |          23676.6 | 16224000 |    16.26 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 1.7
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.73
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 2.45
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.52
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.11
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 0.85
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 186
    cleaning_beam_agent-0_mean: 96.08
    cleaning_beam_agent-0_min: 40
    cleaning_beam_agent-1_max: 413
    cleaning_beam_agent-1_mean: 242.71
    cleaning_beam_agent-1_min: 73
    cleaning_beam_agent-2_max: 162
    cleaning_beam_agent-2_mean: 32.87
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 203
    cleaning_beam_agent-3_mean: 91.61
    cleaning_beam_agent-3_min: 40
    cleaning_beam_agent-4_max: 212
    cleaning_beam_agent-4_mean: 97.53
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 13.37
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 250
    fire_beam_agent-5_mean: 2.58
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-39-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: -30.77
  episode_reward_min: -4385.0
  episodes_this_iter: 96
  episodes_total: 16320
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 13470.343
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 0.6470223665237427
        entropy_coeff: 0.0017600000137463212
        kl: 0.008194026537239552
        model: {}
        policy_loss: -0.009345962665975094
        total_loss: -0.0021683955565094948
        vf_explained_var: 0.0006555318832397461
        vf_loss: 74.96926879882812
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 0.5475126504898071
        entropy_coeff: 0.0017600000137463212
        kl: 0.005047357175499201
        model: {}
        policy_loss: -0.006307944655418396
        total_loss: -0.004452880471944809
        vf_explained_var: 0.0035721808671951294
        vf_loss: 23.13950538635254
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 0.5659745931625366
        entropy_coeff: 0.0017600000137463212
        kl: 0.004295214079320431
        model: {}
        policy_loss: -0.008719749748706818
        total_loss: 0.01304137147963047
        vf_explained_var: 0.002258792519569397
        vf_loss: 218.98196411132812
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 0.7097247838973999
        entropy_coeff: 0.0017600000137463212
        kl: 0.007142194546759129
        model: {}
        policy_loss: -0.009668726474046707
        total_loss: 0.03281015902757645
        vf_explained_var: 0.0019852668046951294
        vf_loss: 430.1378173828125
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 0.7256166934967041
        entropy_coeff: 0.0017600000137463212
        kl: 0.004387244116514921
        model: {}
        policy_loss: -0.008292579092085361
        total_loss: 0.06047900766134262
        vf_explained_var: 0.0027021020650863647
        vf_loss: 691.712158203125
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002476224035490304
        entropy: 0.7798424959182739
        entropy_coeff: 0.0017600000137463212
        kl: 0.00843875389546156
        model: {}
        policy_loss: -0.01303153857588768
        total_loss: -0.012094764038920403
        vf_explained_var: 0.0418836772441864
        vf_loss: 14.654217720031738
    load_time_ms: 14137.198
    num_steps_sampled: 16320000
    num_steps_trained: 16320000
    sample_time_ms: 100764.362
    update_time_ms: 352.585
  iterations_since_restore: 10
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.51741573033708
    ram_util_percent: 13.573033707865168
  pid: 5668
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 16.0
    agent-4: 18.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: -2.7
    agent-1: -1.19
    agent-2: -4.56
    agent-3: -10.51
    agent-4: -9.93
    agent-5: -1.88
  policy_reward_min:
    agent-0: -296.0
    agent-1: -199.0
    agent-2: -846.0
    agent-3: -1396.0
    agent-4: -1399.0
    agent-5: -249.0
  sampler_perf:
    mean_env_wait_ms: 22.961994205430884
    mean_inference_ms: 12.997507414610382
    mean_processing_ms: 57.864101092542484
  time_since_restore: 1291.496250152588
  time_this_iter_s: 125.14526414871216
  time_total_s: 23801.779492616653
  timestamp: 1637221191
  timesteps_since_restore: 960000
  timesteps_this_iter: 96000
  timesteps_total: 16320000
  training_iteration: 170
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    170 |          23801.8 | 16320000 |   -30.77 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 1.96
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.69
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 1.8
    apples_agent-2_min: 0
    apples_agent-3_max: 8
    apples_agent-3_mean: 2.32
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 1.03
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 1.06
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 234
    cleaning_beam_agent-0_mean: 102.56
    cleaning_beam_agent-0_min: 37
    cleaning_beam_agent-1_max: 423
    cleaning_beam_agent-1_mean: 245.41
    cleaning_beam_agent-1_min: 144
    cleaning_beam_agent-2_max: 124
    cleaning_beam_agent-2_mean: 31.55
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 160
    cleaning_beam_agent-3_mean: 85.49
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 411
    cleaning_beam_agent-4_mean: 107.99
    cleaning_beam_agent-4_min: 26
    cleaning_beam_agent-5_max: 63
    cleaning_beam_agent-5_mean: 18.35
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-41-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 69.0
  episode_reward_mean: 15.41
  episode_reward_min: -95.0
  episodes_this_iter: 96
  episodes_total: 16416
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12400.466
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.6539381146430969
        entropy_coeff: 0.0017600000137463212
        kl: 0.0101113086566329
        model: {}
        policy_loss: -0.020661860704421997
        total_loss: -0.020773235708475113
        vf_explained_var: -0.07948058843612671
        vf_loss: 0.28432634472846985
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.5581223964691162
        entropy_coeff: 0.0017600000137463212
        kl: 0.00785839930176735
        model: {}
        policy_loss: -0.016752256080508232
        total_loss: -0.01693381555378437
        vf_explained_var: -0.0584280788898468
        vf_loss: 0.14898116886615753
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.5578289031982422
        entropy_coeff: 0.0017600000137463212
        kl: 0.008840497583150864
        model: {}
        policy_loss: -0.010636313818395138
        total_loss: -0.010560214519500732
        vf_explained_var: -0.042382627725601196
        vf_loss: 1.7383155822753906
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.7001559138298035
        entropy_coeff: 0.0017600000137463212
        kl: 0.007986718788743019
        model: {}
        policy_loss: -0.015887867659330368
        total_loss: -0.016275987029075623
        vf_explained_var: -0.17317062616348267
        vf_loss: 0.45485109090805054
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.7198817729949951
        entropy_coeff: 0.0017600000137463212
        kl: 0.010679859668016434
        model: {}
        policy_loss: -0.024421319365501404
        total_loss: -0.024541135877370834
        vf_explained_var: -0.19698038697242737
        vf_loss: 0.7918625473976135
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002416320057818666
        entropy: 0.7492427825927734
        entropy_coeff: 0.0017600000137463212
        kl: 0.00875693466514349
        model: {}
        policy_loss: -0.012981297448277473
        total_loss: -0.013409115374088287
        vf_explained_var: -0.39319801330566406
        vf_loss: 0.1515602469444275
    load_time_ms: 13712.199
    num_steps_sampled: 16416000
    num_steps_trained: 16416000
    sample_time_ms: 100514.837
    update_time_ms: 17.876
  iterations_since_restore: 11
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.860220994475142
    ram_util_percent: 13.574585635359119
  pid: 5668
  policy_reward_max:
    agent-0: 17.0
    agent-1: 8.0
    agent-2: 18.0
    agent-3: 22.0
    agent-4: 12.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.78
    agent-1: 1.21
    agent-2: 3.19
    agent-3: 3.44
    agent-4: 3.88
    agent-5: 0.91
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: -47.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 22.96255885884939
    mean_inference_ms: 12.988261908406205
    mean_processing_ms: 57.847971594604275
  time_since_restore: 1418.0413973331451
  time_this_iter_s: 126.54514718055725
  time_total_s: 23928.32463979721
  timestamp: 1637221318
  timesteps_since_restore: 1056000
  timesteps_this_iter: 96000
  timesteps_total: 16416000
  training_iteration: 171
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    171 |          23928.3 | 16416000 |    15.41 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 1.64
    apples_agent-0_min: 0
    apples_agent-1_max: 55
    apples_agent-1_mean: 1.29
    apples_agent-1_min: 0
    apples_agent-2_max: 8
    apples_agent-2_mean: 1.47
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.25
    apples_agent-3_min: 0
    apples_agent-4_max: 28
    apples_agent-4_mean: 1.19
    apples_agent-4_min: 0
    apples_agent-5_max: 19
    apples_agent-5_mean: 0.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 177
    cleaning_beam_agent-0_mean: 99.03
    cleaning_beam_agent-0_min: 39
    cleaning_beam_agent-1_max: 479
    cleaning_beam_agent-1_mean: 251.72
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 166
    cleaning_beam_agent-2_mean: 40.15
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 247
    cleaning_beam_agent-3_mean: 88.06
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 341
    cleaning_beam_agent-4_mean: 105.41
    cleaning_beam_agent-4_min: 22
    cleaning_beam_agent-5_max: 102
    cleaning_beam_agent-5_mean: 19.29
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-44-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 16.35
  episode_reward_min: -37.0
  episodes_this_iter: 96
  episodes_total: 16512
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12402.971
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.6349233388900757
        entropy_coeff: 0.0017600000137463212
        kl: 0.0098260547965765
        model: {}
        policy_loss: -0.02109246701002121
        total_loss: -0.021205751225352287
        vf_explained_var: -0.09592154622077942
        vf_loss: 0.2157173752784729
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.5739859342575073
        entropy_coeff: 0.0017600000137463212
        kl: 0.007633491884917021
        model: {}
        policy_loss: -0.015663662925362587
        total_loss: -0.015893906354904175
        vf_explained_var: -0.007863357663154602
        vf_loss: 0.1662442535161972
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.5862141847610474
        entropy_coeff: 0.0017600000137463212
        kl: 0.009561780840158463
        model: {}
        policy_loss: -0.021748725324869156
        total_loss: -0.02178775891661644
        vf_explained_var: -0.15207350254058838
        vf_loss: 0.3652241826057434
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.7052895426750183
        entropy_coeff: 0.0017600000137463212
        kl: 0.007483926136046648
        model: {}
        policy_loss: -0.016845587641000748
        total_loss: -0.017299672588706017
        vf_explained_var: -0.250232458114624
        vf_loss: 0.38831600546836853
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.7397700548171997
        entropy_coeff: 0.0017600000137463212
        kl: 0.007442212663590908
        model: {}
        policy_loss: -0.013345152139663696
        total_loss: -0.013723157346248627
        vf_explained_var: -0.02829146385192871
        vf_loss: 1.7976930141448975
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00023564159346278757
        entropy: 0.7879210114479065
        entropy_coeff: 0.0017600000137463212
        kl: 0.008732017129659653
        model: {}
        policy_loss: -0.014641019515693188
        total_loss: -0.015137049369513988
        vf_explained_var: -0.16515010595321655
        vf_loss: 0.17508459091186523
    load_time_ms: 13671.784
    num_steps_sampled: 16512000
    num_steps_trained: 16512000
    sample_time_ms: 100265.167
    update_time_ms: 17.893
  iterations_since_restore: 12
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.767403314917125
    ram_util_percent: 13.520441988950274
  pid: 5668
  policy_reward_max:
    agent-0: 9.0
    agent-1: 10.0
    agent-2: 11.0
    agent-3: 11.0
    agent-4: 12.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 2.6
    agent-1: 1.84
    agent-2: 3.53
    agent-3: 3.38
    agent-4: 3.44
    agent-5: 1.56
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -46.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.964738703302356
    mean_inference_ms: 12.980044953619847
    mean_processing_ms: 57.83818946894846
  time_since_restore: 1545.039845943451
  time_this_iter_s: 126.99844861030579
  time_total_s: 24055.323088407516
  timestamp: 1637221445
  timesteps_since_restore: 1152000
  timesteps_this_iter: 96000
  timesteps_total: 16512000
  training_iteration: 172
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    172 |          24055.3 | 16512000 |    16.35 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.27
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.74
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.71
    apples_agent-2_min: 0
    apples_agent-3_max: 28
    apples_agent-3_mean: 2.49
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 0.88
    apples_agent-4_min: 0
    apples_agent-5_max: 42
    apples_agent-5_mean: 1.4
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 160
    cleaning_beam_agent-0_mean: 98.02
    cleaning_beam_agent-0_min: 45
    cleaning_beam_agent-1_max: 350
    cleaning_beam_agent-1_mean: 238.64
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 196
    cleaning_beam_agent-2_mean: 39.14
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 183
    cleaning_beam_agent-3_mean: 78.67
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 376
    cleaning_beam_agent-4_mean: 109.44
    cleaning_beam_agent-4_min: 30
    cleaning_beam_agent-5_max: 76
    cleaning_beam_agent-5_mean: 22.8
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-46-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 16.41
  episode_reward_min: -32.0
  episodes_this_iter: 96
  episodes_total: 16608
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12398.532
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.6316355466842651
        entropy_coeff: 0.0017600000137463212
        kl: 0.00909509789198637
        model: {}
        policy_loss: -0.019676735624670982
        total_loss: -0.019848590716719627
        vf_explained_var: -0.02815118432044983
        vf_loss: 0.3031364679336548
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.5454555749893188
        entropy_coeff: 0.0017600000137463212
        kl: 0.0050465138629078865
        model: {}
        policy_loss: -0.007368237711489201
        total_loss: -0.00767655111849308
        vf_explained_var: 0.005012631416320801
        vf_loss: 1.4703553915023804
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.5826362371444702
        entropy_coeff: 0.0017600000137463212
        kl: 0.009095277637243271
        model: {}
        policy_loss: -0.018776798620820045
        total_loss: -0.018835213035345078
        vf_explained_var: -0.06129524111747742
        vf_loss: 0.5750066637992859
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.6979780197143555
        entropy_coeff: 0.0017600000137463212
        kl: 0.006360088009387255
        model: {}
        policy_loss: -0.00989503227174282
        total_loss: -0.010322044603526592
        vf_explained_var: -0.023125648498535156
        vf_loss: 1.6542186737060547
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.7207282185554504
        entropy_coeff: 0.0017600000137463212
        kl: 0.01047822367399931
        model: {}
        policy_loss: -0.022792218253016472
        total_loss: -0.022977717220783234
        vf_explained_var: -0.09880462288856506
        vf_loss: 0.35157719254493713
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022965119569562376
        entropy: 0.8168636560440063
        entropy_coeff: 0.0017600000137463212
        kl: 0.008418571203947067
        model: {}
        policy_loss: -0.014367509633302689
        total_loss: -0.014949006028473377
        vf_explained_var: -0.19984319806098938
        vf_loss: 0.14323943853378296
    load_time_ms: 13669.448
    num_steps_sampled: 16608000
    num_steps_trained: 16608000
    sample_time_ms: 100252.912
    update_time_ms: 17.885
  iterations_since_restore: 13
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.575000000000006
    ram_util_percent: 13.570555555555554
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 9.0
    agent-2: 21.0
    agent-3: 18.0
    agent-4: 14.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.03
    agent-1: 1.36
    agent-2: 3.99
    agent-3: 3.0
    agent-4: 3.5
    agent-5: 1.53
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: -46.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.965396704446572
    mean_inference_ms: 12.971471935185582
    mean_processing_ms: 57.8644414850454
  time_since_restore: 1671.943662405014
  time_this_iter_s: 126.90381646156311
  time_total_s: 24182.22690486908
  timestamp: 1637221572
  timesteps_since_restore: 1248000
  timesteps_this_iter: 96000
  timesteps_total: 16608000
  training_iteration: 173
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    173 |          24182.2 | 16608000 |    16.41 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 1.82
    apples_agent-0_min: 0
    apples_agent-1_max: 3
    apples_agent-1_mean: 0.79
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.68
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 2.7
    apples_agent-3_min: 0
    apples_agent-4_max: 29
    apples_agent-4_mean: 1.06
    apples_agent-4_min: 0
    apples_agent-5_max: 24
    apples_agent-5_mean: 1.12
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 188
    cleaning_beam_agent-0_mean: 97.02
    cleaning_beam_agent-0_min: 37
    cleaning_beam_agent-1_max: 388
    cleaning_beam_agent-1_mean: 248.09
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 117
    cleaning_beam_agent-2_mean: 40.71
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 194
    cleaning_beam_agent-3_mean: 77.36
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 344
    cleaning_beam_agent-4_mean: 104.54
    cleaning_beam_agent-4_min: 21
    cleaning_beam_agent-5_max: 76
    cleaning_beam_agent-5_mean: 19.9
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 9
    fire_beam_agent-1_mean: 0.1
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-48-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 16.27
  episode_reward_min: -40.0
  episodes_this_iter: 96
  episodes_total: 16704
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12404.185
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.6302019357681274
        entropy_coeff: 0.0017600000137463212
        kl: 0.008964589796960354
        model: {}
        policy_loss: -0.02000346966087818
        total_loss: -0.02019209787249565
        vf_explained_var: -0.022028475999832153
        vf_loss: 0.24066433310508728
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.5586116313934326
        entropy_coeff: 0.0017600000137463212
        kl: 0.007679910399019718
        model: {}
        policy_loss: -0.016687288880348206
        total_loss: -0.01688365824520588
        vf_explained_var: 0.00593237578868866
        vf_loss: 0.18793731927871704
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.5883752703666687
        entropy_coeff: 0.0017600000137463212
        kl: 0.008985756896436214
        model: {}
        policy_loss: -0.020361019298434258
        total_loss: -0.020458299666643143
        vf_explained_var: -0.05527624487876892
        vf_loss: 0.396846204996109
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.6952172517776489
        entropy_coeff: 0.0017600000137463212
        kl: 0.006938675418496132
        model: {}
        policy_loss: -0.013643348589539528
        total_loss: -0.014125529676675797
        vf_explained_var: -0.04515412449836731
        vf_loss: 0.4753422737121582
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.7117008566856384
        entropy_coeff: 0.0017600000137463212
        kl: 0.006853848695755005
        model: {}
        policy_loss: -0.011468840762972832
        total_loss: -0.011870461516082287
        vf_explained_var: -0.01923316717147827
        vf_loss: 1.6558630466461182
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00022366079792845994
        entropy: 0.7885739207267761
        entropy_coeff: 0.0017600000137463212
        kl: 0.0089211231097579
        model: {}
        policy_loss: -0.009657272137701511
        total_loss: -0.010013492777943611
        vf_explained_var: -0.023666024208068848
        vf_loss: 1.3955641984939575
    load_time_ms: 13643.199
    num_steps_sampled: 16704000
    num_steps_trained: 16704000
    sample_time_ms: 100189.219
    update_time_ms: 17.464
  iterations_since_restore: 14
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.68121546961326
    ram_util_percent: 13.494475138121548
  pid: 5668
  policy_reward_max:
    agent-0: 11.0
    agent-1: 9.0
    agent-2: 16.0
    agent-3: 18.0
    agent-4: 16.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.91
    agent-1: 2.02
    agent-2: 3.7
    agent-3: 3.8
    agent-4: 2.72
    agent-5: 1.12
  policy_reward_min:
    agent-0: 0.0
    agent-1: -8.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -48.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 22.96548621324509
    mean_inference_ms: 12.959995776217324
    mean_processing_ms: 57.8399195800805
  time_since_restore: 1798.487957239151
  time_this_iter_s: 126.54429483413696
  time_total_s: 24308.771199703217
  timestamp: 1637221699
  timesteps_since_restore: 1344000
  timesteps_this_iter: 96000
  timesteps_total: 16704000
  training_iteration: 174
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    174 |          24308.8 | 16704000 |    16.27 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.23
    apples_agent-0_min: 0
    apples_agent-1_max: 121
    apples_agent-1_mean: 1.89
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.68
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.64
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.34
    apples_agent-4_min: 0
    apples_agent-5_max: 28
    apples_agent-5_mean: 1.34
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 189
    cleaning_beam_agent-0_mean: 98.05
    cleaning_beam_agent-0_min: 40
    cleaning_beam_agent-1_max: 365
    cleaning_beam_agent-1_mean: 249.34
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 167
    cleaning_beam_agent-2_mean: 40.19
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 178
    cleaning_beam_agent-3_mean: 73.88
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 255
    cleaning_beam_agent-4_mean: 109.13
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 68
    cleaning_beam_agent-5_mean: 20.1
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-50-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 46.0
  episode_reward_mean: 17.54
  episode_reward_min: -38.0
  episodes_this_iter: 96
  episodes_total: 16800
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12410.042
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.6286313533782959
        entropy_coeff: 0.0017600000137463212
        kl: 0.009147481061518192
        model: {}
        policy_loss: -0.019152747467160225
        total_loss: -0.019314821809530258
        vf_explained_var: -0.025006026029586792
        vf_loss: 0.295663982629776
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.5483142137527466
        entropy_coeff: 0.0017600000137463212
        kl: 0.006661261431872845
        model: {}
        policy_loss: -0.014980054460465908
        total_loss: -0.01526072807610035
        vf_explained_var: 0.01211114227771759
        vf_loss: 0.18230722844600677
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.5845785140991211
        entropy_coeff: 0.0017600000137463212
        kl: 0.00674524437636137
        model: {}
        policy_loss: -0.01113463006913662
        total_loss: -0.011318841949105263
        vf_explained_var: -0.007122933864593506
        vf_loss: 1.7012460231781006
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.6958943009376526
        entropy_coeff: 0.0017600000137463212
        kl: 0.007486339192837477
        model: {}
        policy_loss: -0.015436800196766853
        total_loss: -0.01587745174765587
        vf_explained_var: -0.09443286061286926
        vf_loss: 0.35489046573638916
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.7161437273025513
        entropy_coeff: 0.0017600000137463212
        kl: 0.009463679045438766
        model: {}
        policy_loss: -0.021324465051293373
        total_loss: -0.02160213328897953
        vf_explained_var: -0.06272575259208679
        vf_loss: 0.36378544569015503
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021767040016129613
        entropy: 0.8000730872154236
        entropy_coeff: 0.0017600000137463212
        kl: 0.008612116798758507
        model: {}
        policy_loss: -0.013212854973971844
        total_loss: -0.01374761201441288
        vf_explained_var: -0.08563026785850525
        vf_loss: 0.1215854063630104
    load_time_ms: 13678.263
    num_steps_sampled: 16800000
    num_steps_trained: 16800000
    sample_time_ms: 100122.762
    update_time_ms: 17.537
  iterations_since_restore: 15
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.524022346368717
    ram_util_percent: 13.530167597765363
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 12.0
    agent-2: 12.0
    agent-3: 10.0
    agent-4: 17.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.07
    agent-1: 2.06
    agent-2: 3.41
    agent-3: 3.6
    agent-4: 3.83
    agent-5: 1.57
  policy_reward_min:
    agent-0: -1.0
    agent-1: 0.0
    agent-2: -47.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.957708007345577
    mean_inference_ms: 12.947089485636818
    mean_processing_ms: 57.84952081887813
  time_since_restore: 1924.1915307044983
  time_this_iter_s: 125.70357346534729
  time_total_s: 24434.474773168564
  timestamp: 1637221825
  timesteps_since_restore: 1440000
  timesteps_this_iter: 96000
  timesteps_total: 16800000
  training_iteration: 175
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    175 |          24434.5 | 16800000 |    17.54 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 1.74
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 0.89
    apples_agent-1_min: 0
    apples_agent-2_max: 7
    apples_agent-2_mean: 1.54
    apples_agent-2_min: 0
    apples_agent-3_max: 38
    apples_agent-3_mean: 3.36
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.9
    apples_agent-4_min: 0
    apples_agent-5_max: 31
    apples_agent-5_mean: 1.36
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 195
    cleaning_beam_agent-0_mean: 100.47
    cleaning_beam_agent-0_min: 41
    cleaning_beam_agent-1_max: 383
    cleaning_beam_agent-1_mean: 243.1
    cleaning_beam_agent-1_min: 152
    cleaning_beam_agent-2_max: 149
    cleaning_beam_agent-2_mean: 38.12
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 233
    cleaning_beam_agent-3_mean: 80.06
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 227
    cleaning_beam_agent-4_mean: 104.17
    cleaning_beam_agent-4_min: 19
    cleaning_beam_agent-5_max: 60
    cleaning_beam_agent-5_mean: 17.06
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-52-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 17.99
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 16896
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12396.29
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.6323541402816772
        entropy_coeff: 0.0017600000137463212
        kl: 0.009006603620946407
        model: {}
        policy_loss: -0.019567564129829407
        total_loss: -0.019754774868488312
        vf_explained_var: -0.021484822034835815
        vf_loss: 0.25069573521614075
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.5442994236946106
        entropy_coeff: 0.0017600000137463212
        kl: 0.0068686287850141525
        model: {}
        policy_loss: -0.015025277622044086
        total_loss: -0.01528105977922678
        vf_explained_var: 0.0032358020544052124
        vf_loss: 0.15320654213428497
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.5856244564056396
        entropy_coeff: 0.0017600000137463212
        kl: 0.008282042108476162
        model: {}
        policy_loss: -0.019076744094491005
        total_loss: -0.01923515647649765
        vf_explained_var: -0.027852296829223633
        vf_loss: 0.44078004360198975
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.6951549649238586
        entropy_coeff: 0.0017600000137463212
        kl: 0.007058863062411547
        model: {}
        policy_loss: -0.013438645750284195
        total_loss: -0.01390649750828743
        vf_explained_var: -0.033924400806427
        vf_loss: 0.4973170757293701
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.7134438753128052
        entropy_coeff: 0.0017600000137463212
        kl: 0.00944835040718317
        model: {}
        policy_loss: -0.02053467370569706
        total_loss: -0.020808666944503784
        vf_explained_var: -0.051242321729660034
        vf_loss: 0.3682977557182312
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00021168000239413232
        entropy: 0.7685121297836304
        entropy_coeff: 0.0017600000137463212
        kl: 0.0081060491502285
        model: {}
        policy_loss: -0.013555653393268585
        total_loss: -0.014085962437093258
        vf_explained_var: -0.045546114444732666
        vf_loss: 0.11666857451200485
    load_time_ms: 13659.36
    num_steps_sampled: 16896000
    num_steps_trained: 16896000
    sample_time_ms: 100114.149
    update_time_ms: 17.512
  iterations_since_restore: 16
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.79722222222222
    ram_util_percent: 13.517777777777777
  pid: 5668
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 17.0
    agent-3: 23.0
    agent-4: 12.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 2.82
    agent-1: 2.04
    agent-2: 3.95
    agent-3: 4.15
    agent-4: 3.63
    agent-5: 1.4
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.956635385454373
    mean_inference_ms: 12.935538005824071
    mean_processing_ms: 57.8282841548014
  time_since_restore: 2050.879799604416
  time_this_iter_s: 126.6882688999176
  time_total_s: 24561.16304206848
  timestamp: 1637221952
  timesteps_since_restore: 1536000
  timesteps_this_iter: 96000
  timesteps_total: 16896000
  training_iteration: 176
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.7/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    176 |          24561.2 | 16896000 |    17.99 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.04
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 1.1
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.87
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 2.78
    apples_agent-3_min: 0
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.86
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 0.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 224
    cleaning_beam_agent-0_mean: 103.49
    cleaning_beam_agent-0_min: 48
    cleaning_beam_agent-1_max: 392
    cleaning_beam_agent-1_mean: 255.9
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 222
    cleaning_beam_agent-2_mean: 33.67
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 193
    cleaning_beam_agent-3_mean: 80.43
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 349
    cleaning_beam_agent-4_mean: 112.64
    cleaning_beam_agent-4_min: 23
    cleaning_beam_agent-5_max: 66
    cleaning_beam_agent-5_mean: 19.89
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 4
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-54-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 15.4
  episode_reward_min: -190.0
  episodes_this_iter: 96
  episodes_total: 16992
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12394.199
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.6270360946655273
        entropy_coeff: 0.0017600000137463212
        kl: 0.006670624483376741
        model: {}
        policy_loss: -0.007494572550058365
        total_loss: -0.007544394116848707
        vf_explained_var: -0.00904051959514618
        vf_loss: 3.8669915199279785
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.5504413843154907
        entropy_coeff: 0.0017600000137463212
        kl: 0.006991635076701641
        model: {}
        policy_loss: -0.014674779027700424
        total_loss: -0.014928130432963371
        vf_explained_var: 0.016162991523742676
        vf_loss: 0.16263209283351898
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.5809122920036316
        entropy_coeff: 0.0017600000137463212
        kl: 0.00826343521475792
        model: {}
        policy_loss: -0.019453175365924835
        total_loss: -0.019606787711381912
        vf_explained_var: -0.010735273361206055
        vf_loss: 0.42448434233665466
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.6890239715576172
        entropy_coeff: 0.0017600000137463212
        kl: 0.006011608988046646
        model: {}
        policy_loss: -0.009495416656136513
        total_loss: -0.009929655119776726
        vf_explained_var: -0.022127658128738403
        vf_loss: 1.7728022336959839
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.7074645161628723
        entropy_coeff: 0.0017600000137463212
        kl: 0.004787578713148832
        model: {}
        policy_loss: -0.007469841279089451
        total_loss: -0.007706589065492153
        vf_explained_var: -0.00286082923412323
        vf_loss: 5.296321868896484
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0002056896046269685
        entropy: 0.7912686467170715
        entropy_coeff: 0.0017600000137463212
        kl: 0.007660882081836462
        model: {}
        policy_loss: -0.013539023697376251
        total_loss: -0.01415021438151598
        vf_explained_var: -0.054885804653167725
        vf_loss: 0.15352153778076172
    load_time_ms: 13639.49
    num_steps_sampled: 16992000
    num_steps_trained: 16992000
    sample_time_ms: 100139.356
    update_time_ms: 17.586
  iterations_since_restore: 17
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.80722222222222
    ram_util_percent: 13.562777777777779
  pid: 5668
  policy_reward_max:
    agent-0: 10.0
    agent-1: 8.0
    agent-2: 17.0
    agent-3: 12.0
    agent-4: 13.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 1.84
    agent-1: 2.01
    agent-2: 4.26
    agent-3: 3.6
    agent-4: 2.03
    agent-5: 1.66
  policy_reward_min:
    agent-0: -99.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -34.0
    agent-4: -97.0
    agent-5: -3.0
  sampler_perf:
    mean_env_wait_ms: 22.957695159190603
    mean_inference_ms: 12.930109097485106
    mean_processing_ms: 57.806003873777485
  time_since_restore: 2177.175616502762
  time_this_iter_s: 126.29581689834595
  time_total_s: 24687.458858966827
  timestamp: 1637222078
  timesteps_since_restore: 1632000
  timesteps_this_iter: 96000
  timesteps_total: 16992000
  training_iteration: 177
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    177 |          24687.5 | 16992000 |     15.4 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 1.81
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 1.09
    apples_agent-1_min: 0
    apples_agent-2_max: 8
    apples_agent-2_mean: 1.81
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.51
    apples_agent-3_min: 0
    apples_agent-4_max: 60
    apples_agent-4_mean: 1.7
    apples_agent-4_min: 0
    apples_agent-5_max: 103
    apples_agent-5_mean: 1.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 236
    cleaning_beam_agent-0_mean: 98.75
    cleaning_beam_agent-0_min: 45
    cleaning_beam_agent-1_max: 381
    cleaning_beam_agent-1_mean: 251.84
    cleaning_beam_agent-1_min: 154
    cleaning_beam_agent-2_max: 121
    cleaning_beam_agent-2_mean: 35.96
    cleaning_beam_agent-2_min: 8
    cleaning_beam_agent-3_max: 168
    cleaning_beam_agent-3_mean: 81.12
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 249
    cleaning_beam_agent-4_mean: 112.35
    cleaning_beam_agent-4_min: 39
    cleaning_beam_agent-5_max: 50
    cleaning_beam_agent-5_mean: 17.22
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-56-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 50.0
  episode_reward_mean: 16.48
  episode_reward_min: -42.0
  episodes_this_iter: 96
  episodes_total: 17088
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12383.451
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.6297280192375183
        entropy_coeff: 0.0017600000137463212
        kl: 0.008207228034734726
        model: {}
        policy_loss: -0.018948420882225037
        total_loss: -0.019216949120163918
        vf_explained_var: -0.012731850147247314
        vf_loss: 0.19070813059806824
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.5475628972053528
        entropy_coeff: 0.0017600000137463212
        kl: 0.004611385054886341
        model: {}
        policy_loss: -0.007760974578559399
        total_loss: -0.008132940158247948
        vf_explained_var: 0.016959652304649353
        vf_loss: 1.3060840368270874
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.5747880339622498
        entropy_coeff: 0.0017600000137463212
        kl: 0.005901615135371685
        model: {}
        policy_loss: -0.0103314109146595
        total_loss: -0.010463589802384377
        vf_explained_var: -0.0050737857818603516
        vf_loss: 2.89289927482605
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.6926852464675903
        entropy_coeff: 0.0017600000137463212
        kl: 0.007490333169698715
        model: {}
        policy_loss: -0.016087086871266365
        total_loss: -0.016523271799087524
        vf_explained_var: -0.11300474405288696
        vf_loss: 0.33911752700805664
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001996992068598047
        entropy: 0.7171704769134521
        entropy_coeff: 0.0017600000137463212
        kl: 0.009067261591553688
        model: {}
        policy_loss: -0.018433894962072372
        total_loss: -0.019191285595297813
        vf_explained_var: -0.013632386922836304
        vf_loss: 0.5146458148956299
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001996992068598047
        entropy: 0.7475156188011169
        entropy_coeff: 0.0017600000137463212
        kl: 0.004739980213344097
        model: {}
        policy_loss: -0.006745229475200176
        total_loss: -0.007441037800163031
        vf_explained_var: -0.0117664635181427
        vf_loss: 1.4581965208053589
    load_time_ms: 13607.732
    num_steps_sampled: 17088000
    num_steps_trained: 17088000
    sample_time_ms: 100169.735
    update_time_ms: 17.804
  iterations_since_restore: 18
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.101111111111116
    ram_util_percent: 13.372777777777774
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 17.0
    agent-3: 11.0
    agent-4: 20.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.78
    agent-1: 1.54
    agent-2: 2.75
    agent-3: 3.76
    agent-4: 4.52
    agent-5: 1.13
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 22.958370898984803
    mean_inference_ms: 12.924917939240583
    mean_processing_ms: 57.79663289792071
  time_since_restore: 2303.34738945961
  time_this_iter_s: 126.17177295684814
  time_total_s: 24813.630631923676
  timestamp: 1637222204
  timesteps_since_restore: 1728000
  timesteps_this_iter: 96000
  timesteps_total: 17088000
  training_iteration: 178
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    178 |          24813.6 | 17088000 |    16.48 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.0
    apples_agent-0_min: 0
    apples_agent-1_max: 3
    apples_agent-1_mean: 0.81
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 1.35
    apples_agent-2_min: 0
    apples_agent-3_max: 29
    apples_agent-3_mean: 2.91
    apples_agent-3_min: 0
    apples_agent-4_max: 29
    apples_agent-4_mean: 1.19
    apples_agent-4_min: 0
    apples_agent-5_max: 40
    apples_agent-5_mean: 1.15
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 175
    cleaning_beam_agent-0_mean: 93.3
    cleaning_beam_agent-0_min: 45
    cleaning_beam_agent-1_max: 374
    cleaning_beam_agent-1_mean: 242.34
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 118
    cleaning_beam_agent-2_mean: 34.06
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 186
    cleaning_beam_agent-3_mean: 71.38
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 270
    cleaning_beam_agent-4_mean: 103.79
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 14.42
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_02-58-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 42.0
  episode_reward_mean: 16.33
  episode_reward_min: -43.0
  episodes_this_iter: 96
  episodes_total: 17184
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12383.693
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.6219267845153809
        entropy_coeff: 0.0017600000137463212
        kl: 0.007954215630888939
        model: {}
        policy_loss: -0.018093932420015335
        total_loss: -0.0183708518743515
        vf_explained_var: -0.012278348207473755
        vf_loss: 0.22250179946422577
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00019370879454072565
        entropy: 0.5426727533340454
        entropy_coeff: 0.0017600000137463212
        kl: 0.007475660648196936
        model: {}
        policy_loss: -0.014653513208031654
        total_loss: -0.015222914516925812
        vf_explained_var: -0.0010503679513931274
        vf_loss: 0.11916001886129379
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.5740604400634766
        entropy_coeff: 0.0017600000137463212
        kl: 0.007758402265608311
        model: {}
        policy_loss: -0.017868340015411377
        total_loss: -0.018063925206661224
        vf_explained_var: -0.01849919557571411
        vf_loss: 0.38918378949165344
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00019370879454072565
        entropy: 0.6777811050415039
        entropy_coeff: 0.0017600000137463212
        kl: 0.006125380285084248
        model: {}
        policy_loss: -0.013092754408717155
        total_loss: -0.013633648864924908
        vf_explained_var: -0.04463320970535278
        vf_loss: 0.3946123719215393
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00019370879454072565
        entropy: 0.7188672423362732
        entropy_coeff: 0.0017600000137463212
        kl: 0.009875781834125519
        model: {}
        policy_loss: -0.020498130470514297
        total_loss: -0.02123415842652321
        vf_explained_var: -0.01981547474861145
        vf_loss: 0.35392528772354126
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00019370879454072565
        entropy: 0.74810791015625
        entropy_coeff: 0.0017600000137463212
        kl: 0.007295501884073019
        model: {}
        policy_loss: -0.011119181290268898
        total_loss: -0.012058007530868053
        vf_explained_var: -0.050579607486724854
        vf_loss: 0.13068099319934845
    load_time_ms: 13646.806
    num_steps_sampled: 17184000
    num_steps_trained: 17184000
    sample_time_ms: 100177.307
    update_time_ms: 17.738
  iterations_since_restore: 19
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.526111111111113
    ram_util_percent: 12.173888888888888
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 6.0
    agent-2: 14.0
    agent-3: 13.0
    agent-4: 13.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.65
    agent-1: 1.76
    agent-2: 3.97
    agent-3: 3.17
    agent-4: 3.9
    agent-5: 0.88
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -42.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 22.95944411372693
    mean_inference_ms: 12.916927434315815
    mean_processing_ms: 57.77486482033357
  time_since_restore: 2429.7457797527313
  time_this_iter_s: 126.39839029312134
  time_total_s: 24940.029022216797
  timestamp: 1637222331
  timesteps_since_restore: 1824000
  timesteps_this_iter: 96000
  timesteps_total: 17184000
  training_iteration: 179
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    179 |            24940 | 17184000 |    16.33 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.0
    apples_agent-0_min: 0
    apples_agent-1_max: 20
    apples_agent-1_mean: 1.26
    apples_agent-1_min: 0
    apples_agent-2_max: 25
    apples_agent-2_mean: 2.16
    apples_agent-2_min: 0
    apples_agent-3_max: 29
    apples_agent-3_mean: 3.48
    apples_agent-3_min: 0
    apples_agent-4_max: 56
    apples_agent-4_mean: 1.88
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 0.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 157
    cleaning_beam_agent-0_mean: 90.06
    cleaning_beam_agent-0_min: 41
    cleaning_beam_agent-1_max: 405
    cleaning_beam_agent-1_mean: 252.94
    cleaning_beam_agent-1_min: 152
    cleaning_beam_agent-2_max: 223
    cleaning_beam_agent-2_mean: 37.28
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 166
    cleaning_beam_agent-3_mean: 71.37
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 280
    cleaning_beam_agent-4_mean: 119.14
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 38
    cleaning_beam_agent-5_mean: 11.83
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-00-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 49.0
  episode_reward_mean: 18.43
  episode_reward_min: -41.0
  episodes_this_iter: 96
  episodes_total: 17280
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12376.014
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.6237691640853882
        entropy_coeff: 0.0017600000137463212
        kl: 0.008328044787049294
        model: {}
        policy_loss: -0.017806563526391983
        total_loss: -0.01804744452238083
        vf_explained_var: -0.0009017884731292725
        vf_loss: 0.24149712920188904
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00018771839677356184
        entropy: 0.547568678855896
        entropy_coeff: 0.0017600000137463212
        kl: 0.007358563598245382
        model: {}
        policy_loss: -0.014810279943048954
        total_loss: -0.015386899001896381
        vf_explained_var: 0.02836798131465912
        vf_loss: 0.1917538046836853
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.5698087215423584
        entropy_coeff: 0.0017600000137463212
        kl: 0.008071860298514366
        model: {}
        policy_loss: -0.018236860632896423
        total_loss: -0.018390066921710968
        vf_explained_var: 0.0031961947679519653
        vf_loss: 0.4247204661369324
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018771839677356184
        entropy: 0.6796585321426392
        entropy_coeff: 0.0017600000137463212
        kl: 0.005460887216031551
        model: {}
        policy_loss: -0.009323874488472939
        total_loss: -0.00979304313659668
        vf_explained_var: -0.006875902414321899
        vf_loss: 1.8094291687011719
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00018771839677356184
        entropy: 0.692552387714386
        entropy_coeff: 0.0017600000137463212
        kl: 0.009029763750731945
        model: {}
        policy_loss: -0.020096836611628532
        total_loss: -0.020823407918214798
        vf_explained_var: 0.014716550707817078
        vf_loss: 0.40833228826522827
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00018771839677356184
        entropy: 0.7625254392623901
        entropy_coeff: 0.0017600000137463212
        kl: 0.007179733831435442
        model: {}
        policy_loss: -0.010928051546216011
        total_loss: -0.011895284987986088
        vf_explained_var: -0.03631424903869629
        vf_loss: 0.15824611485004425
    load_time_ms: 13660.612
    num_steps_sampled: 17280000
    num_steps_trained: 17280000
    sample_time_ms: 100291.49
    update_time_ms: 17.468
  iterations_since_restore: 20
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.375
    ram_util_percent: 12.228888888888886
  pid: 5668
  policy_reward_max:
    agent-0: 12.0
    agent-1: 12.0
    agent-2: 15.0
    agent-3: 17.0
    agent-4: 13.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.75
    agent-1: 2.38
    agent-2: 4.28
    agent-3: 3.64
    agent-4: 4.26
    agent-5: 1.12
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -48.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 22.961418986783645
    mean_inference_ms: 12.909369679895525
    mean_processing_ms: 57.754058144119554
  time_since_restore: 2556.0873646736145
  time_this_iter_s: 126.34158492088318
  time_total_s: 25066.37060713768
  timestamp: 1637222457
  timesteps_since_restore: 1920000
  timesteps_this_iter: 96000
  timesteps_total: 17280000
  training_iteration: 180
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    180 |          25066.4 | 17280000 |    18.43 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 119
    apples_agent-0_mean: 3.25
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.79
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 2.17
    apples_agent-2_min: 0
    apples_agent-3_max: 23
    apples_agent-3_mean: 3.03
    apples_agent-3_min: 0
    apples_agent-4_max: 26
    apples_agent-4_mean: 1.14
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.05
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 200
    cleaning_beam_agent-0_mean: 94.14
    cleaning_beam_agent-0_min: 39
    cleaning_beam_agent-1_max: 401
    cleaning_beam_agent-1_mean: 265.17
    cleaning_beam_agent-1_min: 145
    cleaning_beam_agent-2_max: 94
    cleaning_beam_agent-2_mean: 33.72
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 162
    cleaning_beam_agent-3_mean: 70.2
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 365
    cleaning_beam_agent-4_mean: 109.89
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 44
    cleaning_beam_agent-5_mean: 13.28
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-03-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 19.67
  episode_reward_min: -41.0
  episodes_this_iter: 96
  episodes_total: 17376
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12377.304
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.6223963499069214
        entropy_coeff: 0.0017600000137463212
        kl: 0.007808801718056202
        model: {}
        policy_loss: -0.017832543700933456
        total_loss: -0.018112245947122574
        vf_explained_var: -0.012137562036514282
        vf_loss: 0.34835517406463623
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00018172799900639802
        entropy: 0.5284557342529297
        entropy_coeff: 0.0017600000137463212
        kl: 0.006862020120024681
        model: {}
        policy_loss: -0.013791101053357124
        total_loss: -0.014360319823026657
        vf_explained_var: 0.015386641025543213
        vf_loss: 0.17763909697532654
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.576898455619812
        entropy_coeff: 0.0017600000137463212
        kl: 0.007843097671866417
        model: {}
        policy_loss: -0.017312826588749886
        total_loss: -0.01749950833618641
        vf_explained_var: 0.007997632026672363
        vf_loss: 0.44351786375045776
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00018172799900639802
        entropy: 0.6866697072982788
        entropy_coeff: 0.0017600000137463212
        kl: 0.006151788868010044
        model: {}
        policy_loss: -0.013247092254459858
        total_loss: -0.01380058191716671
        vf_explained_var: -0.025525778532028198
        vf_loss: 0.3987009823322296
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00018172799900639802
        entropy: 0.7112316489219666
        entropy_coeff: 0.0017600000137463212
        kl: 0.009762291796505451
        model: {}
        policy_loss: -0.02023245207965374
        total_loss: -0.020954711362719536
        vf_explained_var: 0.005936577916145325
        vf_loss: 0.41393429040908813
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00018172799900639802
        entropy: 0.7225266098976135
        entropy_coeff: 0.0017600000137463212
        kl: 0.007776825688779354
        model: {}
        policy_loss: -0.012547602877020836
        total_loss: -0.01341765932738781
        vf_explained_var: -0.026755094528198242
        vf_loss: 0.12751001119613647
    load_time_ms: 13689.011
    num_steps_sampled: 17376000
    num_steps_trained: 17376000
    sample_time_ms: 100437.346
    update_time_ms: 17.082
  iterations_since_restore: 21
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.82663043478261
    ram_util_percent: 12.174456521739133
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 12.0
    agent-2: 19.0
    agent-3: 16.0
    agent-4: 14.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.36
    agent-1: 2.29
    agent-2: 4.3
    agent-3: 4.3
    agent-4: 4.25
    agent-5: 1.17
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 22.978697330351903
    mean_inference_ms: 12.908132970395114
    mean_processing_ms: 57.77349182858103
  time_since_restore: 2684.381143093109
  time_this_iter_s: 128.29377841949463
  time_total_s: 25194.664385557175
  timestamp: 1637222586
  timesteps_since_restore: 2016000
  timesteps_this_iter: 96000
  timesteps_total: 17376000
  training_iteration: 181
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    181 |          25194.7 | 17376000 |    19.67 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.24
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.9
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 2.22
    apples_agent-2_min: 0
    apples_agent-3_max: 43
    apples_agent-3_mean: 3.46
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 1.17
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 0.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 200
    cleaning_beam_agent-0_mean: 91.23
    cleaning_beam_agent-0_min: 32
    cleaning_beam_agent-1_max: 383
    cleaning_beam_agent-1_mean: 254.94
    cleaning_beam_agent-1_min: 147
    cleaning_beam_agent-2_max: 137
    cleaning_beam_agent-2_mean: 36.33
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 225
    cleaning_beam_agent-3_mean: 76.98
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 277
    cleaning_beam_agent-4_mean: 93.1
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 36
    cleaning_beam_agent-5_mean: 11.86
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-05-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 17.45
  episode_reward_min: -85.0
  episodes_this_iter: 96
  episodes_total: 17472
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12362.785
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.6151207685470581
        entropy_coeff: 0.0017600000137463212
        kl: 0.0076660229824483395
        model: {}
        policy_loss: -0.01782190427184105
        total_loss: -0.018101921305060387
        vf_explained_var: -0.01632174849510193
        vf_loss: 0.3599269986152649
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001757376012392342
        entropy: 0.5263711214065552
        entropy_coeff: 0.0017600000137463212
        kl: 0.007121631875634193
        model: {}
        policy_loss: -0.008774597197771072
        total_loss: -0.0090554840862751
        vf_explained_var: 0.01291579008102417
        vf_loss: 2.894436836242676
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.5777544379234314
        entropy_coeff: 0.0017600000137463212
        kl: 0.007751562166959047
        model: {}
        policy_loss: -0.01728852093219757
        total_loss: -0.017482900992035866
        vf_explained_var: 0.007126569747924805
        vf_loss: 0.47314736247062683
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001757376012392342
        entropy: 0.6732937097549438
        entropy_coeff: 0.0017600000137463212
        kl: 0.006775825284421444
        model: {}
        policy_loss: -0.013168364763259888
        total_loss: -0.013632778078317642
        vf_explained_var: -0.01763981580734253
        vf_loss: 0.4299991726875305
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001757376012392342
        entropy: 0.6862756013870239
        entropy_coeff: 0.0017600000137463212
        kl: 0.008869439363479614
        model: {}
        policy_loss: -0.01988881826400757
        total_loss: -0.020623186603188515
        vf_explained_var: 0.0017267316579818726
        vf_loss: 0.3000492751598358
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001757376012392342
        entropy: 0.7045512199401855
        entropy_coeff: 0.0017600000137463212
        kl: 0.006943066604435444
        model: {}
        policy_loss: -0.006446461193263531
        total_loss: -0.00693571986630559
        vf_explained_var: 0.004365116357803345
        vf_loss: 4.035975933074951
    load_time_ms: 13718.045
    num_steps_sampled: 17472000
    num_steps_trained: 17472000
    sample_time_ms: 100315.596
    update_time_ms: 17.196
  iterations_since_restore: 22
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.766480446927375
    ram_util_percent: 12.223463687150835
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 14.0
    agent-2: 16.0
    agent-3: 14.0
    agent-4: 11.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 3.65
    agent-1: 1.54
    agent-2: 4.22
    agent-3: 4.22
    agent-4: 3.81
    agent-5: 0.01
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 22.97611596703131
    mean_inference_ms: 12.903753942701291
    mean_processing_ms: 57.74852383785892
  time_since_restore: 2810.324723482132
  time_this_iter_s: 125.94358038902283
  time_total_s: 25320.607965946198
  timestamp: 1637222712
  timesteps_since_restore: 2112000
  timesteps_this_iter: 96000
  timesteps_total: 17472000
  training_iteration: 182
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    182 |          25320.6 | 17472000 |    17.45 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 2.51
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.0
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 1.97
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 3.15
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 0.8
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 0.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 212
    cleaning_beam_agent-0_mean: 93.28
    cleaning_beam_agent-0_min: 42
    cleaning_beam_agent-1_max: 546
    cleaning_beam_agent-1_mean: 258.8
    cleaning_beam_agent-1_min: 140
    cleaning_beam_agent-2_max: 111
    cleaning_beam_agent-2_mean: 39.31
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 243
    cleaning_beam_agent-3_mean: 71.66
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 269
    cleaning_beam_agent-4_mean: 108.3
    cleaning_beam_agent-4_min: 22
    cleaning_beam_agent-5_max: 39
    cleaning_beam_agent-5_mean: 13.19
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-07-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 69.0
  episode_reward_mean: 20.27
  episode_reward_min: -46.0
  episodes_this_iter: 96
  episodes_total: 17568
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12349.473
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.6211487650871277
        entropy_coeff: 0.0017600000137463212
        kl: 0.005640516988933086
        model: {}
        policy_loss: -0.00874527357518673
        total_loss: -0.009110348299145699
        vf_explained_var: -0.0006102025508880615
        vf_loss: 1.6409337520599365
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001697472034720704
        entropy: 0.524591863155365
        entropy_coeff: 0.0017600000137463212
        kl: 0.006277791224420071
        model: {}
        policy_loss: -0.010379110462963581
        total_loss: -0.010949769988656044
        vf_explained_var: 0.00840720534324646
        vf_loss: 0.3873417377471924
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.5893319845199585
        entropy_coeff: 0.0017600000137463212
        kl: 0.007349957711994648
        model: {}
        policy_loss: -0.018543405458331108
        total_loss: -0.018799789249897003
        vf_explained_var: -0.00048103928565979004
        vf_loss: 0.4584284722805023
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001697472034720704
        entropy: 0.6482451558113098
        entropy_coeff: 0.0017600000137463212
        kl: 0.005446585826575756
        model: {}
        policy_loss: -0.011769079603254795
        total_loss: -0.012311970815062523
        vf_explained_var: -0.006062984466552734
        vf_loss: 0.533615231513977
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001697472034720704
        entropy: 0.6955997943878174
        entropy_coeff: 0.0017600000137463212
        kl: 0.00900054257363081
        model: {}
        policy_loss: -0.018864169716835022
        total_loss: -0.019595688208937645
        vf_explained_var: 0.024114981293678284
        vf_loss: 0.42714452743530273
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001697472034720704
        entropy: 0.6650465130805969
        entropy_coeff: 0.0017600000137463212
        kl: 0.006648362148553133
        model: {}
        policy_loss: -0.01108089741319418
        total_loss: -0.01190692838281393
        vf_explained_var: -0.03642001748085022
        vf_loss: 0.12034384906291962
    load_time_ms: 13703.762
    num_steps_sampled: 17568000
    num_steps_trained: 17568000
    sample_time_ms: 100318.364
    update_time_ms: 17.236
  iterations_since_restore: 23
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.831491712707184
    ram_util_percent: 12.162430939226518
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 27.0
    agent-2: 13.0
    agent-3: 17.0
    agent-4: 13.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.9
    agent-1: 2.56
    agent-2: 4.58
    agent-3: 4.36
    agent-4: 4.36
    agent-5: 1.51
  policy_reward_min:
    agent-0: -50.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.97624256057152
    mean_inference_ms: 12.897907723529036
    mean_processing_ms: 57.73113587495789
  time_since_restore: 2936.967304468155
  time_this_iter_s: 126.64258098602295
  time_total_s: 25447.25054693222
  timestamp: 1637222839
  timesteps_since_restore: 2208000
  timesteps_this_iter: 96000
  timesteps_total: 17568000
  training_iteration: 183
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    183 |          25447.3 | 17568000 |    20.27 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.12
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.69
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 1.65
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.92
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 1.17
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 0.96
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 153
    cleaning_beam_agent-0_mean: 90.19
    cleaning_beam_agent-0_min: 35
    cleaning_beam_agent-1_max: 397
    cleaning_beam_agent-1_mean: 268.95
    cleaning_beam_agent-1_min: 193
    cleaning_beam_agent-2_max: 199
    cleaning_beam_agent-2_mean: 42.11
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 153
    cleaning_beam_agent-3_mean: 66.84
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 396
    cleaning_beam_agent-4_mean: 91.81
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 34
    cleaning_beam_agent-5_mean: 11.04
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 4
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 10
    fire_beam_agent-5_mean: 0.14
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-09-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 46.0
  episode_reward_mean: 10.83
  episode_reward_min: -394.0
  episodes_this_iter: 96
  episodes_total: 17664
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12318.859
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.6080847382545471
        entropy_coeff: 0.0017600000137463212
        kl: 0.005232986528426409
        model: {}
        policy_loss: -0.00940790120512247
        total_loss: -0.009798822924494743
        vf_explained_var: -0.00030393898487091064
        vf_loss: 1.5600931644439697
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00016375680570490658
        entropy: 0.5180412530899048
        entropy_coeff: 0.0017600000137463212
        kl: 0.0039247069507837296
        model: {}
        policy_loss: -0.0044794403947889805
        total_loss: -0.004338752944022417
        vf_explained_var: 0.006044432520866394
        vf_loss: 8.562078475952148
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.5917587280273438
        entropy_coeff: 0.0017600000137463212
        kl: 0.006609315052628517
        model: {}
        policy_loss: -0.010583532974123955
        total_loss: -0.01054458785802126
        vf_explained_var: 0.0025443285703659058
        vf_loss: 4.195093631744385
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00016375680570490658
        entropy: 0.6421661376953125
        entropy_coeff: 0.0017600000137463212
        kl: 0.003967443015426397
        model: {}
        policy_loss: -0.0044014910236001015
        total_loss: -0.004512139596045017
        vf_explained_var: -0.0008327960968017578
        vf_loss: 6.22819709777832
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00016375680570490658
        entropy: 0.6909979581832886
        entropy_coeff: 0.0017600000137463212
        kl: 0.005111828446388245
        model: {}
        policy_loss: -0.005790139548480511
        total_loss: -0.005545605905354023
        vf_explained_var: 0.010964125394821167
        vf_loss: 12.050962448120117
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00016375680570490658
        entropy: 0.6725357174873352
        entropy_coeff: 0.0017600000137463212
        kl: 0.006534176878631115
        model: {}
        policy_loss: -0.011550473049283028
        total_loss: -0.012393408454954624
        vf_explained_var: -0.01502467691898346
        vf_loss: 0.14016486704349518
    load_time_ms: 13710.509
    num_steps_sampled: 17664000
    num_steps_trained: 17664000
    sample_time_ms: 100458.649
    update_time_ms: 17.008
  iterations_since_restore: 24
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.13038674033149
    ram_util_percent: 12.186187845303866
  pid: 5668
  policy_reward_max:
    agent-0: 9.0
    agent-1: 7.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 16.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 2.52
    agent-1: -0.06
    agent-2: 2.57
    agent-3: 2.26
    agent-4: 2.19
    agent-5: 1.35
  policy_reward_min:
    agent-0: -50.0
    agent-1: -147.0
    agent-2: -45.0
    agent-3: -93.0
    agent-4: -196.0
    agent-5: -10.0
  sampler_perf:
    mean_env_wait_ms: 22.981453590355628
    mean_inference_ms: 12.894219328685883
    mean_processing_ms: 57.755133814623484
  time_since_restore: 3064.6165566444397
  time_this_iter_s: 127.64925217628479
  time_total_s: 25574.899799108505
  timestamp: 1637222967
  timesteps_since_restore: 2304000
  timesteps_this_iter: 96000
  timesteps_total: 17664000
  training_iteration: 184
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    184 |          25574.9 | 17664000 |    10.83 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 2.43
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.88
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 2.05
    apples_agent-2_min: 0
    apples_agent-3_max: 23
    apples_agent-3_mean: 3.01
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.24
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.84
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 162
    cleaning_beam_agent-0_mean: 93.49
    cleaning_beam_agent-0_min: 41
    cleaning_beam_agent-1_max: 391
    cleaning_beam_agent-1_mean: 261.5
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 315
    cleaning_beam_agent-2_mean: 43.61
    cleaning_beam_agent-2_min: 0
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 58.4
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 287
    cleaning_beam_agent-4_mean: 89.14
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 10.25
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 4
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-11-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 72.0
  episode_reward_mean: 16.55
  episode_reward_min: -182.0
  episodes_this_iter: 96
  episodes_total: 17760
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12297.149
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 0.603935718536377
        entropy_coeff: 0.0017600000137463212
        kl: 0.00708151888102293
        model: {}
        policy_loss: -0.01510467380285263
        total_loss: -0.01542561687529087
        vf_explained_var: -0.005135819315910339
        vf_loss: 0.3383105993270874
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00015776639338582754
        entropy: 0.5327376127243042
        entropy_coeff: 0.0017600000137463212
        kl: 0.006657131947577
        model: {}
        policy_loss: -0.012676186859607697
        total_loss: -0.013426600024104118
        vf_explained_var: 0.0006687790155410767
        vf_loss: 0.20774875581264496
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015776639338582754
        entropy: 0.5993480682373047
        entropy_coeff: 0.0017600000137463212
        kl: 0.0063958908431231976
        model: {}
        policy_loss: -0.012762126512825489
        total_loss: -0.01299856323748827
        vf_explained_var: 0.00950893759727478
        vf_loss: 1.7882616519927979
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015776639338582754
        entropy: 0.6388842463493347
        entropy_coeff: 0.0017600000137463212
        kl: 0.0062523880042135715
        model: {}
        policy_loss: -0.012491046451032162
        total_loss: -0.013270764611661434
        vf_explained_var: -0.027124851942062378
        vf_loss: 0.32098227739334106
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015776639338582754
        entropy: 0.6987993717193604
        entropy_coeff: 0.0017600000137463212
        kl: 0.006816550623625517
        model: {}
        policy_loss: -0.010649960488080978
        total_loss: -0.011384200304746628
        vf_explained_var: 0.005863085389137268
        vf_loss: 1.5481817722320557
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015776639338582754
        entropy: 0.6882079839706421
        entropy_coeff: 0.0017600000137463212
        kl: 0.006386647932231426
        model: {}
        policy_loss: -0.011457178741693497
        total_loss: -0.012338697910308838
        vf_explained_var: -0.028656303882598877
        vf_loss: 0.10397486388683319
    load_time_ms: 13701.532
    num_steps_sampled: 17760000
    num_steps_trained: 17760000
    sample_time_ms: 100439.119
    update_time_ms: 17.041
  iterations_since_restore: 25
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.099441340782125
    ram_util_percent: 12.160335195530726
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 10.0
    agent-2: 20.0
    agent-3: 10.0
    agent-4: 24.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 2.08
    agent-1: 2.4
    agent-2: 3.64
    agent-3: 2.97
    agent-4: 3.91
    agent-5: 1.55
  policy_reward_min:
    agent-0: -50.0
    agent-1: 0.0
    agent-2: -48.0
    agent-3: -93.0
    agent-4: -42.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.973415840114964
    mean_inference_ms: 12.88954584496722
    mean_processing_ms: 57.73152399540033
  time_since_restore: 3189.8280305862427
  time_this_iter_s: 125.21147394180298
  time_total_s: 25700.11127305031
  timestamp: 1637223092
  timesteps_since_restore: 2400000
  timesteps_this_iter: 96000
  timesteps_total: 17760000
  training_iteration: 185
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    185 |          25700.1 | 17760000 |    16.55 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 2.43
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.87
    apples_agent-1_min: 0
    apples_agent-2_max: 33
    apples_agent-2_mean: 2.65
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.84
    apples_agent-3_min: 0
    apples_agent-4_max: 97
    apples_agent-4_mean: 1.96
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 0.82
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 197
    cleaning_beam_agent-0_mean: 97.49
    cleaning_beam_agent-0_min: 32
    cleaning_beam_agent-1_max: 448
    cleaning_beam_agent-1_mean: 261.73
    cleaning_beam_agent-1_min: 168
    cleaning_beam_agent-2_max: 222
    cleaning_beam_agent-2_mean: 41.31
    cleaning_beam_agent-2_min: 7
    cleaning_beam_agent-3_max: 187
    cleaning_beam_agent-3_mean: 67.58
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 268
    cleaning_beam_agent-4_mean: 97.18
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 24
    cleaning_beam_agent-5_mean: 8.75
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 5
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-13-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 70.0
  episode_reward_mean: 19.88
  episode_reward_min: -32.0
  episodes_this_iter: 96
  episodes_total: 17856
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12276.936
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 0.611707329750061
        entropy_coeff: 0.0017600000137463212
        kl: 0.007211265154182911
        model: {}
        policy_loss: -0.01626281999051571
        total_loss: -0.01657826080918312
        vf_explained_var: 0.00037638843059539795
        vf_loss: 0.4003638029098511
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00015177599561866373
        entropy: 0.543590247631073
        entropy_coeff: 0.0017600000137463212
        kl: 0.0066509852185845375
        model: {}
        policy_loss: -0.01260372344404459
        total_loss: -0.013365543447434902
        vf_explained_var: 0.014479368925094604
        vf_loss: 0.28626689314842224
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00015177599561866373
        entropy: 0.5835606455802917
        entropy_coeff: 0.0017600000137463212
        kl: 0.0070616938173770905
        model: {}
        policy_loss: -0.01760970801115036
        total_loss: -0.017884084954857826
        vf_explained_var: -0.00750693678855896
        vf_loss: 0.4652019739151001
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015177599561866373
        entropy: 0.6618472337722778
        entropy_coeff: 0.0017600000137463212
        kl: 0.005928309168666601
        model: {}
        policy_loss: -0.007424043491482735
        total_loss: -0.008114160038530827
        vf_explained_var: -0.0038102716207504272
        vf_loss: 1.7831861972808838
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015177599561866373
        entropy: 0.6921590566635132
        entropy_coeff: 0.0017600000137463212
        kl: 0.007747244089841843
        model: {}
        policy_loss: -0.018143504858016968
        total_loss: -0.018934693187475204
        vf_explained_var: 0.004901483654975891
        vf_loss: 0.396517813205719
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00015177599561866373
        entropy: 0.6733198165893555
        entropy_coeff: 0.0017600000137463212
        kl: 0.006551065016537905
        model: {}
        policy_loss: -0.010558300651609898
        total_loss: -0.01140478067100048
        vf_explained_var: -0.017640113830566406
        vf_loss: 0.11008340865373611
    load_time_ms: 13729.262
    num_steps_sampled: 17856000
    num_steps_trained: 17856000
    sample_time_ms: 100428.185
    update_time_ms: 16.907
  iterations_since_restore: 26
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.48611111111111
    ram_util_percent: 12.213888888888889
  pid: 5668
  policy_reward_max:
    agent-0: 20.0
    agent-1: 16.0
    agent-2: 20.0
    agent-3: 15.0
    agent-4: 12.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.62
    agent-1: 2.32
    agent-2: 4.35
    agent-3: 3.78
    agent-4: 4.27
    agent-5: 1.54
  policy_reward_min:
    agent-0: 0.0
    agent-1: -5.0
    agent-2: 0.0
    agent-3: -48.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.97010720710981
    mean_inference_ms: 12.883687209862703
    mean_processing_ms: 57.71066652401824
  time_since_restore: 3316.486785888672
  time_this_iter_s: 126.6587553024292
  time_total_s: 25826.770028352737
  timestamp: 1637223219
  timesteps_since_restore: 2496000
  timesteps_this_iter: 96000
  timesteps_total: 17856000
  training_iteration: 186
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    186 |          25826.8 | 17856000 |    19.88 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 3.14
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.82
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 1.62
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.92
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.06
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.79
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 226
    cleaning_beam_agent-0_mean: 100.11
    cleaning_beam_agent-0_min: 14
    cleaning_beam_agent-1_max: 408
    cleaning_beam_agent-1_mean: 264.2
    cleaning_beam_agent-1_min: 180
    cleaning_beam_agent-2_max: 100
    cleaning_beam_agent-2_mean: 40.98
    cleaning_beam_agent-2_min: 7
    cleaning_beam_agent-3_max: 180
    cleaning_beam_agent-3_mean: 72.18
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 276
    cleaning_beam_agent-4_mean: 87.23
    cleaning_beam_agent-4_min: 24
    cleaning_beam_agent-5_max: 27
    cleaning_beam_agent-5_mean: 9.9
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-15-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 19.07
  episode_reward_min: -39.0
  episodes_this_iter: 96
  episodes_total: 17952
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12264.434
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00014578559785149992
        entropy: 0.6022939085960388
        entropy_coeff: 0.0017600000137463212
        kl: 0.00700447428971529
        model: {}
        policy_loss: -0.017163902521133423
        total_loss: -0.01749545708298683
        vf_explained_var: -0.0018614083528518677
        vf_loss: 0.2803501486778259
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00014578559785149992
        entropy: 0.5397972464561462
        entropy_coeff: 0.0017600000137463212
        kl: 0.005437181796878576
        model: {}
        policy_loss: -0.0070012896321713924
        total_loss: -0.007681970950216055
        vf_explained_var: 0.007772848010063171
        vf_loss: 1.3343374729156494
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00014578559785149992
        entropy: 0.5837208032608032
        entropy_coeff: 0.0017600000137463212
        kl: 0.006984567269682884
        model: {}
        policy_loss: -0.016651196405291557
        total_loss: -0.016937443986535072
        vf_explained_var: -0.010241344571113586
        vf_loss: 0.4264606833457947
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00014578559785149992
        entropy: 0.660578727722168
        entropy_coeff: 0.0017600000137463212
        kl: 0.006262095179408789
        model: {}
        policy_loss: -0.01271754689514637
        total_loss: -0.013528439216315746
        vf_explained_var: -0.018678516149520874
        vf_loss: 0.38616472482681274
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00014578559785149992
        entropy: 0.6925732493400574
        entropy_coeff: 0.0017600000137463212
        kl: 0.007936449721455574
        model: {}
        policy_loss: -0.017415175214409828
        total_loss: -0.018185485154390335
        vf_explained_var: 0.024837911128997803
        vf_loss: 0.5179617404937744
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00014578559785149992
        entropy: 0.6564370393753052
        entropy_coeff: 0.0017600000137463212
        kl: 0.007018409203737974
        model: {}
        policy_loss: -0.010952578857541084
        total_loss: -0.011745087802410126
        vf_explained_var: -0.000594213604927063
        vf_loss: 0.1190037727355957
    load_time_ms: 13759.798
    num_steps_sampled: 17952000
    num_steps_trained: 17952000
    sample_time_ms: 100445.545
    update_time_ms: 16.805
  iterations_since_restore: 27
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.12111111111111
    ram_util_percent: 12.136666666666663
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 9.0
    agent-2: 14.0
    agent-3: 16.0
    agent-4: 16.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.32
    agent-1: 1.33
    agent-2: 4.12
    agent-3: 4.07
    agent-4: 4.76
    agent-5: 1.47
  policy_reward_min:
    agent-0: 0.0
    agent-1: -48.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.972362249655113
    mean_inference_ms: 12.879661001576444
    mean_processing_ms: 57.7137039575297
  time_since_restore: 3443.123762845993
  time_this_iter_s: 126.63697695732117
  time_total_s: 25953.40700531006
  timestamp: 1637223346
  timesteps_since_restore: 2592000
  timesteps_this_iter: 96000
  timesteps_total: 17952000
  training_iteration: 187
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    187 |          25953.4 | 17952000 |    19.07 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.1
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.8
    apples_agent-1_min: 0
    apples_agent-2_max: 31
    apples_agent-2_mean: 2.06
    apples_agent-2_min: 0
    apples_agent-3_max: 49
    apples_agent-3_mean: 3.4
    apples_agent-3_min: 0
    apples_agent-4_max: 30
    apples_agent-4_mean: 1.49
    apples_agent-4_min: 0
    apples_agent-5_max: 32
    apples_agent-5_mean: 1.57
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 167
    cleaning_beam_agent-0_mean: 97.39
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 401
    cleaning_beam_agent-1_mean: 271.72
    cleaning_beam_agent-1_min: 185
    cleaning_beam_agent-2_max: 142
    cleaning_beam_agent-2_mean: 46.48
    cleaning_beam_agent-2_min: 8
    cleaning_beam_agent-3_max: 166
    cleaning_beam_agent-3_mean: 83.01
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 212
    cleaning_beam_agent-4_mean: 88.2
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 38
    cleaning_beam_agent-5_mean: 9.56
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-17-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 16.93
  episode_reward_min: -110.0
  episodes_this_iter: 96
  episodes_total: 18048
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12256.595
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001397952000843361
        entropy: 0.6031900644302368
        entropy_coeff: 0.0017600000137463212
        kl: 0.005343212280422449
        model: {}
        policy_loss: -0.007032702211290598
        total_loss: -0.0072739520110189915
        vf_explained_var: -0.0007467716932296753
        vf_loss: 2.860438346862793
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0001397952000843361
        entropy: 0.5435357689857483
        entropy_coeff: 0.0017600000137463212
        kl: 0.005801300052553415
        model: {}
        policy_loss: -0.006292033940553665
        total_loss: -0.006958464160561562
        vf_explained_var: 0.008413225412368774
        vf_loss: 1.4516109228134155
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001397952000843361
        entropy: 0.5900261998176575
        entropy_coeff: 0.0017600000137463212
        kl: 0.005592843983322382
        model: {}
        policy_loss: -0.009482145309448242
        total_loss: -0.009643439203500748
        vf_explained_var: -0.0023615360260009766
        vf_loss: 3.178692579269409
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001397952000843361
        entropy: 0.6772769689559937
        entropy_coeff: 0.0017600000137463212
        kl: 0.006441614590585232
        model: {}
        policy_loss: -0.012402839958667755
        total_loss: -0.013236086815595627
        vf_explained_var: -0.0035528242588043213
        vf_loss: 0.36682194471359253
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001397952000843361
        entropy: 0.6917428970336914
        entropy_coeff: 0.0017600000137463212
        kl: 0.00818820670247078
        model: {}
        policy_loss: -0.01802193559706211
        total_loss: -0.018784750252962112
        vf_explained_var: 0.01777760684490204
        vf_loss: 0.45239776372909546
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001397952000843361
        entropy: 0.6766757369041443
        entropy_coeff: 0.0017600000137463212
        kl: 0.004520305432379246
        model: {}
        policy_loss: -0.006865750998258591
        total_loss: -0.007744398433715105
        vf_explained_var: -0.012727946043014526
        vf_loss: 0.8628703355789185
    load_time_ms: 13798.055
    num_steps_sampled: 18048000
    num_steps_trained: 18048000
    sample_time_ms: 100376.912
    update_time_ms: 16.502
  iterations_since_restore: 28
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.175
    ram_util_percent: 12.138333333333332
  pid: 5668
  policy_reward_max:
    agent-0: 11.0
    agent-1: 9.0
    agent-2: 25.0
    agent-3: 14.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.24
    agent-1: 1.44
    agent-2: 3.57
    agent-3: 3.92
    agent-4: 4.77
    agent-5: 0.99
  policy_reward_min:
    agent-0: -46.0
    agent-1: -44.0
    agent-2: -49.0
    agent-3: -1.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 22.969672991083254
    mean_inference_ms: 12.874923773604968
    mean_processing_ms: 57.71947385164829
  time_since_restore: 3568.86146569252
  time_this_iter_s: 125.7377028465271
  time_total_s: 26079.144708156586
  timestamp: 1637223472
  timesteps_since_restore: 2688000
  timesteps_this_iter: 96000
  timesteps_total: 18048000
  training_iteration: 188
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    188 |          26079.1 | 18048000 |    16.93 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.41
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.92
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 2.17
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 3.22
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.46
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 190
    cleaning_beam_agent-0_mean: 94.55
    cleaning_beam_agent-0_min: 35
    cleaning_beam_agent-1_max: 398
    cleaning_beam_agent-1_mean: 279.78
    cleaning_beam_agent-1_min: 171
    cleaning_beam_agent-2_max: 175
    cleaning_beam_agent-2_mean: 47.36
    cleaning_beam_agent-2_min: 10
    cleaning_beam_agent-3_max: 171
    cleaning_beam_agent-3_mean: 77.64
    cleaning_beam_agent-3_min: 30
    cleaning_beam_agent-4_max: 311
    cleaning_beam_agent-4_mean: 90.32
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 25
    cleaning_beam_agent-5_mean: 7.41
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-19-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 19.82
  episode_reward_min: -39.0
  episodes_this_iter: 96
  episodes_total: 18144
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12239.255
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001338048023171723
        entropy: 0.5994682908058167
        entropy_coeff: 0.0017600000137463212
        kl: 0.006758138071745634
        model: {}
        policy_loss: -0.016373353078961372
        total_loss: -0.016721004620194435
        vf_explained_var: 0.006402537226676941
        vf_loss: 0.31597962975502014
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0001338048023171723
        entropy: 0.5361117124557495
        entropy_coeff: 0.0017600000137463212
        kl: 0.0066790711134672165
        model: {}
        policy_loss: -0.012199219316244125
        total_loss: -0.012956374324858189
        vf_explained_var: 0.027219220995903015
        vf_loss: 0.1942346692085266
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001338048023171723
        entropy: 0.585303783416748
        entropy_coeff: 0.0017600000137463212
        kl: 0.004995827563107014
        model: {}
        policy_loss: -0.010427976958453655
        total_loss: -0.010847646743059158
        vf_explained_var: 0.009751006960868835
        vf_loss: 1.1088308095932007
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001338048023171723
        entropy: 0.6647092700004578
        entropy_coeff: 0.0017600000137463212
        kl: 0.005876356270164251
        model: {}
        policy_loss: -0.010998387821018696
        total_loss: -0.011827046982944012
        vf_explained_var: -0.0030578672885894775
        vf_loss: 0.4741283059120178
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0001338048023171723
        entropy: 0.6845022439956665
        entropy_coeff: 0.0017600000137463212
        kl: 0.007860919460654259
        model: {}
        policy_loss: -0.017269916832447052
        total_loss: -0.018039017915725708
        vf_explained_var: -0.005076095461845398
        vf_loss: 0.4257996678352356
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.0001338048023171723
        entropy: 0.6682218909263611
        entropy_coeff: 0.0017600000137463212
        kl: 0.006554822903126478
        model: {}
        policy_loss: -0.010626659728586674
        total_loss: -0.011626101098954678
        vf_explained_var: -0.011911094188690186
        vf_loss: 0.12759971618652344
    load_time_ms: 13787.091
    num_steps_sampled: 18144000
    num_steps_trained: 18144000
    sample_time_ms: 100348.301
    update_time_ms: 16.566
  iterations_since_restore: 29
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.620670391061456
    ram_util_percent: 12.20335195530726
  pid: 5668
  policy_reward_max:
    agent-0: 12.0
    agent-1: 11.0
    agent-2: 12.0
    agent-3: 20.0
    agent-4: 13.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.68
    agent-1: 2.31
    agent-2: 3.73
    agent-3: 4.08
    agent-4: 4.38
    agent-5: 1.64
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -45.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.966639215630547
    mean_inference_ms: 12.868121612828757
    mean_processing_ms: 57.70675050771122
  time_since_restore: 3694.6925411224365
  time_this_iter_s: 125.83107542991638
  time_total_s: 26204.975783586502
  timestamp: 1637223598
  timesteps_since_restore: 2784000
  timesteps_this_iter: 96000
  timesteps_total: 18144000
  training_iteration: 189
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    189 |            26205 | 18144000 |    19.82 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.5
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 1.09
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 2.27
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 3.36
    apples_agent-3_min: 0
    apples_agent-4_max: 30
    apples_agent-4_mean: 1.41
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.09
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 174
    cleaning_beam_agent-0_mean: 93.3
    cleaning_beam_agent-0_min: 33
    cleaning_beam_agent-1_max: 439
    cleaning_beam_agent-1_mean: 281.12
    cleaning_beam_agent-1_min: 171
    cleaning_beam_agent-2_max: 171
    cleaning_beam_agent-2_mean: 51.39
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 168
    cleaning_beam_agent-3_mean: 77.92
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 281
    cleaning_beam_agent-4_mean: 92.26
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 22
    cleaning_beam_agent-5_mean: 7.37
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-22-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 20.87
  episode_reward_min: -31.0
  episodes_this_iter: 96
  episodes_total: 18240
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12227.347
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012781440455000848
        entropy: 0.5935598015785217
        entropy_coeff: 0.0017600000137463212
        kl: 0.006673347670584917
        model: {}
        policy_loss: -0.015450142323970795
        total_loss: -0.01579420268535614
        vf_explained_var: 0.004801332950592041
        vf_loss: 0.33269527554512024
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00012781440455000848
        entropy: 0.5448905825614929
        entropy_coeff: 0.0017600000137463212
        kl: 0.006446590181440115
        model: {}
        policy_loss: -0.013065344654023647
        total_loss: -0.013841413892805576
        vf_explained_var: 0.01812627911567688
        vf_loss: 0.21769526600837708
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012781440455000848
        entropy: 0.5904897451400757
        entropy_coeff: 0.0017600000137463212
        kl: 0.0069862836971879005
        model: {}
        policy_loss: -0.016447141766548157
        total_loss: -0.017088377848267555
        vf_explained_var: -0.0013936758041381836
        vf_loss: 0.4871073067188263
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012781440455000848
        entropy: 0.6585589051246643
        entropy_coeff: 0.0017600000137463212
        kl: 0.005712502170354128
        model: {}
        policy_loss: -0.006587157025933266
        total_loss: -0.007318637799471617
        vf_explained_var: -0.014072626829147339
        vf_loss: 1.419569969177246
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012781440455000848
        entropy: 0.6733181476593018
        entropy_coeff: 0.0017600000137463212
        kl: 0.006164952181279659
        model: {}
        policy_loss: -0.011037524789571762
        total_loss: -0.011730588972568512
        vf_explained_var: 0.0082293301820755
        vf_loss: 1.8372780084609985
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00012781440455000848
        entropy: 0.6764827966690063
        entropy_coeff: 0.0017600000137463212
        kl: 0.006082048639655113
        model: {}
        policy_loss: -0.01041665393859148
        total_loss: -0.011441895738244057
        vf_explained_var: -0.004885807633399963
        vf_loss: 0.1332114189863205
    load_time_ms: 13753.347
    num_steps_sampled: 18240000
    num_steps_trained: 18240000
    sample_time_ms: 100244.053
    update_time_ms: 16.655
  iterations_since_restore: 30
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.34915254237288
    ram_util_percent: 12.225423728813558
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 12.0
    agent-2: 15.0
    agent-3: 20.0
    agent-4: 17.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.54
    agent-1: 2.47
    agent-2: 4.7
    agent-3: 3.9
    agent-4: 4.57
    agent-5: 1.69
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -46.0
    agent-4: -44.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.964415387755324
    mean_inference_ms: 12.862602236531906
    mean_processing_ms: 57.69374891918685
  time_since_restore: 3819.5032229423523
  time_this_iter_s: 124.81068181991577
  time_total_s: 26329.786465406418
  timestamp: 1637223723
  timesteps_since_restore: 2880000
  timesteps_this_iter: 96000
  timesteps_total: 18240000
  training_iteration: 190
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    190 |          26329.8 | 18240000 |    20.87 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.35
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.01
    apples_agent-1_min: 0
    apples_agent-2_max: 7
    apples_agent-2_mean: 1.87
    apples_agent-2_min: 0
    apples_agent-3_max: 28
    apples_agent-3_mean: 3.6
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.04
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 0.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 183
    cleaning_beam_agent-0_mean: 93.18
    cleaning_beam_agent-0_min: 38
    cleaning_beam_agent-1_max: 494
    cleaning_beam_agent-1_mean: 284.02
    cleaning_beam_agent-1_min: 205
    cleaning_beam_agent-2_max: 250
    cleaning_beam_agent-2_mean: 52.15
    cleaning_beam_agent-2_min: 13
    cleaning_beam_agent-3_max: 219
    cleaning_beam_agent-3_mean: 77.33
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 305
    cleaning_beam_agent-4_mean: 91.69
    cleaning_beam_agent-4_min: 19
    cleaning_beam_agent-5_max: 25
    cleaning_beam_agent-5_mean: 5.88
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-24-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 21.59
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 18336
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12212.584
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012182399950688705
        entropy: 0.600163459777832
        entropy_coeff: 0.0017600000137463212
        kl: 0.006870590150356293
        model: {}
        policy_loss: -0.015955671668052673
        total_loss: -0.01629466935992241
        vf_explained_var: 0.00012581050395965576
        vf_loss: 0.3023550510406494
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00012182399950688705
        entropy: 0.5389928817749023
        entropy_coeff: 0.0017600000137463212
        kl: 0.0054502934217453
        model: {}
        policy_loss: -0.011008819565176964
        total_loss: -0.011798015795648098
        vf_explained_var: 0.01649843156337738
        vf_loss: 0.23173421621322632
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012182399950688705
        entropy: 0.5856192708015442
        entropy_coeff: 0.0017600000137463212
        kl: 0.007057751063257456
        model: {}
        policy_loss: -0.01633705012500286
        total_loss: -0.016968347132205963
        vf_explained_var: 0.0011372566223144531
        vf_loss: 0.46503013372421265
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012182399950688705
        entropy: 0.6378493309020996
        entropy_coeff: 0.0017600000137463212
        kl: 0.005495505407452583
        model: {}
        policy_loss: -0.011279325932264328
        total_loss: -0.012085284106433392
        vf_explained_var: -0.021126151084899902
        vf_loss: 0.41882792115211487
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00012182399950688705
        entropy: 0.677013635635376
        entropy_coeff: 0.0017600000137463212
        kl: 0.007082435768097639
        model: {}
        policy_loss: -0.016593001782894135
        total_loss: -0.01739027351140976
        vf_explained_var: 0.0157184898853302
        vf_loss: 0.4015088677406311
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00012182399950688705
        entropy: 0.6684199571609497
        entropy_coeff: 0.0017600000137463212
        kl: 0.00572219118475914
        model: {}
        policy_loss: -0.009693363681435585
        total_loss: -0.01071417797356844
        vf_explained_var: -0.005247429013252258
        vf_loss: 0.12549565732479095
    load_time_ms: 13741.681
    num_steps_sampled: 18336000
    num_steps_trained: 18336000
    sample_time_ms: 99997.808
    update_time_ms: 16.563
  iterations_since_restore: 31
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.877222222222226
    ram_util_percent: 12.164444444444442
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 13.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 12.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 3.61
    agent-1: 2.26
    agent-2: 5.1
    agent-3: 4.43
    agent-4: 4.44
    agent-5: 1.75
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.96685578227418
    mean_inference_ms: 12.858266176576057
    mean_processing_ms: 57.68070768839816
  time_since_restore: 3945.0236303806305
  time_this_iter_s: 125.5204074382782
  time_total_s: 26455.306872844696
  timestamp: 1637223849
  timesteps_since_restore: 2976000
  timesteps_this_iter: 96000
  timesteps_total: 18336000
  training_iteration: 191
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    191 |          26455.3 | 18336000 |    21.59 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 2.62
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.94
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 1.93
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.66
    apples_agent-3_min: 0
    apples_agent-4_max: 28
    apples_agent-4_mean: 1.61
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.15
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 178
    cleaning_beam_agent-0_mean: 88.47
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 412
    cleaning_beam_agent-1_mean: 271.28
    cleaning_beam_agent-1_min: 174
    cleaning_beam_agent-2_max: 182
    cleaning_beam_agent-2_mean: 46.48
    cleaning_beam_agent-2_min: 8
    cleaning_beam_agent-3_max: 148
    cleaning_beam_agent-3_mean: 72.08
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 247
    cleaning_beam_agent-4_mean: 87.11
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 26
    cleaning_beam_agent-5_mean: 6.87
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-26-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 20.68
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 18432
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12199.082
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00011583360173972324
        entropy: 0.580421507358551
        entropy_coeff: 0.0017600000137463212
        kl: 0.006274552084505558
        model: {}
        policy_loss: -0.014932986348867416
        total_loss: -0.015296012163162231
        vf_explained_var: 0.0004217475652694702
        vf_loss: 0.31062471866607666
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00011583360173972324
        entropy: 0.5273417234420776
        entropy_coeff: 0.0017600000137463212
        kl: 0.0058224741369485855
        model: {}
        policy_loss: -0.010940345004200935
        total_loss: -0.01170164905488491
        vf_explained_var: 0.01434779167175293
        vf_loss: 0.21253754198551178
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00011583360173972324
        entropy: 0.5637953281402588
        entropy_coeff: 0.0017600000137463212
        kl: 0.0068000745959579945
        model: {}
        policy_loss: -0.015363922342658043
        total_loss: -0.015973296016454697
        vf_explained_var: -0.004115402698516846
        vf_loss: 0.42905211448669434
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00011583360173972324
        entropy: 0.6482491493225098
        entropy_coeff: 0.0017600000137463212
        kl: 0.005200312007218599
        model: {}
        policy_loss: -0.010772387497127056
        total_loss: -0.01161323394626379
        vf_explained_var: -0.0012212544679641724
        vf_loss: 0.40057456493377686
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00011583360173972324
        entropy: 0.6740338802337646
        entropy_coeff: 0.0017600000137463212
        kl: 0.007215470541268587
        model: {}
        policy_loss: -0.015731116756796837
        total_loss: -0.01651124469935894
        vf_explained_var: -0.0046570152044296265
        vf_loss: 0.45397520065307617
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00011583360173972324
        entropy: 0.6824710369110107
        entropy_coeff: 0.0017600000137463212
        kl: 0.006711120717227459
        model: {}
        policy_loss: -0.009654417634010315
        total_loss: -0.01067478395998478
        vf_explained_var: -0.003344625234603882
        vf_loss: 0.13001756370067596
    load_time_ms: 13742.658
    num_steps_sampled: 18432000
    num_steps_trained: 18432000
    sample_time_ms: 99939.847
    update_time_ms: 17.027
  iterations_since_restore: 32
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.875977653631285
    ram_util_percent: 12.2
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 12.0
    agent-2: 13.0
    agent-3: 13.0
    agent-4: 16.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.46
    agent-1: 2.33
    agent-2: 4.45
    agent-3: 3.97
    agent-4: 4.72
    agent-5: 1.75
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.96175738056264
    mean_inference_ms: 12.852390288980398
    mean_processing_ms: 57.66976829662401
  time_since_restore: 4070.2595217227936
  time_this_iter_s: 125.23589134216309
  time_total_s: 26580.54276418686
  timestamp: 1637223974
  timesteps_since_restore: 3072000
  timesteps_this_iter: 96000
  timesteps_total: 18432000
  training_iteration: 192
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    192 |          26580.5 | 18432000 |    20.68 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.15
    apples_agent-0_min: 0
    apples_agent-1_max: 16
    apples_agent-1_mean: 0.99
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.9
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.9
    apples_agent-3_min: 0
    apples_agent-4_max: 29
    apples_agent-4_mean: 1.3
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.09
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 191
    cleaning_beam_agent-0_mean: 90.13
    cleaning_beam_agent-0_min: 35
    cleaning_beam_agent-1_max: 423
    cleaning_beam_agent-1_mean: 264.05
    cleaning_beam_agent-1_min: 166
    cleaning_beam_agent-2_max: 175
    cleaning_beam_agent-2_mean: 48.03
    cleaning_beam_agent-2_min: 9
    cleaning_beam_agent-3_max: 189
    cleaning_beam_agent-3_mean: 80.59
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 247
    cleaning_beam_agent-4_mean: 73.51
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 18
    cleaning_beam_agent-5_mean: 5.99
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-28-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 68.0
  episode_reward_mean: 19.02
  episode_reward_min: -33.0
  episodes_this_iter: 96
  episodes_total: 18528
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12188.296
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00010984319669660181
        entropy: 0.5910905599594116
        entropy_coeff: 0.0017600000137463212
        kl: 0.00586822722107172
        model: {}
        policy_loss: -0.01314019039273262
        total_loss: -0.013566389679908752
        vf_explained_var: 0.005458220839500427
        vf_loss: 0.27297037839889526
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00010984319669660181
        entropy: 0.5332119464874268
        entropy_coeff: 0.0017600000137463212
        kl: 0.005239170044660568
        model: {}
        policy_loss: -0.009801933541893959
        total_loss: -0.010585036128759384
        vf_explained_var: 0.021111011505126953
        vf_loss: 0.24371294677257538
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00010984319669660181
        entropy: 0.5770244598388672
        entropy_coeff: 0.0017600000137463212
        kl: 0.0063768550753593445
        model: {}
        policy_loss: -0.014810966327786446
        total_loss: -0.01546129398047924
        vf_explained_var: 0.003945276141166687
        vf_loss: 0.4639521539211273
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00010984319669660181
        entropy: 0.6696663498878479
        entropy_coeff: 0.0017600000137463212
        kl: 0.005562030244618654
        model: {}
        policy_loss: -0.011310410685837269
        total_loss: -0.01217818632721901
        vf_explained_var: -0.006889402866363525
        vf_loss: 0.3273400366306305
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.00010984319669660181
        entropy: 0.680131733417511
        entropy_coeff: 0.0017600000137463212
        kl: 0.0071182772517204285
        model: {}
        policy_loss: -0.016164585947990417
        total_loss: -0.016964543610811234
        vf_explained_var: -8.317828178405762e-05
        vf_loss: 0.41161903738975525
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.00010984319669660181
        entropy: 0.7105551958084106
        entropy_coeff: 0.0017600000137463212
        kl: 0.007339076604694128
        model: {}
        policy_loss: -0.007465054746717215
        total_loss: -0.008404882624745369
        vf_explained_var: -0.003025531768798828
        vf_loss: 1.272720456123352
    load_time_ms: 13729.064
    num_steps_sampled: 18528000
    num_steps_trained: 18528000
    sample_time_ms: 99962.847
    update_time_ms: 16.849
  iterations_since_restore: 33
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.197206703910616
    ram_util_percent: 12.153072625698321
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 17.0
    agent-2: 23.0
    agent-3: 14.0
    agent-4: 13.0
    agent-5: 6.0
  policy_reward_mean:
    agent-0: 3.28
    agent-1: 1.86
    agent-2: 4.42
    agent-3: 3.87
    agent-4: 4.61
    agent-5: 0.98
  policy_reward_min:
    agent-0: 0.0
    agent-1: -47.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 22.959503119007227
    mean_inference_ms: 12.84910196728542
    mean_processing_ms: 57.66589280069859
  time_since_restore: 4196.8759388923645
  time_this_iter_s: 126.61641716957092
  time_total_s: 26707.15918135643
  timestamp: 1637224101
  timesteps_since_restore: 3168000
  timesteps_this_iter: 96000
  timesteps_total: 18528000
  training_iteration: 193
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    193 |          26707.2 | 18528000 |    19.02 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.38
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.97
    apples_agent-1_min: 0
    apples_agent-2_max: 40
    apples_agent-2_mean: 2.33
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.81
    apples_agent-3_min: 0
    apples_agent-4_max: 67
    apples_agent-4_mean: 2.11
    apples_agent-4_min: 0
    apples_agent-5_max: 125
    apples_agent-5_mean: 2.63
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 278
    cleaning_beam_agent-0_mean: 91.01
    cleaning_beam_agent-0_min: 30
    cleaning_beam_agent-1_max: 459
    cleaning_beam_agent-1_mean: 271.03
    cleaning_beam_agent-1_min: 177
    cleaning_beam_agent-2_max: 221
    cleaning_beam_agent-2_mean: 52.77
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 204
    cleaning_beam_agent-3_mean: 73.59
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 214
    cleaning_beam_agent-4_mean: 80.24
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 36
    cleaning_beam_agent-5_mean: 5.84
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-30-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 72.0
  episode_reward_mean: 21.16
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 18624
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12190.64
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000103852798929438
        entropy: 0.5814269781112671
        entropy_coeff: 0.0017600000137463212
        kl: 0.005791016388684511
        model: {}
        policy_loss: -0.013254317454993725
        total_loss: -0.013669352047145367
        vf_explained_var: -0.0035790055990219116
        vf_loss: 0.29177749156951904
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.000103852798929438
        entropy: 0.5227829217910767
        entropy_coeff: 0.0017600000137463212
        kl: 0.004954126197844744
        model: {}
        policy_loss: -0.009099789895117283
        total_loss: -0.009874068200588226
        vf_explained_var: 0.030418798327445984
        vf_loss: 0.219655841588974
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000103852798929438
        entropy: 0.5680521130561829
        entropy_coeff: 0.0017600000137463212
        kl: 0.006084691733121872
        model: {}
        policy_loss: -0.013594083487987518
        total_loss: -0.014229312539100647
        vf_explained_var: 0.001853257417678833
        vf_loss: 0.6030791997909546
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000103852798929438
        entropy: 0.6487250328063965
        entropy_coeff: 0.0017600000137463212
        kl: 0.005209871102124453
        model: {}
        policy_loss: -0.010533963330090046
        total_loss: -0.011374399065971375
        vf_explained_var: -0.014222502708435059
        vf_loss: 0.40827542543411255
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000103852798929438
        entropy: 0.6756103038787842
        entropy_coeff: 0.0017600000137463212
        kl: 0.005976717919111252
        model: {}
        policy_loss: -0.014321569353342056
        total_loss: -0.015163936652243137
        vf_explained_var: 0.02069242298603058
        vf_loss: 0.47874361276626587
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 0.000103852798929438
        entropy: 0.68604576587677
        entropy_coeff: 0.0017600000137463212
        kl: 0.00594982597976923
        model: {}
        policy_loss: -0.009358400478959084
        total_loss: -0.010401240549981594
        vf_explained_var: -0.005799427628517151
        vf_loss: 0.15858837962150574
    load_time_ms: 13745.149
    num_steps_sampled: 18624000
    num_steps_trained: 18624000
    sample_time_ms: 99674.951
    update_time_ms: 16.913
  iterations_since_restore: 34
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.056741573033708
    ram_util_percent: 12.23426966292135
  pid: 5668
  policy_reward_max:
    agent-0: 11.0
    agent-1: 13.0
    agent-2: 23.0
    agent-3: 16.0
    agent-4: 17.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.54
    agent-1: 2.25
    agent-2: 4.75
    agent-3: 4.03
    agent-4: 4.85
    agent-5: 1.74
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -2.0
  sampler_perf:
    mean_env_wait_ms: 22.95894606590158
    mean_inference_ms: 12.844656145563468
    mean_processing_ms: 57.6652098490089
  time_since_restore: 4321.827137470245
  time_this_iter_s: 124.95119857788086
  time_total_s: 26832.11037993431
  timestamp: 1637224226
  timesteps_since_restore: 3264000
  timesteps_this_iter: 96000
  timesteps_total: 18624000
  training_iteration: 194
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.6/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    194 |          26832.1 | 18624000 |    21.16 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.93
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 1.28
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 2.75
    apples_agent-2_min: 0
    apples_agent-3_max: 31
    apples_agent-3_mean: 3.92
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.24
    apples_agent-4_min: 0
    apples_agent-5_max: 6
    apples_agent-5_mean: 1.23
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 221
    cleaning_beam_agent-0_mean: 91.52
    cleaning_beam_agent-0_min: 34
    cleaning_beam_agent-1_max: 427
    cleaning_beam_agent-1_mean: 271.79
    cleaning_beam_agent-1_min: 178
    cleaning_beam_agent-2_max: 221
    cleaning_beam_agent-2_mean: 56.25
    cleaning_beam_agent-2_min: 8
    cleaning_beam_agent-3_max: 135
    cleaning_beam_agent-3_mean: 74.38
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 266
    cleaning_beam_agent-4_mean: 97.43
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 24
    cleaning_beam_agent-5_mean: 6.82
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-32-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 78.0
  episode_reward_mean: 24.41
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 18720
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12183.23
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.786240116227418e-05
        entropy: 0.585446834564209
        entropy_coeff: 0.0017600000137463212
        kl: 0.0055759986862540245
        model: {}
        policy_loss: -0.012754186056554317
        total_loss: -0.013180526904761791
        vf_explained_var: 0.002495288848876953
        vf_loss: 0.4644993543624878
      agent-1:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 9.786240116227418e-05
        entropy: 0.524871289730072
        entropy_coeff: 0.0017600000137463212
        kl: 0.005224354099482298
        model: {}
        policy_loss: -0.010298729874193668
        total_loss: -0.01113212201744318
        vf_explained_var: 0.013032615184783936
        vf_loss: 0.2507774531841278
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.786240116227418e-05
        entropy: 0.5759257078170776
        entropy_coeff: 0.0017600000137463212
        kl: 0.005689841695129871
        model: {}
        policy_loss: -0.013129051774740219
        total_loss: -0.013774782419204712
        vf_explained_var: 0.0008445680141448975
        vf_loss: 0.8340457677841187
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.786240116227418e-05
        entropy: 0.6559755802154541
        entropy_coeff: 0.0017600000137463212
        kl: 0.004938866943120956
        model: {}
        policy_loss: -0.010203523561358452
        total_loss: -0.011059286072850227
        vf_explained_var: -0.008894294500350952
        vf_loss: 0.518113374710083
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.786240116227418e-05
        entropy: 0.6769652366638184
        entropy_coeff: 0.0017600000137463212
        kl: 0.005925276316702366
        model: {}
        policy_loss: -0.013020124286413193
        total_loss: -0.013854743912816048
        vf_explained_var: 0.03093351423740387
        vf_loss: 0.605746328830719
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 9.786240116227418e-05
        entropy: 0.6882950067520142
        entropy_coeff: 0.0017600000137463212
        kl: 0.006341069005429745
        model: {}
        policy_loss: -0.010220893658697605
        total_loss: -0.011255327612161636
        vf_explained_var: -0.0059164464473724365
        vf_loss: 0.18433751165866852
    load_time_ms: 13745.151
    num_steps_sampled: 18720000
    num_steps_trained: 18720000
    sample_time_ms: 99620.054
    update_time_ms: 16.791
  iterations_since_restore: 35
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.147191011235954
    ram_util_percent: 12.347752808988767
  pid: 5668
  policy_reward_max:
    agent-0: 19.0
    agent-1: 12.0
    agent-2: 26.0
    agent-3: 20.0
    agent-4: 23.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.2
    agent-1: 2.56
    agent-2: 5.57
    agent-3: 4.9
    agent-4: 5.01
    agent-5: 2.17
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.955279086378578
    mean_inference_ms: 12.839519025676898
    mean_processing_ms: 57.65759153323799
  time_since_restore: 4446.438192129135
  time_this_iter_s: 124.61105465888977
  time_total_s: 26956.7214345932
  timestamp: 1637224351
  timesteps_since_restore: 3360000
  timesteps_this_iter: 96000
  timesteps_total: 18720000
  training_iteration: 195
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 20.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    195 |          26956.7 | 18720000 |    24.41 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.71
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.18
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 2.11
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.18
    apples_agent-3_min: 0
    apples_agent-4_max: 61
    apples_agent-4_mean: 1.96
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 0.88
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 203
    cleaning_beam_agent-0_mean: 88.07
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 406
    cleaning_beam_agent-1_mean: 261.65
    cleaning_beam_agent-1_min: 157
    cleaning_beam_agent-2_max: 185
    cleaning_beam_agent-2_mean: 52.98
    cleaning_beam_agent-2_min: 11
    cleaning_beam_agent-3_max: 158
    cleaning_beam_agent-3_mean: 66.68
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 312
    cleaning_beam_agent-4_mean: 88.71
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 7.91
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-34-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 56.0
  episode_reward_mean: 22.45
  episode_reward_min: -33.0
  episodes_this_iter: 96
  episodes_total: 18816
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12186.656
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.187200339511037e-05
        entropy: 0.5727202296257019
        entropy_coeff: 0.0017600000137463212
        kl: 0.005171085242182016
        model: {}
        policy_loss: -0.012474188581109047
        total_loss: -0.0129320677369833
        vf_explained_var: 0.003550007939338684
        vf_loss: 0.32996174693107605
      agent-1:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 9.187200339511037e-05
        entropy: 0.5273747444152832
        entropy_coeff: 0.0017600000137463212
        kl: 0.005197898484766483
        model: {}
        policy_loss: -0.010048653930425644
        total_loss: -0.010893373750150204
        vf_explained_var: 0.03619714081287384
        vf_loss: 0.18487107753753662
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.187200339511037e-05
        entropy: 0.5623117089271545
        entropy_coeff: 0.0017600000137463212
        kl: 0.00557038513943553
        model: {}
        policy_loss: -0.013657254166901112
        total_loss: -0.014322846196591854
        vf_explained_var: 0.006073325872421265
        vf_loss: 0.455564022064209
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 9.187200339511037e-05
        entropy: 0.6398603320121765
        entropy_coeff: 0.0017600000137463212
        kl: 0.005616180598735809
        model: {}
        policy_loss: -0.009624132886528969
        total_loss: -0.010558138601481915
        vf_explained_var: -0.008578062057495117
        vf_loss: 0.5174652338027954
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.187200339511037e-05
        entropy: 0.6647300124168396
        entropy_coeff: 0.0017600000137463212
        kl: 0.006019277032464743
        model: {}
        policy_loss: -0.013805622234940529
        total_loss: -0.01462383009493351
        vf_explained_var: 0.013873964548110962
        vf_loss: 0.5075346827507019
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 9.187200339511037e-05
        entropy: 0.7001562714576721
        entropy_coeff: 0.0017600000137463212
        kl: 0.007194462697952986
        model: {}
        policy_loss: -0.005036813206970692
        total_loss: -0.005960535258054733
        vf_explained_var: 0.007486239075660706
        vf_loss: 1.2868938446044922
    load_time_ms: 13705.927
    num_steps_sampled: 18816000
    num_steps_trained: 18816000
    sample_time_ms: 99676.453
    update_time_ms: 16.879
  iterations_since_restore: 36
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.79888888888889
    ram_util_percent: 12.248333333333333
  pid: 5668
  policy_reward_max:
    agent-0: 14.0
    agent-1: 9.0
    agent-2: 17.0
    agent-3: 21.0
    agent-4: 23.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.9
    agent-1: 2.35
    agent-2: 4.89
    agent-3: 4.83
    agent-4: 5.24
    agent-5: 1.24
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 22.954441763537684
    mean_inference_ms: 12.837034254186646
    mean_processing_ms: 57.650698774227166
  time_since_restore: 4573.260372400284
  time_this_iter_s: 126.82218027114868
  time_total_s: 27083.54361486435
  timestamp: 1637224478
  timesteps_since_restore: 3456000
  timesteps_this_iter: 96000
  timesteps_total: 18816000
  training_iteration: 196
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.6/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    196 |          27083.5 | 18816000 |    22.45 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 2.57
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 1.25
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.88
    apples_agent-2_min: 0
    apples_agent-3_max: 22
    apples_agent-3_mean: 3.71
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 1.33
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.0
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 217
    cleaning_beam_agent-0_mean: 86.07
    cleaning_beam_agent-0_min: 33
    cleaning_beam_agent-1_max: 426
    cleaning_beam_agent-1_mean: 273.86
    cleaning_beam_agent-1_min: 199
    cleaning_beam_agent-2_max: 169
    cleaning_beam_agent-2_mean: 55.29
    cleaning_beam_agent-2_min: 14
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 70.1
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 320
    cleaning_beam_agent-4_mean: 91.42
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 43
    cleaning_beam_agent-5_mean: 10.14
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-36-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 21.61
  episode_reward_min: -80.0
  episodes_this_iter: 96
  episodes_total: 18912
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12185.305
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 8.588159835198894e-05
        entropy: 0.5741170644760132
        entropy_coeff: 0.0017600000137463212
        kl: 0.005043596960604191
        model: {}
        policy_loss: -0.011862795799970627
        total_loss: -0.01232803426682949
        vf_explained_var: 0.0037571191787719727
        vf_loss: 0.4084780514240265
      agent-1:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 8.588159835198894e-05
        entropy: 0.5376215577125549
        entropy_coeff: 0.0017600000137463212
        kl: 0.004559434950351715
        model: {}
        policy_loss: -0.008990414440631866
        total_loss: -0.00985367689281702
        vf_explained_var: 0.04034845530986786
        vf_loss: 0.25958651304244995
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 8.588159835198894e-05
        entropy: 0.580357551574707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0056190346367657185
        model: {}
        policy_loss: -0.012686745263636112
        total_loss: -0.013368003070354462
        vf_explained_var: -0.0030480623245239258
        vf_loss: 0.5922002792358398
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 8.588159835198894e-05
        entropy: 0.629958987236023
        entropy_coeff: 0.0017600000137463212
        kl: 0.005119801498949528
        model: {}
        policy_loss: -0.009813843294978142
        total_loss: -0.010735912248492241
        vf_explained_var: -0.0015938282012939453
        vf_loss: 0.5866336226463318
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 8.588159835198894e-05
        entropy: 0.6665886044502258
        entropy_coeff: 0.0017600000137463212
        kl: 0.0048356447368860245
        model: {}
        policy_loss: -0.008927982300519943
        total_loss: -0.009704859927296638
        vf_explained_var: 0.015482872724533081
        vf_loss: 1.54532790184021
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 8.588159835198894e-05
        entropy: 0.7186585068702698
        entropy_coeff: 0.0017600000137463212
        kl: 0.005476220510900021
        model: {}
        policy_loss: -0.008282494731247425
        total_loss: -0.009393315762281418
        vf_explained_var: -0.01882532238960266
        vf_loss: 0.1711208075284958
    load_time_ms: 13688.39
    num_steps_sampled: 18912000
    num_steps_trained: 18912000
    sample_time_ms: 99545.526
    update_time_ms: 16.915
  iterations_since_restore: 37
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.069662921348318
    ram_util_percent: 12.220224719101122
  pid: 5668
  policy_reward_max:
    agent-0: 14.0
    agent-1: 13.0
    agent-2: 17.0
    agent-3: 17.0
    agent-4: 16.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.98
    agent-1: 2.55
    agent-2: 4.94
    agent-3: 4.89
    agent-4: 3.86
    agent-5: 1.39
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -50.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 22.9537561293418
    mean_inference_ms: 12.834987075689396
    mean_processing_ms: 57.64496718787204
  time_since_restore: 4698.359710216522
  time_this_iter_s: 125.0993378162384
  time_total_s: 27208.642952680588
  timestamp: 1637224603
  timesteps_since_restore: 3552000
  timesteps_this_iter: 96000
  timesteps_total: 18912000
  training_iteration: 197
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    197 |          27208.6 | 18912000 |    21.61 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.19
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 1.33
    apples_agent-1_min: 0
    apples_agent-2_max: 8
    apples_agent-2_mean: 1.75
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 2.77
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.47
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.17
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 179
    cleaning_beam_agent-0_mean: 92.27
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 592
    cleaning_beam_agent-1_mean: 273.93
    cleaning_beam_agent-1_min: 167
    cleaning_beam_agent-2_max: 181
    cleaning_beam_agent-2_mean: 48.1
    cleaning_beam_agent-2_min: 14
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 62.45
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 252
    cleaning_beam_agent-4_mean: 96.67
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 31
    cleaning_beam_agent-5_mean: 8.93
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 7
    fire_beam_agent-5_mean: 0.13
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-38-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 71.0
  episode_reward_mean: 17.56
  episode_reward_min: -326.0
  episodes_this_iter: 96
  episodes_total: 19008
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12179.968
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.989120058482513e-05
        entropy: 0.5771665573120117
        entropy_coeff: 0.0017600000137463212
        kl: 0.004768085200339556
        model: {}
        policy_loss: -0.011256997473537922
        total_loss: -0.011760957539081573
        vf_explained_var: 0.001820579171180725
        vf_loss: 0.3504379391670227
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 7.989120058482513e-05
        entropy: 0.5323837995529175
        entropy_coeff: 0.0017600000137463212
        kl: 0.0050645433366298676
        model: {}
        policy_loss: -0.009034797549247742
        total_loss: -0.009919259697198868
        vf_explained_var: 0.021804392337799072
        vf_loss: 0.20880825817584991
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 7.989120058482513e-05
        entropy: 0.5630694031715393
        entropy_coeff: 0.0017600000137463212
        kl: 0.005312767811119556
        model: {}
        policy_loss: -0.006029436830431223
        total_loss: -0.0010463269427418709
        vf_explained_var: 0.001993119716644287
        vf_loss: 57.084712982177734
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 7.989120058482513e-05
        entropy: 0.6249175667762756
        entropy_coeff: 0.0017600000137463212
        kl: 0.004878844600170851
        model: {}
        policy_loss: -0.00865664891898632
        total_loss: -0.009588886052370071
        vf_explained_var: -0.01271083950996399
        vf_loss: 0.4564829468727112
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 7.989120058482513e-05
        entropy: 0.6809884309768677
        entropy_coeff: 0.0017600000137463212
        kl: 0.005966623313724995
        model: {}
        policy_loss: -0.012330234050750732
        total_loss: -0.013329708948731422
        vf_explained_var: 0.022607579827308655
        vf_loss: 0.4990173578262329
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 7.989120058482513e-05
        entropy: 0.7072197794914246
        entropy_coeff: 0.0017600000137463212
        kl: 0.004792273975908756
        model: {}
        policy_loss: -0.008790448307991028
        total_loss: -0.009898809716105461
        vf_explained_var: -0.0007758289575576782
        vf_loss: 0.16540272533893585
    load_time_ms: 13695.164
    num_steps_sampled: 19008000
    num_steps_trained: 19008000
    sample_time_ms: 99429.833
    update_time_ms: 17.13
  iterations_since_restore: 38
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.273595505617976
    ram_util_percent: 12.19044943820225
  pid: 5668
  policy_reward_max:
    agent-0: 14.0
    agent-1: 12.0
    agent-2: 25.0
    agent-3: 23.0
    agent-4: 17.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.51
    agent-1: 2.31
    agent-2: 1.33
    agent-3: 4.0
    agent-4: 4.72
    agent-5: 1.69
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -343.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -5.0
  sampler_perf:
    mean_env_wait_ms: 22.94636405052868
    mean_inference_ms: 12.828865034766293
    mean_processing_ms: 57.64527178777611
  time_since_restore: 4823.000694036484
  time_this_iter_s: 124.64098381996155
  time_total_s: 27333.28393650055
  timestamp: 1637224728
  timesteps_since_restore: 3648000
  timesteps_this_iter: 96000
  timesteps_total: 19008000
  training_iteration: 198
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    198 |          27333.3 | 19008000 |    17.56 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 2.55
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.02
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.99
    apples_agent-2_min: 0
    apples_agent-3_max: 29
    apples_agent-3_mean: 3.48
    apples_agent-3_min: 0
    apples_agent-4_max: 56
    apples_agent-4_mean: 1.8
    apples_agent-4_min: 0
    apples_agent-5_max: 26
    apples_agent-5_mean: 1.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 250
    cleaning_beam_agent-0_mean: 93.38
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 379
    cleaning_beam_agent-1_mean: 265.37
    cleaning_beam_agent-1_min: 148
    cleaning_beam_agent-2_max: 180
    cleaning_beam_agent-2_mean: 54.21
    cleaning_beam_agent-2_min: 9
    cleaning_beam_agent-3_max: 170
    cleaning_beam_agent-3_mean: 58.21
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 541
    cleaning_beam_agent-4_mean: 95.44
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 8.83
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-40-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 83.0
  episode_reward_mean: 22.24
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 19104
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12189.399
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 7.390080281766132e-05
        entropy: 0.564690351486206
        entropy_coeff: 0.0017600000137463212
        kl: 0.005200519226491451
        model: {}
        policy_loss: -0.010978439822793007
        total_loss: -0.01168360561132431
        vf_explained_var: -0.0014136135578155518
        vf_loss: 0.28663861751556396
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 7.390080281766132e-05
        entropy: 0.526798665523529
        entropy_coeff: 0.0017600000137463212
        kl: 0.004264299292117357
        model: {}
        policy_loss: -0.008348395116627216
        total_loss: -0.009224179200828075
        vf_explained_var: 0.02958548069000244
        vf_loss: 0.247288316488266
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 7.390080281766132e-05
        entropy: 0.5629103779792786
        entropy_coeff: 0.0017600000137463212
        kl: 0.005201252177357674
        model: {}
        policy_loss: -0.012171745300292969
        total_loss: -0.012835221365094185
        vf_explained_var: -0.004235059022903442
        vf_loss: 0.6718537211418152
      agent-3:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 7.390080281766132e-05
        entropy: 0.5973747968673706
        entropy_coeff: 0.0017600000137463212
        kl: 0.0041416301392018795
        model: {}
        policy_loss: -0.007558019366115332
        total_loss: -0.00849692802876234
        vf_explained_var: -0.004474282264709473
        vf_loss: 0.6070085763931274
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 7.390080281766132e-05
        entropy: 0.6769596338272095
        entropy_coeff: 0.0017600000137463212
        kl: 0.005612663924694061
        model: {}
        policy_loss: -0.01283494383096695
        total_loss: -0.013836630620062351
        vf_explained_var: 0.02287629246711731
        vf_loss: 0.494480699300766
      agent-5:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 7.390080281766132e-05
        entropy: 0.7200160026550293
        entropy_coeff: 0.0017600000137463212
        kl: 0.004608035087585449
        model: {}
        policy_loss: -0.008556385524570942
        total_loss: -0.00975090079009533
        vf_explained_var: 0.005939751863479614
        vf_loss: 0.15113765001296997
    load_time_ms: 13671.038
    num_steps_sampled: 19104000
    num_steps_trained: 19104000
    sample_time_ms: 99376.061
    update_time_ms: 16.996
  iterations_since_restore: 39
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.6808988764045
    ram_util_percent: 12.236516853932583
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 20.0
    agent-2: 25.0
    agent-3: 30.0
    agent-4: 17.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.32
    agent-1: 2.5
    agent-2: 5.31
    agent-3: 4.42
    agent-4: 4.85
    agent-5: 1.84
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.942096659539676
    mean_inference_ms: 12.824855711125418
    mean_processing_ms: 57.625405385132126
  time_since_restore: 4948.136687040329
  time_this_iter_s: 125.13599300384521
  time_total_s: 27458.419929504395
  timestamp: 1637224853
  timesteps_since_restore: 3744000
  timesteps_this_iter: 96000
  timesteps_total: 19104000
  training_iteration: 199
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    199 |          27458.4 | 19104000 |    22.24 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 3.54
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.07
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.93
    apples_agent-2_min: 0
    apples_agent-3_max: 56
    apples_agent-3_mean: 4.47
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.27
    apples_agent-4_min: 0
    apples_agent-5_max: 47
    apples_agent-5_mean: 2.44
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 197
    cleaning_beam_agent-0_mean: 102.18
    cleaning_beam_agent-0_min: 36
    cleaning_beam_agent-1_max: 465
    cleaning_beam_agent-1_mean: 278.35
    cleaning_beam_agent-1_min: 185
    cleaning_beam_agent-2_max: 108
    cleaning_beam_agent-2_mean: 45.31
    cleaning_beam_agent-2_min: 10
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 66.65
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 442
    cleaning_beam_agent-4_mean: 118.39
    cleaning_beam_agent-4_min: 30
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 9.87
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-42-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 79.0
  episode_reward_mean: 24.69
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 19200
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12188.873
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.791039777453989e-05
        entropy: 0.5648173093795776
        entropy_coeff: 0.0017600000137463212
        kl: 0.0047612437047064304
        model: {}
        policy_loss: -0.01052987203001976
        total_loss: -0.011220747604966164
        vf_explained_var: 0.0039012134075164795
        vf_loss: 0.6514354944229126
      agent-1:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 6.791039777453989e-05
        entropy: 0.5332430601119995
        entropy_coeff: 0.0017600000137463212
        kl: 0.004032920114696026
        model: {}
        policy_loss: -0.008330605924129486
        total_loss: -0.009232476353645325
        vf_explained_var: 0.022302374243736267
        vf_loss: 0.2403339147567749
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.791039777453989e-05
        entropy: 0.5628457069396973
        entropy_coeff: 0.0017600000137463212
        kl: 0.0046052150428295135
        model: {}
        policy_loss: -0.011858813464641571
        total_loss: -0.012555944733321667
        vf_explained_var: -0.00558944046497345
        vf_loss: 0.6321734189987183
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 6.791039777453989e-05
        entropy: 0.6151207089424133
        entropy_coeff: 0.0017600000137463212
        kl: 0.004592125304043293
        model: {}
        policy_loss: -0.008410619571805
        total_loss: -0.009400961920619011
        vf_explained_var: -0.006967172026634216
        vf_loss: 0.635673463344574
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 6.791039777453989e-05
        entropy: 0.6705805063247681
        entropy_coeff: 0.0017600000137463212
        kl: 0.005535145290195942
        model: {}
        policy_loss: -0.012209339998662472
        total_loss: -0.013198386877775192
        vf_explained_var: 0.0318705290555954
        vf_loss: 0.5279315710067749
      agent-5:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 6.791039777453989e-05
        entropy: 0.7311477661132812
        entropy_coeff: 0.0017600000137463212
        kl: 0.005199479404836893
        model: {}
        policy_loss: -0.008562627248466015
        total_loss: -0.009800029918551445
        vf_explained_var: -0.008531928062438965
        vf_loss: 0.1692276895046234
    load_time_ms: 13658.569
    num_steps_sampled: 19200000
    num_steps_trained: 19200000
    sample_time_ms: 99342.05
    update_time_ms: 16.782
  iterations_since_restore: 40
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.32090395480226
    ram_util_percent: 12.192655367231637
  pid: 5668
  policy_reward_max:
    agent-0: 22.0
    agent-1: 12.0
    agent-2: 22.0
    agent-3: 20.0
    agent-4: 16.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.82
    agent-1: 2.54
    agent-2: 5.22
    agent-3: 5.16
    agent-4: 4.88
    agent-5: 2.07
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.941545717712177
    mean_inference_ms: 12.820892757763124
    mean_processing_ms: 57.61831353272022
  time_since_restore: 5072.470764636993
  time_this_iter_s: 124.33407759666443
  time_total_s: 27582.75400710106
  timestamp: 1637224977
  timesteps_since_restore: 3840000
  timesteps_this_iter: 96000
  timesteps_total: 19200000
  training_iteration: 200
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    200 |          27582.8 | 19200000 |    24.69 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 2.86
    apples_agent-0_min: 0
    apples_agent-1_max: 14
    apples_agent-1_mean: 1.07
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.31
    apples_agent-2_min: 0
    apples_agent-3_max: 128
    apples_agent-3_mean: 5.33
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 1.27
    apples_agent-4_min: 0
    apples_agent-5_max: 42
    apples_agent-5_mean: 1.42
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 198
    cleaning_beam_agent-0_mean: 88.97
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 502
    cleaning_beam_agent-1_mean: 270.33
    cleaning_beam_agent-1_min: 179
    cleaning_beam_agent-2_max: 197
    cleaning_beam_agent-2_mean: 53.6
    cleaning_beam_agent-2_min: 9
    cleaning_beam_agent-3_max: 136
    cleaning_beam_agent-3_mean: 55.2
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 293
    cleaning_beam_agent-4_mean: 98.98
    cleaning_beam_agent-4_min: 23
    cleaning_beam_agent-5_max: 42
    cleaning_beam_agent-5_mean: 9.18
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-45-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 67.0
  episode_reward_mean: 21.43
  episode_reward_min: -85.0
  episodes_this_iter: 96
  episodes_total: 19296
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12188.515
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 6.192000000737607e-05
        entropy: 0.558958113193512
        entropy_coeff: 0.0017600000137463212
        kl: 0.0043496182188391685
        model: {}
        policy_loss: -0.01017268281430006
        total_loss: -0.011010387912392616
        vf_explained_var: -0.00954475998878479
        vf_loss: 0.37321579456329346
      agent-1:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 6.192000000737607e-05
        entropy: 0.5219602584838867
        entropy_coeff: 0.0017600000137463212
        kl: 0.003928561694920063
        model: {}
        policy_loss: -0.008203092031180859
        total_loss: -0.009098080918192863
        vf_explained_var: 0.025098100304603577
        vf_loss: 0.1752282679080963
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 6.192000000737607e-05
        entropy: 0.5680460929870605
        entropy_coeff: 0.0017600000137463212
        kl: 0.0037107085809111595
        model: {}
        policy_loss: -0.006957673933357
        total_loss: -0.007687962148338556
        vf_explained_var: -0.0037381649017333984
        vf_loss: 1.7670361995697021
      agent-3:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 6.192000000737607e-05
        entropy: 0.5973125100135803
        entropy_coeff: 0.0017600000137463212
        kl: 0.004564405884593725
        model: {}
        policy_loss: -0.007890796288847923
        total_loss: -0.008876917883753777
        vf_explained_var: 7.073581218719482e-05
        vf_loss: 0.5088555812835693
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 6.192000000737607e-05
        entropy: 0.6787804365158081
        entropy_coeff: 0.0017600000137463212
        kl: 0.005066726356744766
        model: {}
        policy_loss: -0.01173624861985445
        total_loss: -0.012754778377711773
        vf_explained_var: 0.026848122477531433
        vf_loss: 0.49457019567489624
      agent-5:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 6.192000000737607e-05
        entropy: 0.7198861837387085
        entropy_coeff: 0.0017600000137463212
        kl: 0.005140273831784725
        model: {}
        policy_loss: -0.0037373944651335478
        total_loss: -0.004825852811336517
        vf_explained_var: 0.0034653395414352417
        vf_loss: 1.4641586542129517
    load_time_ms: 13652.534
    num_steps_sampled: 19296000
    num_steps_trained: 19296000
    sample_time_ms: 99312.627
    update_time_ms: 16.9
  iterations_since_restore: 41
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.669273743016756
    ram_util_percent: 12.242458100558657
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 10.0
    agent-2: 19.0
    agent-3: 20.0
    agent-4: 14.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.84
    agent-1: 2.1
    agent-2: 4.43
    agent-3: 4.9
    agent-4: 4.76
    agent-5: 1.4
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -43.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 22.940830519894707
    mean_inference_ms: 12.817533986824499
    mean_processing_ms: 57.60164771031097
  time_since_restore: 5197.690448999405
  time_this_iter_s: 125.2196843624115
  time_total_s: 27707.97369146347
  timestamp: 1637225103
  timesteps_since_restore: 3936000
  timesteps_this_iter: 96000
  timesteps_total: 19296000
  training_iteration: 201
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    201 |            27708 | 19296000 |    21.43 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.92
    apples_agent-0_min: 0
    apples_agent-1_max: 16
    apples_agent-1_mean: 1.16
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 2.25
    apples_agent-2_min: 0
    apples_agent-3_max: 68
    apples_agent-3_mean: 4.07
    apples_agent-3_min: 0
    apples_agent-4_max: 37
    apples_agent-4_mean: 2.19
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 1.33
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 175
    cleaning_beam_agent-0_mean: 90.65
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 460
    cleaning_beam_agent-1_mean: 267.95
    cleaning_beam_agent-1_min: 179
    cleaning_beam_agent-2_max: 156
    cleaning_beam_agent-2_mean: 43.26
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 143
    cleaning_beam_agent-3_mean: 62.04
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 341
    cleaning_beam_agent-4_mean: 94.18
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 33
    cleaning_beam_agent-5_mean: 9.63
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-47-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 82.0
  episode_reward_mean: 23.97
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 19392
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12188.951
    learner:
      agent-0:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 5.5929598602233455e-05
        entropy: 0.5564666390419006
        entropy_coeff: 0.0017600000137463212
        kl: 0.004094730596989393
        model: {}
        policy_loss: -0.009833713993430138
        total_loss: -0.010723521001636982
        vf_explained_var: -0.0051888227462768555
        vf_loss: 0.38388973474502563
      agent-1:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 5.5929598602233455e-05
        entropy: 0.525505781173706
        entropy_coeff: 0.0017600000137463212
        kl: 0.003492478746920824
        model: {}
        policy_loss: -0.007317704148590565
        total_loss: -0.00821664184331894
        vf_explained_var: 0.01557759940624237
        vf_loss: 0.2322598397731781
      agent-2:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 5.5929598602233455e-05
        entropy: 0.5458531975746155
        entropy_coeff: 0.0017600000137463212
        kl: 0.004463879857212305
        model: {}
        policy_loss: -0.009808884933590889
        total_loss: -0.01064750924706459
        vf_explained_var: 0.0007276684045791626
        vf_loss: 0.6627939939498901
      agent-3:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 5.5929598602233455e-05
        entropy: 0.6219691634178162
        entropy_coeff: 0.0017600000137463212
        kl: 0.0040154908783733845
        model: {}
        policy_loss: -0.00658252090215683
        total_loss: -0.007589032873511314
        vf_explained_var: -0.0012169629335403442
        vf_loss: 0.818780779838562
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 5.5929598602233455e-05
        entropy: 0.6842144727706909
        entropy_coeff: 0.0017600000137463212
        kl: 0.0047558508813381195
        model: {}
        policy_loss: -0.01119493693113327
        total_loss: -0.012236341834068298
        vf_explained_var: 0.02198903262615204
        vf_loss: 0.4391394257545471
      agent-5:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 5.5929598602233455e-05
        entropy: 0.7528119683265686
        entropy_coeff: 0.0017600000137463212
        kl: 0.004854910541325808
        model: {}
        policy_loss: -0.007604709826409817
        total_loss: -0.008880863897502422
        vf_explained_var: -0.016065627336502075
        vf_loss: 0.1845005452632904
    load_time_ms: 13611.833
    num_steps_sampled: 19392000
    num_steps_trained: 19392000
    sample_time_ms: 99348.222
    update_time_ms: 16.166
  iterations_since_restore: 42
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.169662921348312
    ram_util_percent: 12.114044943820224
  pid: 5668
  policy_reward_max:
    agent-0: 18.0
    agent-1: 17.0
    agent-2: 28.0
    agent-3: 38.0
    agent-4: 20.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.15
    agent-1: 2.34
    agent-2: 5.05
    agent-3: 5.19
    agent-4: 5.01
    agent-5: 2.23
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.935734528642858
    mean_inference_ms: 12.81332294630327
    mean_processing_ms: 57.58860280519818
  time_since_restore: 5322.823965549469
  time_this_iter_s: 125.13351655006409
  time_total_s: 27833.107208013535
  timestamp: 1637225229
  timesteps_since_restore: 4032000
  timesteps_this_iter: 96000
  timesteps_total: 19392000
  training_iteration: 202
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    202 |          27833.1 | 19392000 |    23.97 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.82
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.99
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 1.97
    apples_agent-2_min: 0
    apples_agent-3_max: 62
    apples_agent-3_mean: 4.34
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.51
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.25
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 173
    cleaning_beam_agent-0_mean: 89.28
    cleaning_beam_agent-0_min: 38
    cleaning_beam_agent-1_max: 389
    cleaning_beam_agent-1_mean: 256.88
    cleaning_beam_agent-1_min: 170
    cleaning_beam_agent-2_max: 92
    cleaning_beam_agent-2_mean: 39.2
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 155
    cleaning_beam_agent-3_mean: 59.88
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 300
    cleaning_beam_agent-4_mean: 99.32
    cleaning_beam_agent-4_min: 25
    cleaning_beam_agent-5_max: 28
    cleaning_beam_agent-5_mean: 8.44
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-49-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 22.68
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 19488
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12185.821
    learner:
      agent-0:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 4.993920083506964e-05
        entropy: 0.5498389601707458
        entropy_coeff: 0.0017600000137463212
        kl: 0.004389558918774128
        model: {}
        policy_loss: -0.008999952115118504
        total_loss: -0.009902812540531158
        vf_explained_var: -0.0009724348783493042
        vf_loss: 0.374204158782959
      agent-1:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 4.993920083506964e-05
        entropy: 0.5321946144104004
        entropy_coeff: 0.0017600000137463212
        kl: 0.0036465097218751907
        model: {}
        policy_loss: -0.007084928452968597
        total_loss: -0.007996651344001293
        vf_explained_var: 0.017461583018302917
        vf_loss: 0.2351292371749878
      agent-2:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 4.993920083506964e-05
        entropy: 0.5511699914932251
        entropy_coeff: 0.0017600000137463212
        kl: 0.004218796268105507
        model: {}
        policy_loss: -0.010382412932813168
        total_loss: -0.011278122663497925
        vf_explained_var: -0.006736665964126587
        vf_loss: 0.4798099398612976
      agent-3:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 4.993920083506964e-05
        entropy: 0.6062568426132202
        entropy_coeff: 0.0017600000137463212
        kl: 0.004239186178892851
        model: {}
        policy_loss: -0.007085847668349743
        total_loss: -0.00810119416564703
        vf_explained_var: -0.006657391786575317
        vf_loss: 0.4835848808288574
      agent-4:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 4.993920083506964e-05
        entropy: 0.674130916595459
        entropy_coeff: 0.0017600000137463212
        kl: 0.0049374280497431755
        model: {}
        policy_loss: -0.010667824186384678
        total_loss: -0.011745368130505085
        vf_explained_var: 0.010633066296577454
        vf_loss: 0.47208544611930847
      agent-5:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 4.993920083506964e-05
        entropy: 0.7251530885696411
        entropy_coeff: 0.0017600000137463212
        kl: 0.004508749581873417
        model: {}
        policy_loss: -0.007425026502460241
        total_loss: -0.008672796189785004
        vf_explained_var: 0.012417048215866089
        vf_loss: 0.1441303789615631
    load_time_ms: 13619.042
    num_steps_sampled: 19488000
    num_steps_trained: 19488000
    sample_time_ms: 99133.206
    update_time_ms: 16.386
  iterations_since_restore: 43
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.882485875706212
    ram_util_percent: 12.240112994350282
  pid: 5668
  policy_reward_max:
    agent-0: 18.0
    agent-1: 15.0
    agent-2: 18.0
    agent-3: 18.0
    agent-4: 21.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.82
    agent-1: 2.38
    agent-2: 4.87
    agent-3: 4.62
    agent-4: 5.08
    agent-5: 1.91
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.931901403122744
    mean_inference_ms: 12.810902320994767
    mean_processing_ms: 57.583026223748604
  time_since_restore: 5447.337950229645
  time_this_iter_s: 124.51398468017578
  time_total_s: 27957.62119269371
  timestamp: 1637225353
  timesteps_since_restore: 4128000
  timesteps_this_iter: 96000
  timesteps_total: 19488000
  training_iteration: 203
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    203 |          27957.6 | 19488000 |    22.68 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.14
    apples_agent-0_min: 0
    apples_agent-1_max: 30
    apples_agent-1_mean: 1.26
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.9
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 3.1
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.03
    apples_agent-4_min: 0
    apples_agent-5_max: 31
    apples_agent-5_mean: 1.69
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 262
    cleaning_beam_agent-0_mean: 91.08
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 532
    cleaning_beam_agent-1_mean: 259.62
    cleaning_beam_agent-1_min: 158
    cleaning_beam_agent-2_max: 117
    cleaning_beam_agent-2_mean: 35.63
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 160
    cleaning_beam_agent-3_mean: 55.49
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 334
    cleaning_beam_agent-4_mean: 99.43
    cleaning_beam_agent-4_min: 29
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 9.66
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-51-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 21.04
  episode_reward_min: -87.0
  episodes_this_iter: 96
  episodes_total: 19584
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12182.367
    learner:
      agent-0:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 4.394879942992702e-05
        entropy: 0.5543709397315979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0038847983814775944
        model: {}
        policy_loss: -0.008818046189844608
        total_loss: -0.009745887480676174
        vf_explained_var: -0.0012558698654174805
        vf_loss: 0.35710057616233826
      agent-1:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 4.394879942992702e-05
        entropy: 0.540871798992157
        entropy_coeff: 0.0017600000137463212
        kl: 0.002888663671910763
        model: {}
        policy_loss: -0.006424988154321909
        total_loss: -0.00735138263553381
        vf_explained_var: 0.020626172423362732
        vf_loss: 0.2497749626636505
      agent-2:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 4.394879942992702e-05
        entropy: 0.5322537422180176
        entropy_coeff: 0.0017600000137463212
        kl: 0.003972380422055721
        model: {}
        policy_loss: -0.009007609449326992
        total_loss: -0.009880232624709606
        vf_explained_var: -0.005089372396469116
        vf_loss: 0.5173258781433105
      agent-3:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 4.394879942992702e-05
        entropy: 0.5933902263641357
        entropy_coeff: 0.0017600000137463212
        kl: 0.0031631242018193007
        model: {}
        policy_loss: -0.004159606993198395
        total_loss: -0.00503259152173996
        vf_explained_var: -0.004210144281387329
        vf_loss: 1.7014797925949097
      agent-4:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 4.394879942992702e-05
        entropy: 0.6669546365737915
        entropy_coeff: 0.0017600000137463212
        kl: 0.004508716519922018
        model: {}
        policy_loss: -0.009148171171545982
        total_loss: -0.01024631503969431
        vf_explained_var: 0.012259453535079956
        vf_loss: 0.4751804769039154
      agent-5:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 4.394879942992702e-05
        entropy: 0.7186090350151062
        entropy_coeff: 0.0017600000137463212
        kl: 0.004790779203176498
        model: {}
        policy_loss: -0.004506457131356001
        total_loss: -0.005630205385386944
        vf_explained_var: 0.00407622754573822
        vf_loss: 1.3351948261260986
    load_time_ms: 13589.967
    num_steps_sampled: 19584000
    num_steps_trained: 19584000
    sample_time_ms: 99232.595
    update_time_ms: 17.196
  iterations_since_restore: 44
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.12346368715084
    ram_util_percent: 12.20167597765363
  pid: 5668
  policy_reward_max:
    agent-0: 12.0
    agent-1: 12.0
    agent-2: 16.0
    agent-3: 15.0
    agent-4: 16.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.75
    agent-1: 2.31
    agent-2: 5.15
    agent-3: 3.68
    agent-4: 4.63
    agent-5: 1.52
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -49.0
    agent-4: -1.0
    agent-5: -51.0
  sampler_perf:
    mean_env_wait_ms: 22.93362167326187
    mean_inference_ms: 12.810401485011834
    mean_processing_ms: 57.583822268090586
  time_since_restore: 5572.996387720108
  time_this_iter_s: 125.65843749046326
  time_total_s: 28083.279630184174
  timestamp: 1637225479
  timesteps_since_restore: 4224000
  timesteps_this_iter: 96000
  timesteps_total: 19584000
  training_iteration: 204
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    204 |          28083.3 | 19584000 |    21.04 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 3.04
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.96
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 2.7
    apples_agent-2_min: 0
    apples_agent-3_max: 36
    apples_agent-3_mean: 3.9
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.51
    apples_agent-4_min: 0
    apples_agent-5_max: 35
    apples_agent-5_mean: 1.68
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 224
    cleaning_beam_agent-0_mean: 90.35
    cleaning_beam_agent-0_min: 28
    cleaning_beam_agent-1_max: 532
    cleaning_beam_agent-1_mean: 265.29
    cleaning_beam_agent-1_min: 160
    cleaning_beam_agent-2_max: 144
    cleaning_beam_agent-2_mean: 37.8
    cleaning_beam_agent-2_min: 7
    cleaning_beam_agent-3_max: 188
    cleaning_beam_agent-3_mean: 58.57
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 271
    cleaning_beam_agent-4_mean: 94.97
    cleaning_beam_agent-4_min: 31
    cleaning_beam_agent-5_max: 31
    cleaning_beam_agent-5_mean: 10.03
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-53-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 21.4
  episode_reward_min: -76.0
  episodes_this_iter: 96
  episodes_total: 19680
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12188.907
    learner:
      agent-0:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 3.795840166276321e-05
        entropy: 0.550480306148529
        entropy_coeff: 0.0017600000137463212
        kl: 0.0031475187279284
        model: {}
        policy_loss: -0.005499015562236309
        total_loss: -0.006299695000052452
        vf_explained_var: -0.002705812454223633
        vf_loss: 1.6324961185455322
      agent-1:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 3.795840166276321e-05
        entropy: 0.5485151410102844
        entropy_coeff: 0.0017600000137463212
        kl: 0.002613117452710867
        model: {}
        policy_loss: -0.005939091555774212
        total_loss: -0.006889243144541979
        vf_explained_var: 0.02702091634273529
        vf_loss: 0.14978472888469696
      agent-2:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 3.795840166276321e-05
        entropy: 0.5478129386901855
        entropy_coeff: 0.0017600000137463212
        kl: 0.002855845494195819
        model: {}
        policy_loss: -0.0045477598905563354
        total_loss: -0.005326209589838982
        vf_explained_var: -0.003878384828567505
        vf_loss: 1.8123650550842285
      agent-3:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 3.795840166276321e-05
        entropy: 0.629088819026947
        entropy_coeff: 0.0017600000137463212
        kl: 0.003267389489337802
        model: {}
        policy_loss: -0.005872904788702726
        total_loss: -0.0069296699948608875
        vf_explained_var: -0.007618367671966553
        vf_loss: 0.4979315996170044
      agent-4:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 3.795840166276321e-05
        entropy: 0.660702109336853
        entropy_coeff: 0.0017600000137463212
        kl: 0.003769463859498501
        model: {}
        policy_loss: -0.008780444972217083
        total_loss: -0.009877294301986694
        vf_explained_var: 0.02054642140865326
        vf_loss: 0.5420241355895996
      agent-5:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 3.795840166276321e-05
        entropy: 0.7332217693328857
        entropy_coeff: 0.0017600000137463212
        kl: 0.004319826606661081
        model: {}
        policy_loss: -0.004570594523102045
        total_loss: -0.005707908421754837
        vf_explained_var: -0.0026461780071258545
        vf_loss: 1.4978339672088623
    load_time_ms: 13563.414
    num_steps_sampled: 19680000
    num_steps_trained: 19680000
    sample_time_ms: 99298.974
    update_time_ms: 17.111
  iterations_since_restore: 45
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.34269662921348
    ram_util_percent: 12.237640449438201
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 25.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.53
    agent-1: 1.95
    agent-2: 4.23
    agent-3: 4.77
    agent-4: 5.36
    agent-5: 1.56
  policy_reward_min:
    agent-0: -45.0
    agent-1: 0.0
    agent-2: -45.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 22.932924963656113
    mean_inference_ms: 12.806815775454963
    mean_processing_ms: 57.5937078638647
  time_since_restore: 5698.001309394836
  time_this_iter_s: 125.0049216747284
  time_total_s: 28208.284551858902
  timestamp: 1637225604
  timesteps_since_restore: 4320000
  timesteps_this_iter: 96000
  timesteps_total: 19680000
  training_iteration: 205
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    205 |          28208.3 | 19680000 |     21.4 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.53
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.89
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.62
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.91
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.73
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.2
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 218
    cleaning_beam_agent-0_mean: 87.33
    cleaning_beam_agent-0_min: 38
    cleaning_beam_agent-1_max: 410
    cleaning_beam_agent-1_mean: 255.53
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 90
    cleaning_beam_agent-2_mean: 38.5
    cleaning_beam_agent-2_min: 8
    cleaning_beam_agent-3_max: 181
    cleaning_beam_agent-3_mean: 71.52
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 268
    cleaning_beam_agent-4_mean: 83.32
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 28
    cleaning_beam_agent-5_mean: 9.88
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-55-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 65.0
  episode_reward_mean: 20.85
  episode_reward_min: -19.0
  episodes_this_iter: 96
  episodes_total: 19776
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12185.363
    learner:
      agent-0:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 3.196800025762059e-05
        entropy: 0.5560302734375
        entropy_coeff: 0.0017600000137463212
        kl: 0.0033982452005147934
        model: {}
        policy_loss: -0.00738219078630209
        total_loss: -0.008328313939273357
        vf_explained_var: 0.005515187978744507
        vf_loss: 0.2983645796775818
      agent-1:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 3.196800025762059e-05
        entropy: 0.5418071150779724
        entropy_coeff: 0.0017600000137463212
        kl: 0.002462559612467885
        model: {}
        policy_loss: -0.005619524046778679
        total_loss: -0.006556590087711811
        vf_explained_var: 0.0276414155960083
        vf_loss: 0.1639796793460846
      agent-2:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 3.196800025762059e-05
        entropy: 0.5477909445762634
        entropy_coeff: 0.0017600000137463212
        kl: 0.0031678909435868263
        model: {}
        policy_loss: -0.008114732801914215
        total_loss: -0.009019503369927406
        vf_explained_var: -0.009458690881729126
        vf_loss: 0.5687167644500732
      agent-3:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 3.196800025762059e-05
        entropy: 0.6240953207015991
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028339684940874577
        model: {}
        policy_loss: -0.00565499160438776
        total_loss: -0.006715110037475824
        vf_explained_var: 0.006939515471458435
        vf_loss: 0.38018250465393066
      agent-4:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 3.196800025762059e-05
        entropy: 0.6606953740119934
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030074906535446644
        model: {}
        policy_loss: -0.007552884519100189
        total_loss: -0.008663812652230263
        vf_explained_var: 0.01517467200756073
        vf_loss: 0.4720157980918884
      agent-5:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 3.196800025762059e-05
        entropy: 0.7376615405082703
        entropy_coeff: 0.0017600000137463212
        kl: 0.002622167579829693
        model: {}
        policy_loss: -0.0021202494390308857
        total_loss: -0.003269495675340295
        vf_explained_var: 0.003284841775894165
        vf_loss: 1.4801098108291626
    load_time_ms: 13581.006
    num_steps_sampled: 19776000
    num_steps_trained: 19776000
    sample_time_ms: 99042.707
    update_time_ms: 17.273
  iterations_since_restore: 46
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.81129943502825
    ram_util_percent: 12.209604519774011
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 10.0
    agent-2: 17.0
    agent-3: 21.0
    agent-4: 16.0
    agent-5: 7.0
  policy_reward_mean:
    agent-0: 3.67
    agent-1: 2.17
    agent-2: 4.62
    agent-3: 4.19
    agent-4: 4.64
    agent-5: 1.56
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 22.927446441400953
    mean_inference_ms: 12.803830335286102
    mean_processing_ms: 57.58558935294406
  time_since_restore: 5822.445412158966
  time_this_iter_s: 124.44410276412964
  time_total_s: 28332.72865462303
  timestamp: 1637225729
  timesteps_since_restore: 4416000
  timesteps_this_iter: 96000
  timesteps_total: 19776000
  training_iteration: 206
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    206 |          28332.7 | 19776000 |    20.85 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.73
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.05
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.23
    apples_agent-2_min: 0
    apples_agent-3_max: 30
    apples_agent-3_mean: 3.69
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.59
    apples_agent-4_min: 0
    apples_agent-5_max: 114
    apples_agent-5_mean: 2.85
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 189
    cleaning_beam_agent-0_mean: 89.49
    cleaning_beam_agent-0_min: 34
    cleaning_beam_agent-1_max: 687
    cleaning_beam_agent-1_mean: 259.18
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 119
    cleaning_beam_agent-2_mean: 38.24
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 206
    cleaning_beam_agent-3_mean: 69.73
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 340
    cleaning_beam_agent-4_mean: 106.99
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 28
    cleaning_beam_agent-5_mean: 9.9
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-57-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 73.0
  episode_reward_mean: 25.06
  episode_reward_min: -22.0
  episodes_this_iter: 96
  episodes_total: 19872
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12179.223
    learner:
      agent-0:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 2.597760067146737e-05
        entropy: 0.557060182094574
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028380281291902065
        model: {}
        policy_loss: -0.006095550954341888
        total_loss: -0.00702109606936574
        vf_explained_var: -0.0022002607583999634
        vf_loss: 0.5377258062362671
      agent-1:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 2.597760067146737e-05
        entropy: 0.5288810729980469
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016516398172825575
        model: {}
        policy_loss: -0.004368656314909458
        total_loss: -0.005267834756523371
        vf_explained_var: 0.023818686604499817
        vf_loss: 0.31614843010902405
      agent-2:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 2.597760067146737e-05
        entropy: 0.5411827564239502
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026466739363968372
        model: {}
        policy_loss: -0.006744571030139923
        total_loss: -0.007621557451784611
        vf_explained_var: -0.0017934143543243408
        vf_loss: 0.7446169257164001
      agent-3:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 2.597760067146737e-05
        entropy: 0.6189443469047546
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023039779625833035
        model: {}
        policy_loss: -0.004925865214318037
        total_loss: -0.005968174897134304
        vf_explained_var: -0.007199525833129883
        vf_loss: 0.4692026674747467
      agent-4:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 2.597760067146737e-05
        entropy: 0.6639026999473572
        entropy_coeff: 0.0017600000137463212
        kl: 0.002835699124261737
        model: {}
        policy_loss: -0.006639101076871157
        total_loss: -0.0077572353184223175
        vf_explained_var: 0.020600050687789917
        vf_loss: 0.4812072217464447
      agent-5:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 2.597760067146737e-05
        entropy: 0.752960205078125
        entropy_coeff: 0.0017600000137463212
        kl: 0.0035610958002507687
        model: {}
        policy_loss: -0.0054140519350767136
        total_loss: -0.006715663708746433
        vf_explained_var: 0.009312257170677185
        vf_loss: 0.22907105088233948
    load_time_ms: 13573.566
    num_steps_sampled: 19872000
    num_steps_trained: 19872000
    sample_time_ms: 99090.923
    update_time_ms: 17.917
  iterations_since_restore: 47
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.25754189944134
    ram_util_percent: 12.169273743016758
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 21.0
    agent-2: 27.0
    agent-3: 17.0
    agent-4: 15.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.93
    agent-1: 2.71
    agent-2: 5.87
    agent-3: 4.9
    agent-4: 5.05
    agent-5: 2.6
  policy_reward_min:
    agent-0: -42.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.92676987370066
    mean_inference_ms: 12.803742162983893
    mean_processing_ms: 57.587352461243334
  time_since_restore: 5947.938851118088
  time_this_iter_s: 125.4934389591217
  time_total_s: 28458.222093582153
  timestamp: 1637225854
  timesteps_since_restore: 4512000
  timesteps_this_iter: 96000
  timesteps_total: 19872000
  training_iteration: 207
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    207 |          28458.2 | 19872000 |    25.06 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.82
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 1.08
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 2.55
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.16
    apples_agent-3_min: 0
    apples_agent-4_max: 48
    apples_agent-4_mean: 1.5
    apples_agent-4_min: 0
    apples_agent-5_max: 103
    apples_agent-5_mean: 2.36
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 243
    cleaning_beam_agent-0_mean: 89.9
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 410
    cleaning_beam_agent-1_mean: 254.48
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 248
    cleaning_beam_agent-2_mean: 39.98
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 160
    cleaning_beam_agent-3_mean: 68.53
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 301
    cleaning_beam_agent-4_mean: 93.34
    cleaning_beam_agent-4_min: 28
    cleaning_beam_agent-5_max: 45
    cleaning_beam_agent-5_mean: 11.02
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_03-59-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 71.0
  episode_reward_mean: 23.83
  episode_reward_min: 5.0
  episodes_this_iter: 96
  episodes_total: 19968
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12180.188
    learner:
      agent-0:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.998719926632475e-05
        entropy: 0.5596987009048462
        entropy_coeff: 0.0017600000137463212
        kl: 0.002081349492073059
        model: {}
        policy_loss: -0.005419258028268814
        total_loss: -0.00636440422385931
        vf_explained_var: 0.004708826541900635
        vf_loss: 0.3951745331287384
      agent-1:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.998719926632475e-05
        entropy: 0.5267162322998047
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017850088188424706
        model: {}
        policy_loss: -0.0041235946118831635
        total_loss: -0.00503051932901144
        vf_explained_var: 0.03458781540393829
        vf_loss: 0.2007734179496765
      agent-2:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.998719926632475e-05
        entropy: 0.5266534686088562
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022981595247983932
        model: {}
        policy_loss: -0.005294477567076683
        total_loss: -0.0061619896441698074
        vf_explained_var: 0.00218924880027771
        vf_loss: 0.589511513710022
      agent-3:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.998719926632475e-05
        entropy: 0.6211994886398315
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019754075910896063
        model: {}
        policy_loss: -0.004327830858528614
        total_loss: -0.005373836029320955
        vf_explained_var: -0.007893770933151245
        vf_loss: 0.47262871265411377
      agent-4:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.998719926632475e-05
        entropy: 0.668730616569519
        entropy_coeff: 0.0017600000137463212
        kl: 0.002710952889174223
        model: {}
        policy_loss: -0.006025703623890877
        total_loss: -0.007151089608669281
        vf_explained_var: 0.011819705367088318
        vf_loss: 0.505230188369751
      agent-5:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.998719926632475e-05
        entropy: 0.7810038924217224
        entropy_coeff: 0.0017600000137463212
        kl: 0.0027933460660278797
        model: {}
        policy_loss: -0.004413746763020754
        total_loss: -0.005770225543528795
        vf_explained_var: 0.006954267621040344
        vf_loss: 0.17814689874649048
    load_time_ms: 13547.92
    num_steps_sampled: 19968000
    num_steps_trained: 19968000
    sample_time_ms: 99203.151
    update_time_ms: 17.61
  iterations_since_restore: 48
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.655307262569828
    ram_util_percent: 12.24972067039106
  pid: 5668
  policy_reward_max:
    agent-0: 17.0
    agent-1: 11.0
    agent-2: 25.0
    agent-3: 19.0
    agent-4: 18.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.13
    agent-1: 2.36
    agent-2: 5.26
    agent-3: 4.73
    agent-4: 5.21
    agent-5: 2.14
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.923228631753453
    mean_inference_ms: 12.802091580950568
    mean_processing_ms: 57.57807704135679
  time_since_restore: 6073.481752872467
  time_this_iter_s: 125.54290175437927
  time_total_s: 28583.764995336533
  timestamp: 1637225980
  timesteps_since_restore: 4608000
  timesteps_this_iter: 96000
  timesteps_total: 19968000
  training_iteration: 208
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    208 |          28583.8 | 19968000 |    23.83 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 3.25
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 1.14
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 2.2
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.29
    apples_agent-3_min: 0
    apples_agent-4_max: 40
    apples_agent-4_mean: 2.05
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.3
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 195
    cleaning_beam_agent-0_mean: 78.12
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 402
    cleaning_beam_agent-1_mean: 239.24
    cleaning_beam_agent-1_min: 161
    cleaning_beam_agent-2_max: 120
    cleaning_beam_agent-2_mean: 38.36
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 175
    cleaning_beam_agent-3_mean: 77.11
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 230
    cleaning_beam_agent-4_mean: 92.75
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 29
    cleaning_beam_agent-5_mean: 11.37
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-01-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 72.0
  episode_reward_mean: 22.96
  episode_reward_min: -90.0
  episodes_this_iter: 96
  episodes_total: 20064
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12181.824
    learner:
      agent-0:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.5520620346069336
        entropy_coeff: 0.0017600000137463212
        kl: 0.001582488650456071
        model: {}
        policy_loss: -0.0030595893040299416
        total_loss: -0.003859042190015316
        vf_explained_var: -6.890296936035156e-05
        vf_loss: 1.7202363014221191
      agent-1:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.3996799680171534e-05
        entropy: 0.525739848613739
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018310323357582092
        model: {}
        policy_loss: -0.003580742981284857
        total_loss: -0.004484193399548531
        vf_explained_var: 0.04007871448993683
        vf_loss: 0.2184213399887085
      agent-2:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.5413669943809509
        entropy_coeff: 0.0017600000137463212
        kl: 0.002051915042102337
        model: {}
        policy_loss: -0.004886834882199764
        total_loss: -0.005781513638794422
        vf_explained_var: -0.0025553107261657715
        vf_loss: 0.579287588596344
      agent-3:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.627932608127594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012183890212327242
        model: {}
        policy_loss: -0.0020262899342924356
        total_loss: -0.0029494294431060553
        vf_explained_var: 0.0019437819719314575
        vf_loss: 1.8201134204864502
      agent-4:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.3996799680171534e-05
        entropy: 0.6630090475082397
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018041954608634114
        model: {}
        policy_loss: -0.004550922196358442
        total_loss: -0.00566694512963295
        vf_explained_var: 0.013002127408981323
        vf_loss: 0.5052379369735718
      agent-5:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.7669664025306702
        entropy_coeff: 0.0017600000137463212
        kl: 0.001939985086210072
        model: {}
        policy_loss: -0.0032389014959335327
        total_loss: -0.00456839008256793
        vf_explained_var: 0.009946227073669434
        vf_loss: 0.20282123982906342
    load_time_ms: 13550.53
    num_steps_sampled: 20064000
    num_steps_trained: 20064000
    sample_time_ms: 99312.917
    update_time_ms: 18.278
  iterations_since_restore: 49
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.35722222222222
    ram_util_percent: 12.160555555555554
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 10.0
    agent-2: 19.0
    agent-3: 19.0
    agent-4: 14.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.64
    agent-1: 2.52
    agent-2: 5.19
    agent-3: 4.23
    agent-4: 5.14
    agent-5: 2.24
  policy_reward_min:
    agent-0: -50.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.92581291036471
    mean_inference_ms: 12.802468061649295
    mean_processing_ms: 57.58122903635526
  time_since_restore: 6199.775607585907
  time_this_iter_s: 126.29385471343994
  time_total_s: 28710.058850049973
  timestamp: 1637226107
  timesteps_since_restore: 4704000
  timesteps_this_iter: 96000
  timesteps_total: 20064000
  training_iteration: 209
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    209 |          28710.1 | 20064000 |    22.96 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.24
    apples_agent-0_min: 0
    apples_agent-1_max: 29
    apples_agent-1_mean: 1.39
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 2.09
    apples_agent-2_min: 0
    apples_agent-3_max: 28
    apples_agent-3_mean: 3.83
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 0.94
    apples_agent-4_min: 0
    apples_agent-5_max: 38
    apples_agent-5_mean: 1.99
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 156
    cleaning_beam_agent-0_mean: 78.05
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 374
    cleaning_beam_agent-1_mean: 232.91
    cleaning_beam_agent-1_min: 154
    cleaning_beam_agent-2_max: 163
    cleaning_beam_agent-2_mean: 36.3
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 164
    cleaning_beam_agent-3_mean: 80.12
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 292
    cleaning_beam_agent-4_mean: 96.74
    cleaning_beam_agent-4_min: 26
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 13.06
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-03-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 89.0
  episode_reward_mean: 22.2
  episode_reward_min: -90.0
  episodes_this_iter: 96
  episodes_total: 20160
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12179.136
    learner:
      agent-0:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5564122200012207
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014661041786894202
        model: {}
        policy_loss: -0.0042264023795723915
        total_loss: -0.005152357742190361
        vf_explained_var: 0.0004858970642089844
        vf_loss: 0.5325881242752075
      agent-1:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5316920280456543
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016203220002353191
        model: {}
        policy_loss: -0.0032909910660237074
        total_loss: -0.004208037164062262
        vf_explained_var: 0.025206491351127625
        vf_loss: 0.187294602394104
      agent-2:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5378098487854004
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014984507579356432
        model: {}
        policy_loss: -0.004102481063455343
        total_loss: -0.004983566235750914
        vf_explained_var: -0.01654064655303955
        vf_loss: 0.6538874506950378
      agent-3:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6307016015052795
        entropy_coeff: 0.0017600000137463212
        kl: 0.001565215876325965
        model: {}
        policy_loss: -0.003100485075265169
        total_loss: -0.004155749920755625
        vf_explained_var: -0.014222145080566406
        vf_loss: 0.5476173162460327
      agent-4:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6661663055419922
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015311394818127155
        model: {}
        policy_loss: -0.004164381884038448
        total_loss: -0.0052920444868505
        vf_explained_var: 0.01906806230545044
        vf_loss: 0.44640886783599854
      agent-5:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7769050002098083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021189346443861723
        model: {}
        policy_loss: -0.0033162476029247046
        total_loss: -0.004666236229240894
        vf_explained_var: -0.0021810382604599
        vf_loss: 0.1731228232383728
    load_time_ms: 13547.633
    num_steps_sampled: 20160000
    num_steps_trained: 20160000
    sample_time_ms: 99291.533
    update_time_ms: 18.652
  iterations_since_restore: 50
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.946022727272727
    ram_util_percent: 12.19715909090909
  pid: 5668
  policy_reward_max:
    agent-0: 23.0
    agent-1: 9.0
    agent-2: 33.0
    agent-3: 19.0
    agent-4: 11.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.33
    agent-1: 2.3
    agent-2: 5.46
    agent-3: 4.47
    agent-4: 4.54
    agent-5: 2.1
  policy_reward_min:
    agent-0: -50.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.922244106027144
    mean_inference_ms: 12.80003018205125
    mean_processing_ms: 57.576528022569526
  time_since_restore: 6323.843040943146
  time_this_iter_s: 124.06743335723877
  time_total_s: 28834.12628340721
  timestamp: 1637226231
  timesteps_since_restore: 4800000
  timesteps_this_iter: 96000
  timesteps_total: 20160000
  training_iteration: 210
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    210 |          28834.1 | 20160000 |     22.2 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 2.57
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.92
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 1.88
    apples_agent-2_min: 0
    apples_agent-3_max: 38
    apples_agent-3_mean: 3.46
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.11
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 1.54
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 162
    cleaning_beam_agent-0_mean: 86.51
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 314
    cleaning_beam_agent-1_mean: 227.54
    cleaning_beam_agent-1_min: 148
    cleaning_beam_agent-2_max: 146
    cleaning_beam_agent-2_mean: 37.12
    cleaning_beam_agent-2_min: 9
    cleaning_beam_agent-3_max: 169
    cleaning_beam_agent-3_mean: 73.27
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 318
    cleaning_beam_agent-4_mean: 93.45
    cleaning_beam_agent-4_min: 21
    cleaning_beam_agent-5_max: 39
    cleaning_beam_agent-5_mean: 13.12
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-05-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 19.81
  episode_reward_min: -91.0
  episodes_this_iter: 96
  episodes_total: 20256
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12175.268
    learner:
      agent-0:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5610147714614868
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016896818997338414
        model: {}
        policy_loss: -0.0022449404932558537
        total_loss: -0.0030723470263183117
        vf_explained_var: -0.0034558475017547607
        vf_loss: 1.5994350910186768
      agent-1:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5207228660583496
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008264239877462387
        model: {}
        policy_loss: -0.001957861240953207
        total_loss: -0.002727605402469635
        vf_explained_var: 0.0025066733360290527
        vf_loss: 1.4672609567642212
      agent-2:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5431461334228516
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009885833133012056
        model: {}
        policy_loss: -0.0026572835631668568
        total_loss: -0.0034874877892434597
        vf_explained_var: -0.00042842328548431396
        vf_loss: 1.2571070194244385
      agent-3:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.641608715057373
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022996487095952034
        model: {}
        policy_loss: -0.0026885264087468386
        total_loss: -0.003694205777719617
        vf_explained_var: 0.0033408701419830322
        vf_loss: 1.235475778579712
      agent-4:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6732147932052612
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016532920999452472
        model: {}
        policy_loss: -0.004028280731290579
        total_loss: -0.005172224249690771
        vf_explained_var: -0.006212279200553894
        vf_loss: 0.4084107577800751
      agent-5:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.788284182548523
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021686404943466187
        model: {}
        policy_loss: -0.003423484042286873
        total_loss: -0.004797553643584251
        vf_explained_var: -0.004039973020553589
        vf_loss: 0.13289882242679596
    load_time_ms: 13523.301
    num_steps_sampled: 20256000
    num_steps_trained: 20256000
    sample_time_ms: 99277.933
    update_time_ms: 18.545
  iterations_since_restore: 51
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.218539325842695
    ram_util_percent: 12.212921348314605
  pid: 5668
  policy_reward_max:
    agent-0: 14.0
    agent-1: 8.0
    agent-2: 20.0
    agent-3: 15.0
    agent-4: 17.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.25
    agent-1: 1.66
    agent-2: 4.4
    agent-3: 3.91
    agent-4: 4.6
    agent-5: 1.99
  policy_reward_min:
    agent-0: -48.0
    agent-1: -47.0
    agent-2: -47.0
    agent-3: -48.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.91777938161956
    mean_inference_ms: 12.798700185257553
    mean_processing_ms: 57.57097077724506
  time_since_restore: 6448.588258266449
  time_this_iter_s: 124.74521732330322
  time_total_s: 28958.871500730515
  timestamp: 1637226356
  timesteps_since_restore: 4896000
  timesteps_this_iter: 96000
  timesteps_total: 20256000
  training_iteration: 211
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    211 |          28958.9 | 20256000 |    19.81 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 2.82
    apples_agent-0_min: 0
    apples_agent-1_max: 25
    apples_agent-1_mean: 1.27
    apples_agent-1_min: 0
    apples_agent-2_max: 50
    apples_agent-2_mean: 2.73
    apples_agent-2_min: 0
    apples_agent-3_max: 45
    apples_agent-3_mean: 3.15
    apples_agent-3_min: 0
    apples_agent-4_max: 46
    apples_agent-4_mean: 1.63
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.34
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 166
    cleaning_beam_agent-0_mean: 87.53
    cleaning_beam_agent-0_min: 35
    cleaning_beam_agent-1_max: 412
    cleaning_beam_agent-1_mean: 221.97
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 125
    cleaning_beam_agent-2_mean: 36.32
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 68.42
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 318
    cleaning_beam_agent-4_mean: 81.82
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 44
    cleaning_beam_agent-5_mean: 13.56
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-08-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 79.0
  episode_reward_mean: 21.69
  episode_reward_min: -36.0
  episodes_this_iter: 96
  episodes_total: 20352
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12171.817
    learner:
      agent-0:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5487538576126099
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013779937289655209
        model: {}
        policy_loss: -0.0037723700515925884
        total_loss: -0.004701085388660431
        vf_explained_var: -0.0015100836753845215
        vf_loss: 0.3708251416683197
      agent-1:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5277760624885559
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009868396446108818
        model: {}
        policy_loss: -0.0028093308210372925
        total_loss: -0.003717901185154915
        vf_explained_var: 0.02213016152381897
        vf_loss: 0.2031937837600708
      agent-2:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5300988554954529
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015236649196594954
        model: {}
        policy_loss: -0.00402251398190856
        total_loss: -0.004895206540822983
        vf_explained_var: 0.003631964325904846
        vf_loss: 0.6026652455329895
      agent-3:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6366019248962402
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016259277472272515
        model: {}
        policy_loss: -0.0030617322772741318
        total_loss: -0.004138023592531681
        vf_explained_var: -0.007349669933319092
        vf_loss: 0.44127917289733887
      agent-4:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6702408790588379
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017040871316567063
        model: {}
        policy_loss: -0.00378126697614789
        total_loss: -0.004915313795208931
        vf_explained_var: 0.019186556339263916
        vf_loss: 0.45538032054901123
      agent-5:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7905160188674927
        entropy_coeff: 0.0017600000137463212
        kl: 0.002072670264169574
        model: {}
        policy_loss: -0.002889089984819293
        total_loss: -0.004260114394128323
        vf_explained_var: 0.0030201077461242676
        vf_loss: 0.20274768769741058
    load_time_ms: 13523.132
    num_steps_sampled: 20352000
    num_steps_trained: 20352000
    sample_time_ms: 99136.235
    update_time_ms: 19.432
  iterations_since_restore: 52
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.31468926553672
    ram_util_percent: 12.261581920903954
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 13.0
    agent-2: 19.0
    agent-3: 19.0
    agent-4: 16.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.1
    agent-1: 2.2
    agent-2: 4.66
    agent-3: 3.72
    agent-4: 4.71
    agent-5: 2.3
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -34.0
    agent-3: -46.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.90724716473023
    mean_inference_ms: 12.797098591704739
    mean_processing_ms: 57.56508058863048
  time_since_restore: 6572.321990966797
  time_this_iter_s: 123.7337327003479
  time_total_s: 29082.605233430862
  timestamp: 1637226480
  timesteps_since_restore: 4992000
  timesteps_this_iter: 96000
  timesteps_total: 20352000
  training_iteration: 212
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    212 |          29082.6 | 20352000 |    21.69 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.9
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.97
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 2.37
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 3.51
    apples_agent-3_min: 0
    apples_agent-4_max: 26
    apples_agent-4_mean: 1.75
    apples_agent-4_min: 0
    apples_agent-5_max: 28
    apples_agent-5_mean: 1.72
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 181
    cleaning_beam_agent-0_mean: 86.55
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 392
    cleaning_beam_agent-1_mean: 221.58
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 177
    cleaning_beam_agent-2_mean: 36.63
    cleaning_beam_agent-2_min: 8
    cleaning_beam_agent-3_max: 149
    cleaning_beam_agent-3_mean: 63.91
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 278
    cleaning_beam_agent-4_mean: 90.72
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 14.93
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-10-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 83.0
  episode_reward_mean: 22.9
  episode_reward_min: -83.0
  episodes_this_iter: 96
  episodes_total: 20448
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12182.604
    learner:
      agent-0:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5459696054458618
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009578689932823181
        model: {}
        policy_loss: -0.0022449095267802477
        total_loss: -0.0030298896599560976
        vf_explained_var: 0.002129793167114258
        vf_loss: 1.7592108249664307
      agent-1:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5189679861068726
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010839374735951424
        model: {}
        policy_loss: -0.0029523158445954323
        total_loss: -0.0038489578291773796
        vf_explained_var: 0.03841906785964966
        vf_loss: 0.16742275655269623
      agent-2:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5205291509628296
        entropy_coeff: 0.0017600000137463212
        kl: 0.00112367351539433
        model: {}
        policy_loss: -0.003808397799730301
        total_loss: -0.004669561050832272
        vf_explained_var: -0.0026600807905197144
        vf_loss: 0.5496466159820557
      agent-3:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6246880888938904
        entropy_coeff: 0.0017600000137463212
        kl: 0.002165447222068906
        model: {}
        policy_loss: -0.0023138122633099556
        total_loss: -0.0032368022948503494
        vf_explained_var: -0.0019666850566864014
        vf_loss: 1.7646121978759766
      agent-4:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.657531201839447
        entropy_coeff: 0.0017600000137463212
        kl: 0.001583760604262352
        model: {}
        policy_loss: -0.0038704564794898033
        total_loss: -0.004973570350557566
        vf_explained_var: 0.01620928943157196
        vf_loss: 0.5411962866783142
      agent-5:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.812740683555603
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023416238836944103
        model: {}
        policy_loss: -0.0034338929690420628
        total_loss: -0.004845310002565384
        vf_explained_var: 0.006224200129508972
        vf_loss: 0.19001363217830658
    load_time_ms: 13496.827
    num_steps_sampled: 20448000
    num_steps_trained: 20448000
    sample_time_ms: 99493.879
    update_time_ms: 19.309
  iterations_since_restore: 53
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.006043956043953
    ram_util_percent: 12.230219780219779
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 11.0
    agent-2: 17.0
    agent-3: 14.0
    agent-4: 23.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.8
    agent-1: 2.15
    agent-2: 5.22
    agent-3: 4.16
    agent-4: 5.26
    agent-5: 2.31
  policy_reward_min:
    agent-0: -48.0
    agent-1: 0.0
    agent-2: -34.0
    agent-3: -46.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.909604811919525
    mean_inference_ms: 12.797860662057984
    mean_processing_ms: 57.58057097286873
  time_since_restore: 6700.257359981537
  time_this_iter_s: 127.93536901473999
  time_total_s: 29210.540602445602
  timestamp: 1637226608
  timesteps_since_restore: 5088000
  timesteps_this_iter: 96000
  timesteps_total: 20448000
  training_iteration: 213
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    213 |          29210.5 | 20448000 |     22.9 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.73
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.03
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 1.95
    apples_agent-2_min: 0
    apples_agent-3_max: 41
    apples_agent-3_mean: 3.56
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 1.24
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 1.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 193
    cleaning_beam_agent-0_mean: 78.93
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 400
    cleaning_beam_agent-1_mean: 233.71
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 106
    cleaning_beam_agent-2_mean: 38.61
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 153
    cleaning_beam_agent-3_mean: 66.12
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 356
    cleaning_beam_agent-4_mean: 91.42
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 50
    cleaning_beam_agent-5_mean: 14.05
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-12-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 65.0
  episode_reward_mean: 23.44
  episode_reward_min: -15.0
  episodes_this_iter: 96
  episodes_total: 20544
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12185.403
    learner:
      agent-0:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5551996231079102
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017690514214336872
        model: {}
        policy_loss: -0.00400351220741868
        total_loss: -0.00493877986446023
        vf_explained_var: 0.0018234848976135254
        vf_loss: 0.41877979040145874
      agent-1:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5149350762367249
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009279890218749642
        model: {}
        policy_loss: -0.0027583171613514423
        total_loss: -0.00364495231769979
        vf_explained_var: 0.03494645655155182
        vf_loss: 0.19651009142398834
      agent-2:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5366824865341187
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014890957390889525
        model: {}
        policy_loss: -0.004123155493289232
        total_loss: -0.005018158350139856
        vf_explained_var: -0.003503084182739258
        vf_loss: 0.49555617570877075
      agent-3:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6349527835845947
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012654748279601336
        model: {}
        policy_loss: -0.002612731885164976
        total_loss: -0.0036773094907402992
        vf_explained_var: -0.004964321851730347
        vf_loss: 0.5293945074081421
      agent-4:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6617096662521362
        entropy_coeff: 0.0017600000137463212
        kl: 0.001376919448375702
        model: {}
        policy_loss: -0.0019759605638682842
        total_loss: -0.0029751667752861977
        vf_explained_var: 0.010532841086387634
        vf_loss: 1.6539192199707031
      agent-5:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8162249326705933
        entropy_coeff: 0.0017600000137463212
        kl: 0.001278405892662704
        model: {}
        policy_loss: -0.0029294772539287806
        total_loss: -0.004343823529779911
        vf_explained_var: 0.005150973796844482
        vf_loss: 0.2220773696899414
    load_time_ms: 13486.166
    num_steps_sampled: 20544000
    num_steps_trained: 20544000
    sample_time_ms: 99459.571
    update_time_ms: 19.088
  iterations_since_restore: 54
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.710112359550564
    ram_util_percent: 12.181460674157302
  pid: 5668
  policy_reward_max:
    agent-0: 18.0
    agent-1: 12.0
    agent-2: 15.0
    agent-3: 22.0
    agent-4: 15.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 4.37
    agent-1: 2.35
    agent-2: 4.87
    agent-3: 4.86
    agent-4: 4.48
    agent-5: 2.51
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -43.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.906195429140798
    mean_inference_ms: 12.798256433393274
    mean_processing_ms: 57.57408297224449
  time_since_restore: 6825.461730480194
  time_this_iter_s: 125.20437049865723
  time_total_s: 29335.74497294426
  timestamp: 1637226733
  timesteps_since_restore: 5184000
  timesteps_this_iter: 96000
  timesteps_total: 20544000
  training_iteration: 214
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    214 |          29335.7 | 20544000 |    23.44 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 2.7
    apples_agent-0_min: 0
    apples_agent-1_max: 17
    apples_agent-1_mean: 1.22
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 2.37
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.41
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.03
    apples_agent-4_min: 0
    apples_agent-5_max: 22
    apples_agent-5_mean: 1.84
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 192
    cleaning_beam_agent-0_mean: 86.08
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 389
    cleaning_beam_agent-1_mean: 236.74
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 129
    cleaning_beam_agent-2_mean: 39.42
    cleaning_beam_agent-2_min: 7
    cleaning_beam_agent-3_max: 172
    cleaning_beam_agent-3_mean: 63.37
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 221
    cleaning_beam_agent-4_mean: 86.24
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 45
    cleaning_beam_agent-5_mean: 14.6
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-14-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 65.0
  episode_reward_mean: 22.79
  episode_reward_min: -88.0
  episodes_this_iter: 96
  episodes_total: 20640
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12182.133
    learner:
      agent-0:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5551073551177979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017409204738214612
        model: {}
        policy_loss: -0.003934537060558796
        total_loss: -0.004872114397585392
        vf_explained_var: 0.0002094656229019165
        vf_loss: 0.3940839171409607
      agent-1:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5319149494171143
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011656928109005094
        model: {}
        policy_loss: -0.002899094019085169
        total_loss: -0.003818230237811804
        vf_explained_var: 0.014737725257873535
        vf_loss: 0.1703331172466278
      agent-2:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5347620248794556
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017399517819285393
        model: {}
        policy_loss: -0.004310613498091698
        total_loss: -0.005197578109800816
        vf_explained_var: -0.0063936710357666016
        vf_loss: 0.5421689748764038
      agent-3:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6183054447174072
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011586020700633526
        model: {}
        policy_loss: -0.0028157001361250877
        total_loss: -0.0038522311951965094
        vf_explained_var: -0.0017551183700561523
        vf_loss: 0.5168576240539551
      agent-4:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6654309630393982
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016602181131020188
        model: {}
        policy_loss: -0.0038059509824961424
        total_loss: -0.004921792075037956
        vf_explained_var: 0.005857363343238831
        vf_loss: 0.5531818270683289
      agent-5:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7939671277999878
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012037073029205203
        model: {}
        policy_loss: -0.002905597910284996
        total_loss: -0.004284157883375883
        vf_explained_var: 0.009261831641197205
        vf_loss: 0.1882082223892212
    load_time_ms: 13491.046
    num_steps_sampled: 20640000
    num_steps_trained: 20640000
    sample_time_ms: 99200.531
    update_time_ms: 19.262
  iterations_since_restore: 55
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.632758620689657
    ram_util_percent: 12.200574712643675
  pid: 5668
  policy_reward_max:
    agent-0: 14.0
    agent-1: 10.0
    agent-2: 17.0
    agent-3: 12.0
    agent-4: 18.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 4.13
    agent-1: 2.1
    agent-2: 5.48
    agent-3: 3.78
    agent-4: 4.71
    agent-5: 2.59
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -46.0
    agent-4: -46.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.89821681832939
    mean_inference_ms: 12.795492095035
    mean_processing_ms: 57.56288397955636
  time_since_restore: 6947.955365896225
  time_this_iter_s: 122.49363541603088
  time_total_s: 29458.23860836029
  timestamp: 1637226856
  timesteps_since_restore: 5280000
  timesteps_this_iter: 96000
  timesteps_total: 20640000
  training_iteration: 215
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    215 |          29458.2 | 20640000 |    22.79 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 7
    apples_agent-0_mean: 2.43
    apples_agent-0_min: 0
    apples_agent-1_max: 40
    apples_agent-1_mean: 1.6
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.71
    apples_agent-2_min: 0
    apples_agent-3_max: 106
    apples_agent-3_mean: 4.31
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.99
    apples_agent-4_min: 0
    apples_agent-5_max: 27
    apples_agent-5_mean: 1.79
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 184
    cleaning_beam_agent-0_mean: 93.89
    cleaning_beam_agent-0_min: 34
    cleaning_beam_agent-1_max: 414
    cleaning_beam_agent-1_mean: 236.07
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 164
    cleaning_beam_agent-2_mean: 41.19
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 163
    cleaning_beam_agent-3_mean: 63.16
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 327
    cleaning_beam_agent-4_mean: 94.49
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 14.32
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-16-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 22.3
  episode_reward_min: -37.0
  episodes_this_iter: 96
  episodes_total: 20736
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12190.613
    learner:
      agent-0:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5508502721786499
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014284667558968067
        model: {}
        policy_loss: -0.003255629912018776
        total_loss: -0.004180056042969227
        vf_explained_var: 0.0005182325839996338
        vf_loss: 0.45071524381637573
      agent-1:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5213142037391663
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010803912300616503
        model: {}
        policy_loss: -0.002821116242557764
        total_loss: -0.0037181172519922256
        vf_explained_var: 0.023704424500465393
        vf_loss: 0.20512719452381134
      agent-2:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5334212183952332
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012465594336390495
        model: {}
        policy_loss: -0.002078190678730607
        total_loss: -0.002830873942002654
        vf_explained_var: 0.004577130079269409
        vf_loss: 1.8613717555999756
      agent-3:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6153361797332764
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011783447116613388
        model: {}
        policy_loss: -0.0016743512824177742
        total_loss: -0.0025915084406733513
        vf_explained_var: 7.399916648864746e-05
        vf_loss: 1.658342957496643
      agent-4:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6695715188980103
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014205072075128555
        model: {}
        policy_loss: -0.004098016303032637
        total_loss: -0.00523132411763072
        vf_explained_var: 0.008106634020805359
        vf_loss: 0.45139387249946594
      agent-5:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8223874568939209
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012877825647592545
        model: {}
        policy_loss: -0.0029906490817666054
        total_loss: -0.004416001960635185
        vf_explained_var: -0.0006183832883834839
        vf_loss: 0.22047418355941772
    load_time_ms: 13482.52
    num_steps_sampled: 20736000
    num_steps_trained: 20736000
    sample_time_ms: 99267.277
    update_time_ms: 19.212
  iterations_since_restore: 56
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.350561797752807
    ram_util_percent: 12.248314606741571
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 13.0
    agent-2: 23.0
    agent-3: 18.0
    agent-4: 16.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.94
    agent-1: 2.29
    agent-2: 4.32
    agent-3: 4.02
    agent-4: 4.99
    agent-5: 2.74
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -47.0
    agent-3: -42.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.89329293198948
    mean_inference_ms: 12.792530855610117
    mean_processing_ms: 57.565971821978394
  time_since_restore: 7073.12913274765
  time_this_iter_s: 125.17376685142517
  time_total_s: 29583.412375211716
  timestamp: 1637226981
  timesteps_since_restore: 5376000
  timesteps_this_iter: 96000
  timesteps_total: 20736000
  training_iteration: 216
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    216 |          29583.4 | 20736000 |     22.3 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.83
    apples_agent-0_min: 0
    apples_agent-1_max: 44
    apples_agent-1_mean: 1.59
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 2.01
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 3.3
    apples_agent-3_min: 0
    apples_agent-4_max: 80
    apples_agent-4_mean: 2.26
    apples_agent-4_min: 0
    apples_agent-5_max: 73
    apples_agent-5_mean: 2.42
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 150
    cleaning_beam_agent-0_mean: 81.65
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 339
    cleaning_beam_agent-1_mean: 237.35
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 142
    cleaning_beam_agent-2_mean: 43.18
    cleaning_beam_agent-2_min: 8
    cleaning_beam_agent-3_max: 182
    cleaning_beam_agent-3_mean: 69.22
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 257
    cleaning_beam_agent-4_mean: 88.04
    cleaning_beam_agent-4_min: 27
    cleaning_beam_agent-5_max: 59
    cleaning_beam_agent-5_mean: 16.52
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-18-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 66.0
  episode_reward_mean: 23.25
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 20832
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12203.137
    learner:
      agent-0:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5337343811988831
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014474918134510517
        model: {}
        policy_loss: -0.003511781571432948
        total_loss: -0.004415813367813826
        vf_explained_var: -0.00032261013984680176
        vf_loss: 0.353385329246521
      agent-1:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5238165855407715
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011396582704037428
        model: {}
        policy_loss: -0.002969783963635564
        total_loss: -0.003873839508742094
        vf_explained_var: 0.013386785984039307
        vf_loss: 0.17859935760498047
      agent-2:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5442858934402466
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015629369299858809
        model: {}
        policy_loss: -0.0040304819121956825
        total_loss: -0.004934291820973158
        vf_explained_var: 0.0024309903383255005
        vf_loss: 0.5413369536399841
      agent-3:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6263193488121033
        entropy_coeff: 0.0017600000137463212
        kl: 0.001476841513067484
        model: {}
        policy_loss: -0.0029122009873390198
        total_loss: -0.003971701022237539
        vf_explained_var: -0.011383414268493652
        vf_loss: 0.42821556329727173
      agent-4:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6694599986076355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016760139260441065
        model: {}
        policy_loss: -0.0038649514317512512
        total_loss: -0.004994979128241539
        vf_explained_var: 0.011758863925933838
        vf_loss: 0.48220330476760864
      agent-5:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.815147876739502
        entropy_coeff: 0.0017600000137463212
        kl: 0.00139368767850101
        model: {}
        policy_loss: -0.0031476272270083427
        total_loss: -0.0045676035806536674
        vf_explained_var: -0.01329869031906128
        vf_loss: 0.14682790637016296
    load_time_ms: 13454.317
    num_steps_sampled: 20832000
    num_steps_trained: 20832000
    sample_time_ms: 99206.329
    update_time_ms: 18.588
  iterations_since_restore: 57
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.61573033707865
    ram_util_percent: 12.24438202247191
  pid: 5668
  policy_reward_max:
    agent-0: 14.0
    agent-1: 13.0
    agent-2: 23.0
    agent-3: 15.0
    agent-4: 15.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.11
    agent-1: 2.35
    agent-2: 5.16
    agent-3: 4.28
    agent-4: 5.07
    agent-5: 2.28
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.888798095392357
    mean_inference_ms: 12.789901797529664
    mean_processing_ms: 57.553770458680226
  time_since_restore: 7197.845485925674
  time_this_iter_s: 124.71635317802429
  time_total_s: 29708.12872838974
  timestamp: 1637227106
  timesteps_since_restore: 5472000
  timesteps_this_iter: 96000
  timesteps_total: 20832000
  training_iteration: 217
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    217 |          29708.1 | 20832000 |    23.25 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 3.17
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.12
    apples_agent-1_min: 0
    apples_agent-2_max: 45
    apples_agent-2_mean: 2.77
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.42
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 1.7
    apples_agent-4_min: 0
    apples_agent-5_max: 63
    apples_agent-5_mean: 2.42
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 174
    cleaning_beam_agent-0_mean: 89.04
    cleaning_beam_agent-0_min: 39
    cleaning_beam_agent-1_max: 395
    cleaning_beam_agent-1_mean: 237.38
    cleaning_beam_agent-1_min: 155
    cleaning_beam_agent-2_max: 107
    cleaning_beam_agent-2_mean: 41.72
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 200
    cleaning_beam_agent-3_mean: 72.65
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 274
    cleaning_beam_agent-4_mean: 90.15
    cleaning_beam_agent-4_min: 25
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 15.52
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-20-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 71.0
  episode_reward_mean: 25.69
  episode_reward_min: 5.0
  episodes_this_iter: 96
  episodes_total: 20928
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12208.538
    learner:
      agent-0:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5443027019500732
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012968145310878754
        model: {}
        policy_loss: -0.0037474893033504486
        total_loss: -0.0046553583815693855
        vf_explained_var: 4.537403583526611e-05
        vf_loss: 0.5010164976119995
      agent-1:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5233522653579712
        entropy_coeff: 0.0017600000137463212
        kl: 0.001239860663190484
        model: {}
        policy_loss: -0.00304398313164711
        total_loss: -0.003948400728404522
        vf_explained_var: 0.02375081181526184
        vf_loss: 0.16682368516921997
      agent-2:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5387167930603027
        entropy_coeff: 0.0017600000137463212
        kl: 0.002086431486532092
        model: {}
        policy_loss: -0.004372243769466877
        total_loss: -0.0052705565467476845
        vf_explained_var: -0.0005483180284500122
        vf_loss: 0.4982760548591614
      agent-3:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.627415120601654
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012623497750610113
        model: {}
        policy_loss: -0.00318222027271986
        total_loss: -0.0042353481985628605
        vf_explained_var: -0.0034612268209457397
        vf_loss: 0.5112318992614746
      agent-4:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6593353748321533
        entropy_coeff: 0.0017600000137463212
        kl: 0.001650561927817762
        model: {}
        policy_loss: -0.003818362485617399
        total_loss: -0.00491744838654995
        vf_explained_var: 0.03389917314052582
        vf_loss: 0.613455593585968
      agent-5:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8173789978027344
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010491266148164868
        model: {}
        policy_loss: -0.0027871388010680676
        total_loss: -0.004204431548714638
        vf_explained_var: 0.010662510991096497
        vf_loss: 0.21293193101882935
    load_time_ms: 13442.269
    num_steps_sampled: 20928000
    num_steps_trained: 20928000
    sample_time_ms: 99078.624
    update_time_ms: 18.736
  iterations_since_restore: 58
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.141242937853107
    ram_util_percent: 12.245762711864407
  pid: 5668
  policy_reward_max:
    agent-0: 20.0
    agent-1: 12.0
    agent-2: 19.0
    agent-3: 21.0
    agent-4: 20.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 4.73
    agent-1: 2.4
    agent-2: 5.33
    agent-3: 4.79
    agent-4: 5.83
    agent-5: 2.61
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.886774248445576
    mean_inference_ms: 12.789076037414258
    mean_processing_ms: 57.54885809007699
  time_since_restore: 7321.969545841217
  time_this_iter_s: 124.1240599155426
  time_total_s: 29832.252788305283
  timestamp: 1637227230
  timesteps_since_restore: 5568000
  timesteps_this_iter: 96000
  timesteps_total: 20928000
  training_iteration: 218
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    218 |          29832.3 | 20928000 |    25.69 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 3.49
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 1.04
    apples_agent-1_min: 0
    apples_agent-2_max: 98
    apples_agent-2_mean: 3.32
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 3.33
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.35
    apples_agent-4_min: 0
    apples_agent-5_max: 26
    apples_agent-5_mean: 2.15
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 184
    cleaning_beam_agent-0_mean: 93.67
    cleaning_beam_agent-0_min: 46
    cleaning_beam_agent-1_max: 322
    cleaning_beam_agent-1_mean: 228.39
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 104
    cleaning_beam_agent-2_mean: 37.23
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 158
    cleaning_beam_agent-3_mean: 71.71
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 340
    cleaning_beam_agent-4_mean: 87.86
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 39
    cleaning_beam_agent-5_mean: 13.97
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-22-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 77.0
  episode_reward_mean: 22.84
  episode_reward_min: -42.0
  episodes_this_iter: 96
  episodes_total: 21024
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12203.513
    learner:
      agent-0:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5499815344810486
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015175602165982127
        model: {}
        policy_loss: -0.0026448494754731655
        total_loss: -0.0033478543628007174
        vf_explained_var: -0.0005540698766708374
        vf_loss: 2.6496455669403076
      agent-1:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5363959074020386
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005921956617385149
        model: {}
        policy_loss: -0.0019780430011451244
        total_loss: -0.002771098166704178
        vf_explained_var: 0.009150207042694092
        vf_loss: 1.510014533996582
      agent-2:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5285623669624329
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016360499430447817
        model: {}
        policy_loss: -0.0040433891117572784
        total_loss: -0.004900242201983929
        vf_explained_var: -0.00029405951499938965
        vf_loss: 0.7341709733009338
      agent-3:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6204580664634705
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014445189153775573
        model: {}
        policy_loss: -0.0032584909349679947
        total_loss: -0.004282617010176182
        vf_explained_var: -0.010455012321472168
        vf_loss: 0.6787800788879395
      agent-4:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6719385385513306
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021208804100751877
        model: {}
        policy_loss: -0.0041073462925851345
        total_loss: -0.00524243013933301
        vf_explained_var: 0.016221463680267334
        vf_loss: 0.47528237104415894
      agent-5:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8058497905731201
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021051978692412376
        model: {}
        policy_loss: -0.0029973711352795362
        total_loss: -0.004391867201775312
        vf_explained_var: -0.00359383225440979
        vf_loss: 0.23798778653144836
    load_time_ms: 13441.609
    num_steps_sampled: 21024000
    num_steps_trained: 21024000
    sample_time_ms: 98946.318
    update_time_ms: 18.242
  iterations_since_restore: 59
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.08595505617977
    ram_util_percent: 12.27247191011236
  pid: 5668
  policy_reward_max:
    agent-0: 18.0
    agent-1: 10.0
    agent-2: 24.0
    agent-3: 24.0
    agent-4: 21.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.0
    agent-1: 1.67
    agent-2: 5.6
    agent-3: 5.04
    agent-4: 4.98
    agent-5: 2.55
  policy_reward_min:
    agent-0: -46.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.886683872489254
    mean_inference_ms: 12.788280838187934
    mean_processing_ms: 57.55219357095053
  time_since_restore: 7446.880473136902
  time_this_iter_s: 124.91092729568481
  time_total_s: 29957.163715600967
  timestamp: 1637227356
  timesteps_since_restore: 5664000
  timesteps_this_iter: 96000
  timesteps_total: 21024000
  training_iteration: 219
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    219 |          29957.2 | 21024000 |    22.84 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 3.4
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.2
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 2.16
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 3.26
    apples_agent-3_min: 0
    apples_agent-4_max: 29
    apples_agent-4_mean: 1.77
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 1.56
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 249
    cleaning_beam_agent-0_mean: 94.28
    cleaning_beam_agent-0_min: 37
    cleaning_beam_agent-1_max: 359
    cleaning_beam_agent-1_mean: 235.33
    cleaning_beam_agent-1_min: 151
    cleaning_beam_agent-2_max: 173
    cleaning_beam_agent-2_mean: 40.29
    cleaning_beam_agent-2_min: 7
    cleaning_beam_agent-3_max: 180
    cleaning_beam_agent-3_mean: 72.22
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 223
    cleaning_beam_agent-4_mean: 89.93
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 15.33
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 5
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-24-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 75.0
  episode_reward_mean: 23.96
  episode_reward_min: -89.0
  episodes_this_iter: 96
  episodes_total: 21120
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12199.945
    learner:
      agent-0:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5502448678016663
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015862799482420087
        model: {}
        policy_loss: -0.0034356899559497833
        total_loss: -0.004335908219218254
        vf_explained_var: 0.00014656782150268555
        vf_loss: 0.6821624636650085
      agent-1:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5268684029579163
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011078206589445472
        model: {}
        policy_loss: -0.0027848100289702415
        total_loss: -0.003679270390421152
        vf_explained_var: 0.02083304524421692
        vf_loss: 0.3282874822616577
      agent-2:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5322815179824829
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018498143181204796
        model: {}
        policy_loss: -0.003040582872927189
        total_loss: -0.0037761819548904896
        vf_explained_var: 0.0018788725137710571
        vf_loss: 2.0121893882751465
      agent-3:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6349642276763916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015257039340212941
        model: {}
        policy_loss: -0.002852258738130331
        total_loss: -0.003920968621969223
        vf_explained_var: -0.004051506519317627
        vf_loss: 0.4882546067237854
      agent-4:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6742849349975586
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016256666276603937
        model: {}
        policy_loss: -0.003936678171157837
        total_loss: -0.0050711436197161674
        vf_explained_var: 0.0034368038177490234
        vf_loss: 0.5227342844009399
      agent-5:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8243079781532288
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024994341656565666
        model: {}
        policy_loss: -0.0024773976765573025
        total_loss: -0.0037715304642915726
        vf_explained_var: 0.0034755170345306396
        vf_loss: 1.5664739608764648
    load_time_ms: 13444.302
    num_steps_sampled: 21120000
    num_steps_trained: 21120000
    sample_time_ms: 99039.674
    update_time_ms: 18.21
  iterations_since_restore: 60
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.279213483146066
    ram_util_percent: 12.164044943820224
  pid: 5668
  policy_reward_max:
    agent-0: 23.0
    agent-1: 20.0
    agent-2: 22.0
    agent-3: 18.0
    agent-4: 16.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.7
    agent-1: 2.37
    agent-2: 5.35
    agent-3: 4.62
    agent-4: 5.32
    agent-5: 1.6
  policy_reward_min:
    agent-0: 0.0
    agent-1: -3.0
    agent-2: -48.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 22.883191227135235
    mean_inference_ms: 12.78620136246817
    mean_processing_ms: 57.5567902833504
  time_since_restore: 7571.90586233139
  time_this_iter_s: 125.02538919448853
  time_total_s: 30082.189104795456
  timestamp: 1637227481
  timesteps_since_restore: 5760000
  timesteps_this_iter: 96000
  timesteps_total: 21120000
  training_iteration: 220
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    220 |          30082.2 | 21120000 |    23.96 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 80
    apples_agent-0_mean: 3.87
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.05
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 2.15
    apples_agent-2_min: 0
    apples_agent-3_max: 30
    apples_agent-3_mean: 3.36
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 1.37
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 1.43
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 227
    cleaning_beam_agent-0_mean: 92.06
    cleaning_beam_agent-0_min: 36
    cleaning_beam_agent-1_max: 325
    cleaning_beam_agent-1_mean: 227.73
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 200
    cleaning_beam_agent-2_mean: 40.31
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 167
    cleaning_beam_agent-3_mean: 68.73
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 359
    cleaning_beam_agent-4_mean: 92.54
    cleaning_beam_agent-4_min: 28
    cleaning_beam_agent-5_max: 33
    cleaning_beam_agent-5_mean: 14.39
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-26-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 66.0
  episode_reward_mean: 24.43
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 21216
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12203.315
    learner:
      agent-0:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5367920398712158
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020609295461326838
        model: {}
        policy_loss: -0.0035843648947775364
        total_loss: -0.004485960118472576
        vf_explained_var: -0.0001834779977798462
        vf_loss: 0.43158629536628723
      agent-1:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.519329309463501
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011663150507956743
        model: {}
        policy_loss: -0.0027905991300940514
        total_loss: -0.0036836271174252033
        vf_explained_var: 0.02858389914035797
        vf_loss: 0.2099214792251587
      agent-2:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5287412405014038
        entropy_coeff: 0.0017600000137463212
        kl: 0.001233801362104714
        model: {}
        policy_loss: -0.0034326124005019665
        total_loss: -0.004293755628168583
        vf_explained_var: 0.00021895766258239746
        vf_loss: 0.6943764090538025
      agent-3:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6113348007202148
        entropy_coeff: 0.0017600000137463212
        kl: 0.001212957315146923
        model: {}
        policy_loss: -0.002658726181834936
        total_loss: -0.003687644610181451
        vf_explained_var: -0.011489152908325195
        vf_loss: 0.4703218340873718
      agent-4:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6712385416030884
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014686272479593754
        model: {}
        policy_loss: -0.003988049458712339
        total_loss: -0.005125058349221945
        vf_explained_var: 0.0278204083442688
        vf_loss: 0.4437218904495239
      agent-5:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8333581686019897
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015910478541627526
        model: {}
        policy_loss: -0.003328719874843955
        total_loss: -0.0047713760286569595
        vf_explained_var: -0.0013971775770187378
        vf_loss: 0.24056312441825867
    load_time_ms: 13446.028
    num_steps_sampled: 21216000
    num_steps_trained: 21216000
    sample_time_ms: 99134.634
    update_time_ms: 18.498
  iterations_since_restore: 61
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.716111111111108
    ram_util_percent: 12.23611111111111
  pid: 5668
  policy_reward_max:
    agent-0: 18.0
    agent-1: 10.0
    agent-2: 24.0
    agent-3: 16.0
    agent-4: 17.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.96
    agent-1: 2.43
    agent-2: 5.79
    agent-3: 4.48
    agent-4: 5.18
    agent-5: 2.59
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.88099913231882
    mean_inference_ms: 12.784611528909103
    mean_processing_ms: 57.555538616196166
  time_since_restore: 7697.681171178818
  time_this_iter_s: 125.77530884742737
  time_total_s: 30207.964413642883
  timestamp: 1637227607
  timesteps_since_restore: 5856000
  timesteps_this_iter: 96000
  timesteps_total: 21216000
  training_iteration: 221
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    221 |            30208 | 21216000 |    24.43 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.37
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.91
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 2.13
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.99
    apples_agent-3_min: 0
    apples_agent-4_max: 34
    apples_agent-4_mean: 1.63
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 1.35
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 202
    cleaning_beam_agent-0_mean: 94.92
    cleaning_beam_agent-0_min: 34
    cleaning_beam_agent-1_max: 411
    cleaning_beam_agent-1_mean: 224.07
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 192
    cleaning_beam_agent-2_mean: 40.19
    cleaning_beam_agent-2_min: 7
    cleaning_beam_agent-3_max: 202
    cleaning_beam_agent-3_mean: 75.55
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 259
    cleaning_beam_agent-4_mean: 81.46
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 14.01
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-28-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 21.88
  episode_reward_min: -25.0
  episodes_this_iter: 96
  episodes_total: 21312
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12207.758
    learner:
      agent-0:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5360229015350342
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014620872680097818
        model: {}
        policy_loss: -0.003834755625575781
        total_loss: -0.0047492836602032185
        vf_explained_var: -0.00013443827629089355
        vf_loss: 0.28873470425605774
      agent-1:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5353660583496094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009277745266444981
        model: {}
        policy_loss: -0.0025461912155151367
        total_loss: -0.003473915159702301
        vf_explained_var: 0.0029626786708831787
        vf_loss: 0.1451806128025055
      agent-2:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5289958715438843
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015692143933847547
        model: {}
        policy_loss: -0.0037462220061570406
        total_loss: -0.0046205949038267136
        vf_explained_var: 0.006086677312850952
        vf_loss: 0.5666061639785767
      agent-3:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6157032251358032
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013865565415471792
        model: {}
        policy_loss: -0.0028284345753490925
        total_loss: -0.0038725032936781645
        vf_explained_var: -0.005491539835929871
        vf_loss: 0.39569544792175293
      agent-4:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6714276671409607
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014444936532527208
        model: {}
        policy_loss: -0.002571658231317997
        total_loss: -0.003602102864533663
        vf_explained_var: 0.007603555917739868
        vf_loss: 1.5126376152038574
      agent-5:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8342002034187317
        entropy_coeff: 0.0017600000137463212
        kl: 0.001095442334190011
        model: {}
        policy_loss: -0.0028916194569319487
        total_loss: -0.004338596016168594
        vf_explained_var: -0.008780509233474731
        vf_loss: 0.2121485024690628
    load_time_ms: 13454.915
    num_steps_sampled: 21312000
    num_steps_trained: 21312000
    sample_time_ms: 99156.423
    update_time_ms: 17.844
  iterations_since_restore: 62
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.2375
    ram_util_percent: 12.24602272727273
  pid: 5668
  policy_reward_max:
    agent-0: 11.0
    agent-1: 10.0
    agent-2: 20.0
    agent-3: 18.0
    agent-4: 14.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.8
    agent-1: 1.88
    agent-2: 5.08
    agent-3: 4.34
    agent-4: 4.4
    agent-5: 2.38
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -48.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.87872414360327
    mean_inference_ms: 12.783613296970664
    mean_processing_ms: 57.550615323004784
  time_since_restore: 7821.717575550079
  time_this_iter_s: 124.0364043712616
  time_total_s: 30332.000818014145
  timestamp: 1637227731
  timesteps_since_restore: 5952000
  timesteps_this_iter: 96000
  timesteps_total: 21312000
  training_iteration: 222
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    222 |            30332 | 21312000 |    21.88 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 2.96
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.86
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 1.87
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 3.34
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.25
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.73
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 171
    cleaning_beam_agent-0_mean: 93.16
    cleaning_beam_agent-0_min: 45
    cleaning_beam_agent-1_max: 310
    cleaning_beam_agent-1_mean: 214.06
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 175
    cleaning_beam_agent-2_mean: 42.86
    cleaning_beam_agent-2_min: 8
    cleaning_beam_agent-3_max: 204
    cleaning_beam_agent-3_mean: 72.58
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 269
    cleaning_beam_agent-4_mean: 83.43
    cleaning_beam_agent-4_min: 21
    cleaning_beam_agent-5_max: 42
    cleaning_beam_agent-5_mean: 14.87
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-30-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 79.0
  episode_reward_mean: 23.5
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 21408
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12198.73
    learner:
      agent-0:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5354663133621216
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017484280979260802
        model: {}
        policy_loss: -0.003787788562476635
        total_loss: -0.004688627552241087
        vf_explained_var: -0.003187835216522217
        vf_loss: 0.41582682728767395
      agent-1:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5299752950668335
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012145289219915867
        model: {}
        policy_loss: -0.0028586513362824917
        total_loss: -0.0037756930105388165
        vf_explained_var: 0.02496546506881714
        vf_loss: 0.15714368224143982
      agent-2:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5226143002510071
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013179101515561342
        model: {}
        policy_loss: -0.0039148651994764805
        total_loss: -0.004770574159920216
        vf_explained_var: 0.004591986536979675
        vf_loss: 0.640913724899292
      agent-3:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6143085956573486
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016564165707677603
        model: {}
        policy_loss: -0.0034003211185336113
        total_loss: -0.00443251570686698
        vf_explained_var: -0.007171690464019775
        vf_loss: 0.4898837208747864
      agent-4:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6739857196807861
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019042633939534426
        model: {}
        policy_loss: -0.0038020526990294456
        total_loss: -0.004929960705339909
        vf_explained_var: 0.012382760643959045
        vf_loss: 0.5830826759338379
      agent-5:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8272595405578613
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016489812405779958
        model: {}
        policy_loss: -0.0032940967939794064
        total_loss: -0.004726880695670843
        vf_explained_var: 0.012606784701347351
        vf_loss: 0.231941819190979
    load_time_ms: 13474.193
    num_steps_sampled: 21408000
    num_steps_trained: 21408000
    sample_time_ms: 98964.837
    update_time_ms: 18.077
  iterations_since_restore: 63
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.782681564245813
    ram_util_percent: 12.24972067039106
  pid: 5668
  policy_reward_max:
    agent-0: 18.0
    agent-1: 12.0
    agent-2: 23.0
    agent-3: 19.0
    agent-4: 24.0
    agent-5: 16.0
  policy_reward_mean:
    agent-0: 4.08
    agent-1: 1.92
    agent-2: 5.37
    agent-3: 4.57
    agent-4: 5.1
    agent-5: 2.46
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -1.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.875284878674087
    mean_inference_ms: 12.782308857729323
    mean_processing_ms: 57.54882787642007
  time_since_restore: 7947.848427057266
  time_this_iter_s: 126.13085150718689
  time_total_s: 30458.13166952133
  timestamp: 1637227858
  timesteps_since_restore: 6048000
  timesteps_this_iter: 96000
  timesteps_total: 21408000
  training_iteration: 223
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    223 |          30458.1 | 21408000 |     23.5 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 3.31
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.12
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 2.29
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 3.44
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.14
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.42
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 177
    cleaning_beam_agent-0_mean: 91.08
    cleaning_beam_agent-0_min: 36
    cleaning_beam_agent-1_max: 300
    cleaning_beam_agent-1_mean: 206.92
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 258
    cleaning_beam_agent-2_mean: 42.59
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 188
    cleaning_beam_agent-3_mean: 74.31
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 193
    cleaning_beam_agent-4_mean: 80.91
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 45
    cleaning_beam_agent-5_mean: 14.1
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-33-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 23.2
  episode_reward_min: 7.0
  episodes_this_iter: 96
  episodes_total: 21504
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12197.383
    learner:
      agent-0:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5361768007278442
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015286668203771114
        model: {}
        policy_loss: -0.0035049500875175
        total_loss: -0.004417555872350931
        vf_explained_var: 0.006356880068778992
        vf_loss: 0.31069856882095337
      agent-1:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5369532704353333
        entropy_coeff: 0.0017600000137463212
        kl: 0.001701920060440898
        model: {}
        policy_loss: -0.0030041607096791267
        total_loss: -0.0039364127442240715
        vf_explained_var: 0.012636303901672363
        vf_loss: 0.12782570719718933
      agent-2:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5240228176116943
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013459182810038328
        model: {}
        policy_loss: -0.0038190840277820826
        total_loss: -0.004693346563726664
        vf_explained_var: 0.0034282952547073364
        vf_loss: 0.4801921248435974
      agent-3:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6182296872138977
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013822962064296007
        model: {}
        policy_loss: -0.0028171390295028687
        total_loss: -0.0038676299154758453
        vf_explained_var: -0.007575765252113342
        vf_loss: 0.3759142756462097
      agent-4:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6743915677070618
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010694164084270597
        model: {}
        policy_loss: -0.003865946317091584
        total_loss: -0.005006267223507166
        vf_explained_var: 0.023427531123161316
        vf_loss: 0.46608617901802063
      agent-5:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8458535671234131
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017052317271009088
        model: {}
        policy_loss: -0.003010706277564168
        total_loss: -0.004479449242353439
        vf_explained_var: 0.005548983812332153
        vf_loss: 0.1995885670185089
    load_time_ms: 13467.776
    num_steps_sampled: 21504000
    num_steps_trained: 21504000
    sample_time_ms: 99014.168
    update_time_ms: 17.364
  iterations_since_restore: 64
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.671508379888266
    ram_util_percent: 12.165363128491617
  pid: 5668
  policy_reward_max:
    agent-0: 9.0
    agent-1: 7.0
    agent-2: 20.0
    agent-3: 17.0
    agent-4: 19.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.74
    agent-1: 2.08
    agent-2: 5.15
    agent-3: 4.54
    agent-4: 5.22
    agent-5: 2.47
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.872903062486373
    mean_inference_ms: 12.780901229144963
    mean_processing_ms: 57.54962295104393
  time_since_restore: 8073.458890676498
  time_this_iter_s: 125.61046361923218
  time_total_s: 30583.742133140564
  timestamp: 1637227983
  timesteps_since_restore: 6144000
  timesteps_this_iter: 96000
  timesteps_total: 21504000
  training_iteration: 224
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    224 |          30583.7 | 21504000 |     23.2 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 3.09
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.9
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 2.24
    apples_agent-2_min: 0
    apples_agent-3_max: 71
    apples_agent-3_mean: 3.85
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.54
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 217
    cleaning_beam_agent-0_mean: 85.83
    cleaning_beam_agent-0_min: 30
    cleaning_beam_agent-1_max: 356
    cleaning_beam_agent-1_mean: 220.37
    cleaning_beam_agent-1_min: 148
    cleaning_beam_agent-2_max: 112
    cleaning_beam_agent-2_mean: 38.06
    cleaning_beam_agent-2_min: 8
    cleaning_beam_agent-3_max: 176
    cleaning_beam_agent-3_mean: 81.66
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 303
    cleaning_beam_agent-4_mean: 79.61
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 15.08
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-35-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 84.0
  episode_reward_mean: 21.95
  episode_reward_min: -30.0
  episodes_this_iter: 96
  episodes_total: 21600
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12194.56
    learner:
      agent-0:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5257167220115662
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018186841625720263
        model: {}
        policy_loss: -0.003299388103187084
        total_loss: -0.004182317294180393
        vf_explained_var: 0.002144157886505127
        vf_loss: 0.4233401417732239
      agent-1:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5292314291000366
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011416884372010827
        model: {}
        policy_loss: -0.00247112731449306
        total_loss: -0.0033854523207992315
        vf_explained_var: 0.02295336127281189
        vf_loss: 0.17121966183185577
      agent-2:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5167492628097534
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017906168941408396
        model: {}
        policy_loss: -0.003223614301532507
        total_loss: -0.003980154171586037
        vf_explained_var: -0.005541026592254639
        vf_loss: 1.5294153690338135
      agent-3:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6323752999305725
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011772832367569208
        model: {}
        policy_loss: -0.002776352921500802
        total_loss: -0.003842696314677596
        vf_explained_var: -0.008332580327987671
        vf_loss: 0.4663654565811157
      agent-4:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6717318892478943
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016139918006956577
        model: {}
        policy_loss: -0.003950309939682484
        total_loss: -0.0050945160910487175
        vf_explained_var: 0.006888791918754578
        vf_loss: 0.38041210174560547
      agent-5:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.849503219127655
        entropy_coeff: 0.0017600000137463212
        kl: 0.002814470324665308
        model: {}
        policy_loss: -0.0027755307964980602
        total_loss: -0.0041193775832653046
        vf_explained_var: 0.0025977790355682373
        vf_loss: 1.5128202438354492
    load_time_ms: 13466.865
    num_steps_sampled: 21600000
    num_steps_trained: 21600000
    sample_time_ms: 99253.149
    update_time_ms: 17.228
  iterations_since_restore: 65
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.610112359550563
    ram_util_percent: 12.257865168539324
  pid: 5668
  policy_reward_max:
    agent-0: 19.0
    agent-1: 10.0
    agent-2: 19.0
    agent-3: 18.0
    agent-4: 13.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.04
    agent-1: 2.17
    agent-2: 4.69
    agent-3: 4.35
    agent-4: 4.5
    agent-5: 2.2
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -44.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 22.872359711023424
    mean_inference_ms: 12.780137393246676
    mean_processing_ms: 57.54742398067201
  time_since_restore: 8198.245115995407
  time_this_iter_s: 124.78622531890869
  time_total_s: 30708.528358459473
  timestamp: 1637228108
  timesteps_since_restore: 6240000
  timesteps_this_iter: 96000
  timesteps_total: 21600000
  training_iteration: 225
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    225 |          30708.5 | 21600000 |    21.95 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.2
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.12
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 1.84
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 3.13
    apples_agent-3_min: 0
    apples_agent-4_max: 42
    apples_agent-4_mean: 1.67
    apples_agent-4_min: 0
    apples_agent-5_max: 30
    apples_agent-5_mean: 1.62
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 159
    cleaning_beam_agent-0_mean: 84.74
    cleaning_beam_agent-0_min: 33
    cleaning_beam_agent-1_max: 351
    cleaning_beam_agent-1_mean: 216.29
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 143
    cleaning_beam_agent-2_mean: 37.29
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 176
    cleaning_beam_agent-3_mean: 78.96
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 253
    cleaning_beam_agent-4_mean: 80.85
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 46
    cleaning_beam_agent-5_mean: 15.86
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-37-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 88.0
  episode_reward_mean: 19.69
  episode_reward_min: -83.0
  episodes_this_iter: 96
  episodes_total: 21696
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12185.0
    learner:
      agent-0:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5289742946624756
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010478682816028595
        model: {}
        policy_loss: -0.0019058118341490626
        total_loss: -0.0025782741140574217
        vf_explained_var: -0.0007562041282653809
        vf_loss: 2.585313320159912
      agent-1:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5401051640510559
        entropy_coeff: 0.0017600000137463212
        kl: 0.001079497393220663
        model: {}
        policy_loss: -0.002890896052122116
        total_loss: -0.003822833299636841
        vf_explained_var: 0.019651591777801514
        vf_loss: 0.18649494647979736
      agent-2:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5206112861633301
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013337827986106277
        model: {}
        policy_loss: -0.0029507428407669067
        total_loss: -0.0035123585257679224
        vf_explained_var: -0.0006300359964370728
        vf_loss: 3.546581983566284
      agent-3:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6164007186889648
        entropy_coeff: 0.0017600000137463212
        kl: 0.000845122616738081
        model: {}
        policy_loss: -0.0019945777021348476
        total_loss: -0.003000450786203146
        vf_explained_var: 0.0051109641790390015
        vf_loss: 0.7899256944656372
      agent-4:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6751135587692261
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016013863496482372
        model: {}
        policy_loss: -0.004107285290956497
        total_loss: -0.005238385405391455
        vf_explained_var: 0.02400343120098114
        vf_loss: 0.571006178855896
      agent-5:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8646478652954102
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016371377278119326
        model: {}
        policy_loss: -0.0031470023095607758
        total_loss: -0.004650717601180077
        vf_explained_var: 0.0024588406085968018
        vf_loss: 0.1806332767009735
    load_time_ms: 13465.042
    num_steps_sampled: 21696000
    num_steps_trained: 21696000
    sample_time_ms: 99316.598
    update_time_ms: 17.584
  iterations_since_restore: 66
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.350279329608938
    ram_util_percent: 12.197206703910613
  pid: 5668
  policy_reward_max:
    agent-0: 11.0
    agent-1: 12.0
    agent-2: 18.0
    agent-3: 18.0
    agent-4: 16.0
    agent-5: 16.0
  policy_reward_mean:
    agent-0: 2.63
    agent-1: 2.12
    agent-2: 3.17
    agent-3: 4.23
    agent-4: 5.36
    agent-5: 2.18
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: -50.0
    agent-3: -50.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.869497147120807
    mean_inference_ms: 12.778865434729301
    mean_processing_ms: 57.54768222480964
  time_since_restore: 8323.870805740356
  time_this_iter_s: 125.62568974494934
  time_total_s: 30834.154048204422
  timestamp: 1637228234
  timesteps_since_restore: 6336000
  timesteps_this_iter: 96000
  timesteps_total: 21696000
  training_iteration: 226
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    226 |          30834.2 | 21696000 |    19.69 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.46
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.05
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.49
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 3.36
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 0.91
    apples_agent-4_min: 0
    apples_agent-5_max: 22
    apples_agent-5_mean: 1.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 173
    cleaning_beam_agent-0_mean: 89.07
    cleaning_beam_agent-0_min: 32
    cleaning_beam_agent-1_max: 325
    cleaning_beam_agent-1_mean: 210.62
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 90
    cleaning_beam_agent-2_mean: 40.33
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 202
    cleaning_beam_agent-3_mean: 77.66
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 188
    cleaning_beam_agent-4_mean: 74.77
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 46
    cleaning_beam_agent-5_mean: 15.69
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-39-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 67.0
  episode_reward_mean: 20.53
  episode_reward_min: -81.0
  episodes_this_iter: 96
  episodes_total: 21792
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12176.982
    learner:
      agent-0:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.536167323589325
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015506641939282417
        model: {}
        policy_loss: -0.0036031443160027266
        total_loss: -0.004519512876868248
        vf_explained_var: -0.005137771368026733
        vf_loss: 0.2728652358055115
      agent-1:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5368587374687195
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011587501503527164
        model: {}
        policy_loss: -0.0030744140967726707
        total_loss: -0.0040045203641057014
        vf_explained_var: 0.014164894819259644
        vf_loss: 0.14763137698173523
      agent-2:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5203486680984497
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013053214643150568
        model: {}
        policy_loss: -0.0027960764709860086
        total_loss: -0.0035410027485340834
        vf_explained_var: -0.00213468074798584
        vf_loss: 1.708827018737793
      agent-3:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6104682683944702
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011157228145748377
        model: {}
        policy_loss: -0.0022151870653033257
        total_loss: -0.003118029795587063
        vf_explained_var: -0.003366142511367798
        vf_loss: 1.7158095836639404
      agent-4:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6840710639953613
        entropy_coeff: 0.0017600000137463212
        kl: 0.001651320606470108
        model: {}
        policy_loss: -0.0037361099384725094
        total_loss: -0.004893792327493429
        vf_explained_var: 0.016999676823616028
        vf_loss: 0.4628254175186157
      agent-5:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8614618182182312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014136226382106543
        model: {}
        policy_loss: -0.002160123083740473
        total_loss: -0.0035334639251232147
        vf_explained_var: -0.00064106285572052
        vf_loss: 1.4282928705215454
    load_time_ms: 13490.459
    num_steps_sampled: 21792000
    num_steps_trained: 21792000
    sample_time_ms: 99407.747
    update_time_ms: 17.451
  iterations_since_restore: 67
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.110614525139667
    ram_util_percent: 12.227374301675976
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 10.0
    agent-2: 15.0
    agent-3: 14.0
    agent-4: 18.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.67
    agent-1: 1.9
    agent-2: 4.17
    agent-3: 3.61
    agent-4: 4.96
    agent-5: 2.22
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: -47.0
    agent-3: -42.0
    agent-4: 0.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 22.86472194614344
    mean_inference_ms: 12.77748374593478
    mean_processing_ms: 57.542259723387126
  time_since_restore: 8449.669537305832
  time_this_iter_s: 125.79873156547546
  time_total_s: 30959.952779769897
  timestamp: 1637228360
  timesteps_since_restore: 6432000
  timesteps_this_iter: 96000
  timesteps_total: 21792000
  training_iteration: 227
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    227 |            30960 | 21792000 |    20.53 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.31
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.97
    apples_agent-1_min: 0
    apples_agent-2_max: 6
    apples_agent-2_mean: 1.58
    apples_agent-2_min: 0
    apples_agent-3_max: 40
    apples_agent-3_mean: 3.37
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.15
    apples_agent-4_min: 0
    apples_agent-5_max: 33
    apples_agent-5_mean: 1.57
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 143
    cleaning_beam_agent-0_mean: 83.53
    cleaning_beam_agent-0_min: 38
    cleaning_beam_agent-1_max: 315
    cleaning_beam_agent-1_mean: 205.35
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 190
    cleaning_beam_agent-2_mean: 38.95
    cleaning_beam_agent-2_min: 7
    cleaning_beam_agent-3_max: 176
    cleaning_beam_agent-3_mean: 72.27
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 196
    cleaning_beam_agent-4_mean: 74.08
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 36
    cleaning_beam_agent-5_mean: 15.3
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-41-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 88.0
  episode_reward_mean: 20.39
  episode_reward_min: -95.0
  episodes_this_iter: 96
  episodes_total: 21888
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12171.11
    learner:
      agent-0:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5314483642578125
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014268080703914165
        model: {}
        policy_loss: -0.003538551041856408
        total_loss: -0.004447473678737879
        vf_explained_var: -0.003951266407966614
        vf_loss: 0.2642652094364166
      agent-1:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5365509390830994
        entropy_coeff: 0.0017600000137463212
        kl: 0.001154756173491478
        model: {}
        policy_loss: -0.0023305430077016354
        total_loss: -0.0031258799135684967
        vf_explained_var: 0.002187371253967285
        vf_loss: 1.4899178743362427
      agent-2:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5286454558372498
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014164496678858995
        model: {}
        policy_loss: -0.0040065208449959755
        total_loss: -0.004892237484455109
        vf_explained_var: -0.004811882972717285
        vf_loss: 0.44701313972473145
      agent-3:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5989106893539429
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011353406589478254
        model: {}
        policy_loss: -0.0026466951239854097
        total_loss: -0.003646652679890394
        vf_explained_var: -0.009842246770858765
        vf_loss: 0.5412573218345642
      agent-4:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6722556352615356
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015691027510911226
        model: {}
        policy_loss: -0.0025214035995304585
        total_loss: -0.00352648738771677
        vf_explained_var: 0.007804811000823975
        vf_loss: 1.7808668613433838
      agent-5:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8629770278930664
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018312062602490187
        model: {}
        policy_loss: -0.0033581191673874855
        total_loss: -0.004856927786022425
        vf_explained_var: 0.01032327115535736
        vf_loss: 0.20034411549568176
    load_time_ms: 13490.818
    num_steps_sampled: 21888000
    num_steps_trained: 21888000
    sample_time_ms: 99529.561
    update_time_ms: 17.408
  iterations_since_restore: 68
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.84044943820225
    ram_util_percent: 12.185393258426966
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 16.0
    agent-2: 14.0
    agent-3: 27.0
    agent-4: 14.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 3.38
    agent-1: 1.31
    agent-2: 4.57
    agent-3: 4.54
    agent-4: 4.14
    agent-5: 2.45
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -49.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.862324223668647
    mean_inference_ms: 12.777739736790533
    mean_processing_ms: 57.54358375334169
  time_since_restore: 8574.956048965454
  time_this_iter_s: 125.28651165962219
  time_total_s: 31085.23929142952
  timestamp: 1637228485
  timesteps_since_restore: 6528000
  timesteps_this_iter: 96000
  timesteps_total: 21888000
  training_iteration: 228
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    228 |          31085.2 | 21888000 |    20.39 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 2.5
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.76
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.04
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.68
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 1.22
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 1.46
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 159
    cleaning_beam_agent-0_mean: 85.32
    cleaning_beam_agent-0_min: 32
    cleaning_beam_agent-1_max: 312
    cleaning_beam_agent-1_mean: 208.94
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 118
    cleaning_beam_agent-2_mean: 36.97
    cleaning_beam_agent-2_min: 7
    cleaning_beam_agent-3_max: 181
    cleaning_beam_agent-3_mean: 67.27
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 264
    cleaning_beam_agent-4_mean: 80.36
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 15.12
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 6
    fire_beam_agent-1_mean: 0.08
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-43-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 133.0
  episode_reward_mean: 20.75
  episode_reward_min: -28.0
  episodes_this_iter: 96
  episodes_total: 21984
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12165.928
    learner:
      agent-0:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5278720855712891
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014765494270250201
        model: {}
        policy_loss: -0.0036179940216243267
        total_loss: -0.004518347326666117
        vf_explained_var: -0.002820611000061035
        vf_loss: 0.2870512008666992
      agent-1:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.541657567024231
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012340897228568792
        model: {}
        policy_loss: -0.001985901966691017
        total_loss: -0.0027869967743754387
        vf_explained_var: 0.006088152527809143
        vf_loss: 1.5222206115722656
      agent-2:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5270112752914429
        entropy_coeff: 0.0017600000137463212
        kl: 0.001387071330100298
        model: {}
        policy_loss: -0.003699874971061945
        total_loss: -0.004551450721919537
        vf_explained_var: -0.0014835000038146973
        vf_loss: 0.7596478462219238
      agent-3:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6021519899368286
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012735791970044374
        model: {}
        policy_loss: -0.00274910987354815
        total_loss: -0.003753904951736331
        vf_explained_var: -0.0001904815435409546
        vf_loss: 0.549913763999939
      agent-4:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6727844476699829
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013559766812250018
        model: {}
        policy_loss: -0.0038301516324281693
        total_loss: -0.0049717118963599205
        vf_explained_var: 0.028220966458320618
        vf_loss: 0.42542630434036255
      agent-5:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.873397946357727
        entropy_coeff: 0.0017600000137463212
        kl: 0.002131099347025156
        model: {}
        policy_loss: -0.0033143635373562574
        total_loss: -0.004816094413399696
        vf_explained_var: -0.00254252552986145
        vf_loss: 0.354476660490036
    load_time_ms: 13488.91
    num_steps_sampled: 21984000
    num_steps_trained: 21984000
    sample_time_ms: 99547.895
    update_time_ms: 17.281
  iterations_since_restore: 69
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.29606741573034
    ram_util_percent: 12.252247191011234
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 16.0
    agent-2: 42.0
    agent-3: 34.0
    agent-4: 15.0
    agent-5: 23.0
  policy_reward_mean:
    agent-0: 3.51
    agent-1: 1.33
    agent-2: 4.78
    agent-3: 4.09
    agent-4: 4.31
    agent-5: 2.73
  policy_reward_min:
    agent-0: 0.0
    agent-1: -48.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.858360677848832
    mean_inference_ms: 12.777281337561725
    mean_processing_ms: 57.53947970208106
  time_since_restore: 8699.984935760498
  time_this_iter_s: 125.02888679504395
  time_total_s: 31210.268178224564
  timestamp: 1637228611
  timesteps_since_restore: 6624000
  timesteps_this_iter: 96000
  timesteps_total: 21984000
  training_iteration: 229
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    229 |          31210.3 | 21984000 |    20.75 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 3.06
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 1.07
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 2.21
    apples_agent-2_min: 0
    apples_agent-3_max: 21
    apples_agent-3_mean: 3.74
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.04
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.62
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 178
    cleaning_beam_agent-0_mean: 80.71
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 328
    cleaning_beam_agent-1_mean: 215.93
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 192
    cleaning_beam_agent-2_mean: 40.03
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 139
    cleaning_beam_agent-3_mean: 60.14
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 446
    cleaning_beam_agent-4_mean: 87.03
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 47
    cleaning_beam_agent-5_mean: 14.91
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-45-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 88.0
  episode_reward_mean: 22.31
  episode_reward_min: -137.0
  episodes_this_iter: 96
  episodes_total: 22080
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12180.619
    learner:
      agent-0:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5328793525695801
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018300453666597605
        model: {}
        policy_loss: -0.0034252835903316736
        total_loss: -0.004308492876589298
        vf_explained_var: 0.0029726773500442505
        vf_loss: 0.5465736985206604
      agent-1:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5433393120765686
        entropy_coeff: 0.0017600000137463212
        kl: 0.001258555450476706
        model: {}
        policy_loss: -0.0024159729946404696
        total_loss: -0.003346651326864958
        vf_explained_var: 0.0229681134223938
        vf_loss: 0.2559850811958313
      agent-2:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5308479070663452
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016788453795015812
        model: {}
        policy_loss: -0.0037193498574197292
        total_loss: -0.004580663051456213
        vf_explained_var: -0.006546765565872192
        vf_loss: 0.7297757267951965
      agent-3:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5879217982292175
        entropy_coeff: 0.0017600000137463212
        kl: 0.000830656208563596
        model: {}
        policy_loss: -0.002295311540365219
        total_loss: -0.003137734951451421
        vf_explained_var: -0.003786414861679077
        vf_loss: 1.9232094287872314
      agent-4:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6641035676002502
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013348986394703388
        model: {}
        policy_loss: -0.002663895022124052
        total_loss: -0.003661078866571188
        vf_explained_var: 0.0079251229763031
        vf_loss: 1.716383695602417
      agent-5:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.863834798336029
        entropy_coeff: 0.0017600000137463212
        kl: 0.001460839295759797
        model: {}
        policy_loss: -0.0030862153507769108
        total_loss: -0.00458403117954731
        vf_explained_var: 0.0038256198167800903
        vf_loss: 0.2253207266330719
    load_time_ms: 13494.261
    num_steps_sampled: 22080000
    num_steps_trained: 22080000
    sample_time_ms: 99635.064
    update_time_ms: 16.992
  iterations_since_restore: 70
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.72178770949721
    ram_util_percent: 12.204469273743015
  pid: 5668
  policy_reward_max:
    agent-0: 22.0
    agent-1: 11.0
    agent-2: 35.0
    agent-3: 22.0
    agent-4: 13.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.21
    agent-1: 1.77
    agent-2: 5.68
    agent-3: 3.68
    agent-4: 4.21
    agent-5: 2.76
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: -49.0
    agent-4: -44.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.855242456215635
    mean_inference_ms: 12.776265651583127
    mean_processing_ms: 57.54112452185495
  time_since_restore: 8826.09825015068
  time_this_iter_s: 126.1133143901825
  time_total_s: 31336.381492614746
  timestamp: 1637228737
  timesteps_since_restore: 6720000
  timesteps_this_iter: 96000
  timesteps_total: 22080000
  training_iteration: 230
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    230 |          31336.4 | 22080000 |    22.31 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 2.93
    apples_agent-0_min: 0
    apples_agent-1_max: 35
    apples_agent-1_mean: 1.2
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 1.6
    apples_agent-2_min: 0
    apples_agent-3_max: 28
    apples_agent-3_mean: 3.02
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.16
    apples_agent-4_min: 0
    apples_agent-5_max: 5
    apples_agent-5_mean: 1.16
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 217
    cleaning_beam_agent-0_mean: 80.91
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 373
    cleaning_beam_agent-1_mean: 209.27
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 101
    cleaning_beam_agent-2_mean: 40.54
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 159
    cleaning_beam_agent-3_mean: 66.59
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 164
    cleaning_beam_agent-4_mean: 75.74
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 42
    cleaning_beam_agent-5_mean: 14.6
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-47-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 20.48
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 22176
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12174.108
    learner:
      agent-0:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5286659598350525
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014348800759762526
        model: {}
        policy_loss: -0.0033931895159184933
        total_loss: -0.00429639033973217
        vf_explained_var: -0.0029425770044326782
        vf_loss: 0.2725365161895752
      agent-1:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5546084642410278
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012124779168516397
        model: {}
        policy_loss: -0.0027768437284976244
        total_loss: -0.00374012952670455
        vf_explained_var: 0.011248156428337097
        vf_loss: 0.1282707005739212
      agent-2:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.547741711139679
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015913477400317788
        model: {}
        policy_loss: -0.003892104607075453
        total_loss: -0.004806007724255323
        vf_explained_var: 0.0042185187339782715
        vf_loss: 0.5012372136116028
      agent-3:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6011387705802917
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010298527777194977
        model: {}
        policy_loss: -0.002597341313958168
        total_loss: -0.0036130440421402454
        vf_explained_var: 0.005764797329902649
        vf_loss: 0.42299672961235046
      agent-4:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6801764965057373
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010086469119414687
        model: {}
        policy_loss: -0.003959766123443842
        total_loss: -0.005119954235851765
        vf_explained_var: 0.018632665276527405
        vf_loss: 0.3692336678504944
      agent-5:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8816028833389282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016605770215392113
        model: {}
        policy_loss: -0.003198292339220643
        total_loss: -0.0047327084466814995
        vf_explained_var: 0.006740167737007141
        vf_loss: 0.17205187678337097
    load_time_ms: 13475.629
    num_steps_sampled: 22176000
    num_steps_trained: 22176000
    sample_time_ms: 99773.321
    update_time_ms: 16.869
  iterations_since_restore: 71
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.912087912087912
    ram_util_percent: 12.259890109890112
  pid: 5668
  policy_reward_max:
    agent-0: 10.0
    agent-1: 7.0
    agent-2: 15.0
    agent-3: 18.0
    agent-4: 12.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 3.32
    agent-1: 1.67
    agent-2: 4.61
    agent-3: 4.01
    agent-4: 4.42
    agent-5: 2.45
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.854040628613575
    mean_inference_ms: 12.77659597999926
    mean_processing_ms: 57.54267886258889
  time_since_restore: 8952.974581956863
  time_this_iter_s: 126.87633180618286
  time_total_s: 31463.25782442093
  timestamp: 1637228864
  timesteps_since_restore: 6816000
  timesteps_this_iter: 96000
  timesteps_total: 22176000
  training_iteration: 231
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    231 |          31463.3 | 22176000 |    20.48 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 2.48
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 0.98
    apples_agent-1_min: 0
    apples_agent-2_max: 34
    apples_agent-2_mean: 1.88
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 2.94
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.52
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.18
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 157
    cleaning_beam_agent-0_mean: 79.11
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 381
    cleaning_beam_agent-1_mean: 208.97
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 152
    cleaning_beam_agent-2_mean: 39.64
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 162
    cleaning_beam_agent-3_mean: 69.73
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 298
    cleaning_beam_agent-4_mean: 78.55
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 47
    cleaning_beam_agent-5_mean: 14.6
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-49-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 21.43
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 22272
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12171.476
    learner:
      agent-0:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5320343971252441
        entropy_coeff: 0.0017600000137463212
        kl: 0.001322336494922638
        model: {}
        policy_loss: -0.003406209871172905
        total_loss: -0.004299940541386604
        vf_explained_var: 0.004280492663383484
        vf_loss: 0.42648592591285706
      agent-1:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5463424921035767
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013978087808936834
        model: {}
        policy_loss: -0.0028869877569377422
        total_loss: -0.003835509531199932
        vf_explained_var: 0.011137783527374268
        vf_loss: 0.13040445744991302
      agent-2:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5338839888572693
        entropy_coeff: 0.0017600000137463212
        kl: 0.001587606966495514
        model: {}
        policy_loss: -0.0040682097896933556
        total_loss: -0.0049719782546162605
        vf_explained_var: -0.005540698766708374
        vf_loss: 0.3586786985397339
      agent-3:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6037618517875671
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013372973771765828
        model: {}
        policy_loss: -0.002873662393540144
        total_loss: -0.0038987360894680023
        vf_explained_var: -0.01984301209449768
        vf_loss: 0.37545856833457947
      agent-4:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6719547510147095
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013949156273156404
        model: {}
        policy_loss: -0.0038726951461285353
        total_loss: -0.005006132647395134
        vf_explained_var: 0.010839149355888367
        vf_loss: 0.4919750690460205
      agent-5:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8695802092552185
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017919304082170129
        model: {}
        policy_loss: -0.003231823444366455
        total_loss: -0.0047417692840099335
        vf_explained_var: -0.0019815266132354736
        vf_loss: 0.20518548786640167
    load_time_ms: 13473.195
    num_steps_sampled: 22272000
    num_steps_trained: 22272000
    sample_time_ms: 99970.456
    update_time_ms: 16.587
  iterations_since_restore: 72
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.353631284916197
    ram_util_percent: 12.262569832402235
  pid: 5668
  policy_reward_max:
    agent-0: 20.0
    agent-1: 7.0
    agent-2: 15.0
    agent-3: 19.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.96
    agent-1: 1.78
    agent-2: 4.24
    agent-3: 3.81
    agent-4: 5.15
    agent-5: 2.49
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.854564543271664
    mean_inference_ms: 12.777236706811694
    mean_processing_ms: 57.549448970289376
  time_since_restore: 9078.974726200104
  time_this_iter_s: 126.00014424324036
  time_total_s: 31589.25796866417
  timestamp: 1637228991
  timesteps_since_restore: 6912000
  timesteps_this_iter: 96000
  timesteps_total: 22272000
  training_iteration: 232
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    232 |          31589.3 | 22272000 |    21.43 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 3.01
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 0.95
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 1.86
    apples_agent-2_min: 0
    apples_agent-3_max: 24
    apples_agent-3_mean: 3.21
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.01
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 1.56
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 210
    cleaning_beam_agent-0_mean: 82.33
    cleaning_beam_agent-0_min: 30
    cleaning_beam_agent-1_max: 334
    cleaning_beam_agent-1_mean: 216.94
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 137
    cleaning_beam_agent-2_mean: 40.83
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 63.39
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 188
    cleaning_beam_agent-4_mean: 75.3
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 47
    cleaning_beam_agent-5_mean: 13.38
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-51-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 70.0
  episode_reward_mean: 20.74
  episode_reward_min: -72.0
  episodes_this_iter: 96
  episodes_total: 22368
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12170.755
    learner:
      agent-0:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5241935849189758
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015884563326835632
        model: {}
        policy_loss: -0.0032819730695337057
        total_loss: -0.004161157179623842
        vf_explained_var: 0.0002769380807876587
        vf_loss: 0.4339376986026764
      agent-1:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.550049901008606
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011092970380559564
        model: {}
        policy_loss: -0.0030665637459605932
        total_loss: -0.004014770500361919
        vf_explained_var: 0.016096726059913635
        vf_loss: 0.19879291951656342
      agent-2:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5376741886138916
        entropy_coeff: 0.0017600000137463212
        kl: 0.001494681229814887
        model: {}
        policy_loss: -0.003853891044855118
        total_loss: -0.004748305305838585
        vf_explained_var: 0.0017313659191131592
        vf_loss: 0.5189199447631836
      agent-3:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5888057351112366
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013510850258171558
        model: {}
        policy_loss: -0.0028395599219948053
        total_loss: -0.003824681742116809
        vf_explained_var: -0.01040920615196228
        vf_loss: 0.5117638111114502
      agent-4:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6689209938049316
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010109745198860765
        model: {}
        policy_loss: -0.003333294065669179
        total_loss: -0.0044737448915839195
        vf_explained_var: 0.01627776026725769
        vf_loss: 0.3684804439544678
      agent-5:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8871139287948608
        entropy_coeff: 0.0017600000137463212
        kl: 0.001393586746416986
        model: {}
        policy_loss: -0.0027116588316857815
        total_loss: -0.004247285891324282
        vf_explained_var: -0.006081104278564453
        vf_loss: 0.25692427158355713
    load_time_ms: 13472.065
    num_steps_sampled: 22368000
    num_steps_trained: 22368000
    sample_time_ms: 99942.412
    update_time_ms: 16.736
  iterations_since_restore: 73
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.614525139664803
    ram_util_percent: 12.200000000000001
  pid: 5668
  policy_reward_max:
    agent-0: 17.0
    agent-1: 10.0
    agent-2: 14.0
    agent-3: 18.0
    agent-4: 13.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.41
    agent-1: 1.95
    agent-2: 4.08
    agent-3: 4.22
    agent-4: 4.35
    agent-5: 2.73
  policy_reward_min:
    agent-0: -48.0
    agent-1: 0.0
    agent-2: -45.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.850372459740083
    mean_inference_ms: 12.776218950797267
    mean_processing_ms: 57.550190303253075
  time_since_restore: 9204.825247526169
  time_this_iter_s: 125.85052132606506
  time_total_s: 31715.108489990234
  timestamp: 1637229117
  timesteps_since_restore: 7008000
  timesteps_this_iter: 96000
  timesteps_total: 22368000
  training_iteration: 233
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    233 |          31715.1 | 22368000 |    20.74 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 3.17
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 1.03
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 1.85
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 3.46
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 0.84
    apples_agent-4_min: 0
    apples_agent-5_max: 27
    apples_agent-5_mean: 1.8
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 151
    cleaning_beam_agent-0_mean: 80.38
    cleaning_beam_agent-0_min: 30
    cleaning_beam_agent-1_max: 297
    cleaning_beam_agent-1_mean: 213.69
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 138
    cleaning_beam_agent-2_mean: 35.52
    cleaning_beam_agent-2_min: 7
    cleaning_beam_agent-3_max: 143
    cleaning_beam_agent-3_mean: 58.04
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 279
    cleaning_beam_agent-4_mean: 81.86
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 36
    cleaning_beam_agent-5_mean: 15.03
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-54-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 23.05
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 22464
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12171.785
    learner:
      agent-0:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5317790508270264
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013753533130511642
        model: {}
        policy_loss: -0.003931253217160702
        total_loss: -0.004833301994949579
        vf_explained_var: -0.0009534507989883423
        vf_loss: 0.3388289213180542
      agent-1:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5516316294670105
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008415703196078539
        model: {}
        policy_loss: -0.0028605563566088676
        total_loss: -0.0038161827251315117
        vf_explained_var: 0.019038483500480652
        vf_loss: 0.15245214104652405
      agent-2:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5266042351722717
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014739729231223464
        model: {}
        policy_loss: -0.0036852825433015823
        total_loss: -0.004557999782264233
        vf_explained_var: -0.0022292733192443848
        vf_loss: 0.5410587191581726
      agent-3:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5875656604766846
        entropy_coeff: 0.0017600000137463212
        kl: 0.001257359515875578
        model: {}
        policy_loss: -0.00278255483135581
        total_loss: -0.003761221654713154
        vf_explained_var: 0.005052059888839722
        vf_loss: 0.5544506311416626
      agent-4:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6636013984680176
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014637423446401954
        model: {}
        policy_loss: -0.003915374167263508
        total_loss: -0.005042380653321743
        vf_explained_var: 0.00391775369644165
        vf_loss: 0.4093213677406311
      agent-5:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8942127823829651
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015208420809358358
        model: {}
        policy_loss: -0.0027918792329728603
        total_loss: -0.004342188593000174
        vf_explained_var: 0.003869175910949707
        vf_loss: 0.23502600193023682
    load_time_ms: 13491.324
    num_steps_sampled: 22464000
    num_steps_trained: 22464000
    sample_time_ms: 100015.792
    update_time_ms: 16.939
  iterations_since_restore: 74
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.113333333333337
    ram_util_percent: 12.238888888888889
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 10.0
    agent-2: 18.0
    agent-3: 22.0
    agent-4: 17.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.05
    agent-1: 1.87
    agent-2: 4.87
    agent-3: 4.8
    agent-4: 4.54
    agent-5: 2.92
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.846481037448303
    mean_inference_ms: 12.775488735685109
    mean_processing_ms: 57.55711330089916
  time_since_restore: 9331.40915298462
  time_this_iter_s: 126.58390545845032
  time_total_s: 31841.692395448685
  timestamp: 1637229243
  timesteps_since_restore: 7104000
  timesteps_this_iter: 96000
  timesteps_total: 22464000
  training_iteration: 234
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    234 |          31841.7 | 22464000 |    23.05 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 2.65
    apples_agent-0_min: 0
    apples_agent-1_max: 56
    apples_agent-1_mean: 1.72
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 2.09
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 3.2
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 1.08
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 1.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 315
    cleaning_beam_agent-0_mean: 82.56
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 334
    cleaning_beam_agent-1_mean: 222.06
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 155
    cleaning_beam_agent-2_mean: 37.13
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 62.85
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 184
    cleaning_beam_agent-4_mean: 74.93
    cleaning_beam_agent-4_min: 19
    cleaning_beam_agent-5_max: 40
    cleaning_beam_agent-5_mean: 14.09
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-56-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 23.01
  episode_reward_min: 5.0
  episodes_this_iter: 96
  episodes_total: 22560
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12175.232
    learner:
      agent-0:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5305061936378479
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016907916869968176
        model: {}
        policy_loss: -0.0036930497735738754
        total_loss: -0.004591900855302811
        vf_explained_var: -0.00031344592571258545
        vf_loss: 0.3483752906322479
      agent-1:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5420414209365845
        entropy_coeff: 0.0017600000137463212
        kl: 0.000985379796475172
        model: {}
        policy_loss: -0.0028647789731621742
        total_loss: -0.003800960723310709
        vf_explained_var: 0.027284175157546997
        vf_loss: 0.17811891436576843
      agent-2:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.529165506362915
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014291120460256934
        model: {}
        policy_loss: -0.0038095770869404078
        total_loss: -0.004678637720644474
        vf_explained_var: -0.0006687194108963013
        vf_loss: 0.6227059364318848
      agent-3:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6029955148696899
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016115492908284068
        model: {}
        policy_loss: -0.0031143780797719955
        total_loss: -0.004137152805924416
        vf_explained_var: -0.012727349996566772
        vf_loss: 0.38497233390808105
      agent-4:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6604400277137756
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012940450105816126
        model: {}
        policy_loss: -0.003995127510279417
        total_loss: -0.0051169428043067455
        vf_explained_var: 0.021362483501434326
        vf_loss: 0.40559104084968567
      agent-5:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8816236257553101
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019582489039748907
        model: {}
        policy_loss: -0.003020340343937278
        total_loss: -0.004544532857835293
        vf_explained_var: 0.011653482913970947
        vf_loss: 0.27465227246284485
    load_time_ms: 13478.182
    num_steps_sampled: 22560000
    num_steps_trained: 22560000
    sample_time_ms: 100031.527
    update_time_ms: 17.293
  iterations_since_restore: 75
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.72584269662921
    ram_util_percent: 12.169101123595508
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 21.0
    agent-3: 15.0
    agent-4: 13.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.75
    agent-1: 2.25
    agent-2: 4.97
    agent-3: 3.98
    agent-4: 5.12
    agent-5: 2.94
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.8433735620067
    mean_inference_ms: 12.77515552851812
    mean_processing_ms: 57.55695998578186
  time_since_restore: 9456.258411884308
  time_this_iter_s: 124.84925889968872
  time_total_s: 31966.541654348373
  timestamp: 1637229368
  timesteps_since_restore: 7200000
  timesteps_this_iter: 96000
  timesteps_total: 22560000
  training_iteration: 235
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    235 |          31966.5 | 22560000 |    23.01 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.58
    apples_agent-0_min: 0
    apples_agent-1_max: 56
    apples_agent-1_mean: 1.28
    apples_agent-1_min: 0
    apples_agent-2_max: 7
    apples_agent-2_mean: 1.73
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 2.85
    apples_agent-3_min: 0
    apples_agent-4_max: 32
    apples_agent-4_mean: 1.61
    apples_agent-4_min: 0
    apples_agent-5_max: 17
    apples_agent-5_mean: 1.58
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 188
    cleaning_beam_agent-0_mean: 81.08
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 392
    cleaning_beam_agent-1_mean: 217.28
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 83
    cleaning_beam_agent-2_mean: 38.93
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 71.34
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 196
    cleaning_beam_agent-4_mean: 75.96
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 41
    cleaning_beam_agent-5_mean: 13.83
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_04-58-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 21.81
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 22656
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12181.753
    learner:
      agent-0:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5325968265533447
        entropy_coeff: 0.0017600000137463212
        kl: 0.001680479384958744
        model: {}
        policy_loss: -0.004006038419902325
        total_loss: -0.004903522320091724
        vf_explained_var: -0.001092568039894104
        vf_loss: 0.3988473415374756
      agent-1:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.541471004486084
        entropy_coeff: 0.0017600000137463212
        kl: 0.001378660905174911
        model: {}
        policy_loss: -0.002904419554397464
        total_loss: -0.003842322388663888
        vf_explained_var: 0.021510004997253418
        vf_loss: 0.1508987545967102
      agent-2:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5275481939315796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019227191805839539
        model: {}
        policy_loss: -0.0038645542226731777
        total_loss: -0.004741318058222532
        vf_explained_var: 2.3767352104187012e-05
        vf_loss: 0.5172010660171509
      agent-3:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6316297650337219
        entropy_coeff: 0.0017600000137463212
        kl: 0.001602847594767809
        model: {}
        policy_loss: -0.0029416228644549847
        total_loss: -0.004018818959593773
        vf_explained_var: -0.011549592018127441
        vf_loss: 0.3447248637676239
      agent-4:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6638211607933044
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019171623280271888
        model: {}
        policy_loss: -0.0039853863418102264
        total_loss: -0.0051156990230083466
        vf_explained_var: 0.003018319606781006
        vf_loss: 0.3801285922527313
      agent-5:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8751180171966553
        entropy_coeff: 0.0017600000137463212
        kl: 0.001396634615957737
        model: {}
        policy_loss: -0.002876858226954937
        total_loss: -0.004397459328174591
        vf_explained_var: -0.0014895349740982056
        vf_loss: 0.19603660702705383
    load_time_ms: 13475.785
    num_steps_sampled: 22656000
    num_steps_trained: 22656000
    sample_time_ms: 100039.598
    update_time_ms: 16.815
  iterations_since_restore: 76
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.126815642458105
    ram_util_percent: 12.254748603351954
  pid: 5668
  policy_reward_max:
    agent-0: 17.0
    agent-1: 8.0
    agent-2: 14.0
    agent-3: 13.0
    agent-4: 13.0
    agent-5: 8.0
  policy_reward_mean:
    agent-0: 4.37
    agent-1: 1.88
    agent-2: 4.58
    agent-3: 4.09
    agent-4: 4.39
    agent-5: 2.5
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.839769282032798
    mean_inference_ms: 12.774670161817301
    mean_processing_ms: 57.5556531491118
  time_since_restore: 9582.002971410751
  time_this_iter_s: 125.74455952644348
  time_total_s: 32092.286213874817
  timestamp: 1637229494
  timesteps_since_restore: 7296000
  timesteps_this_iter: 96000
  timesteps_total: 22656000
  training_iteration: 236
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    236 |          32092.3 | 22656000 |    21.81 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 3.2
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.09
    apples_agent-1_min: 0
    apples_agent-2_max: 32
    apples_agent-2_mean: 2.22
    apples_agent-2_min: 0
    apples_agent-3_max: 22
    apples_agent-3_mean: 3.3
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 0.96
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.97
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 171
    cleaning_beam_agent-0_mean: 80.54
    cleaning_beam_agent-0_min: 14
    cleaning_beam_agent-1_max: 335
    cleaning_beam_agent-1_mean: 207.23
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 122
    cleaning_beam_agent-2_mean: 37.92
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 148
    cleaning_beam_agent-3_mean: 63.17
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 292
    cleaning_beam_agent-4_mean: 87.29
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 46
    cleaning_beam_agent-5_mean: 14.8
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-00-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 77.0
  episode_reward_mean: 22.28
  episode_reward_min: -32.0
  episodes_this_iter: 96
  episodes_total: 22752
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12185.737
    learner:
      agent-0:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5312027931213379
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014302218332886696
        model: {}
        policy_loss: -0.0032728286460042
        total_loss: -0.004171386826783419
        vf_explained_var: -0.0015377700328826904
        vf_loss: 0.36355674266815186
      agent-1:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5406322479248047
        entropy_coeff: 0.0017600000137463212
        kl: 0.001413423684425652
        model: {}
        policy_loss: -0.0030175000429153442
        total_loss: -0.003951948136091232
        vf_explained_var: 0.02075798809528351
        vf_loss: 0.1706552505493164
      agent-2:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5328617095947266
        entropy_coeff: 0.0017600000137463212
        kl: 0.001529978821054101
        model: {}
        policy_loss: -0.003695226740092039
        total_loss: -0.004569456912577152
        vf_explained_var: 0.0008317083120346069
        vf_loss: 0.6360713243484497
      agent-3:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6111910343170166
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013309031492099166
        model: {}
        policy_loss: -0.002471230924129486
        total_loss: -0.003479478880763054
        vf_explained_var: -0.00841134786605835
        vf_loss: 0.6744894981384277
      agent-4:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6543821096420288
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011511312332004309
        model: {}
        policy_loss: -0.003582000732421875
        total_loss: -0.0046952832490205765
        vf_explained_var: 0.023300960659980774
        vf_loss: 0.38429608941078186
      agent-5:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8874406814575195
        entropy_coeff: 0.0017600000137463212
        kl: 0.001449035364203155
        model: {}
        policy_loss: -0.0030002216808497906
        total_loss: -0.004539568442851305
        vf_explained_var: 0.002213895320892334
        vf_loss: 0.22552868723869324
    load_time_ms: 13480.684
    num_steps_sampled: 22752000
    num_steps_trained: 22752000
    sample_time_ms: 100039.018
    update_time_ms: 17.291
  iterations_since_restore: 77
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.90614525139665
    ram_util_percent: 12.28994413407821
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 9.0
    agent-2: 22.0
    agent-3: 24.0
    agent-4: 12.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.62
    agent-1: 1.99
    agent-2: 5.14
    agent-3: 4.67
    agent-4: 4.32
    agent-5: 2.54
  policy_reward_min:
    agent-0: -45.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.83881696659062
    mean_inference_ms: 12.774011163139239
    mean_processing_ms: 57.55439636912429
  time_since_restore: 9707.893528938293
  time_this_iter_s: 125.89055752754211
  time_total_s: 32218.17677140236
  timestamp: 1637229620
  timesteps_since_restore: 7392000
  timesteps_this_iter: 96000
  timesteps_total: 22752000
  training_iteration: 237
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    237 |          32218.2 | 22752000 |    22.28 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 3.02
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.07
    apples_agent-1_min: 0
    apples_agent-2_max: 36
    apples_agent-2_mean: 2.62
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 3.0
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.25
    apples_agent-4_min: 0
    apples_agent-5_max: 18
    apples_agent-5_mean: 1.48
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 197
    cleaning_beam_agent-0_mean: 80.21
    cleaning_beam_agent-0_min: 33
    cleaning_beam_agent-1_max: 335
    cleaning_beam_agent-1_mean: 208.26
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 117
    cleaning_beam_agent-2_mean: 37.83
    cleaning_beam_agent-2_min: 7
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 60.47
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 237
    cleaning_beam_agent-4_mean: 82.65
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 37
    cleaning_beam_agent-5_mean: 16.16
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-02-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 22.36
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 22848
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12185.554
    learner:
      agent-0:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5291198492050171
        entropy_coeff: 0.0017600000137463212
        kl: 0.001638457179069519
        model: {}
        policy_loss: -0.003515320597216487
        total_loss: -0.004412030801177025
        vf_explained_var: -0.0049796998500823975
        vf_loss: 0.345398873090744
      agent-1:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5359386801719666
        entropy_coeff: 0.0017600000137463212
        kl: 0.00128072127699852
        model: {}
        policy_loss: -0.002858377993106842
        total_loss: -0.0037885885685682297
        vf_explained_var: 0.030938148498535156
        vf_loss: 0.13042399287223816
      agent-2:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5288757681846619
        entropy_coeff: 0.0017600000137463212
        kl: 0.001711674383841455
        model: {}
        policy_loss: -0.004226792138069868
        total_loss: -0.005112401209771633
        vf_explained_var: 0.00046233832836151123
        vf_loss: 0.4521387815475464
      agent-3:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5975948572158813
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008973486837930977
        model: {}
        policy_loss: -0.0025678309611976147
        total_loss: -0.0035716595593839884
        vf_explained_var: 0.003723651170730591
        vf_loss: 0.47937583923339844
      agent-4:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6574022173881531
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017971249762922525
        model: {}
        policy_loss: -0.004014287609606981
        total_loss: -0.005130890291184187
        vf_explained_var: 0.014228418469429016
        vf_loss: 0.4042645990848541
      agent-5:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8798593282699585
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015652603469789028
        model: {}
        policy_loss: -0.0030431849882006645
        total_loss: -0.004568429198116064
        vf_explained_var: -0.00025540590286254883
        vf_loss: 0.2330610603094101
    load_time_ms: 13472.152
    num_steps_sampled: 22848000
    num_steps_trained: 22848000
    sample_time_ms: 100060.249
    update_time_ms: 17.343
  iterations_since_restore: 78
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.378770949720668
    ram_util_percent: 12.176536312849164
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 15.0
    agent-3: 16.0
    agent-4: 12.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 4.09
    agent-1: 1.87
    agent-2: 4.57
    agent-3: 4.56
    agent-4: 4.69
    agent-5: 2.58
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.836184786623967
    mean_inference_ms: 12.773381168758032
    mean_processing_ms: 57.55383312010483
  time_since_restore: 9833.304233312607
  time_this_iter_s: 125.41070437431335
  time_total_s: 32343.587475776672
  timestamp: 1637229746
  timesteps_since_restore: 7488000
  timesteps_this_iter: 96000
  timesteps_total: 22848000
  training_iteration: 238
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    238 |          32343.6 | 22848000 |    22.36 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 2.95
    apples_agent-0_min: 0
    apples_agent-1_max: 18
    apples_agent-1_mean: 1.22
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 1.98
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.55
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 1.22
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.46
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 193
    cleaning_beam_agent-0_mean: 83.57
    cleaning_beam_agent-0_min: 24
    cleaning_beam_agent-1_max: 369
    cleaning_beam_agent-1_mean: 213.36
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 99
    cleaning_beam_agent-2_mean: 35.49
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 205
    cleaning_beam_agent-3_mean: 58.38
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 285
    cleaning_beam_agent-4_mean: 81.91
    cleaning_beam_agent-4_min: 20
    cleaning_beam_agent-5_max: 45
    cleaning_beam_agent-5_mean: 16.46
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-04-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 22.61
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 22944
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12186.971
    learner:
      agent-0:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5261836647987366
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011914244387298822
        model: {}
        policy_loss: -0.0033075809478759766
        total_loss: -0.004201486706733704
        vf_explained_var: 0.00040918588638305664
        vf_loss: 0.32177695631980896
      agent-1:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5446356534957886
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013096330221742392
        model: {}
        policy_loss: -0.0025952421128749847
        total_loss: -0.0035369442775845528
        vf_explained_var: 0.02218000590801239
        vf_loss: 0.16859029233455658
      agent-2:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5124853849411011
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010959028732031584
        model: {}
        policy_loss: -0.003522887360304594
        total_loss: -0.004379449877887964
        vf_explained_var: -0.001261398196220398
        vf_loss: 0.45410922169685364
      agent-3:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6010626554489136
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013200901448726654
        model: {}
        policy_loss: -0.002723600948229432
        total_loss: -0.0037331741768866777
        vf_explained_var: -0.004321962594985962
        vf_loss: 0.48295754194259644
      agent-4:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6574530005455017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014387641567736864
        model: {}
        policy_loss: -0.0039009014144539833
        total_loss: -0.0050186887383461
        vf_explained_var: 0.010590344667434692
        vf_loss: 0.393293559551239
      agent-5:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9028657674789429
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014844998950138688
        model: {}
        policy_loss: -0.003018992720171809
        total_loss: -0.004584729205816984
        vf_explained_var: 0.0058211833238601685
        vf_loss: 0.23305025696754456
    load_time_ms: 13464.84
    num_steps_sampled: 22944000
    num_steps_trained: 22944000
    sample_time_ms: 100270.396
    update_time_ms: 17.63
  iterations_since_restore: 79
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.355248618784533
    ram_util_percent: 12.247513812154692
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 10.0
    agent-2: 18.0
    agent-3: 24.0
    agent-4: 13.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.71
    agent-1: 1.82
    agent-2: 5.0
    agent-3: 4.66
    agent-4: 4.55
    agent-5: 2.87
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.836164484098575
    mean_inference_ms: 12.774168386237722
    mean_processing_ms: 57.56234634629579
  time_since_restore: 9960.367206811905
  time_this_iter_s: 127.0629734992981
  time_total_s: 32470.65044927597
  timestamp: 1637229873
  timesteps_since_restore: 7584000
  timesteps_this_iter: 96000
  timesteps_total: 22944000
  training_iteration: 239
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    239 |          32470.7 | 22944000 |    22.61 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 2.85
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.96
    apples_agent-1_min: 0
    apples_agent-2_max: 23
    apples_agent-2_mean: 1.89
    apples_agent-2_min: 0
    apples_agent-3_max: 33
    apples_agent-3_mean: 3.85
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.14
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.68
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 141
    cleaning_beam_agent-0_mean: 74.86
    cleaning_beam_agent-0_min: 27
    cleaning_beam_agent-1_max: 295
    cleaning_beam_agent-1_mean: 197.07
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 219
    cleaning_beam_agent-2_mean: 41.8
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 127
    cleaning_beam_agent-3_mean: 49.74
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 281
    cleaning_beam_agent-4_mean: 87.37
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 18.98
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-06-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 65.0
  episode_reward_mean: 23.28
  episode_reward_min: -25.0
  episodes_this_iter: 96
  episodes_total: 23040
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12180.863
    learner:
      agent-0:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5244636535644531
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019044820219278336
        model: {}
        policy_loss: -0.0023848332930356264
        total_loss: -0.0031395964324474335
        vf_explained_var: 0.0010062307119369507
        vf_loss: 1.6829087734222412
      agent-1:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5519492626190186
        entropy_coeff: 0.0017600000137463212
        kl: 0.001188760856166482
        model: {}
        policy_loss: -0.002777300775051117
        total_loss: -0.0037352945655584335
        vf_explained_var: 0.010124638676643372
        vf_loss: 0.13435643911361694
      agent-2:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5230883359909058
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017440628726035357
        model: {}
        policy_loss: -0.003671540878713131
        total_loss: -0.00455348240211606
        vf_explained_var: -0.002971738576889038
        vf_loss: 0.3869418501853943
      agent-3:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5715978145599365
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016059224726632237
        model: {}
        policy_loss: -0.002732490887865424
        total_loss: -0.0036856774240732193
        vf_explained_var: 0.007051438093185425
        vf_loss: 0.5282607078552246
      agent-4:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6593467593193054
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018619615584611893
        model: {}
        policy_loss: -0.003919634502381086
        total_loss: -0.005036733578890562
        vf_explained_var: 0.00939546525478363
        vf_loss: 0.4335145950317383
      agent-5:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9070563316345215
        entropy_coeff: 0.0017600000137463212
        kl: 0.001964747440069914
        model: {}
        policy_loss: -0.0031466837972402573
        total_loss: -0.004720207769423723
        vf_explained_var: -0.0033953338861465454
        vf_loss: 0.2289215624332428
    load_time_ms: 13443.945
    num_steps_sampled: 23040000
    num_steps_trained: 23040000
    sample_time_ms: 100215.049
    update_time_ms: 17.57
  iterations_since_restore: 80
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.240449438202248
    ram_util_percent: 12.228089887640447
  pid: 5668
  policy_reward_max:
    agent-0: 18.0
    agent-1: 11.0
    agent-2: 18.0
    agent-3: 18.0
    agent-4: 15.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.44
    agent-1: 1.86
    agent-2: 4.66
    agent-3: 5.05
    agent-4: 5.24
    agent-5: 3.03
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.83177966512984
    mean_inference_ms: 12.773366504810237
    mean_processing_ms: 57.56190210866144
  time_since_restore: 10085.648469686508
  time_this_iter_s: 125.28126287460327
  time_total_s: 32595.931712150574
  timestamp: 1637229998
  timesteps_since_restore: 7680000
  timesteps_this_iter: 96000
  timesteps_total: 23040000
  training_iteration: 240
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    240 |          32595.9 | 23040000 |    23.28 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 3.49
    apples_agent-0_min: 0
    apples_agent-1_max: 16
    apples_agent-1_mean: 1.11
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.07
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 3.4
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.42
    apples_agent-4_min: 0
    apples_agent-5_max: 23
    apples_agent-5_mean: 1.79
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 192
    cleaning_beam_agent-0_mean: 83.1
    cleaning_beam_agent-0_min: 34
    cleaning_beam_agent-1_max: 357
    cleaning_beam_agent-1_mean: 191.28
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 124
    cleaning_beam_agent-2_mean: 40.17
    cleaning_beam_agent-2_min: 9
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 51.78
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 322
    cleaning_beam_agent-4_mean: 76.1
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 17.53
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-08-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 23.53
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 23136
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12186.48
    learner:
      agent-0:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.529634952545166
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016479630721732974
        model: {}
        policy_loss: -0.0034074229188263416
        total_loss: -0.004303535912185907
        vf_explained_var: -0.003033667802810669
        vf_loss: 0.3604670464992523
      agent-1:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5483027100563049
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011696883011609316
        model: {}
        policy_loss: -0.0029061520472168922
        total_loss: -0.003854685463011265
        vf_explained_var: 0.02424071729183197
        vf_loss: 0.16480295360088348
      agent-2:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5055555105209351
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015242446679621935
        model: {}
        policy_loss: -0.003812034847214818
        total_loss: -0.004648272879421711
        vf_explained_var: -0.005829513072967529
        vf_loss: 0.5353769063949585
      agent-3:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5898275375366211
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015712485183030367
        model: {}
        policy_loss: -0.002707206178456545
        total_loss: -0.003702404908835888
        vf_explained_var: -0.008213788270950317
        vf_loss: 0.42898404598236084
      agent-4:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.669321596622467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014260848984122276
        model: {}
        policy_loss: -0.004046524874866009
        total_loss: -0.005173030309379101
        vf_explained_var: 0.009534046053886414
        vf_loss: 0.5150153636932373
      agent-5:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.890835702419281
        entropy_coeff: 0.0017600000137463212
        kl: 0.001707948511466384
        model: {}
        policy_loss: -0.0029914684128016233
        total_loss: -0.00454135425388813
        vf_explained_var: -0.005668893456459045
        vf_loss: 0.17984524369239807
    load_time_ms: 13460.001
    num_steps_sampled: 23136000
    num_steps_trained: 23136000
    sample_time_ms: 100168.798
    update_time_ms: 17.528
  iterations_since_restore: 81
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.614917127071823
    ram_util_percent: 12.190055248618782
  pid: 5668
  policy_reward_max:
    agent-0: 11.0
    agent-1: 9.0
    agent-2: 17.0
    agent-3: 19.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.39
    agent-1: 2.03
    agent-2: 4.72
    agent-3: 4.35
    agent-4: 5.44
    agent-5: 2.6
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.830653351826268
    mean_inference_ms: 12.774722674306656
    mean_processing_ms: 57.57138083443107
  time_since_restore: 10212.277022838593
  time_this_iter_s: 126.62855315208435
  time_total_s: 32722.560265302658
  timestamp: 1637230126
  timesteps_since_restore: 7776000
  timesteps_this_iter: 96000
  timesteps_total: 23136000
  training_iteration: 241
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    241 |          32722.6 | 23136000 |    23.53 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 3.28
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 1.0
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 1.91
    apples_agent-2_min: 0
    apples_agent-3_max: 35
    apples_agent-3_mean: 3.51
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.2
    apples_agent-4_min: 0
    apples_agent-5_max: 26
    apples_agent-5_mean: 1.82
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 268
    cleaning_beam_agent-0_mean: 77.16
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 336
    cleaning_beam_agent-1_mean: 197.25
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 165
    cleaning_beam_agent-2_mean: 36.0
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 56.91
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 206
    cleaning_beam_agent-4_mean: 69.83
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 66
    cleaning_beam_agent-5_mean: 15.73
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 4
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-10-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 49.0
  episode_reward_mean: 20.28
  episode_reward_min: -172.0
  episodes_this_iter: 96
  episodes_total: 23232
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12202.823
    learner:
      agent-0:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5287120938301086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007968843565322459
        model: {}
        policy_loss: -0.002067562425509095
        total_loss: -0.0014430130831897259
        vf_explained_var: -0.00015822052955627441
        vf_loss: 15.550846099853516
      agent-1:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5369336605072021
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009525595814920962
        model: {}
        policy_loss: -0.0025689145550131798
        total_loss: -0.0035028241109102964
        vf_explained_var: 0.024767279624938965
        vf_loss: 0.11094939708709717
      agent-2:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5061218738555908
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012487908825278282
        model: {}
        policy_loss: -0.0034569904673844576
        total_loss: -0.004301806446164846
        vf_explained_var: -0.005192458629608154
        vf_loss: 0.45960721373558044
      agent-3:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6049549579620361
        entropy_coeff: 0.0017600000137463212
        kl: 0.001428057556040585
        model: {}
        policy_loss: -0.002928510308265686
        total_loss: -0.003954581916332245
        vf_explained_var: -0.003653228282928467
        vf_loss: 0.3864617943763733
      agent-4:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6651849150657654
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015170767437666655
        model: {}
        policy_loss: -0.0028343764133751392
        total_loss: -0.003821050748229027
        vf_explained_var: 0.006485790014266968
        vf_loss: 1.840506672859192
      agent-5:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8877472281455994
        entropy_coeff: 0.0017600000137463212
        kl: 0.001954486360773444
        model: {}
        policy_loss: -0.0032400223426520824
        total_loss: -0.004781830590218306
        vf_explained_var: 0.006114020943641663
        vf_loss: 0.2063019871711731
    load_time_ms: 13470.136
    num_steps_sampled: 23232000
    num_steps_trained: 23232000
    sample_time_ms: 100260.601
    update_time_ms: 17.738
  iterations_since_restore: 82
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.011049723756905
    ram_util_percent: 12.27513812154696
  pid: 5668
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 15.0
    agent-3: 12.0
    agent-4: 14.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 2.15
    agent-1: 1.7
    agent-2: 4.83
    agent-3: 4.34
    agent-4: 4.51
    agent-5: 2.75
  policy_reward_min:
    agent-0: -197.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -46.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.827909862548548
    mean_inference_ms: 12.774811416269685
    mean_processing_ms: 57.57131676685187
  time_since_restore: 10339.449832439423
  time_this_iter_s: 127.17280960083008
  time_total_s: 32849.73307490349
  timestamp: 1637230253
  timesteps_since_restore: 7872000
  timesteps_this_iter: 96000
  timesteps_total: 23232000
  training_iteration: 242
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    242 |          32849.7 | 23232000 |    20.28 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.53
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.9
    apples_agent-1_min: 0
    apples_agent-2_max: 8
    apples_agent-2_mean: 1.61
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.66
    apples_agent-3_min: 0
    apples_agent-4_max: 16
    apples_agent-4_mean: 1.3
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.64
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 147
    cleaning_beam_agent-0_mean: 74.15
    cleaning_beam_agent-0_min: 13
    cleaning_beam_agent-1_max: 334
    cleaning_beam_agent-1_mean: 193.91
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 88
    cleaning_beam_agent-2_mean: 33.93
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 146
    cleaning_beam_agent-3_mean: 61.08
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 291
    cleaning_beam_agent-4_mean: 72.63
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 45
    cleaning_beam_agent-5_mean: 16.26
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-13-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 19.8
  episode_reward_min: -39.0
  episodes_this_iter: 96
  episodes_total: 23328
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12206.251
    learner:
      agent-0:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.53325355052948
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007796993013471365
        model: {}
        policy_loss: -0.0021156021393835545
        total_loss: -0.002891778014600277
        vf_explained_var: -0.0003441721200942993
        vf_loss: 1.6234829425811768
      agent-1:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5509452819824219
        entropy_coeff: 0.0017600000137463212
        kl: 0.001569710555486381
        model: {}
        policy_loss: -0.0028675543144345284
        total_loss: -0.003825617954134941
        vf_explained_var: 0.029679447412490845
        vf_loss: 0.11597393453121185
      agent-2:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49485546350479126
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007371096289716661
        model: {}
        policy_loss: -0.0019693649373948574
        total_loss: -0.0026689895894378424
        vf_explained_var: -0.003741517663002014
        vf_loss: 1.7132153511047363
      agent-3:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5992388129234314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008739198674447834
        model: {}
        policy_loss: -0.0027256980538368225
        total_loss: -0.003745352616533637
        vf_explained_var: -0.03016480803489685
        vf_loss: 0.35002246499061584
      agent-4:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6653764247894287
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014180137077346444
        model: {}
        policy_loss: -0.0026075895875692368
        total_loss: -0.0036190301179885864
        vf_explained_var: 0.009040549397468567
        vf_loss: 1.5962597131729126
      agent-5:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8845563530921936
        entropy_coeff: 0.0017600000137463212
        kl: 0.001867916900664568
        model: {}
        policy_loss: -0.0031113987788558006
        total_loss: -0.004646732471883297
        vf_explained_var: -0.0027064085006713867
        vf_loss: 0.21486717462539673
    load_time_ms: 13449.169
    num_steps_sampled: 23328000
    num_steps_trained: 23328000
    sample_time_ms: 100419.53
    update_time_ms: 17.373
  iterations_since_restore: 83
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.06353591160221
    ram_util_percent: 12.167955801104974
  pid: 5668
  policy_reward_max:
    agent-0: 14.0
    agent-1: 7.0
    agent-2: 13.0
    agent-3: 19.0
    agent-4: 14.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.33
    agent-1: 1.72
    agent-2: 3.98
    agent-3: 3.55
    agent-4: 4.45
    agent-5: 2.77
  policy_reward_min:
    agent-0: -47.0
    agent-1: 0.0
    agent-2: -47.0
    agent-3: 0.0
    agent-4: -42.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.825946417280957
    mean_inference_ms: 12.774985959542958
    mean_processing_ms: 57.572249689006874
  time_since_restore: 10466.651214122772
  time_this_iter_s: 127.20138168334961
  time_total_s: 32976.93445658684
  timestamp: 1637230380
  timesteps_since_restore: 7968000
  timesteps_this_iter: 96000
  timesteps_total: 23328000
  training_iteration: 243
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    243 |          32976.9 | 23328000 |     19.8 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 6
    apples_agent-0_mean: 1.94
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.93
    apples_agent-1_min: 0
    apples_agent-2_max: 37
    apples_agent-2_mean: 1.78
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 2.51
    apples_agent-3_min: 0
    apples_agent-4_max: 31
    apples_agent-4_mean: 1.26
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 1.38
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 148
    cleaning_beam_agent-0_mean: 76.28
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 276
    cleaning_beam_agent-1_mean: 192.84
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 109
    cleaning_beam_agent-2_mean: 33.64
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 127
    cleaning_beam_agent-3_mean: 56.66
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 238
    cleaning_beam_agent-4_mean: 67.14
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 65
    cleaning_beam_agent-5_mean: 18.38
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-15-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 44.0
  episode_reward_mean: 19.39
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 23424
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12206.16
    learner:
      agent-0:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5305821299552917
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013941744109615684
        model: {}
        policy_loss: -0.0038566598668694496
        total_loss: -0.004770837724208832
        vf_explained_var: -0.006194800138473511
        vf_loss: 0.19646964967250824
      agent-1:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5437431335449219
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011107667814940214
        model: {}
        policy_loss: -0.0028649182058870792
        total_loss: -0.0038072457537055016
        vf_explained_var: 0.02189093828201294
        vf_loss: 0.14661507308483124
      agent-2:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5019437074661255
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012751221656799316
        model: {}
        policy_loss: -0.0038755261339247227
        total_loss: -0.00472539896145463
        vf_explained_var: -0.0020478367805480957
        vf_loss: 0.33545243740081787
      agent-3:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6011486053466797
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010168554726988077
        model: {}
        policy_loss: -0.0026196460239589214
        total_loss: -0.003643500152975321
        vf_explained_var: -0.01214289665222168
        vf_loss: 0.34165841341018677
      agent-4:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6685347557067871
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012348032323643565
        model: {}
        policy_loss: -0.003624726552516222
        total_loss: -0.004758072085678577
        vf_explained_var: 0.027355343103408813
        vf_loss: 0.43271365761756897
      agent-5:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8980647921562195
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015733292093500495
        model: {}
        policy_loss: -0.0029966586735099554
        total_loss: -0.0045599378645420074
        vf_explained_var: -0.012749522924423218
        vf_loss: 0.17314578592777252
    load_time_ms: 13436.654
    num_steps_sampled: 23424000
    num_steps_trained: 23424000
    sample_time_ms: 100295.21
    update_time_ms: 17.222
  iterations_since_restore: 84
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.752808988764045
    ram_util_percent: 12.280337078651684
  pid: 5668
  policy_reward_max:
    agent-0: 9.0
    agent-1: 9.0
    agent-2: 13.0
    agent-3: 14.0
    agent-4: 14.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 2.92
    agent-1: 1.99
    agent-2: 4.0
    agent-3: 3.39
    agent-4: 4.66
    agent-5: 2.43
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.821812593556743
    mean_inference_ms: 12.775396572967754
    mean_processing_ms: 57.570857129007564
  time_since_restore: 10591.874515771866
  time_this_iter_s: 125.22330164909363
  time_total_s: 33102.15775823593
  timestamp: 1637230506
  timesteps_since_restore: 8064000
  timesteps_this_iter: 96000
  timesteps_total: 23424000
  training_iteration: 244
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    244 |          33102.2 | 23424000 |    19.39 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.58
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.93
    apples_agent-1_min: 0
    apples_agent-2_max: 29
    apples_agent-2_mean: 2.13
    apples_agent-2_min: 0
    apples_agent-3_max: 22
    apples_agent-3_mean: 3.4
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 0.97
    apples_agent-4_min: 0
    apples_agent-5_max: 41
    apples_agent-5_mean: 2.63
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 188
    cleaning_beam_agent-0_mean: 74.28
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 340
    cleaning_beam_agent-1_mean: 189.92
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 85
    cleaning_beam_agent-2_mean: 37.62
    cleaning_beam_agent-2_min: 7
    cleaning_beam_agent-3_max: 162
    cleaning_beam_agent-3_mean: 55.98
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 265
    cleaning_beam_agent-4_mean: 84.63
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 62
    cleaning_beam_agent-5_mean: 18.53
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-17-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 66.0
  episode_reward_mean: 23.12
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 23520
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12203.195
    learner:
      agent-0:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.536733865737915
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019529415294528008
        model: {}
        policy_loss: -0.0037575745955109596
        total_loss: -0.004657289013266563
        vf_explained_var: 0.0035127997398376465
        vf_loss: 0.4493888020515442
      agent-1:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5441944599151611
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006946786306798458
        model: {}
        policy_loss: -0.002303582616150379
        total_loss: -0.0032450207509100437
        vf_explained_var: 0.024021849036216736
        vf_loss: 0.16342821717262268
      agent-2:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5019099712371826
        entropy_coeff: 0.0017600000137463212
        kl: 0.001467011054046452
        model: {}
        policy_loss: -0.003797279205173254
        total_loss: -0.004616908263415098
        vf_explained_var: 0.0023808330297470093
        vf_loss: 0.6373426914215088
      agent-3:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5938037633895874
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011678456794470549
        model: {}
        policy_loss: -0.002524270210415125
        total_loss: -0.0035117766819894314
        vf_explained_var: -0.004579901695251465
        vf_loss: 0.5758742094039917
      agent-4:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6603657007217407
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015639399643987417
        model: {}
        policy_loss: -0.003934320993721485
        total_loss: -0.005051474552601576
        vf_explained_var: 0.005748957395553589
        vf_loss: 0.4509061574935913
      agent-5:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.883709728717804
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011554810917004943
        model: {}
        policy_loss: -0.0028128684498369694
        total_loss: -0.004342036787420511
        vf_explained_var: 0.001738905906677246
        vf_loss: 0.2616191804409027
    load_time_ms: 13451.713
    num_steps_sampled: 23520000
    num_steps_trained: 23520000
    sample_time_ms: 100459.628
    update_time_ms: 17.295
  iterations_since_restore: 85
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.071270718232043
    ram_util_percent: 12.282872928176795
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 13.0
    agent-2: 24.0
    agent-3: 23.0
    agent-4: 13.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.99
    agent-1: 1.84
    agent-2: 4.93
    agent-3: 4.62
    agent-4: 4.89
    agent-5: 2.85
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.818708014584733
    mean_inference_ms: 12.775359100741333
    mean_processing_ms: 57.57270678529124
  time_since_restore: 10718.541461706161
  time_this_iter_s: 126.66694593429565
  time_total_s: 33228.82470417023
  timestamp: 1637230632
  timesteps_since_restore: 8160000
  timesteps_this_iter: 96000
  timesteps_total: 23520000
  training_iteration: 245
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    245 |          33228.8 | 23520000 |    23.12 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.71
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.18
    apples_agent-1_min: 0
    apples_agent-2_max: 29
    apples_agent-2_mean: 1.92
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.8
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 0.97
    apples_agent-4_min: 0
    apples_agent-5_max: 14
    apples_agent-5_mean: 1.78
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 154
    cleaning_beam_agent-0_mean: 70.03
    cleaning_beam_agent-0_min: 26
    cleaning_beam_agent-1_max: 300
    cleaning_beam_agent-1_mean: 187.34
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 123
    cleaning_beam_agent-2_mean: 37.78
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 116
    cleaning_beam_agent-3_mean: 56.79
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 266
    cleaning_beam_agent-4_mean: 75.5
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 68
    cleaning_beam_agent-5_mean: 20.82
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-19-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 47.0
  episode_reward_mean: 21.44
  episode_reward_min: 1.0
  episodes_this_iter: 96
  episodes_total: 23616
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12198.758
    learner:
      agent-0:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5322258472442627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011376631446182728
        model: {}
        policy_loss: -0.0036492422223091125
        total_loss: -0.0045459456741809845
        vf_explained_var: -0.004803478717803955
        vf_loss: 0.4001465141773224
      agent-1:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5464651584625244
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009519815794192255
        model: {}
        policy_loss: -0.0028135557658970356
        total_loss: -0.003761580213904381
        vf_explained_var: 0.03050987422466278
        vf_loss: 0.13753321766853333
      agent-2:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.502045750617981
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011574581731110811
        model: {}
        policy_loss: -0.0035379088949412107
        total_loss: -0.004384220577776432
        vf_explained_var: -0.0008265823125839233
        vf_loss: 0.37288615107536316
      agent-3:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5937576293945312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012294539483264089
        model: {}
        policy_loss: -0.0028920602053403854
        total_loss: -0.0039041144773364067
        vf_explained_var: -0.0029504597187042236
        vf_loss: 0.32960769534111023
      agent-4:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6743075847625732
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019217610824853182
        model: {}
        policy_loss: -0.003914178349077702
        total_loss: -0.005061468109488487
        vf_explained_var: 0.019275173544883728
        vf_loss: 0.3949005603790283
      agent-5:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8868995904922485
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019825345370918512
        model: {}
        policy_loss: -0.0031580741051584482
        total_loss: -0.00469505600631237
        vf_explained_var: -0.00686393678188324
        vf_loss: 0.23962336778640747
    load_time_ms: 13434.208
    num_steps_sampled: 23616000
    num_steps_trained: 23616000
    sample_time_ms: 100446.806
    update_time_ms: 17.496
  iterations_since_restore: 86
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.28932584269663
    ram_util_percent: 12.200000000000003
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 6.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 19.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.84
    agent-1: 2.0
    agent-2: 4.28
    agent-3: 3.9
    agent-4: 4.64
    agent-5: 2.78
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.815252255048772
    mean_inference_ms: 12.775337896678634
    mean_processing_ms: 57.57144186987036
  time_since_restore: 10843.906886100769
  time_this_iter_s: 125.36542439460754
  time_total_s: 33354.190128564835
  timestamp: 1637230758
  timesteps_since_restore: 8256000
  timesteps_this_iter: 96000
  timesteps_total: 23616000
  training_iteration: 246
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    246 |          33354.2 | 23616000 |    21.44 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 3.2
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.81
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 2.02
    apples_agent-2_min: 0
    apples_agent-3_max: 46
    apples_agent-3_mean: 3.36
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.25
    apples_agent-4_min: 0
    apples_agent-5_max: 29
    apples_agent-5_mean: 2.28
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 145
    cleaning_beam_agent-0_mean: 66.81
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 529
    cleaning_beam_agent-1_mean: 202.52
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 158
    cleaning_beam_agent-2_mean: 36.81
    cleaning_beam_agent-2_min: 7
    cleaning_beam_agent-3_max: 109
    cleaning_beam_agent-3_mean: 58.16
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 276
    cleaning_beam_agent-4_mean: 73.4
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 49
    cleaning_beam_agent-5_mean: 19.82
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-21-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 22.7
  episode_reward_min: -39.0
  episodes_this_iter: 96
  episodes_total: 23712
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12188.005
    learner:
      agent-0:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5251609683036804
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012870925711467862
        model: {}
        policy_loss: -0.003383321687579155
        total_loss: -0.00427277572453022
        vf_explained_var: 0.000767052173614502
        vf_loss: 0.34826797246932983
      agent-1:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5652421712875366
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010790397645905614
        model: {}
        policy_loss: -0.002699356758967042
        total_loss: -0.0036812410689890385
        vf_explained_var: 0.011850833892822266
        vf_loss: 0.12940549850463867
      agent-2:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5045788884162903
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014189640060067177
        model: {}
        policy_loss: -0.003703879425302148
        total_loss: -0.004539105109870434
        vf_explained_var: 0.009588927030563354
        vf_loss: 0.5283153057098389
      agent-3:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5837416648864746
        entropy_coeff: 0.0017600000137463212
        kl: 0.001139721367508173
        model: {}
        policy_loss: -0.0026474199257791042
        total_loss: -0.0036185849457979202
        vf_explained_var: -0.0019283443689346313
        vf_loss: 0.5622192025184631
      agent-4:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6600539088249207
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016320798313245177
        model: {}
        policy_loss: -0.002978518372401595
        total_loss: -0.003986292984336615
        vf_explained_var: 0.002303197979927063
        vf_loss: 1.5392019748687744
      agent-5:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8892198204994202
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019190878374502063
        model: {}
        policy_loss: -0.0029031415469944477
        total_loss: -0.004439950920641422
        vf_explained_var: 0.0011954456567764282
        vf_loss: 0.28218939900398254
    load_time_ms: 13409.093
    num_steps_sampled: 23712000
    num_steps_trained: 23712000
    sample_time_ms: 100439.737
    update_time_ms: 17.151
  iterations_since_restore: 87
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.69217877094972
    ram_util_percent: 12.2927374301676
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 7.0
    agent-2: 19.0
    agent-3: 20.0
    agent-4: 16.0
    agent-5: 15.0
  policy_reward_mean:
    agent-0: 3.98
    agent-1: 1.87
    agent-2: 5.02
    agent-3: 4.74
    agent-4: 4.17
    agent-5: 2.92
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -45.0
    agent-5: -2.0
  sampler_perf:
    mean_env_wait_ms: 22.81276639252966
    mean_inference_ms: 12.775026501407984
    mean_processing_ms: 57.57253176815461
  time_since_restore: 10969.36335325241
  time_this_iter_s: 125.45646715164185
  time_total_s: 33479.64659571648
  timestamp: 1637230884
  timesteps_since_restore: 8352000
  timesteps_this_iter: 96000
  timesteps_total: 23712000
  training_iteration: 247
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    247 |          33479.6 | 23712000 |     22.7 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 3.03
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.83
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 2.06
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 3.15
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.37
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.62
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 121
    cleaning_beam_agent-0_mean: 65.67
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 209.37
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 80
    cleaning_beam_agent-2_mean: 31.18
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 146
    cleaning_beam_agent-3_mean: 55.87
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 308
    cleaning_beam_agent-4_mean: 73.06
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 47
    cleaning_beam_agent-5_mean: 20.32
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-23-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 23.03
  episode_reward_min: -87.0
  episodes_this_iter: 96
  episodes_total: 23808
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12189.895
    learner:
      agent-0:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.532598078250885
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014388574054464698
        model: {}
        policy_loss: -0.003412877209484577
        total_loss: -0.0042998394928872585
        vf_explained_var: 0.007351040840148926
        vf_loss: 0.5041152238845825
      agent-1:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5468735098838806
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010176708456128836
        model: {}
        policy_loss: -0.0027785561978816986
        total_loss: -0.0037281406112015247
        vf_explained_var: 0.0297783762216568
        vf_loss: 0.1291361153125763
      agent-2:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48845529556274414
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013275959063321352
        model: {}
        policy_loss: -0.003525175154209137
        total_loss: -0.004315298981964588
        vf_explained_var: -0.0017386078834533691
        vf_loss: 0.6955512762069702
      agent-3:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5936970114707947
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012188085820525885
        model: {}
        policy_loss: -0.002163821831345558
        total_loss: -0.0030492786318063736
        vf_explained_var: -0.0015248209238052368
        vf_loss: 1.5944987535476685
      agent-4:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6632678508758545
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017436654306948185
        model: {}
        policy_loss: -0.003144484478980303
        total_loss: -0.004145138431340456
        vf_explained_var: 0.0013298839330673218
        vf_loss: 1.6669378280639648
      agent-5:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8933873176574707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013267265167087317
        model: {}
        policy_loss: -0.0031941866036504507
        total_loss: -0.004742551129311323
        vf_explained_var: 0.007786914706230164
        vf_loss: 0.23999252915382385
    load_time_ms: 13430.452
    num_steps_sampled: 23808000
    num_steps_trained: 23808000
    sample_time_ms: 100432.869
    update_time_ms: 17.164
  iterations_since_restore: 88
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.124022346368715
    ram_util_percent: 12.30223463687151
  pid: 5668
  policy_reward_max:
    agent-0: 18.0
    agent-1: 7.0
    agent-2: 20.0
    agent-3: 13.0
    agent-4: 15.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.51
    agent-1: 1.96
    agent-2: 5.36
    agent-3: 3.94
    agent-4: 4.6
    agent-5: 2.66
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -49.0
    agent-4: -48.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.808584309267204
    mean_inference_ms: 12.774179824620028
    mean_processing_ms: 57.567692408093784
  time_since_restore: 11095.043973207474
  time_this_iter_s: 125.68061995506287
  time_total_s: 33605.32721567154
  timestamp: 1637231009
  timesteps_since_restore: 8448000
  timesteps_this_iter: 96000
  timesteps_total: 23808000
  training_iteration: 248
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    248 |          33605.3 | 23808000 |    23.03 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.81
    apples_agent-0_min: 0
    apples_agent-1_max: 26
    apples_agent-1_mean: 1.3
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 1.85
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.83
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.13
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.77
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 137
    cleaning_beam_agent-0_mean: 64.99
    cleaning_beam_agent-0_min: 11
    cleaning_beam_agent-1_max: 347
    cleaning_beam_agent-1_mean: 208.87
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 193
    cleaning_beam_agent-2_mean: 36.1
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 191
    cleaning_beam_agent-3_mean: 50.63
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 442
    cleaning_beam_agent-4_mean: 75.22
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 45
    cleaning_beam_agent-5_mean: 18.29
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-25-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 68.0
  episode_reward_mean: 21.67
  episode_reward_min: -87.0
  episodes_this_iter: 96
  episodes_total: 23904
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12196.506
    learner:
      agent-0:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.531764566898346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012536616995930672
        model: {}
        policy_loss: -0.0033355234190821648
        total_loss: -0.0042265066877007484
        vf_explained_var: -0.0018590092658996582
        vf_loss: 0.44922590255737305
      agent-1:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.548954963684082
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009974148124456406
        model: {}
        policy_loss: -0.0027175163850188255
        total_loss: -0.003668787656351924
        vf_explained_var: 0.03180116415023804
        vf_loss: 0.14889512956142426
      agent-2:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5024851560592651
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013287847395986319
        model: {}
        policy_loss: -0.003515480551868677
        total_loss: -0.004364861641079187
        vf_explained_var: 0.0032524168491363525
        vf_loss: 0.34993329644203186
      agent-3:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5743739008903503
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012386501766741276
        model: {}
        policy_loss: -0.0026220856234431267
        total_loss: -0.0035946425050497055
        vf_explained_var: 0.00507761538028717
        vf_loss: 0.38344335556030273
      agent-4:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6593624353408813
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017627583583816886
        model: {}
        policy_loss: -0.003716064617037773
        total_loss: -0.004832166247069836
        vf_explained_var: 0.003832608461380005
        vf_loss: 0.44377201795578003
      agent-5:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8847934603691101
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012926597846671939
        model: {}
        policy_loss: -0.002591959200799465
        total_loss: -0.004126652143895626
        vf_explained_var: 0.0002704113721847534
        vf_loss: 0.2254326045513153
    load_time_ms: 13426.64
    num_steps_sampled: 23904000
    num_steps_trained: 23904000
    sample_time_ms: 100460.417
    update_time_ms: 16.842
  iterations_since_restore: 89
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.56022099447514
    ram_util_percent: 12.299447513812154
  pid: 5668
  policy_reward_max:
    agent-0: 22.0
    agent-1: 9.0
    agent-2: 14.0
    agent-3: 16.0
    agent-4: 13.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.37
    agent-1: 1.96
    agent-2: 4.3
    agent-3: 3.64
    agent-4: 4.4
    agent-5: 3.0
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -49.0
    agent-4: -48.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.807838323114307
    mean_inference_ms: 12.774725257559464
    mean_processing_ms: 57.57075394367893
  time_since_restore: 11222.373717546463
  time_this_iter_s: 127.32974433898926
  time_total_s: 33732.65696001053
  timestamp: 1637231137
  timesteps_since_restore: 8544000
  timesteps_this_iter: 96000
  timesteps_total: 23904000
  training_iteration: 249
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    249 |          33732.7 | 23904000 |    21.67 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 2.74
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 1.02
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.77
    apples_agent-2_min: 0
    apples_agent-3_max: 21
    apples_agent-3_mean: 3.12
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.17
    apples_agent-4_min: 0
    apples_agent-5_max: 47
    apples_agent-5_mean: 1.98
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 125
    cleaning_beam_agent-0_mean: 63.14
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 308
    cleaning_beam_agent-1_mean: 202.89
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 193
    cleaning_beam_agent-2_mean: 33.84
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 188
    cleaning_beam_agent-3_mean: 58.4
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 442
    cleaning_beam_agent-4_mean: 74.75
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 46
    cleaning_beam_agent-5_mean: 20.45
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-27-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 78.0
  episode_reward_mean: 22.92
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 24000
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12187.637
    learner:
      agent-0:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5176246166229248
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012583635980263352
        model: {}
        policy_loss: -0.003213306190446019
        total_loss: -0.00409025140106678
        vf_explained_var: 0.008922502398490906
        vf_loss: 0.34075337648391724
      agent-1:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5555321574211121
        entropy_coeff: 0.0017600000137463212
        kl: 0.001093333587050438
        model: {}
        policy_loss: -0.002600259380415082
        total_loss: -0.003563419682905078
        vf_explained_var: 0.010465607047080994
        vf_loss: 0.14574037492275238
      agent-2:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48211365938186646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012461832957342267
        model: {}
        policy_loss: -0.0031330715864896774
        total_loss: -0.003922919277101755
        vf_explained_var: 0.0005799233913421631
        vf_loss: 0.5867142081260681
      agent-3:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6064207553863525
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010110439034178853
        model: {}
        policy_loss: -0.002435698639601469
        total_loss: -0.0034593408927321434
        vf_explained_var: -0.0021572113037109375
        vf_loss: 0.4365807771682739
      agent-4:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6640006303787231
        entropy_coeff: 0.0017600000137463212
        kl: 0.001405029557645321
        model: {}
        policy_loss: -0.0036714388988912106
        total_loss: -0.004789481405168772
        vf_explained_var: 0.02148088812828064
        vf_loss: 0.5059702396392822
      agent-5:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9062081575393677
        entropy_coeff: 0.0017600000137463212
        kl: 0.002228913363069296
        model: {}
        policy_loss: -0.0032337838783860207
        total_loss: -0.004805670119822025
        vf_explained_var: 0.00102996826171875
        vf_loss: 0.23040349781513214
    load_time_ms: 13437.734
    num_steps_sampled: 24000000
    num_steps_trained: 24000000
    sample_time_ms: 100409.283
    update_time_ms: 17.015
  iterations_since_restore: 90
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.31067415730337
    ram_util_percent: 12.22078651685393
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 7.0
    agent-2: 22.0
    agent-3: 24.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 3.78
    agent-1: 1.7
    agent-2: 5.09
    agent-3: 4.29
    agent-4: 5.03
    agent-5: 3.03
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.804931259235186
    mean_inference_ms: 12.774777916230612
    mean_processing_ms: 57.56935069626557
  time_since_restore: 11347.127693891525
  time_this_iter_s: 124.75397634506226
  time_total_s: 33857.41093635559
  timestamp: 1637231262
  timesteps_since_restore: 8640000
  timesteps_this_iter: 96000
  timesteps_total: 24000000
  training_iteration: 250
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    250 |          33857.4 | 24000000 |    22.92 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 2.75
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.95
    apples_agent-1_min: 0
    apples_agent-2_max: 31
    apples_agent-2_mean: 2.02
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 2.85
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.2
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.77
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 144
    cleaning_beam_agent-0_mean: 71.98
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 419
    cleaning_beam_agent-1_mean: 196.93
    cleaning_beam_agent-1_min: 65
    cleaning_beam_agent-2_max: 121
    cleaning_beam_agent-2_mean: 35.53
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 57.65
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 184
    cleaning_beam_agent-4_mean: 72.15
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 46
    cleaning_beam_agent-5_mean: 17.46
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-29-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 91.0
  episode_reward_mean: 22.76
  episode_reward_min: -73.0
  episodes_this_iter: 96
  episodes_total: 24096
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12188.307
    learner:
      agent-0:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.525793194770813
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017553131328895688
        model: {}
        policy_loss: -0.003417195752263069
        total_loss: -0.004309432581067085
        vf_explained_var: -2.911686897277832e-05
        vf_loss: 0.33160871267318726
      agent-1:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5472562313079834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009145664516836405
        model: {}
        policy_loss: -0.002428346313536167
        total_loss: -0.003373288083821535
        vf_explained_var: 0.027894407510757446
        vf_loss: 0.18227839469909668
      agent-2:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4904339909553528
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015659088967368007
        model: {}
        policy_loss: -0.003192429430782795
        total_loss: -0.003969665616750717
        vf_explained_var: 0.004426047205924988
        vf_loss: 0.8592933416366577
      agent-3:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6008793115615845
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010251359781250358
        model: {}
        policy_loss: -0.0022772764787077904
        total_loss: -0.0032854732125997543
        vf_explained_var: 0.006453290581703186
        vf_loss: 0.49347150325775146
      agent-4:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6679657697677612
        entropy_coeff: 0.0017600000137463212
        kl: 0.001298579154536128
        model: {}
        policy_loss: -0.0035728481598198414
        total_loss: -0.004698192700743675
        vf_explained_var: 0.016043230891227722
        vf_loss: 0.5027560591697693
      agent-5:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8917331695556641
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020093750208616257
        model: {}
        policy_loss: -0.003100612433627248
        total_loss: -0.004642294719815254
        vf_explained_var: -0.001963704824447632
        vf_loss: 0.2776657044887543
    load_time_ms: 13454.491
    num_steps_sampled: 24096000
    num_steps_trained: 24096000
    sample_time_ms: 100416.232
    update_time_ms: 17.002
  iterations_since_restore: 91
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.377900552486192
    ram_util_percent: 12.295580110497237
  pid: 5668
  policy_reward_max:
    agent-0: 23.0
    agent-1: 12.0
    agent-2: 32.0
    agent-3: 17.0
    agent-4: 15.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.75
    agent-1: 1.97
    agent-2: 5.37
    agent-3: 3.71
    agent-4: 4.64
    agent-5: 3.32
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -42.0
    agent-4: -45.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.803582192875837
    mean_inference_ms: 12.774545535426943
    mean_processing_ms: 57.574275366475334
  time_since_restore: 11474.046155452728
  time_this_iter_s: 126.918461561203
  time_total_s: 33984.329397916794
  timestamp: 1637231389
  timesteps_since_restore: 8736000
  timesteps_this_iter: 96000
  timesteps_total: 24096000
  training_iteration: 251
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    251 |          33984.3 | 24096000 |    22.76 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 2.68
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.91
    apples_agent-1_min: 0
    apples_agent-2_max: 31
    apples_agent-2_mean: 1.82
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 3.02
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.16
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 157
    cleaning_beam_agent-0_mean: 72.11
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 280
    cleaning_beam_agent-1_mean: 190.6
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 101
    cleaning_beam_agent-2_mean: 38.12
    cleaning_beam_agent-2_min: 8
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 55.08
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 291
    cleaning_beam_agent-4_mean: 76.08
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 40
    cleaning_beam_agent-5_mean: 16.64
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-31-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 21.34
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 24192
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12174.228
    learner:
      agent-0:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5259950160980225
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013555418699979782
        model: {}
        policy_loss: -0.003435715101659298
        total_loss: -0.0043360330164432526
        vf_explained_var: -0.0016579478979110718
        vf_loss: 0.25432342290878296
      agent-1:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5592864751815796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009190069395117462
        model: {}
        policy_loss: -0.002790117869153619
        total_loss: -0.003764163935557008
        vf_explained_var: 0.003620356321334839
        vf_loss: 0.10297280550003052
      agent-2:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.500912070274353
        entropy_coeff: 0.0017600000137463212
        kl: 0.00113381736446172
        model: {}
        policy_loss: -0.003565650898963213
        total_loss: -0.0044103385880589485
        vf_explained_var: -0.001139223575592041
        vf_loss: 0.3691677153110504
      agent-3:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5901415944099426
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012095454148948193
        model: {}
        policy_loss: -0.0025048106908798218
        total_loss: -0.003509053960442543
        vf_explained_var: -0.01085430383682251
        vf_loss: 0.34406763315200806
      agent-4:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6638548374176025
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019519224297255278
        model: {}
        policy_loss: -0.004266656003892422
        total_loss: -0.005388069432228804
        vf_explained_var: 0.017288580536842346
        vf_loss: 0.4697263538837433
      agent-5:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8802765607833862
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014420392690226436
        model: {}
        policy_loss: -0.0029881224036216736
        total_loss: -0.004510892555117607
        vf_explained_var: 0.0008780807256698608
        vf_loss: 0.2651881277561188
    load_time_ms: 13429.134
    num_steps_sampled: 24192000
    num_steps_trained: 24192000
    sample_time_ms: 100483.66
    update_time_ms: 16.954
  iterations_since_restore: 92
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.36978021978022
    ram_util_percent: 12.285164835164835
  pid: 5668
  policy_reward_max:
    agent-0: 14.0
    agent-1: 8.0
    agent-2: 14.0
    agent-3: 11.0
    agent-4: 16.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 3.48
    agent-1: 1.75
    agent-2: 4.23
    agent-3: 3.81
    agent-4: 5.17
    agent-5: 2.9
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.802075076421136
    mean_inference_ms: 12.774978591388376
    mean_processing_ms: 57.577216160587525
  time_since_restore: 11601.462899923325
  time_this_iter_s: 127.41674447059631
  time_total_s: 34111.74614238739
  timestamp: 1637231517
  timesteps_since_restore: 8832000
  timesteps_this_iter: 96000
  timesteps_total: 24192000
  training_iteration: 252
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 20.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    252 |          34111.7 | 24192000 |    21.34 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 2.56
    apples_agent-0_min: 0
    apples_agent-1_max: 39
    apples_agent-1_mean: 1.39
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 1.69
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 3.27
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 1.34
    apples_agent-4_min: 0
    apples_agent-5_max: 40
    apples_agent-5_mean: 2.34
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 148
    cleaning_beam_agent-0_mean: 67.48
    cleaning_beam_agent-0_min: 29
    cleaning_beam_agent-1_max: 311
    cleaning_beam_agent-1_mean: 191.29
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 103
    cleaning_beam_agent-2_mean: 35.23
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 151
    cleaning_beam_agent-3_mean: 60.28
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 291
    cleaning_beam_agent-4_mean: 76.07
    cleaning_beam_agent-4_min: 16
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 17.51
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-34-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 65.0
  episode_reward_mean: 21.54
  episode_reward_min: -1.0
  episodes_this_iter: 96
  episodes_total: 24288
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12172.148
    learner:
      agent-0:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5262291431427002
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013679764233529568
        model: {}
        policy_loss: -0.00343526853248477
        total_loss: -0.004329276271164417
        vf_explained_var: 0.0003790557384490967
        vf_loss: 0.3215498626232147
      agent-1:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5429063439369202
        entropy_coeff: 0.0017600000137463212
        kl: 0.000763115705922246
        model: {}
        policy_loss: -0.002931416966021061
        total_loss: -0.003871424123644829
        vf_explained_var: 0.013574853539466858
        vf_loss: 0.15508650243282318
      agent-2:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49556827545166016
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012794239446520805
        model: {}
        policy_loss: -0.003878568299114704
        total_loss: -0.004710047505795956
        vf_explained_var: -0.004101783037185669
        vf_loss: 0.40720218420028687
      agent-3:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5884533524513245
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011554730590432882
        model: {}
        policy_loss: -0.0029673269018530846
        total_loss: -0.0039672572165727615
        vf_explained_var: -0.00852307677268982
        vf_loss: 0.3574718236923218
      agent-4:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6596966981887817
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013084833044558764
        model: {}
        policy_loss: -0.003798846621066332
        total_loss: -0.0049178581684827805
        vf_explained_var: 0.000600859522819519
        vf_loss: 0.42053860425949097
      agent-5:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8860307931900024
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017680299933999777
        model: {}
        policy_loss: -0.0027346250135451555
        total_loss: -0.004266081377863884
        vf_explained_var: 0.004423975944519043
        vf_loss: 0.2795637547969818
    load_time_ms: 13446.733
    num_steps_sampled: 24288000
    num_steps_trained: 24288000
    sample_time_ms: 100287.992
    update_time_ms: 16.84
  iterations_since_restore: 93
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.721910112359552
    ram_util_percent: 12.312921348314605
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 12.0
    agent-2: 19.0
    agent-3: 12.0
    agent-4: 14.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.67
    agent-1: 2.19
    agent-2: 4.48
    agent-3: 4.02
    agent-4: 4.01
    agent-5: 3.17
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -39.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.799153695571263
    mean_inference_ms: 12.774742143977381
    mean_processing_ms: 57.58043434507752
  time_since_restore: 11726.893340826035
  time_this_iter_s: 125.43044090270996
  time_total_s: 34237.1765832901
  timestamp: 1637231642
  timesteps_since_restore: 8928000
  timesteps_this_iter: 96000
  timesteps_total: 24288000
  training_iteration: 253
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    253 |          34237.2 | 24288000 |    21.54 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.62
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.12
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 1.5
    apples_agent-2_min: 0
    apples_agent-3_max: 36
    apples_agent-3_mean: 3.19
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.02
    apples_agent-4_min: 0
    apples_agent-5_max: 21
    apples_agent-5_mean: 2.25
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 154
    cleaning_beam_agent-0_mean: 66.96
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 329
    cleaning_beam_agent-1_mean: 193.35
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 129
    cleaning_beam_agent-2_mean: 37.9
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 56.36
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 174
    cleaning_beam_agent-4_mean: 63.19
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 21.05
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-36-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 77.0
  episode_reward_mean: 22.43
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 24384
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12179.073
    learner:
      agent-0:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5225244760513306
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013085816754028201
        model: {}
        policy_loss: -0.0031650494784116745
        total_loss: -0.004041173029690981
        vf_explained_var: -0.0022308528423309326
        vf_loss: 0.43519431352615356
      agent-1:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5470154285430908
        entropy_coeff: 0.0017600000137463212
        kl: 0.000996356480754912
        model: {}
        policy_loss: -0.0027645917143672705
        total_loss: -0.00371552468277514
        vf_explained_var: 0.016834408044815063
        vf_loss: 0.1181010901927948
      agent-2:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.491554319858551
        entropy_coeff: 0.0017600000137463212
        kl: 0.001228150213137269
        model: {}
        policy_loss: -0.003486294997856021
        total_loss: -0.004312192555516958
        vf_explained_var: -0.00545087456703186
        vf_loss: 0.3923710882663727
      agent-3:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5827012062072754
        entropy_coeff: 0.0017600000137463212
        kl: 0.001258002477698028
        model: {}
        policy_loss: -0.0027719789650291204
        total_loss: -0.00375352636910975
        vf_explained_var: -0.006877273321151733
        vf_loss: 0.4400629699230194
      agent-4:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6567736864089966
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018322397954761982
        model: {}
        policy_loss: -0.003804817795753479
        total_loss: -0.004921993240714073
        vf_explained_var: 0.012868210673332214
        vf_loss: 0.38743361830711365
      agent-5:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.912230372428894
        entropy_coeff: 0.0017600000137463212
        kl: 0.002371632494032383
        model: {}
        policy_loss: -0.002695668488740921
        total_loss: -0.00425691157579422
        vf_explained_var: 0.004201576113700867
        vf_loss: 0.4427914619445801
    load_time_ms: 13465.123
    num_steps_sampled: 24384000
    num_steps_trained: 24384000
    sample_time_ms: 100443.418
    update_time_ms: 17.143
  iterations_since_restore: 94
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.048618784530387
    ram_util_percent: 12.2414364640884
  pid: 5668
  policy_reward_max:
    agent-0: 17.0
    agent-1: 7.0
    agent-2: 19.0
    agent-3: 15.0
    agent-4: 15.0
    agent-5: 21.0
  policy_reward_mean:
    agent-0: 3.92
    agent-1: 1.85
    agent-2: 4.46
    agent-3: 4.3
    agent-4: 4.56
    agent-5: 3.34
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.795344332962745
    mean_inference_ms: 12.774663822250194
    mean_processing_ms: 57.579770412584054
  time_since_restore: 11853.948795557022
  time_this_iter_s: 127.05545473098755
  time_total_s: 34364.23203802109
  timestamp: 1637231770
  timesteps_since_restore: 9024000
  timesteps_this_iter: 96000
  timesteps_total: 24384000
  training_iteration: 254
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    254 |          34364.2 | 24384000 |    22.43 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.88
    apples_agent-0_min: 0
    apples_agent-1_max: 22
    apples_agent-1_mean: 1.3
    apples_agent-1_min: 0
    apples_agent-2_max: 36
    apples_agent-2_mean: 2.34
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 2.84
    apples_agent-3_min: 0
    apples_agent-4_max: 30
    apples_agent-4_mean: 1.65
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.97
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 212
    cleaning_beam_agent-0_mean: 63.62
    cleaning_beam_agent-0_min: 14
    cleaning_beam_agent-1_max: 356
    cleaning_beam_agent-1_mean: 192.05
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 111
    cleaning_beam_agent-2_mean: 35.01
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 139
    cleaning_beam_agent-3_mean: 61.12
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 236
    cleaning_beam_agent-4_mean: 72.5
    cleaning_beam_agent-4_min: 20
    cleaning_beam_agent-5_max: 58
    cleaning_beam_agent-5_mean: 19.49
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-38-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 22.01
  episode_reward_min: -44.0
  episodes_this_iter: 96
  episodes_total: 24480
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12179.774
    learner:
      agent-0:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5009601712226868
        entropy_coeff: 0.0017600000137463212
        kl: 0.001160366926342249
        model: {}
        policy_loss: -0.00320992199704051
        total_loss: -0.00405144551768899
        vf_explained_var: 0.0030413568019866943
        vf_loss: 0.4016849100589752
      agent-1:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.53838711977005
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010352464159950614
        model: {}
        policy_loss: -0.002753762062638998
        total_loss: -0.003689658362418413
        vf_explained_var: 0.015798941254615784
        vf_loss: 0.11666591465473175
      agent-2:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4915093183517456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014959762338548899
        model: {}
        policy_loss: -0.0026769088581204414
        total_loss: -0.003355784108862281
        vf_explained_var: -0.0009106993675231934
        vf_loss: 1.8618109226226807
      agent-3:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.620567798614502
        entropy_coeff: 0.0017600000137463212
        kl: 0.001735877012833953
        model: {}
        policy_loss: -0.0024954318068921566
        total_loss: -0.003544268663972616
        vf_explained_var: -0.002631828188896179
        vf_loss: 0.43362754583358765
      agent-4:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6615630984306335
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017266895156353712
        model: {}
        policy_loss: -0.003996600396931171
        total_loss: -0.005115695297718048
        vf_explained_var: 0.03151221573352814
        vf_loss: 0.4525720477104187
      agent-5:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9077651500701904
        entropy_coeff: 0.0017600000137463212
        kl: 0.001939312438480556
        model: {}
        policy_loss: -0.0028612688183784485
        total_loss: -0.004423231817781925
        vf_explained_var: 0.0002187788486480713
        vf_loss: 0.3570292294025421
    load_time_ms: 13446.674
    num_steps_sampled: 24480000
    num_steps_trained: 24480000
    sample_time_ms: 100337.568
    update_time_ms: 16.797
  iterations_since_restore: 95
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.73575418994414
    ram_util_percent: 12.239664804469271
  pid: 5668
  policy_reward_max:
    agent-0: 19.0
    agent-1: 7.0
    agent-2: 20.0
    agent-3: 14.0
    agent-4: 14.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 4.09
    agent-1: 1.52
    agent-2: 4.48
    agent-3: 3.67
    agent-4: 4.89
    agent-5: 3.36
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: -44.0
    agent-3: -50.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.792825700342416
    mean_inference_ms: 12.774309731770433
    mean_processing_ms: 57.58167185316087
  time_since_restore: 11979.324141263962
  time_this_iter_s: 125.3753457069397
  time_total_s: 34489.60738372803
  timestamp: 1637231895
  timesteps_since_restore: 9120000
  timesteps_this_iter: 96000
  timesteps_total: 24480000
  training_iteration: 255
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    255 |          34489.6 | 24480000 |    22.01 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.7
    apples_agent-0_min: 0
    apples_agent-1_max: 17
    apples_agent-1_mean: 1.07
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 2.01
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.1
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.15
    apples_agent-4_min: 0
    apples_agent-5_max: 42
    apples_agent-5_mean: 2.13
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 153
    cleaning_beam_agent-0_mean: 67.74
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 342
    cleaning_beam_agent-1_mean: 204.13
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 99
    cleaning_beam_agent-2_mean: 35.94
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 58.32
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 197
    cleaning_beam_agent-4_mean: 70.24
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 19.44
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-40-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 66.0
  episode_reward_mean: 21.06
  episode_reward_min: -97.0
  episodes_this_iter: 96
  episodes_total: 24576
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12181.022
    learner:
      agent-0:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5127819776535034
        entropy_coeff: 0.0017600000137463212
        kl: 0.001391432131640613
        model: {}
        policy_loss: -0.0035224230960011482
        total_loss: -0.0043922001495957375
        vf_explained_var: -0.0055890679359436035
        vf_loss: 0.32720327377319336
      agent-1:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5390486121177673
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009743924019858241
        model: {}
        policy_loss: -0.0027006324380636215
        total_loss: -0.0036368854343891144
        vf_explained_var: 0.011832058429718018
        vf_loss: 0.12472762167453766
      agent-2:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4933527112007141
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012906190240755677
        model: {}
        policy_loss: -0.003802538849413395
        total_loss: -0.004629573784768581
        vf_explained_var: 0.0004541128873825073
        vf_loss: 0.4126843512058258
      agent-3:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5988287925720215
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012891631340608
        model: {}
        policy_loss: -0.002859958214685321
        total_loss: -0.0038777056615799665
        vf_explained_var: -0.004694938659667969
        vf_loss: 0.3619232773780823
      agent-4:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6497411131858826
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011693377746269107
        model: {}
        policy_loss: -0.002461184049025178
        total_loss: -0.003120001405477524
        vf_explained_var: 0.0008189082145690918
        vf_loss: 4.847241401672363
      agent-5:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8858537077903748
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011886777356266975
        model: {}
        policy_loss: -0.0024146409705281258
        total_loss: -0.003908859565854073
        vf_explained_var: 0.003958210349082947
        vf_loss: 0.6488327980041504
    load_time_ms: 13466.954
    num_steps_sampled: 24576000
    num_steps_trained: 24576000
    sample_time_ms: 100264.123
    update_time_ms: 16.414
  iterations_since_restore: 96
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.81741573033708
    ram_util_percent: 12.311797752808989
  pid: 5668
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 12.0
    agent-3: 17.0
    agent-4: 14.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.79
    agent-1: 1.88
    agent-2: 4.8
    agent-3: 4.13
    agent-4: 3.82
    agent-5: 2.64
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -100.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 22.790258963500108
    mean_inference_ms: 12.773809961730825
    mean_processing_ms: 57.58118721850295
  time_since_restore: 12104.192840099335
  time_this_iter_s: 124.86869883537292
  time_total_s: 34614.4760825634
  timestamp: 1637232020
  timesteps_since_restore: 9216000
  timesteps_this_iter: 96000
  timesteps_total: 24576000
  training_iteration: 256
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 20.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    256 |          34614.5 | 24576000 |    21.06 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 3.16
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 1.11
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 1.67
    apples_agent-2_min: 0
    apples_agent-3_max: 21
    apples_agent-3_mean: 3.27
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.12
    apples_agent-4_min: 0
    apples_agent-5_max: 24
    apples_agent-5_mean: 1.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 153
    cleaning_beam_agent-0_mean: 65.07
    cleaning_beam_agent-0_min: 25
    cleaning_beam_agent-1_max: 333
    cleaning_beam_agent-1_mean: 199.13
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 195
    cleaning_beam_agent-2_mean: 32.41
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 108
    cleaning_beam_agent-3_mean: 57.5
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 191
    cleaning_beam_agent-4_mean: 67.81
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 60
    cleaning_beam_agent-5_mean: 20.83
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-42-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 22.43
  episode_reward_min: -40.0
  episodes_this_iter: 96
  episodes_total: 24672
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12182.872
    learner:
      agent-0:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5209670662879944
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013987758429720998
        model: {}
        policy_loss: -0.003528699278831482
        total_loss: -0.004409742075949907
        vf_explained_var: 0.002084687352180481
        vf_loss: 0.35858821868896484
      agent-1:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5358380079269409
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008547439938411117
        model: {}
        policy_loss: -0.002534573432058096
        total_loss: -0.003462825668975711
        vf_explained_var: 0.02761174738407135
        vf_loss: 0.1482255458831787
      agent-2:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4768211543560028
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015204858500510454
        model: {}
        policy_loss: -0.003563472768291831
        total_loss: -0.0043573398143053055
        vf_explained_var: -0.004326224327087402
        vf_loss: 0.45336025953292847
      agent-3:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6283011436462402
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010800735326483846
        model: {}
        policy_loss: -0.0025042202323675156
        total_loss: -0.003573034889996052
        vf_explained_var: -0.0022756606340408325
        vf_loss: 0.36994054913520813
      agent-4:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6539504528045654
        entropy_coeff: 0.0017600000137463212
        kl: 0.001165241003036499
        model: {}
        policy_loss: -0.0033427695743739605
        total_loss: -0.004446325358003378
        vf_explained_var: 0.010732799768447876
        vf_loss: 0.4739540219306946
      agent-5:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8986369371414185
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018398337997496128
        model: {}
        policy_loss: -0.0030913674272596836
        total_loss: -0.0046462989412248135
        vf_explained_var: 0.0019490718841552734
        vf_loss: 0.2667187452316284
    load_time_ms: 13503.11
    num_steps_sampled: 24672000
    num_steps_trained: 24672000
    sample_time_ms: 100355.449
    update_time_ms: 16.402
  iterations_since_restore: 97
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.019444444444446
    ram_util_percent: 12.108888888888892
  pid: 5668
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 19.0
    agent-3: 15.0
    agent-4: 15.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.07
    agent-1: 2.03
    agent-2: 4.59
    agent-3: 4.09
    agent-4: 5.01
    agent-5: 2.64
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -50.0
  sampler_perf:
    mean_env_wait_ms: 22.787233406265578
    mean_inference_ms: 12.77404360472733
    mean_processing_ms: 57.57997462876865
  time_since_restore: 12230.954211235046
  time_this_iter_s: 126.76137113571167
  time_total_s: 34741.23745369911
  timestamp: 1637232147
  timesteps_since_restore: 9312000
  timesteps_this_iter: 96000
  timesteps_total: 24672000
  training_iteration: 257
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    257 |          34741.2 | 24672000 |    22.43 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.85
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.97
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 2.42
    apples_agent-2_min: 0
    apples_agent-3_max: 9
    apples_agent-3_mean: 2.87
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 1.19
    apples_agent-4_min: 0
    apples_agent-5_max: 26
    apples_agent-5_mean: 2.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 125
    cleaning_beam_agent-0_mean: 67.61
    cleaning_beam_agent-0_min: 37
    cleaning_beam_agent-1_max: 353
    cleaning_beam_agent-1_mean: 200.88
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 126
    cleaning_beam_agent-2_mean: 32.28
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 112
    cleaning_beam_agent-3_mean: 53.55
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 248
    cleaning_beam_agent-4_mean: 74.66
    cleaning_beam_agent-4_min: 19
    cleaning_beam_agent-5_max: 68
    cleaning_beam_agent-5_mean: 20.65
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-44-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 92.0
  episode_reward_mean: 23.18
  episode_reward_min: -7.0
  episodes_this_iter: 96
  episodes_total: 24768
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12179.629
    learner:
      agent-0:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5293542146682739
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011796936159953475
        model: {}
        policy_loss: -0.0033959599677473307
        total_loss: -0.004288995638489723
        vf_explained_var: -0.002304866909980774
        vf_loss: 0.38624340295791626
      agent-1:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5328425168991089
        entropy_coeff: 0.0017600000137463212
        kl: 0.001154160825535655
        model: {}
        policy_loss: -0.0029586846940219402
        total_loss: -0.0038769326638430357
        vf_explained_var: 0.026088207960128784
        vf_loss: 0.19554300606250763
      agent-2:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48776787519454956
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014919384848326445
        model: {}
        policy_loss: -0.003537970595061779
        total_loss: -0.004331722855567932
        vf_explained_var: -0.005023747682571411
        vf_loss: 0.647215723991394
      agent-3:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5994689464569092
        entropy_coeff: 0.0017600000137463212
        kl: 0.001410018652677536
        model: {}
        policy_loss: -0.0030116578564047813
        total_loss: -0.004027768969535828
        vf_explained_var: 0.0011112391948699951
        vf_loss: 0.38953468203544617
      agent-4:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6468430757522583
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018350688042119145
        model: {}
        policy_loss: -0.003904860932379961
        total_loss: -0.005004486534744501
        vf_explained_var: -0.004079505801200867
        vf_loss: 0.3881790041923523
      agent-5:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8885630369186401
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012478805147111416
        model: {}
        policy_loss: -0.00257924385368824
        total_loss: -0.004109411500394344
        vf_explained_var: -0.00522206723690033
        vf_loss: 0.33703070878982544
    load_time_ms: 13482.399
    num_steps_sampled: 24768000
    num_steps_trained: 24768000
    sample_time_ms: 100406.007
    update_time_ms: 16.206
  iterations_since_restore: 98
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.832222222222224
    ram_util_percent: 11.137777777777778
  pid: 5668
  policy_reward_max:
    agent-0: 17.0
    agent-1: 13.0
    agent-2: 26.0
    agent-3: 18.0
    agent-4: 11.0
    agent-5: 15.0
  policy_reward_mean:
    agent-0: 4.19
    agent-1: 2.03
    agent-2: 5.32
    agent-3: 4.23
    agent-4: 4.63
    agent-5: 2.78
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 22.782597379304097
    mean_inference_ms: 12.772404216074744
    mean_processing_ms: 57.57450425944823
  time_since_restore: 12356.793343305588
  time_this_iter_s: 125.83913207054138
  time_total_s: 34867.07658576965
  timestamp: 1637232273
  timesteps_since_restore: 9408000
  timesteps_this_iter: 96000
  timesteps_total: 24768000
  training_iteration: 258
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.1/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    258 |          34867.1 | 24768000 |    23.18 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 2.87
    apples_agent-0_min: 0
    apples_agent-1_max: 13
    apples_agent-1_mean: 1.02
    apples_agent-1_min: 0
    apples_agent-2_max: 35
    apples_agent-2_mean: 2.42
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 3.0
    apples_agent-3_min: 0
    apples_agent-4_max: 61
    apples_agent-4_mean: 1.56
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.75
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 172
    cleaning_beam_agent-0_mean: 69.17
    cleaning_beam_agent-0_min: 22
    cleaning_beam_agent-1_max: 299
    cleaning_beam_agent-1_mean: 199.47
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 146
    cleaning_beam_agent-2_mean: 35.03
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 124
    cleaning_beam_agent-3_mean: 54.15
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 222
    cleaning_beam_agent-4_mean: 70.5
    cleaning_beam_agent-4_min: 22
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 20.65
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-46-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 21.45
  episode_reward_min: -89.0
  episodes_this_iter: 96
  episodes_total: 24864
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12169.708
    learner:
      agent-0:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5269225835800171
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013091990258544683
        model: {}
        policy_loss: -0.0032634204253554344
        total_loss: -0.004160571843385696
        vf_explained_var: 0.00021016597747802734
        vf_loss: 0.3022984564304352
      agent-1:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.536443829536438
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008747121319174767
        model: {}
        policy_loss: -0.0017238683067262173
        total_loss: -0.002524206880480051
        vf_explained_var: 0.011059671640396118
        vf_loss: 1.4380109310150146
      agent-2:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48370039463043213
        entropy_coeff: 0.0017600000137463212
        kl: 0.001641719602048397
        model: {}
        policy_loss: -0.0034922754857689142
        total_loss: -0.004304208792746067
        vf_explained_var: -0.0035957396030426025
        vf_loss: 0.39380747079849243
      agent-3:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6016949415206909
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010622651316225529
        model: {}
        policy_loss: -0.0027980729937553406
        total_loss: -0.00381748890504241
        vf_explained_var: -0.0023544281721115112
        vf_loss: 0.39567479491233826
      agent-4:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6521071195602417
        entropy_coeff: 0.0017600000137463212
        kl: 0.001313412911258638
        model: {}
        policy_loss: -0.0031765024177730083
        total_loss: -0.004135889932513237
        vf_explained_var: 0.0064249187707901
        vf_loss: 1.883193850517273
      agent-5:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8924472332000732
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018884611781686544
        model: {}
        policy_loss: -0.0031502307392656803
        total_loss: -0.004698203876614571
        vf_explained_var: 0.0019620656967163086
        vf_loss: 0.22734849154949188
    load_time_ms: 13485.211
    num_steps_sampled: 24864000
    num_steps_trained: 24864000
    sample_time_ms: 100072.713
    update_time_ms: 16.19
  iterations_since_restore: 99
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.091477272727275
    ram_util_percent: 10.989772727272728
  pid: 5668
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 12.0
    agent-3: 13.0
    agent-4: 14.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.77
    agent-1: 1.34
    agent-2: 3.92
    agent-3: 4.37
    agent-4: 5.17
    agent-5: 2.88
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: -41.0
    agent-3: 0.0
    agent-4: -49.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.77751283802606
    mean_inference_ms: 12.770428773337729
    mean_processing_ms: 57.565897568428355
  time_since_restore: 12480.756831407547
  time_this_iter_s: 123.96348810195923
  time_total_s: 34991.04007387161
  timestamp: 1637232397
  timesteps_since_restore: 9504000
  timesteps_this_iter: 96000
  timesteps_total: 24864000
  training_iteration: 259
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 18.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    259 |            34991 | 24864000 |    21.45 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.48
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.16
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 2.99
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 2.9
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.01
    apples_agent-4_min: 0
    apples_agent-5_max: 21
    apples_agent-5_mean: 2.14
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 163
    cleaning_beam_agent-0_mean: 60.21
    cleaning_beam_agent-0_min: 6
    cleaning_beam_agent-1_max: 346
    cleaning_beam_agent-1_mean: 195.22
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 109
    cleaning_beam_agent-2_mean: 31.83
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 130
    cleaning_beam_agent-3_mean: 54.91
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 233
    cleaning_beam_agent-4_mean: 68.1
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 77
    cleaning_beam_agent-5_mean: 19.2
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-48-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 62.0
  episode_reward_mean: 20.86
  episode_reward_min: -25.0
  episodes_this_iter: 96
  episodes_total: 24960
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12164.889
    learner:
      agent-0:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5089789032936096
        entropy_coeff: 0.0017600000137463212
        kl: 0.001136204693466425
        model: {}
        policy_loss: -0.0032463406678289175
        total_loss: -0.004109013359993696
        vf_explained_var: 0.0007183104753494263
        vf_loss: 0.3312772810459137
      agent-1:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5393362641334534
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015628589317202568
        model: {}
        policy_loss: -0.0028206005226820707
        total_loss: -0.0037534902803599834
        vf_explained_var: 0.012998893857002258
        vf_loss: 0.1634175181388855
      agent-2:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48040464520454407
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013863876229152083
        model: {}
        policy_loss: -0.003448443952947855
        total_loss: -0.004258676432073116
        vf_explained_var: -0.0003439486026763916
        vf_loss: 0.35280510783195496
      agent-3:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6000045537948608
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011278403690084815
        model: {}
        policy_loss: -0.0025510573759675026
        total_loss: -0.003567781299352646
        vf_explained_var: -0.011697888374328613
        vf_loss: 0.3928266167640686
      agent-4:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6586689949035645
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018929274519905448
        model: {}
        policy_loss: -0.0037383141461759806
        total_loss: -0.0048642694018781185
        vf_explained_var: 0.01169578731060028
        vf_loss: 0.3329995572566986
      agent-5:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8745408058166504
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013638540403917432
        model: {}
        policy_loss: -0.002621279563754797
        total_loss: -0.004140931647270918
        vf_explained_var: -0.008155032992362976
        vf_loss: 0.19539354741573334
    load_time_ms: 13508.058
    num_steps_sampled: 24960000
    num_steps_trained: 24960000
    sample_time_ms: 100045.828
    update_time_ms: 16.941
  iterations_since_restore: 100
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.93258426966292
    ram_util_percent: 11.148876404494382
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 9.0
    agent-2: 11.0
    agent-3: 20.0
    agent-4: 14.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.75
    agent-1: 1.99
    agent-2: 4.31
    agent-3: 3.53
    agent-4: 4.71
    agent-5: 2.57
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -46.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.77130842756802
    mean_inference_ms: 12.768894637724298
    mean_processing_ms: 57.55682269802889
  time_since_restore: 12605.501666784286
  time_this_iter_s: 124.7448353767395
  time_total_s: 35115.78490924835
  timestamp: 1637232522
  timesteps_since_restore: 9600000
  timesteps_this_iter: 96000
  timesteps_total: 24960000
  training_iteration: 260
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    260 |          35115.8 | 24960000 |    20.86 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.29
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.03
    apples_agent-1_min: 0
    apples_agent-2_max: 21
    apples_agent-2_mean: 2.1
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 3.17
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.19
    apples_agent-4_min: 0
    apples_agent-5_max: 26
    apples_agent-5_mean: 1.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 117
    cleaning_beam_agent-0_mean: 61.11
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 318
    cleaning_beam_agent-1_mean: 185.47
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 131
    cleaning_beam_agent-2_mean: 33.76
    cleaning_beam_agent-2_min: 7
    cleaning_beam_agent-3_max: 139
    cleaning_beam_agent-3_mean: 49.98
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 188
    cleaning_beam_agent-4_mean: 66.19
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 19.4
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-50-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 20.37
  episode_reward_min: -109.0
  episodes_this_iter: 96
  episodes_total: 25056
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12157.782
    learner:
      agent-0:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5149928331375122
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016839548479765654
        model: {}
        policy_loss: -0.003221728838980198
        total_loss: -0.004103015176951885
        vf_explained_var: -0.0004653036594390869
        vf_loss: 0.25104159116744995
      agent-1:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5193583369255066
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011237177532166243
        model: {}
        policy_loss: -0.0026182555593550205
        total_loss: -0.0035210917703807354
        vf_explained_var: 0.013468056917190552
        vf_loss: 0.1123451441526413
      agent-2:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.480324923992157
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012812960194423795
        model: {}
        policy_loss: -0.00282795587554574
        total_loss: -0.0035712039098143578
        vf_explained_var: 0.0016700774431228638
        vf_loss: 1.0212547779083252
      agent-3:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5679277181625366
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008027873118408024
        model: {}
        policy_loss: -0.0020873527973890305
        total_loss: -0.002996153198182583
        vf_explained_var: -0.004055336117744446
        vf_loss: 0.9074962139129639
      agent-4:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6543912887573242
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012957529397681355
        model: {}
        policy_loss: -0.0029458990320563316
        total_loss: -0.004006135743111372
        vf_explained_var: 0.012270569801330566
        vf_loss: 0.9149454832077026
      agent-5:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8948900103569031
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013005033833906054
        model: {}
        policy_loss: -0.0024652353022247553
        total_loss: -0.0040175579488277435
        vf_explained_var: -0.008549898862838745
        vf_loss: 0.22683143615722656
    load_time_ms: 13479.206
    num_steps_sampled: 25056000
    num_steps_trained: 25056000
    sample_time_ms: 99859.733
    update_time_ms: 16.951
  iterations_since_restore: 101
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.93258426966292
    ram_util_percent: 11.067977528089887
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 8.0
    agent-2: 22.0
    agent-3: 20.0
    agent-4: 13.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.48
    agent-1: 1.79
    agent-2: 3.72
    agent-3: 3.84
    agent-4: 4.57
    agent-5: 2.97
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: -49.0
    agent-3: -42.0
    agent-4: -43.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.764236116753878
    mean_inference_ms: 12.766932100317653
    mean_processing_ms: 57.54696801877892
  time_since_restore: 12730.157831668854
  time_this_iter_s: 124.65616488456726
  time_total_s: 35240.44107413292
  timestamp: 1637232647
  timesteps_since_restore: 9696000
  timesteps_this_iter: 96000
  timesteps_total: 25056000
  training_iteration: 261
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    261 |          35240.4 | 25056000 |    20.37 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.28
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.96
    apples_agent-1_min: 0
    apples_agent-2_max: 13
    apples_agent-2_mean: 1.86
    apples_agent-2_min: 0
    apples_agent-3_max: 31
    apples_agent-3_mean: 3.16
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.11
    apples_agent-4_min: 0
    apples_agent-5_max: 12
    apples_agent-5_mean: 1.98
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 139
    cleaning_beam_agent-0_mean: 63.27
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 330
    cleaning_beam_agent-1_mean: 189.6
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 95
    cleaning_beam_agent-2_mean: 34.62
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 116
    cleaning_beam_agent-3_mean: 50.81
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 189
    cleaning_beam_agent-4_mean: 68.12
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 66
    cleaning_beam_agent-5_mean: 20.66
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-52-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 78.0
  episode_reward_mean: 22.25
  episode_reward_min: -38.0
  episodes_this_iter: 96
  episodes_total: 25152
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12151.453
    learner:
      agent-0:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5129939317703247
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010485617676749825
        model: {}
        policy_loss: -0.0019170697778463364
        total_loss: -0.0026593171060085297
        vf_explained_var: -0.0007980465888977051
        vf_loss: 1.6062132120132446
      agent-1:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5277275443077087
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011598397977650166
        model: {}
        policy_loss: -0.0022783330641686916
        total_loss: -0.003185122739523649
        vf_explained_var: 0.010766714811325073
        vf_loss: 0.22011668980121613
      agent-2:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4846971035003662
        entropy_coeff: 0.0017600000137463212
        kl: 0.001503865234553814
        model: {}
        policy_loss: -0.003618585877120495
        total_loss: -0.004420632030814886
        vf_explained_var: -0.003744661808013916
        vf_loss: 0.510242223739624
      agent-3:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5885036587715149
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012066212948411703
        model: {}
        policy_loss: -0.002765827812254429
        total_loss: -0.003759710118174553
        vf_explained_var: -0.008430063724517822
        vf_loss: 0.4188356399536133
      agent-4:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6599419713020325
        entropy_coeff: 0.0017600000137463212
        kl: 0.00126547715626657
        model: {}
        policy_loss: -0.003645103657618165
        total_loss: -0.004756194073706865
        vf_explained_var: 0.0061371177434921265
        vf_loss: 0.5040766000747681
      agent-5:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8841040730476379
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020686357747763395
        model: {}
        policy_loss: -0.003182593733072281
        total_loss: -0.004696267191320658
        vf_explained_var: -0.006211787462234497
        vf_loss: 0.42349371314048767
    load_time_ms: 13499.9
    num_steps_sampled: 25152000
    num_steps_trained: 25152000
    sample_time_ms: 99400.364
    update_time_ms: 16.651
  iterations_since_restore: 102
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.010285714285715
    ram_util_percent: 11.06
  pid: 5668
  policy_reward_max:
    agent-0: 11.0
    agent-1: 15.0
    agent-2: 16.0
    agent-3: 26.0
    agent-4: 19.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 3.21
    agent-1: 2.02
    agent-2: 4.5
    agent-3: 4.04
    agent-4: 4.86
    agent-5: 3.62
  policy_reward_min:
    agent-0: -45.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.756686064836252
    mean_inference_ms: 12.764742496472184
    mean_processing_ms: 57.53524470809488
  time_since_restore: 12853.180815935135
  time_this_iter_s: 123.02298426628113
  time_total_s: 35363.4640583992
  timestamp: 1637232771
  timesteps_since_restore: 9792000
  timesteps_this_iter: 96000
  timesteps_total: 25152000
  training_iteration: 262
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 18.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    262 |          35363.5 | 25152000 |    22.25 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.19
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.02
    apples_agent-1_min: 0
    apples_agent-2_max: 7
    apples_agent-2_mean: 1.65
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 3.18
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 1.27
    apples_agent-4_min: 0
    apples_agent-5_max: 29
    apples_agent-5_mean: 1.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 131
    cleaning_beam_agent-0_mean: 62.37
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 295
    cleaning_beam_agent-1_mean: 181.85
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 209
    cleaning_beam_agent-2_mean: 38.26
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 51.1
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 260
    cleaning_beam_agent-4_mean: 73.47
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 49
    cleaning_beam_agent-5_mean: 18.59
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-54-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 52.0
  episode_reward_mean: 21.4
  episode_reward_min: -44.0
  episodes_this_iter: 96
  episodes_total: 25248
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12165.013
    learner:
      agent-0:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5187368392944336
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015348570886999369
        model: {}
        policy_loss: -0.003575804177671671
        total_loss: -0.004451369866728783
        vf_explained_var: 0.0013519972562789917
        vf_loss: 0.37415462732315063
      agent-1:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5247756838798523
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009559374302625656
        model: {}
        policy_loss: -0.0024719047360122204
        total_loss: -0.0033763062674552202
        vf_explained_var: 0.009875565767288208
        vf_loss: 0.19205312430858612
      agent-2:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48061084747314453
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008835986955091357
        model: {}
        policy_loss: -0.0036289039999246597
        total_loss: -0.004434444010257721
        vf_explained_var: -0.003844156861305237
        vf_loss: 0.403341680765152
      agent-3:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5812453627586365
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013471972197294235
        model: {}
        policy_loss: -0.0029051657766103745
        total_loss: -0.0038906149566173553
        vf_explained_var: -0.006548792123794556
        vf_loss: 0.37539300322532654
      agent-4:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6631315350532532
        entropy_coeff: 0.0017600000137463212
        kl: 0.001785017317160964
        model: {}
        policy_loss: -0.004069645423442125
        total_loss: -0.005188642535358667
        vf_explained_var: 0.020308777689933777
        vf_loss: 0.4811425805091858
      agent-5:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.855518102645874
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013605679851025343
        model: {}
        policy_loss: -0.0028593987226486206
        total_loss: -0.004333793185651302
        vf_explained_var: 0.005993098020553589
        vf_loss: 0.31315740942955017
    load_time_ms: 13492.239
    num_steps_sampled: 25248000
    num_steps_trained: 25248000
    sample_time_ms: 99308.577
    update_time_ms: 16.793
  iterations_since_restore: 103
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.147457627118648
    ram_util_percent: 11.00677966101695
  pid: 5668
  policy_reward_max:
    agent-0: 14.0
    agent-1: 11.0
    agent-2: 16.0
    agent-3: 10.0
    agent-4: 16.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 3.19
    agent-1: 1.93
    agent-2: 4.27
    agent-3: 3.61
    agent-4: 5.12
    agent-5: 3.28
  policy_reward_min:
    agent-0: -45.0
    agent-1: 0.0
    agent-2: -1.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.75103454970239
    mean_inference_ms: 12.76379350078039
    mean_processing_ms: 57.52676047477026
  time_since_restore: 12977.757987260818
  time_this_iter_s: 124.5771713256836
  time_total_s: 35488.041229724884
  timestamp: 1637232895
  timesteps_since_restore: 9888000
  timesteps_this_iter: 96000
  timesteps_total: 25248000
  training_iteration: 263
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    263 |            35488 | 25248000 |     21.4 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 2.23
    apples_agent-0_min: 0
    apples_agent-1_max: 21
    apples_agent-1_mean: 1.12
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 1.8
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.93
    apples_agent-3_min: 0
    apples_agent-4_max: 52
    apples_agent-4_mean: 1.66
    apples_agent-4_min: 0
    apples_agent-5_max: 35
    apples_agent-5_mean: 2.09
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 132
    cleaning_beam_agent-0_mean: 62.47
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 328
    cleaning_beam_agent-1_mean: 185.31
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 117
    cleaning_beam_agent-2_mean: 31.26
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 136
    cleaning_beam_agent-3_mean: 46.49
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 190
    cleaning_beam_agent-4_mean: 68.06
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 51
    cleaning_beam_agent-5_mean: 19.12
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-57-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 75.0
  episode_reward_mean: 21.37
  episode_reward_min: -36.0
  episodes_this_iter: 96
  episodes_total: 25344
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12154.581
    learner:
      agent-0:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.506432056427002
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014417264610528946
        model: {}
        policy_loss: -0.003194976132363081
        total_loss: -0.0040497370064258575
        vf_explained_var: 0.009516850113868713
        vf_loss: 0.36561065912246704
      agent-1:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.524949312210083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010133732575923204
        model: {}
        policy_loss: -0.0027890331111848354
        total_loss: -0.003701914567500353
        vf_explained_var: 0.034362152218818665
        vf_loss: 0.1102718710899353
      agent-2:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4586740732192993
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017495418433099985
        model: {}
        policy_loss: -0.0026124659925699234
        total_loss: -0.0032526981085538864
        vf_explained_var: -0.00128994882106781
        vf_loss: 1.6703226566314697
      agent-3:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5680624842643738
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010648241732269526
        model: {}
        policy_loss: -0.0017762945499271154
        total_loss: -0.0026030021253973246
        vf_explained_var: 0.00272466242313385
        vf_loss: 1.7308744192123413
      agent-4:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.664900541305542
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011008367873728275
        model: {}
        policy_loss: -0.003679192392155528
        total_loss: -0.004802786745131016
        vf_explained_var: 0.00954534113407135
        vf_loss: 0.46634119749069214
      agent-5:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8892407417297363
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013594976626336575
        model: {}
        policy_loss: -0.0027302242815494537
        total_loss: -0.004268157295882702
        vf_explained_var: -0.0021717995405197144
        vf_loss: 0.2713008224964142
    load_time_ms: 13468.601
    num_steps_sampled: 25344000
    num_steps_trained: 25344000
    sample_time_ms: 99133.252
    update_time_ms: 16.53
  iterations_since_restore: 104
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.953932584269666
    ram_util_percent: 11.084831460674156
  pid: 5668
  policy_reward_max:
    agent-0: 20.0
    agent-1: 6.0
    agent-2: 17.0
    agent-3: 22.0
    agent-4: 13.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.65
    agent-1: 1.72
    agent-2: 4.01
    agent-3: 3.62
    agent-4: 5.12
    agent-5: 3.25
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -48.0
    agent-3: -47.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.7449784430422
    mean_inference_ms: 12.762801444627803
    mean_processing_ms: 57.520290080620754
  time_since_restore: 13102.655782699585
  time_this_iter_s: 124.89779543876648
  time_total_s: 35612.93902516365
  timestamp: 1637233020
  timesteps_since_restore: 9984000
  timesteps_this_iter: 96000
  timesteps_total: 25344000
  training_iteration: 264
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    264 |          35612.9 | 25344000 |    21.37 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.38
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.09
    apples_agent-1_min: 0
    apples_agent-2_max: 9
    apples_agent-2_mean: 1.94
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.97
    apples_agent-3_min: 0
    apples_agent-4_max: 35
    apples_agent-4_mean: 2.05
    apples_agent-4_min: 0
    apples_agent-5_max: 23
    apples_agent-5_mean: 1.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 157
    cleaning_beam_agent-0_mean: 63.25
    cleaning_beam_agent-0_min: 12
    cleaning_beam_agent-1_max: 310
    cleaning_beam_agent-1_mean: 193.38
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 92
    cleaning_beam_agent-2_mean: 35.09
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 96
    cleaning_beam_agent-3_mean: 45.47
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 190
    cleaning_beam_agent-4_mean: 72.38
    cleaning_beam_agent-4_min: 18
    cleaning_beam_agent-5_max: 51
    cleaning_beam_agent-5_mean: 19.2
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_05-59-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 59.0
  episode_reward_mean: 21.12
  episode_reward_min: -32.0
  episodes_this_iter: 96
  episodes_total: 25440
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12154.333
    learner:
      agent-0:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5130401849746704
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020336105953902006
        model: {}
        policy_loss: -0.0025774193927645683
        total_loss: -0.003307634498924017
        vf_explained_var: 0.0008677244186401367
        vf_loss: 1.7273499965667725
      agent-1:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5208674073219299
        entropy_coeff: 0.0017600000137463212
        kl: 0.001149856369011104
        model: {}
        policy_loss: -0.00278805592097342
        total_loss: -0.0036906623281538486
        vf_explained_var: 0.019188717007637024
        vf_loss: 0.14121876657009125
      agent-2:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47437286376953125
        entropy_coeff: 0.0017600000137463212
        kl: 0.001797043951228261
        model: {}
        policy_loss: -0.003520375583320856
        total_loss: -0.004314994905143976
        vf_explained_var: -0.0005980879068374634
        vf_loss: 0.4027794599533081
      agent-3:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5854053497314453
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008679645834490657
        model: {}
        policy_loss: -0.002506635384634137
        total_loss: -0.0034989132545888424
        vf_explained_var: -0.006550431251525879
        vf_loss: 0.38039469718933105
      agent-4:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.670708179473877
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014696697471663356
        model: {}
        policy_loss: -0.0039107464253902435
        total_loss: -0.005051116459071636
        vf_explained_var: 0.0017630457878112793
        vf_loss: 0.40078747272491455
      agent-5:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8802235722541809
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014674225822091103
        model: {}
        policy_loss: -0.002035678829997778
        total_loss: -0.0034316026140004396
        vf_explained_var: 0.0011933892965316772
        vf_loss: 1.5327039957046509
    load_time_ms: 13487.205
    num_steps_sampled: 25440000
    num_steps_trained: 25440000
    sample_time_ms: 98964.974
    update_time_ms: 16.343
  iterations_since_restore: 105
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.12215909090909
    ram_util_percent: 11.069886363636362
  pid: 5668
  policy_reward_max:
    agent-0: 20.0
    agent-1: 8.0
    agent-2: 15.0
    agent-3: 14.0
    agent-4: 15.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.28
    agent-1: 1.78
    agent-2: 4.79
    agent-3: 4.18
    agent-4: 4.56
    agent-5: 2.53
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 22.740213580891645
    mean_inference_ms: 12.761095739258055
    mean_processing_ms: 57.51180397377824
  time_since_restore: 13226.56568479538
  time_this_iter_s: 123.90990209579468
  time_total_s: 35736.848927259445
  timestamp: 1637233144
  timesteps_since_restore: 10080000
  timesteps_this_iter: 96000
  timesteps_total: 25440000
  training_iteration: 265
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    265 |          35736.8 | 25440000 |    21.12 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.13
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.92
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 1.73
    apples_agent-2_min: 0
    apples_agent-3_max: 26
    apples_agent-3_mean: 3.41
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.09
    apples_agent-4_min: 0
    apples_agent-5_max: 82
    apples_agent-5_mean: 2.7
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 132
    cleaning_beam_agent-0_mean: 58.21
    cleaning_beam_agent-0_min: 10
    cleaning_beam_agent-1_max: 321
    cleaning_beam_agent-1_mean: 188.0
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 180
    cleaning_beam_agent-2_mean: 31.81
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 44.11
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 192
    cleaning_beam_agent-4_mean: 73.21
    cleaning_beam_agent-4_min: 17
    cleaning_beam_agent-5_max: 62
    cleaning_beam_agent-5_mean: 20.7
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-01-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 56.0
  episode_reward_mean: 21.75
  episode_reward_min: 2.0
  episodes_this_iter: 96
  episodes_total: 25536
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12160.194
    learner:
      agent-0:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.512489914894104
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012847826583310962
        model: {}
        policy_loss: -0.0033337485510855913
        total_loss: -0.004196692258119583
        vf_explained_var: -0.0003618597984313965
        vf_loss: 0.3903685510158539
      agent-1:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5202937126159668
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010215111542493105
        model: {}
        policy_loss: -0.0028571607545018196
        total_loss: -0.003756405785679817
        vf_explained_var: 0.02239842712879181
        vf_loss: 0.16474494338035583
      agent-2:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45526400208473206
        entropy_coeff: 0.0017600000137463212
        kl: 0.00142328639049083
        model: {}
        policy_loss: -0.0038204065058380365
        total_loss: -0.004586466588079929
        vf_explained_var: -0.007442891597747803
        vf_loss: 0.3520650267601013
      agent-3:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5759148597717285
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014237964060157537
        model: {}
        policy_loss: -0.002556117717176676
        total_loss: -0.003527748631313443
        vf_explained_var: -0.006615668535232544
        vf_loss: 0.4197987914085388
      agent-4:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6667557954788208
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018962589092552662
        model: {}
        policy_loss: -0.0037529543042182922
        total_loss: -0.004882107488811016
        vf_explained_var: 0.007128357887268066
        vf_loss: 0.44339853525161743
      agent-5:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.881016731262207
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014265517238527536
        model: {}
        policy_loss: -0.0028281081467866898
        total_loss: -0.0043550776317715645
        vf_explained_var: 0.0015687942504882812
        vf_loss: 0.23619234561920166
    load_time_ms: 13493.525
    num_steps_sampled: 25536000
    num_steps_trained: 25536000
    sample_time_ms: 98861.176
    update_time_ms: 16.55
  iterations_since_restore: 106
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.462711864406778
    ram_util_percent: 11.127683615819208
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 11.0
    agent-2: 12.0
    agent-3: 13.0
    agent-4: 13.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.86
    agent-1: 1.77
    agent-2: 4.21
    agent-3: 4.28
    agent-4: 4.72
    agent-5: 2.91
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.733875217492358
    mean_inference_ms: 12.759871790620144
    mean_processing_ms: 57.503912860761154
  time_since_restore: 13350.537438869476
  time_this_iter_s: 123.97175407409668
  time_total_s: 35860.82068133354
  timestamp: 1637233269
  timesteps_since_restore: 10176000
  timesteps_this_iter: 96000
  timesteps_total: 25536000
  training_iteration: 266
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    266 |          35860.8 | 25536000 |    21.75 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 8
    apples_agent-0_mean: 2.34
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 1.05
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 1.49
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.96
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.03
    apples_agent-4_min: 0
    apples_agent-5_max: 25
    apples_agent-5_mean: 2.16
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 139
    cleaning_beam_agent-0_mean: 59.56
    cleaning_beam_agent-0_min: 14
    cleaning_beam_agent-1_max: 315
    cleaning_beam_agent-1_mean: 190.55
    cleaning_beam_agent-1_min: 78
    cleaning_beam_agent-2_max: 90
    cleaning_beam_agent-2_mean: 29.34
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 97
    cleaning_beam_agent-3_mean: 41.25
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 183
    cleaning_beam_agent-4_mean: 67.88
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 71
    cleaning_beam_agent-5_mean: 21.07
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-03-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 51.0
  episode_reward_mean: 22.17
  episode_reward_min: -80.0
  episodes_this_iter: 96
  episodes_total: 25632
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12155.483
    learner:
      agent-0:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5035704970359802
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011620334116742015
        model: {}
        policy_loss: -0.0030830465257167816
        total_loss: -0.003935512155294418
        vf_explained_var: -0.0020578354597091675
        vf_loss: 0.33815670013427734
      agent-1:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5174494385719299
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009364114957861602
        model: {}
        policy_loss: -0.002282856497913599
        total_loss: -0.0031781564466655254
        vf_explained_var: 0.019800707697868347
        vf_loss: 0.1541195958852768
      agent-2:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46103501319885254
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014073420315980911
        model: {}
        policy_loss: -0.0035257162526249886
        total_loss: -0.004292724654078484
        vf_explained_var: -0.0027783215045928955
        vf_loss: 0.44416630268096924
      agent-3:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.56401526927948
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021001778077334166
        model: {}
        policy_loss: -0.002004338428378105
        total_loss: -0.0028253798373043537
        vf_explained_var: 0.0004090219736099243
        vf_loss: 1.7162305116653442
      agent-4:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6691001653671265
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014487078879028559
        model: {}
        policy_loss: -0.002932874485850334
        total_loss: -0.004042483400553465
        vf_explained_var: 0.01639781892299652
        vf_loss: 0.6800829172134399
      agent-5:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8875247240066528
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016022745985537767
        model: {}
        policy_loss: -0.003227190114557743
        total_loss: -0.004761026240885258
        vf_explained_var: 0.00038601458072662354
        vf_loss: 0.28209352493286133
    load_time_ms: 13481.246
    num_steps_sampled: 25632000
    num_steps_trained: 25632000
    sample_time_ms: 98712.2
    update_time_ms: 16.447
  iterations_since_restore: 107
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.00337078651685
    ram_util_percent: 11.070786516853932
  pid: 5668
  policy_reward_max:
    agent-0: 11.0
    agent-1: 9.0
    agent-2: 17.0
    agent-3: 11.0
    agent-4: 24.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.03
    agent-1: 1.38
    agent-2: 4.68
    agent-3: 3.92
    agent-4: 4.84
    agent-5: 3.32
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: -42.0
    agent-4: -44.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.72804727298061
    mean_inference_ms: 12.75791816637005
    mean_processing_ms: 57.49676695739325
  time_since_restore: 13475.58688735962
  time_this_iter_s: 125.04944849014282
  time_total_s: 35985.870129823685
  timestamp: 1637233394
  timesteps_since_restore: 10272000
  timesteps_this_iter: 96000
  timesteps_total: 25632000
  training_iteration: 267
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    267 |          35985.9 | 25632000 |    22.17 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 2.55
    apples_agent-0_min: 0
    apples_agent-1_max: 22
    apples_agent-1_mean: 1.39
    apples_agent-1_min: 0
    apples_agent-2_max: 37
    apples_agent-2_mean: 2.39
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 3.46
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.1
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.86
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 123
    cleaning_beam_agent-0_mean: 61.0
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 303
    cleaning_beam_agent-1_mean: 185.62
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 132
    cleaning_beam_agent-2_mean: 34.74
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 39.38
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 198
    cleaning_beam_agent-4_mean: 62.69
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 55
    cleaning_beam_agent-5_mean: 19.27
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-05-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 71.0
  episode_reward_mean: 22.0
  episode_reward_min: -61.0
  episodes_this_iter: 96
  episodes_total: 25728
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12159.915
    learner:
      agent-0:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5148588418960571
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010703650768846273
        model: {}
        policy_loss: -0.002960770856589079
        total_loss: -0.003831479698419571
        vf_explained_var: 0.004041582345962524
        vf_loss: 0.3544272780418396
      agent-1:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.509814977645874
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010028404649347067
        model: {}
        policy_loss: -0.002804472576826811
        total_loss: -0.0036882723215967417
        vf_explained_var: 0.03178085386753082
        vf_loss: 0.13476064801216125
      agent-2:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4718037247657776
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010814788984134793
        model: {}
        policy_loss: -0.0033901561982929707
        total_loss: -0.0041707344353199005
        vf_explained_var: -0.001082092523574829
        vf_loss: 0.4979719817638397
      agent-3:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5596978664398193
        entropy_coeff: 0.0017600000137463212
        kl: 0.001067389384843409
        model: {}
        policy_loss: -0.00232783704996109
        total_loss: -0.003257577307522297
        vf_explained_var: -0.0037080049514770508
        vf_loss: 0.5532655715942383
      agent-4:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6764160394668579
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013095393078401685
        model: {}
        policy_loss: -0.0036086158361285925
        total_loss: -0.0047518848441541195
        vf_explained_var: 0.01991795003414154
        vf_loss: 0.4722389280796051
      agent-5:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8655056953430176
        entropy_coeff: 0.0017600000137463212
        kl: 0.001974759390577674
        model: {}
        policy_loss: -0.0032371501438319683
        total_loss: -0.004723432473838329
        vf_explained_var: 0.009147420525550842
        vf_loss: 0.370092511177063
    load_time_ms: 13500.37
    num_steps_sampled: 25728000
    num_steps_trained: 25728000
    sample_time_ms: 98651.112
    update_time_ms: 16.488
  iterations_since_restore: 108
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.887709497206703
    ram_util_percent: 11.07486033519553
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 10.0
    agent-2: 17.0
    agent-3: 23.0
    agent-4: 15.0
    agent-5: 17.0
  policy_reward_mean:
    agent-0: 3.92
    agent-1: 1.94
    agent-2: 4.21
    agent-3: 3.06
    agent-4: 5.41
    agent-5: 3.46
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -40.0
    agent-3: -44.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.7222537451002
    mean_inference_ms: 12.756329960256664
    mean_processing_ms: 57.49143429068366
  time_since_restore: 13601.129798173904
  time_this_iter_s: 125.54291081428528
  time_total_s: 36111.41304063797
  timestamp: 1637233519
  timesteps_since_restore: 10368000
  timesteps_this_iter: 96000
  timesteps_total: 25728000
  training_iteration: 268
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    268 |          36111.4 | 25728000 |       22 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 2.79
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.94
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 1.82
    apples_agent-2_min: 0
    apples_agent-3_max: 55
    apples_agent-3_mean: 3.14
    apples_agent-3_min: 0
    apples_agent-4_max: 35
    apples_agent-4_mean: 1.8
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 1.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 168
    cleaning_beam_agent-0_mean: 68.55
    cleaning_beam_agent-0_min: 13
    cleaning_beam_agent-1_max: 262
    cleaning_beam_agent-1_mean: 192.32
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 148
    cleaning_beam_agent-2_mean: 38.84
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 171
    cleaning_beam_agent-3_mean: 40.3
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 154
    cleaning_beam_agent-4_mean: 62.59
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 65
    cleaning_beam_agent-5_mean: 19.07
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-07-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 56.0
  episode_reward_mean: 21.31
  episode_reward_min: -38.0
  episodes_this_iter: 96
  episodes_total: 25824
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12157.614
    learner:
      agent-0:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5277541875839233
        entropy_coeff: 0.0017600000137463212
        kl: 0.001371652353554964
        model: {}
        policy_loss: -0.0034033809788525105
        total_loss: -0.004285870585590601
        vf_explained_var: 0.002286657691001892
        vf_loss: 0.4636061191558838
      agent-1:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5031291246414185
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008605577168054879
        model: {}
        policy_loss: -0.0027405740693211555
        total_loss: -0.0036125285550951958
        vf_explained_var: 0.031175121665000916
        vf_loss: 0.13550810515880585
      agent-2:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47300758957862854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014337358297780156
        model: {}
        policy_loss: -0.0037793354131281376
        total_loss: -0.00456458842381835
        vf_explained_var: -0.0029094666242599487
        vf_loss: 0.47240331768989563
      agent-3:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5702480673789978
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005345427780412138
        model: {}
        policy_loss: -0.0010298136621713638
        total_loss: -0.0017398613272234797
        vf_explained_var: 0.000850185751914978
        vf_loss: 2.9359092712402344
      agent-4:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.663414478302002
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019145847763866186
        model: {}
        policy_loss: -0.003229288151487708
        total_loss: -0.004235538654029369
        vf_explained_var: 0.0075699687004089355
        vf_loss: 1.6135904788970947
      agent-5:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8616205453872681
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013426317600533366
        model: {}
        policy_loss: -0.003084715688601136
        total_loss: -0.004575743339955807
        vf_explained_var: 0.004491448402404785
        vf_loss: 0.254252165555954
    load_time_ms: 13507.606
    num_steps_sampled: 25824000
    num_steps_trained: 25824000
    sample_time_ms: 98795.446
    update_time_ms: 16.602
  iterations_since_restore: 109
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.874301675977655
    ram_util_percent: 11.083240223463688
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 8.0
    agent-2: 19.0
    agent-3: 14.0
    agent-4: 12.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 3.52
    agent-1: 1.9
    agent-2: 5.0
    agent-3: 2.78
    agent-4: 4.82
    agent-5: 3.29
  policy_reward_min:
    agent-0: -45.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: -48.0
    agent-4: -47.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.71775193962164
    mean_inference_ms: 12.754904760429822
    mean_processing_ms: 57.486209780799534
  time_since_restore: 13726.547269105911
  time_this_iter_s: 125.41747093200684
  time_total_s: 36236.83051156998
  timestamp: 1637233645
  timesteps_since_restore: 10464000
  timesteps_this_iter: 96000
  timesteps_total: 25824000
  training_iteration: 269
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    269 |          36236.8 | 25824000 |    21.31 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.61
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.18
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 1.83
    apples_agent-2_min: 0
    apples_agent-3_max: 8
    apples_agent-3_mean: 2.41
    apples_agent-3_min: 0
    apples_agent-4_max: 49
    apples_agent-4_mean: 1.81
    apples_agent-4_min: 0
    apples_agent-5_max: 37
    apples_agent-5_mean: 2.04
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 150
    cleaning_beam_agent-0_mean: 61.42
    cleaning_beam_agent-0_min: 21
    cleaning_beam_agent-1_max: 282
    cleaning_beam_agent-1_mean: 194.28
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 104
    cleaning_beam_agent-2_mean: 34.52
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 101
    cleaning_beam_agent-3_mean: 38.53
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 151
    cleaning_beam_agent-4_mean: 59.49
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 68
    cleaning_beam_agent-5_mean: 20.07
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-09-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 22.19
  episode_reward_min: 6.0
  episodes_this_iter: 96
  episodes_total: 25920
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12158.661
    learner:
      agent-0:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.507568359375
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012991982512176037
        model: {}
        policy_loss: -0.0031549781560897827
        total_loss: -0.00401337631046772
        vf_explained_var: 0.0031538456678390503
        vf_loss: 0.3492260277271271
      agent-1:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5048171281814575
        entropy_coeff: 0.0017600000137463212
        kl: 0.000899486884009093
        model: {}
        policy_loss: -0.0028006117790937424
        total_loss: -0.0036775521002709866
        vf_explained_var: 0.033718883991241455
        vf_loss: 0.1153746023774147
      agent-2:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4718545079231262
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010532615706324577
        model: {}
        policy_loss: -0.003399401903152466
        total_loss: -0.004193561151623726
        vf_explained_var: -0.0029012560844421387
        vf_loss: 0.3630129098892212
      agent-3:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.568110466003418
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012082051252946258
        model: {}
        policy_loss: -0.0027891735080629587
        total_loss: -0.003754926146939397
        vf_explained_var: 0.0023455768823623657
        vf_loss: 0.34120887517929077
      agent-4:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6607613563537598
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015510637313127518
        model: {}
        policy_loss: -0.003775329329073429
        total_loss: -0.004894674755632877
        vf_explained_var: 0.011152595281600952
        vf_loss: 0.4359396696090698
      agent-5:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8906949758529663
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015672179870307446
        model: {}
        policy_loss: -0.003085043281316757
        total_loss: -0.004633168689906597
        vf_explained_var: -0.0014541447162628174
        vf_loss: 0.19498661160469055
    load_time_ms: 13480.023
    num_steps_sampled: 25920000
    num_steps_trained: 25920000
    sample_time_ms: 98763.356
    update_time_ms: 15.783
  iterations_since_restore: 110
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.040909090909093
    ram_util_percent: 11.070454545454545
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 7.0
    agent-2: 19.0
    agent-3: 13.0
    agent-4: 15.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.01
    agent-1: 1.73
    agent-2: 4.74
    agent-3: 3.89
    agent-4: 5.15
    agent-5: 2.67
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.713504366740505
    mean_inference_ms: 12.753373926486466
    mean_processing_ms: 57.478647429811716
  time_since_restore: 13850.671991825104
  time_this_iter_s: 124.1247227191925
  time_total_s: 36360.95523428917
  timestamp: 1637233769
  timesteps_since_restore: 10560000
  timesteps_this_iter: 96000
  timesteps_total: 25920000
  training_iteration: 270
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    270 |            36361 | 25920000 |    22.19 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.51
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.95
    apples_agent-1_min: 0
    apples_agent-2_max: 6
    apples_agent-2_mean: 1.51
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 2.99
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.5
    apples_agent-4_min: 0
    apples_agent-5_max: 27
    apples_agent-5_mean: 2.6
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 167
    cleaning_beam_agent-0_mean: 60.97
    cleaning_beam_agent-0_min: 14
    cleaning_beam_agent-1_max: 311
    cleaning_beam_agent-1_mean: 197.65
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 124
    cleaning_beam_agent-2_mean: 37.76
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 39.98
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 190
    cleaning_beam_agent-4_mean: 63.06
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 45
    cleaning_beam_agent-5_mean: 19.54
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-11-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 88.0
  episode_reward_mean: 22.49
  episode_reward_min: -88.0
  episodes_this_iter: 96
  episodes_total: 26016
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12161.324
    learner:
      agent-0:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4970821738243103
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012910650111734867
        model: {}
        policy_loss: -0.0032606993336230516
        total_loss: -0.0041048163548111916
        vf_explained_var: 0.007792189717292786
        vf_loss: 0.30745166540145874
      agent-1:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5141246318817139
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009229808347299695
        model: {}
        policy_loss: -0.002458726055920124
        total_loss: -0.003347511403262615
        vf_explained_var: 0.012364804744720459
        vf_loss: 0.1607309728860855
      agent-2:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.475321888923645
        entropy_coeff: 0.0017600000137463212
        kl: 0.001504038693383336
        model: {}
        policy_loss: -0.003185773268342018
        total_loss: -0.003972189500927925
        vf_explained_var: 0.0006804615259170532
        vf_loss: 0.5015151500701904
      agent-3:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5642291903495789
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015320084057748318
        model: {}
        policy_loss: -0.0028778898995369673
        total_loss: -0.0038332794792950153
        vf_explained_var: -0.0018161088228225708
        vf_loss: 0.3765358030796051
      agent-4:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6648968458175659
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015017811674624681
        model: {}
        policy_loss: -0.0028409152291715145
        total_loss: -0.003822368336841464
        vf_explained_var: 0.0057931095361709595
        vf_loss: 1.8876566886901855
      agent-5:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8951871395111084
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010204152204096317
        model: {}
        policy_loss: -0.0014718594029545784
        total_loss: -0.002887326292693615
        vf_explained_var: 0.003695577383041382
        vf_loss: 1.6006190776824951
    load_time_ms: 13489.056
    num_steps_sampled: 26016000
    num_steps_trained: 26016000
    sample_time_ms: 98782.393
    update_time_ms: 15.751
  iterations_since_restore: 111
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.84245810055866
    ram_util_percent: 11.08994413407821
  pid: 5668
  policy_reward_max:
    agent-0: 12.0
    agent-1: 12.0
    agent-2: 25.0
    agent-3: 12.0
    agent-4: 24.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 3.9
    agent-1: 1.66
    agent-2: 4.93
    agent-3: 4.2
    agent-4: 5.08
    agent-5: 2.72
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -48.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 22.708700681493475
    mean_inference_ms: 12.752275717556506
    mean_processing_ms: 57.472539207135576
  time_since_restore: 13975.673522472382
  time_this_iter_s: 125.00153064727783
  time_total_s: 36485.95676493645
  timestamp: 1637233895
  timesteps_since_restore: 10656000
  timesteps_this_iter: 96000
  timesteps_total: 26016000
  training_iteration: 271
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.7/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    271 |            36486 | 26016000 |    22.49 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.59
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.79
    apples_agent-1_min: 0
    apples_agent-2_max: 18
    apples_agent-2_mean: 2.16
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.99
    apples_agent-3_min: 0
    apples_agent-4_max: 40
    apples_agent-4_mean: 1.79
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 1.96
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 143
    cleaning_beam_agent-0_mean: 58.6
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 380
    cleaning_beam_agent-1_mean: 205.31
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 137
    cleaning_beam_agent-2_mean: 34.88
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 43.43
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 162
    cleaning_beam_agent-4_mean: 64.19
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 45
    cleaning_beam_agent-5_mean: 19.1
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-13-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 54.0
  episode_reward_mean: 22.54
  episode_reward_min: -23.0
  episodes_this_iter: 96
  episodes_total: 26112
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12162.5
    learner:
      agent-0:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5001863837242126
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011374615132808685
        model: {}
        policy_loss: -0.003142695873975754
        total_loss: -0.003986183553934097
        vf_explained_var: -0.0009713619947433472
        vf_loss: 0.36841675639152527
      agent-1:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5229304432868958
        entropy_coeff: 0.0017600000137463212
        kl: 0.000986838131211698
        model: {}
        policy_loss: -0.0025976691395044327
        total_loss: -0.0035017989575862885
        vf_explained_var: 0.026756957173347473
        vf_loss: 0.1622869372367859
      agent-2:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46352478861808777
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012883084127679467
        model: {}
        policy_loss: -0.0034469214733690023
        total_loss: -0.004226529970765114
        vf_explained_var: -0.006386220455169678
        vf_loss: 0.3619340658187866
      agent-3:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5788239240646362
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010305013274773955
        model: {}
        policy_loss: -0.0026498506776988506
        total_loss: -0.003634426277130842
        vf_explained_var: 0.003584563732147217
        vf_loss: 0.3415520489215851
      agent-4:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6689819097518921
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014470777241513133
        model: {}
        policy_loss: -0.004013472702354193
        total_loss: -0.005145970731973648
        vf_explained_var: -0.00062599778175354
        vf_loss: 0.4490859806537628
      agent-5:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8936737775802612
        entropy_coeff: 0.0017600000137463212
        kl: 0.001481186947785318
        model: {}
        policy_loss: -0.0030936175025999546
        total_loss: -0.0046421536244452
        vf_explained_var: -0.004286050796508789
        vf_loss: 0.24329763650894165
    load_time_ms: 13491.23
    num_steps_sampled: 26112000
    num_steps_trained: 26112000
    sample_time_ms: 99018.983
    update_time_ms: 15.963
  iterations_since_restore: 112
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.25642458100559
    ram_util_percent: 11.131843575418996
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 9.0
    agent-2: 12.0
    agent-3: 12.0
    agent-4: 18.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.29
    agent-1: 1.17
    agent-2: 4.77
    agent-3: 4.01
    agent-4: 5.07
    agent-5: 3.23
  policy_reward_min:
    agent-0: 0.0
    agent-1: -46.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.705423716017073
    mean_inference_ms: 12.75159878678663
    mean_processing_ms: 57.47094243140314
  time_since_restore: 14101.040804386139
  time_this_iter_s: 125.36728191375732
  time_total_s: 36611.324046850204
  timestamp: 1637234021
  timesteps_since_restore: 10752000
  timesteps_this_iter: 96000
  timesteps_total: 26112000
  training_iteration: 272
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 18.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    272 |          36611.3 | 26112000 |    22.54 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 2.92
    apples_agent-0_min: 0
    apples_agent-1_max: 15
    apples_agent-1_mean: 1.1
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 2.1
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 2.64
    apples_agent-3_min: 0
    apples_agent-4_max: 11
    apples_agent-4_mean: 1.26
    apples_agent-4_min: 0
    apples_agent-5_max: 30
    apples_agent-5_mean: 2.08
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 119
    cleaning_beam_agent-0_mean: 54.44
    cleaning_beam_agent-0_min: 19
    cleaning_beam_agent-1_max: 319
    cleaning_beam_agent-1_mean: 201.83
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 86
    cleaning_beam_agent-2_mean: 31.18
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 45.95
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 193
    cleaning_beam_agent-4_mean: 63.53
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 47
    cleaning_beam_agent-5_mean: 18.5
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-15-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 68.0
  episode_reward_mean: 23.34
  episode_reward_min: 6.0
  episodes_this_iter: 96
  episodes_total: 26208
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12155.485
    learner:
      agent-0:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5053695440292358
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013821364846080542
        model: {}
        policy_loss: -0.003471887204796076
        total_loss: -0.004324831534177065
        vf_explained_var: 0.004113644361495972
        vf_loss: 0.36504605412483215
      agent-1:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5038044452667236
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008323308429680765
        model: {}
        policy_loss: -0.0023994669318199158
        total_loss: -0.0032707946375012398
        vf_explained_var: 0.029379546642303467
        vf_loss: 0.15368583798408508
      agent-2:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47248294949531555
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012685711262747645
        model: {}
        policy_loss: -0.0033339769579470158
        total_loss: -0.0041165524162352085
        vf_explained_var: -0.0019152909517288208
        vf_loss: 0.4899766743183136
      agent-3:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5915989875793457
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009788783499971032
        model: {}
        policy_loss: -0.002767649944871664
        total_loss: -0.003779513295739889
        vf_explained_var: -0.0038365721702575684
        vf_loss: 0.293474942445755
      agent-4:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6668301820755005
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014226778876036406
        model: {}
        policy_loss: -0.003633707296103239
        total_loss: -0.004755182191729546
        vf_explained_var: 0.020232677459716797
        vf_loss: 0.5214505195617676
      agent-5:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9038190841674805
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015403586439788342
        model: {}
        policy_loss: -0.0029762773774564266
        total_loss: -0.0045381649397313595
        vf_explained_var: -0.002190634608268738
        vf_loss: 0.28835076093673706
    load_time_ms: 13509.307
    num_steps_sampled: 26208000
    num_steps_trained: 26208000
    sample_time_ms: 99117.128
    update_time_ms: 15.984
  iterations_since_restore: 113
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.052247191011237
    ram_util_percent: 11.075280898876402
  pid: 5668
  policy_reward_max:
    agent-0: 12.0
    agent-1: 8.0
    agent-2: 19.0
    agent-3: 10.0
    agent-4: 18.0
    agent-5: 15.0
  policy_reward_mean:
    agent-0: 4.07
    agent-1: 1.91
    agent-2: 4.69
    agent-3: 3.81
    agent-4: 5.62
    agent-5: 3.24
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.701838554416472
    mean_inference_ms: 12.750745391152993
    mean_processing_ms: 57.465814920703636
  time_since_restore: 14226.743536949158
  time_this_iter_s: 125.7027325630188
  time_total_s: 36737.02677941322
  timestamp: 1637234146
  timesteps_since_restore: 10848000
  timesteps_this_iter: 96000
  timesteps_total: 26208000
  training_iteration: 273
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    273 |            36737 | 26208000 |    23.34 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 3.56
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 0.94
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 2.84
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 3.15
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.08
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 2.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 160
    cleaning_beam_agent-0_mean: 53.42
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 343
    cleaning_beam_agent-1_mean: 207.22
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 123
    cleaning_beam_agent-2_mean: 29.96
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 127
    cleaning_beam_agent-3_mean: 47.33
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 193
    cleaning_beam_agent-4_mean: 68.79
    cleaning_beam_agent-4_min: 13
    cleaning_beam_agent-5_max: 50
    cleaning_beam_agent-5_mean: 19.39
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-17-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 108.0
  episode_reward_mean: 22.96
  episode_reward_min: -44.0
  episodes_this_iter: 96
  episodes_total: 26304
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12146.963
    learner:
      agent-0:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5006613731384277
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017658723518252373
        model: {}
        policy_loss: -0.0034120893105864525
        total_loss: -0.004251532256603241
        vf_explained_var: 0.005212739109992981
        vf_loss: 0.41729700565338135
      agent-1:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5004757046699524
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009599413606338203
        model: {}
        policy_loss: -0.0022828830406069756
        total_loss: -0.0031454218551516533
        vf_explained_var: 0.0269850492477417
        vf_loss: 0.18297994136810303
      agent-2:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45105794072151184
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010735284304246306
        model: {}
        policy_loss: -0.003022069577127695
        total_loss: -0.00374401081353426
        vf_explained_var: 0.001965269446372986
        vf_loss: 0.7192087173461914
      agent-3:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5836768746376038
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010503929806873202
        model: {}
        policy_loss: -0.0027082962915301323
        total_loss: -0.003690185025334358
        vf_explained_var: 0.014944285154342651
        vf_loss: 0.4538089334964752
      agent-4:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6652441024780273
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011374953901395202
        model: {}
        policy_loss: -0.0023347819223999977
        total_loss: -0.0033252357970923185
        vf_explained_var: 0.012547492980957031
        vf_loss: 1.803737998008728
      agent-5:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8823469281196594
        entropy_coeff: 0.0017600000137463212
        kl: 0.001442587235942483
        model: {}
        policy_loss: -0.002758791670203209
        total_loss: -0.00427373219281435
        vf_explained_var: 0.00023549795150756836
        vf_loss: 0.37990713119506836
    load_time_ms: 13499.912
    num_steps_sampled: 26304000
    num_steps_trained: 26304000
    sample_time_ms: 99236.771
    update_time_ms: 16.072
  iterations_since_restore: 114
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.81611111111111
    ram_util_percent: 11.139444444444445
  pid: 5668
  policy_reward_max:
    agent-0: 17.0
    agent-1: 11.0
    agent-2: 34.0
    agent-3: 17.0
    agent-4: 19.0
    agent-5: 18.0
  policy_reward_mean:
    agent-0: 4.25
    agent-1: 1.43
    agent-2: 4.85
    agent-3: 4.61
    agent-4: 4.7
    agent-5: 3.12
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: -49.0
    agent-3: 0.0
    agent-4: -43.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.697874695413574
    mean_inference_ms: 12.750135729339377
    mean_processing_ms: 57.46062982880544
  time_since_restore: 14352.653490781784
  time_this_iter_s: 125.90995383262634
  time_total_s: 36862.93673324585
  timestamp: 1637234272
  timesteps_since_restore: 10944000
  timesteps_this_iter: 96000
  timesteps_total: 26304000
  training_iteration: 274
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    274 |          36862.9 | 26304000 |    22.96 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.86
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.0
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 2.01
    apples_agent-2_min: 0
    apples_agent-3_max: 32
    apples_agent-3_mean: 3.65
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.41
    apples_agent-4_min: 0
    apples_agent-5_max: 43
    apples_agent-5_mean: 2.73
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 113
    cleaning_beam_agent-0_mean: 52.0
    cleaning_beam_agent-0_min: 10
    cleaning_beam_agent-1_max: 326
    cleaning_beam_agent-1_mean: 212.39
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 117
    cleaning_beam_agent-2_mean: 33.29
    cleaning_beam_agent-2_min: 8
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 48.28
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 151
    cleaning_beam_agent-4_mean: 58.92
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 71
    cleaning_beam_agent-5_mean: 23.05
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-19-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 53.0
  episode_reward_mean: 23.09
  episode_reward_min: -80.0
  episodes_this_iter: 96
  episodes_total: 26400
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12159.155
    learner:
      agent-0:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4998931884765625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010523800738155842
        model: {}
        policy_loss: -0.0027720448561012745
        total_loss: -0.0036161919124424458
        vf_explained_var: -0.001978650689125061
        vf_loss: 0.35663291811943054
      agent-1:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49327924847602844
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011996510438621044
        model: {}
        policy_loss: -0.002243142342194915
        total_loss: -0.0029653587844222784
        vf_explained_var: 0.003374263644218445
        vf_loss: 1.4595532417297363
      agent-2:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4651186764240265
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011454580817371607
        model: {}
        policy_loss: -0.0035941265523433685
        total_loss: -0.004368227906525135
        vf_explained_var: 0.0037229806184768677
        vf_loss: 0.4450410008430481
      agent-3:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5989348888397217
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013756055850535631
        model: {}
        policy_loss: -0.0031427768990397453
        total_loss: -0.004160058684647083
        vf_explained_var: -0.005355805158615112
        vf_loss: 0.3684289753437042
      agent-4:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6585828065872192
        entropy_coeff: 0.0017600000137463212
        kl: 0.001458550221286714
        model: {}
        policy_loss: -0.0038038059137761593
        total_loss: -0.004909113049507141
        vf_explained_var: 0.02683909237384796
        vf_loss: 0.5379727482795715
      agent-5:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9182490706443787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017457144567742944
        model: {}
        policy_loss: -0.0023814146406948566
        total_loss: -0.0038431533612310886
        vf_explained_var: -0.0011169612407684326
        vf_loss: 1.543781042098999
    load_time_ms: 13516.0
    num_steps_sampled: 26400000
    num_steps_trained: 26400000
    sample_time_ms: 99333.734
    update_time_ms: 16.187
  iterations_since_restore: 115
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.913483146067414
    ram_util_percent: 11.08258426966292
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 13.0
    agent-3: 14.0
    agent-4: 16.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.11
    agent-1: 0.7
    agent-2: 5.24
    agent-3: 4.33
    agent-4: 6.04
    agent-5: 2.67
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -43.0
  sampler_perf:
    mean_env_wait_ms: 22.69434931985594
    mean_inference_ms: 12.749134390037876
    mean_processing_ms: 57.45723105045175
  time_since_restore: 14477.872573137283
  time_this_iter_s: 125.21908235549927
  time_total_s: 36988.15581560135
  timestamp: 1637234398
  timesteps_since_restore: 11040000
  timesteps_this_iter: 96000
  timesteps_total: 26400000
  training_iteration: 275
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    275 |          36988.2 | 26400000 |    23.09 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.66
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.97
    apples_agent-1_min: 0
    apples_agent-2_max: 36
    apples_agent-2_mean: 2.94
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.99
    apples_agent-3_min: 0
    apples_agent-4_max: 4
    apples_agent-4_mean: 0.98
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 1.7
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 131
    cleaning_beam_agent-0_mean: 54.51
    cleaning_beam_agent-0_min: 12
    cleaning_beam_agent-1_max: 316
    cleaning_beam_agent-1_mean: 216.2
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 142
    cleaning_beam_agent-2_mean: 33.39
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 46.1
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 212
    cleaning_beam_agent-4_mean: 61.35
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 73
    cleaning_beam_agent-5_mean: 21.84
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-22-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 24.45
  episode_reward_min: -39.0
  episodes_this_iter: 96
  episodes_total: 26496
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12147.366
    learner:
      agent-0:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5046850442886353
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013643328566104174
        model: {}
        policy_loss: -0.003380295354872942
        total_loss: -0.00423161406069994
        vf_explained_var: -0.00217650830745697
        vf_loss: 0.3692440092563629
      agent-1:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4983111321926117
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010290862992405891
        model: {}
        policy_loss: -0.0026075737550854683
        total_loss: -0.003470342606306076
        vf_explained_var: 0.016006752848625183
        vf_loss: 0.14261309802532196
      agent-2:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46256059408187866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013383497716858983
        model: {}
        policy_loss: -0.003723517060279846
        total_loss: -0.0044900947250425816
        vf_explained_var: 0.0013031363487243652
        vf_loss: 0.47529473900794983
      agent-3:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5740604400634766
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015079113654792309
        model: {}
        policy_loss: -0.0027750025037676096
        total_loss: -0.00374020216986537
        vf_explained_var: -0.00427091121673584
        vf_loss: 0.4514894485473633
      agent-4:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6608966588973999
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021281810477375984
        model: {}
        policy_loss: -0.003003700403496623
        total_loss: -0.003987266682088375
        vf_explained_var: -0.0037634074687957764
        vf_loss: 1.7961143255233765
      agent-5:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8956345319747925
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012767643202096224
        model: {}
        policy_loss: -0.002634100615978241
        total_loss: -0.004185936413705349
        vf_explained_var: -0.005215942859649658
        vf_loss: 0.24480894207954407
    load_time_ms: 13521.499
    num_steps_sampled: 26496000
    num_steps_trained: 26496000
    sample_time_ms: 99437.936
    update_time_ms: 16.133
  iterations_since_restore: 116
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.11685393258427
    ram_util_percent: 11.082022471910111
  pid: 5668
  policy_reward_max:
    agent-0: 17.0
    agent-1: 7.0
    agent-2: 16.0
    agent-3: 15.0
    agent-4: 16.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.27
    agent-1: 1.89
    agent-2: 5.25
    agent-3: 4.51
    agent-4: 5.41
    agent-5: 3.12
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -46.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.691117210107464
    mean_inference_ms: 12.748494444463363
    mean_processing_ms: 57.45216244663132
  time_since_restore: 14602.779394626617
  time_this_iter_s: 124.9068214893341
  time_total_s: 37113.06263709068
  timestamp: 1637234523
  timesteps_since_restore: 11136000
  timesteps_this_iter: 96000
  timesteps_total: 26496000
  training_iteration: 276
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    276 |          37113.1 | 26496000 |    24.45 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 2.97
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.94
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 2.26
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.97
    apples_agent-3_min: 0
    apples_agent-4_max: 13
    apples_agent-4_mean: 1.12
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 142
    cleaning_beam_agent-0_mean: 57.62
    cleaning_beam_agent-0_min: 17
    cleaning_beam_agent-1_max: 359
    cleaning_beam_agent-1_mean: 210.38
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 119
    cleaning_beam_agent-2_mean: 30.39
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 113
    cleaning_beam_agent-3_mean: 47.26
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 201
    cleaning_beam_agent-4_mean: 59.92
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 19.72
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-24-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 45.0
  episode_reward_mean: 23.91
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 26592
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12147.402
    learner:
      agent-0:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5136814117431641
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012485925108194351
        model: {}
        policy_loss: -0.003036125563085079
        total_loss: -0.0038989041931927204
        vf_explained_var: 0.0015810132026672363
        vf_loss: 0.4129929840564728
      agent-1:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49428078532218933
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007326384074985981
        model: {}
        policy_loss: -0.002322126179933548
        total_loss: -0.0031792884692549706
        vf_explained_var: 0.03160421550273895
        vf_loss: 0.1277039349079132
      agent-2:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45394036173820496
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013472153805196285
        model: {}
        policy_loss: -0.0035143326967954636
        total_loss: -0.004272050224244595
        vf_explained_var: -0.007233351469039917
        vf_loss: 0.41215595602989197
      agent-3:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.580666184425354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013041800120845437
        model: {}
        policy_loss: -0.0027358313091099262
        total_loss: -0.003726211842149496
        vf_explained_var: -0.004383385181427002
        vf_loss: 0.3158781826496124
      agent-4:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6651333570480347
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013270387426018715
        model: {}
        policy_loss: -0.00375187024474144
        total_loss: -0.004879700019955635
        vf_explained_var: 0.011306241154670715
        vf_loss: 0.42804616689682007
      agent-5:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8767486214637756
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010847190860658884
        model: {}
        policy_loss: -0.0027070259675383568
        total_loss: -0.0042254747822880745
        vf_explained_var: 0.00883018970489502
        vf_loss: 0.24627211689949036
    load_time_ms: 13513.697
    num_steps_sampled: 26592000
    num_steps_trained: 26592000
    sample_time_ms: 99354.518
    update_time_ms: 16.314
  iterations_since_restore: 117
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.017514124293783
    ram_util_percent: 11.146327683615821
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 7.0
    agent-2: 13.0
    agent-3: 13.0
    agent-4: 14.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.24
    agent-1: 1.8
    agent-2: 4.99
    agent-3: 4.18
    agent-4: 5.58
    agent-5: 3.12
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.68733101070143
    mean_inference_ms: 12.74754800704522
    mean_processing_ms: 57.444889257119165
  time_since_restore: 14726.986346960068
  time_this_iter_s: 124.20695233345032
  time_total_s: 37237.26958942413
  timestamp: 1637234647
  timesteps_since_restore: 11232000
  timesteps_this_iter: 96000
  timesteps_total: 26592000
  training_iteration: 277
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    277 |          37237.3 | 26592000 |    23.91 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.9
    apples_agent-0_min: 0
    apples_agent-1_max: 21
    apples_agent-1_mean: 1.03
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 2.26
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.03
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.06
    apples_agent-4_min: 0
    apples_agent-5_max: 72
    apples_agent-5_mean: 3.0
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 141
    cleaning_beam_agent-0_mean: 55.97
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 308
    cleaning_beam_agent-1_mean: 198.68
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 78
    cleaning_beam_agent-2_mean: 24.81
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 48.33
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 154
    cleaning_beam_agent-4_mean: 56.73
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 66
    cleaning_beam_agent-5_mean: 20.57
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-26-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 78.0
  episode_reward_mean: 22.91
  episode_reward_min: -79.0
  episodes_this_iter: 96
  episodes_total: 26688
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12145.323
    learner:
      agent-0:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5133718252182007
        entropy_coeff: 0.0017600000137463212
        kl: 0.001542546204291284
        model: {}
        policy_loss: -0.0034944345243275166
        total_loss: -0.004363086074590683
        vf_explained_var: 0.010044768452644348
        vf_loss: 0.34884634613990784
      agent-1:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4975915551185608
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007611290784552693
        model: {}
        policy_loss: -0.0014725630171597004
        total_loss: -0.0021931848023086786
        vf_explained_var: 0.003533586859703064
        vf_loss: 1.5513999462127686
      agent-2:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44664904475212097
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013076890027150512
        model: {}
        policy_loss: -0.003471087198704481
        total_loss: -0.0042075044475495815
        vf_explained_var: 0.0003924816846847534
        vf_loss: 0.49684807658195496
      agent-3:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5833790898323059
        entropy_coeff: 0.0017600000137463212
        kl: 0.001069741090759635
        model: {}
        policy_loss: -0.002970488276332617
        total_loss: -0.003951544873416424
        vf_explained_var: -0.005033522844314575
        vf_loss: 0.4569188952445984
      agent-4:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6726520657539368
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012375199003145099
        model: {}
        policy_loss: -0.0036337943747639656
        total_loss: -0.004757622256875038
        vf_explained_var: 0.01845794916152954
        vf_loss: 0.6004081964492798
      agent-5:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8917168974876404
        entropy_coeff: 0.0017600000137463212
        kl: 0.00154194759670645
        model: {}
        policy_loss: -0.0014608795754611492
        total_loss: -0.0028751303907483816
        vf_explained_var: 0.0007543563842773438
        vf_loss: 1.55171537399292
    load_time_ms: 13498.757
    num_steps_sampled: 26688000
    num_steps_trained: 26688000
    sample_time_ms: 99355.041
    update_time_ms: 16.624
  iterations_since_restore: 118
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.119662921348315
    ram_util_percent: 11.071910112359548
  pid: 5668
  policy_reward_max:
    agent-0: 14.0
    agent-1: 14.0
    agent-2: 16.0
    agent-3: 14.0
    agent-4: 20.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.4
    agent-1: 1.55
    agent-2: 5.02
    agent-3: 4.28
    agent-4: 5.0
    agent-5: 2.66
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -49.0
    agent-5: -46.0
  sampler_perf:
    mean_env_wait_ms: 22.683023265429846
    mean_inference_ms: 12.746581290705373
    mean_processing_ms: 57.44034584005077
  time_since_restore: 14852.322919607162
  time_this_iter_s: 125.33657264709473
  time_total_s: 37362.60616207123
  timestamp: 1637234773
  timesteps_since_restore: 11328000
  timesteps_this_iter: 96000
  timesteps_total: 26688000
  training_iteration: 278
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    278 |          37362.6 | 26688000 |    22.91 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.43
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.92
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 1.74
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 2.89
    apples_agent-3_min: 0
    apples_agent-4_max: 20
    apples_agent-4_mean: 1.48
    apples_agent-4_min: 0
    apples_agent-5_max: 23
    apples_agent-5_mean: 2.32
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 164
    cleaning_beam_agent-0_mean: 64.48
    cleaning_beam_agent-0_min: 12
    cleaning_beam_agent-1_max: 305
    cleaning_beam_agent-1_mean: 203.65
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 94
    cleaning_beam_agent-2_mean: 29.39
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 158
    cleaning_beam_agent-3_mean: 50.58
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 136
    cleaning_beam_agent-4_mean: 52.68
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 55
    cleaning_beam_agent-5_mean: 20.3
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-28-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 67.0
  episode_reward_mean: 21.9
  episode_reward_min: -92.0
  episodes_this_iter: 96
  episodes_total: 26784
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12146.542
    learner:
      agent-0:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5084902048110962
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013719769194722176
        model: {}
        policy_loss: -0.0032814498990774155
        total_loss: -0.004142853431403637
        vf_explained_var: 0.0005486756563186646
        vf_loss: 0.3353899121284485
      agent-1:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4945914149284363
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011401859810575843
        model: {}
        policy_loss: -0.0012538940645754337
        total_loss: -0.0019824684131890535
        vf_explained_var: 0.0008751600980758667
        vf_loss: 1.4190651178359985
      agent-2:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45274728536605835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015043421881273389
        model: {}
        policy_loss: -0.003496326506137848
        total_loss: -0.004257054999470711
        vf_explained_var: -0.0008720606565475464
        vf_loss: 0.36105722188949585
      agent-3:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6112614870071411
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013451066333800554
        model: {}
        policy_loss: -0.0027643502689898014
        total_loss: -0.003800263162702322
        vf_explained_var: -0.00040271878242492676
        vf_loss: 0.3990699052810669
      agent-4:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6736899614334106
        entropy_coeff: 0.0017600000137463212
        kl: 0.001632514875382185
        model: {}
        policy_loss: -0.0033588558435440063
        total_loss: -0.004484492354094982
        vf_explained_var: 0.024047240614891052
        vf_loss: 0.6005921363830566
      agent-5:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9008172750473022
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024080637376755476
        model: {}
        policy_loss: -0.0019256873056292534
        total_loss: -0.0028262468986213207
        vf_explained_var: 0.0020597130060195923
        vf_loss: 6.848769664764404
    load_time_ms: 13507.656
    num_steps_sampled: 26784000
    num_steps_trained: 26784000
    sample_time_ms: 99296.395
    update_time_ms: 16.821
  iterations_since_restore: 119
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.947191011235958
    ram_util_percent: 11.07696629213483
  pid: 5668
  policy_reward_max:
    agent-0: 14.0
    agent-1: 9.0
    agent-2: 13.0
    agent-3: 13.0
    agent-4: 20.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.95
    agent-1: 1.13
    agent-2: 4.51
    agent-3: 4.39
    agent-4: 5.92
    agent-5: 2.0
  policy_reward_min:
    agent-0: 0.0
    agent-1: -48.0
    agent-2: 0.0
    agent-3: -1.0
    agent-4: 0.0
    agent-5: -99.0
  sampler_perf:
    mean_env_wait_ms: 22.680179041762454
    mean_inference_ms: 12.745745330962304
    mean_processing_ms: 57.43636742740312
  time_since_restore: 14977.263222694397
  time_this_iter_s: 124.9403030872345
  time_total_s: 37487.54646515846
  timestamp: 1637234898
  timesteps_since_restore: 11424000
  timesteps_this_iter: 96000
  timesteps_total: 26784000
  training_iteration: 279
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    279 |          37487.5 | 26784000 |     21.9 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.64
    apples_agent-0_min: 0
    apples_agent-1_max: 22
    apples_agent-1_mean: 1.23
    apples_agent-1_min: 0
    apples_agent-2_max: 16
    apples_agent-2_mean: 2.36
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 3.69
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.44
    apples_agent-4_min: 0
    apples_agent-5_max: 40
    apples_agent-5_mean: 2.42
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 227
    cleaning_beam_agent-0_mean: 60.12
    cleaning_beam_agent-0_min: 23
    cleaning_beam_agent-1_max: 331
    cleaning_beam_agent-1_mean: 219.0
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 74
    cleaning_beam_agent-2_mean: 28.55
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 153
    cleaning_beam_agent-3_mean: 49.31
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 302
    cleaning_beam_agent-4_mean: 60.01
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 49
    cleaning_beam_agent-5_mean: 21.12
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-30-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 99.0
  episode_reward_mean: 26.57
  episode_reward_min: 7.0
  episodes_this_iter: 96
  episodes_total: 26880
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12146.575
    learner:
      agent-0:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5027811527252197
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012258562492206693
        model: {}
        policy_loss: -0.0029457462951540947
        total_loss: -0.0037887906655669212
        vf_explained_var: 0.0010850578546524048
        vf_loss: 0.4184859097003937
      agent-1:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48551464080810547
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008627711213193834
        model: {}
        policy_loss: -0.0026407865807414055
        total_loss: -0.0034830747172236443
        vf_explained_var: 0.03275541961193085
        vf_loss: 0.12220026552677155
      agent-2:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4527469277381897
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014261140022426844
        model: {}
        policy_loss: -0.003504685591906309
        total_loss: -0.004244975280016661
        vf_explained_var: -0.0050623416900634766
        vf_loss: 0.5654598474502563
      agent-3:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5989830493927002
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009164345683529973
        model: {}
        policy_loss: -0.0025436393916606903
        total_loss: -0.0035500084049999714
        vf_explained_var: 0.003954306244850159
        vf_loss: 0.478423535823822
      agent-4:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.658235490322113
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020518433302640915
        model: {}
        policy_loss: -0.00386255607008934
        total_loss: -0.004971237853169441
        vf_explained_var: 0.01633693277835846
        vf_loss: 0.49815109372138977
      agent-5:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9137794971466064
        entropy_coeff: 0.0017600000137463212
        kl: 0.001796061173081398
        model: {}
        policy_loss: -0.0030711202416568995
        total_loss: -0.004648876376450062
        vf_explained_var: 0.0052237361669540405
        vf_loss: 0.30496060848236084
    load_time_ms: 13514.134
    num_steps_sampled: 26880000
    num_steps_trained: 26880000
    sample_time_ms: 99486.663
    update_time_ms: 17.15
  iterations_since_restore: 120
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.38
    ram_util_percent: 11.113888888888887
  pid: 5668
  policy_reward_max:
    agent-0: 17.0
    agent-1: 8.0
    agent-2: 23.0
    agent-3: 24.0
    agent-4: 17.0
    agent-5: 18.0
  policy_reward_mean:
    agent-0: 4.62
    agent-1: 1.87
    agent-2: 5.7
    agent-3: 4.99
    agent-4: 5.81
    agent-5: 3.58
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.677823641480018
    mean_inference_ms: 12.74494274024405
    mean_processing_ms: 57.43244228235364
  time_since_restore: 15103.310985565186
  time_this_iter_s: 126.04776287078857
  time_total_s: 37613.59422802925
  timestamp: 1637235024
  timesteps_since_restore: 11520000
  timesteps_this_iter: 96000
  timesteps_total: 26880000
  training_iteration: 280
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    280 |          37613.6 | 26880000 |    26.57 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 3.19
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.04
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 2.47
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 3.51
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 1.93
    apples_agent-4_min: 0
    apples_agent-5_max: 121
    apples_agent-5_mean: 3.46
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 143
    cleaning_beam_agent-0_mean: 58.39
    cleaning_beam_agent-0_min: 12
    cleaning_beam_agent-1_max: 329
    cleaning_beam_agent-1_mean: 212.37
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 110
    cleaning_beam_agent-2_mean: 32.32
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 42.49
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 244
    cleaning_beam_agent-4_mean: 56.55
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 20.89
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-32-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 70.0
  episode_reward_mean: 27.07
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 26976
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12148.115
    learner:
      agent-0:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49898990988731384
        entropy_coeff: 0.0017600000137463212
        kl: 0.001358624780550599
        model: {}
        policy_loss: -0.0034833913668990135
        total_loss: -0.004317995626479387
        vf_explained_var: 0.00048023462295532227
        vf_loss: 0.4361400008201599
      agent-1:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4970645308494568
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014244860503822565
        model: {}
        policy_loss: -0.002726963022723794
        total_loss: -0.0035893868189305067
        vf_explained_var: 0.023053258657455444
        vf_loss: 0.12411680817604065
      agent-2:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46508705615997314
        entropy_coeff: 0.0017600000137463212
        kl: 0.00147541135083884
        model: {}
        policy_loss: -0.0025187472347170115
        total_loss: -0.0031419522128999233
        vf_explained_var: 0.003786325454711914
        vf_loss: 1.9534848928451538
      agent-3:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.597072958946228
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015522002940997481
        model: {}
        policy_loss: -0.003004687372595072
        total_loss: -0.004007356241345406
        vf_explained_var: 0.0020331144332885742
        vf_loss: 0.4817984104156494
      agent-4:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6627086400985718
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014962457353249192
        model: {}
        policy_loss: -0.0035352446138858795
        total_loss: -0.004641672596335411
        vf_explained_var: 0.029845789074897766
        vf_loss: 0.5994075536727905
      agent-5:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8989779949188232
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014035184867680073
        model: {}
        policy_loss: -0.0028748521581292152
        total_loss: -0.004423713777214289
        vf_explained_var: 0.0010778307914733887
        vf_loss: 0.33340519666671753
    load_time_ms: 13539.399
    num_steps_sampled: 26976000
    num_steps_trained: 26976000
    sample_time_ms: 99468.973
    update_time_ms: 17.238
  iterations_since_restore: 121
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.864804469273743
    ram_util_percent: 11.07486033519553
  pid: 5668
  policy_reward_max:
    agent-0: 14.0
    agent-1: 8.0
    agent-2: 20.0
    agent-3: 16.0
    agent-4: 16.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 4.77
    agent-1: 1.85
    agent-2: 5.5
    agent-3: 4.97
    agent-4: 6.31
    agent-5: 3.67
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -34.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.67485394363574
    mean_inference_ms: 12.744201267654297
    mean_processing_ms: 57.42779788201471
  time_since_restore: 15228.425455093384
  time_this_iter_s: 125.11446952819824
  time_total_s: 37738.70869755745
  timestamp: 1637235150
  timesteps_since_restore: 11616000
  timesteps_this_iter: 96000
  timesteps_total: 26976000
  training_iteration: 281
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    281 |          37738.7 | 26976000 |    27.07 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 154
    apples_agent-0_mean: 4.01
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 0.9
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 2.6
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 3.35
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.29
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 1.92
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 132
    cleaning_beam_agent-0_mean: 59.33
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 207.04
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 128
    cleaning_beam_agent-2_mean: 30.2
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 129
    cleaning_beam_agent-3_mean: 46.49
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 234
    cleaning_beam_agent-4_mean: 52.0
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 58
    cleaning_beam_agent-5_mean: 23.14
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-34-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 63.0
  episode_reward_mean: 25.6
  episode_reward_min: 5.0
  episodes_this_iter: 96
  episodes_total: 27072
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12150.824
    learner:
      agent-0:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.510539710521698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013379566371440887
        model: {}
        policy_loss: -0.0033729602582752705
        total_loss: -0.004243792034685612
        vf_explained_var: 0.006027430295944214
        vf_loss: 0.2771911025047302
      agent-1:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48433127999305725
        entropy_coeff: 0.0017600000137463212
        kl: 0.001163593726232648
        model: {}
        policy_loss: -0.0026762234047055244
        total_loss: -0.00351509265601635
        vf_explained_var: 0.009827658534049988
        vf_loss: 0.1355372816324234
      agent-2:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4673646092414856
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013643810525536537
        model: {}
        policy_loss: -0.003732461016625166
        total_loss: -0.004504133015871048
        vf_explained_var: -0.004286527633666992
        vf_loss: 0.5088950991630554
      agent-3:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6017582416534424
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010906842071563005
        model: {}
        policy_loss: -0.002687961095944047
        total_loss: -0.00370568735525012
        vf_explained_var: 0.0018099844455718994
        vf_loss: 0.4136943519115448
      agent-4:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.659573495388031
        entropy_coeff: 0.0017600000137463212
        kl: 0.002195583190768957
        model: {}
        policy_loss: -0.004108686000108719
        total_loss: -0.005220872350037098
        vf_explained_var: 0.010521620512008667
        vf_loss: 0.48663318157196045
      agent-5:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9074763059616089
        entropy_coeff: 0.0017600000137463212
        kl: 0.002329411683604121
        model: {}
        policy_loss: -0.0028054057620465755
        total_loss: -0.0043713185004889965
        vf_explained_var: -0.004101961851119995
        vf_loss: 0.3124549388885498
    load_time_ms: 13531.175
    num_steps_sampled: 27072000
    num_steps_trained: 27072000
    sample_time_ms: 99409.508
    update_time_ms: 17.376
  iterations_since_restore: 122
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.023595505617976
    ram_util_percent: 11.154494382022474
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 16.0
    agent-3: 18.0
    agent-4: 17.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 3.64
    agent-1: 1.87
    agent-2: 5.59
    agent-3: 4.89
    agent-4: 5.99
    agent-5: 3.62
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.67003207813185
    mean_inference_ms: 12.743002391365524
    mean_processing_ms: 57.42076592927593
  time_since_restore: 15353.142843484879
  time_this_iter_s: 124.71738839149475
  time_total_s: 37863.426085948944
  timestamp: 1637235275
  timesteps_since_restore: 11712000
  timesteps_this_iter: 96000
  timesteps_total: 27072000
  training_iteration: 282
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    282 |          37863.4 | 27072000 |     25.6 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 3.13
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 0.79
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 1.96
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 2.78
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 1.18
    apples_agent-4_min: 0
    apples_agent-5_max: 9
    apples_agent-5_mean: 1.68
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 146
    cleaning_beam_agent-0_mean: 56.11
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 318
    cleaning_beam_agent-1_mean: 197.17
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 69
    cleaning_beam_agent-2_mean: 29.07
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 47.22
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 191
    cleaning_beam_agent-4_mean: 51.24
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 103
    cleaning_beam_agent-5_mean: 23.59
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-36-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 24.26
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 27168
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12137.627
    learner:
      agent-0:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49968650937080383
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013794191181659698
        model: {}
        policy_loss: -0.0030745454132556915
        total_loss: -0.003915501292794943
        vf_explained_var: -0.0016898661851882935
        vf_loss: 0.384949654340744
      agent-1:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4763340651988983
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008563052979297936
        model: {}
        policy_loss: -0.0026673059910535812
        total_loss: -0.0034951288253068924
        vf_explained_var: 0.018375977873802185
        vf_loss: 0.10526898503303528
      agent-2:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4553907811641693
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012521094176918268
        model: {}
        policy_loss: -0.0033352470491081476
        total_loss: -0.004093182273209095
        vf_explained_var: -0.0025548934936523438
        vf_loss: 0.43552833795547485
      agent-3:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6038501262664795
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013803611509501934
        model: {}
        policy_loss: -0.002565935254096985
        total_loss: -0.003590531414374709
        vf_explained_var: -0.012083709239959717
        vf_loss: 0.3818221092224121
      agent-4:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6546520590782166
        entropy_coeff: 0.0017600000137463212
        kl: 0.001587562495842576
        model: {}
        policy_loss: -0.0038960669189691544
        total_loss: -0.004995894618332386
        vf_explained_var: 0.011666819453239441
        vf_loss: 0.5236009359359741
      agent-5:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8901949524879456
        entropy_coeff: 0.0017600000137463212
        kl: 0.001824432285502553
        model: {}
        policy_loss: -0.0028591009322553873
        total_loss: -0.0044041583314538
        vf_explained_var: 0.004353925585746765
        vf_loss: 0.21683140099048615
    load_time_ms: 13526.061
    num_steps_sampled: 27168000
    num_steps_trained: 27168000
    sample_time_ms: 99295.502
    update_time_ms: 17.633
  iterations_since_restore: 123
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.971751412429377
    ram_util_percent: 11.154237288135594
  pid: 5668
  policy_reward_max:
    agent-0: 19.0
    agent-1: 6.0
    agent-2: 14.0
    agent-3: 18.0
    agent-4: 19.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.15
    agent-1: 1.43
    agent-2: 5.28
    agent-3: 4.27
    agent-4: 6.11
    agent-5: 3.02
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.666291320061145
    mean_inference_ms: 12.741829966487025
    mean_processing_ms: 57.41482365110009
  time_since_restore: 15477.502099514008
  time_this_iter_s: 124.35925602912903
  time_total_s: 37987.78534197807
  timestamp: 1637235399
  timesteps_since_restore: 11808000
  timesteps_this_iter: 96000
  timesteps_total: 27168000
  training_iteration: 283
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    283 |          37987.8 | 27168000 |    24.26 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 3.15
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 0.95
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 2.34
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 3.57
    apples_agent-3_min: 0
    apples_agent-4_max: 15
    apples_agent-4_mean: 2.08
    apples_agent-4_min: 0
    apples_agent-5_max: 21
    apples_agent-5_mean: 2.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 102
    cleaning_beam_agent-0_mean: 49.69
    cleaning_beam_agent-0_min: 13
    cleaning_beam_agent-1_max: 299
    cleaning_beam_agent-1_mean: 196.43
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 91
    cleaning_beam_agent-2_mean: 32.05
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 144
    cleaning_beam_agent-3_mean: 47.46
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 174
    cleaning_beam_agent-4_mean: 50.97
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 71
    cleaning_beam_agent-5_mean: 20.28
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-38-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 70.0
  episode_reward_mean: 25.43
  episode_reward_min: -34.0
  episodes_this_iter: 96
  episodes_total: 27264
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12145.17
    learner:
      agent-0:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4982284903526306
        entropy_coeff: 0.0017600000137463212
        kl: 0.001147174509242177
        model: {}
        policy_loss: -0.0027294468600302935
        total_loss: -0.003559317672625184
        vf_explained_var: 0.0024831295013427734
        vf_loss: 0.4701070487499237
      agent-1:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47346219420433044
        entropy_coeff: 0.0017600000137463212
        kl: 0.001110558514483273
        model: {}
        policy_loss: -0.002582393353804946
        total_loss: -0.0033983318135142326
        vf_explained_var: 0.03745712339878082
        vf_loss: 0.17353886365890503
      agent-2:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4617886245250702
        entropy_coeff: 0.0017600000137463212
        kl: 0.001156807178631425
        model: {}
        policy_loss: -0.003382114227861166
        total_loss: -0.0041403574869036674
        vf_explained_var: -0.0018191039562225342
        vf_loss: 0.5450522303581238
      agent-3:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5906925201416016
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009817895479500294
        model: {}
        policy_loss: -0.002857829909771681
        total_loss: -0.0038553071208298206
        vf_explained_var: -0.0017580986022949219
        vf_loss: 0.42144495248794556
      agent-4:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6499522924423218
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012346984585747123
        model: {}
        policy_loss: -0.0034902531187981367
        total_loss: -0.0045782774686813354
        vf_explained_var: 0.017189472913742065
        vf_loss: 0.5589565634727478
      agent-5:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8709151744842529
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015181689523160458
        model: {}
        policy_loss: -0.002618849743157625
        total_loss: -0.004110916052013636
        vf_explained_var: 0.004268363118171692
        vf_loss: 0.40743783116340637
    load_time_ms: 13542.368
    num_steps_sampled: 27264000
    num_steps_trained: 27264000
    sample_time_ms: 99157.114
    update_time_ms: 17.965
  iterations_since_restore: 124
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.000564971751416
    ram_util_percent: 11.077401129943501
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 13.0
    agent-2: 20.0
    agent-3: 16.0
    agent-4: 23.0
    agent-5: 16.0
  policy_reward_mean:
    agent-0: 4.03
    agent-1: 2.21
    agent-2: 5.17
    agent-3: 4.67
    agent-4: 6.06
    agent-5: 3.29
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 22.661716787592628
    mean_inference_ms: 12.740404734240995
    mean_processing_ms: 57.40861838733597
  time_since_restore: 15602.27656674385
  time_this_iter_s: 124.77446722984314
  time_total_s: 38112.559809207916
  timestamp: 1637235524
  timesteps_since_restore: 11904000
  timesteps_this_iter: 96000
  timesteps_total: 27264000
  training_iteration: 284
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 18.6/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    284 |          38112.6 | 27264000 |    25.43 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.58
    apples_agent-0_min: 0
    apples_agent-1_max: 18
    apples_agent-1_mean: 1.08
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 2.28
    apples_agent-2_min: 0
    apples_agent-3_max: 35
    apples_agent-3_mean: 3.45
    apples_agent-3_min: 0
    apples_agent-4_max: 30
    apples_agent-4_mean: 1.87
    apples_agent-4_min: 0
    apples_agent-5_max: 13
    apples_agent-5_mean: 1.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 135
    cleaning_beam_agent-0_mean: 52.9
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 292
    cleaning_beam_agent-1_mean: 193.43
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 131
    cleaning_beam_agent-2_mean: 31.71
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 41.99
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 241
    cleaning_beam_agent-4_mean: 44.81
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 69
    cleaning_beam_agent-5_mean: 21.42
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-40-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 48.0
  episode_reward_mean: 25.06
  episode_reward_min: 9.0
  episodes_this_iter: 96
  episodes_total: 27360
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12132.345
    learner:
      agent-0:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5013357400894165
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013865211512893438
        model: {}
        policy_loss: -0.003330655861645937
        total_loss: -0.004180566407740116
        vf_explained_var: 0.008281707763671875
        vf_loss: 0.3244122266769409
      agent-1:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4765944480895996
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009273230098187923
        model: {}
        policy_loss: -0.0026228507049381733
        total_loss: -0.003450461197644472
        vf_explained_var: 0.01689840853214264
        vf_loss: 0.11194755136966705
      agent-2:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46425890922546387
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010768608190119267
        model: {}
        policy_loss: -0.0035223811864852905
        total_loss: -0.00429095234721899
        vf_explained_var: 0.002825140953063965
        vf_loss: 0.48521119356155396
      agent-3:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6049792170524597
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015722938114777207
        model: {}
        policy_loss: -0.0025983424857258797
        total_loss: -0.0036225777585059404
        vf_explained_var: -0.008374899625778198
        vf_loss: 0.4052767753601074
      agent-4:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6643853783607483
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016903772484511137
        model: {}
        policy_loss: -0.003970231860876083
        total_loss: -0.005097340326756239
        vf_explained_var: 0.0068143755197525024
        vf_loss: 0.42210638523101807
      agent-5:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8897256851196289
        entropy_coeff: 0.0017600000137463212
        kl: 0.001791277900338173
        model: {}
        policy_loss: -0.0030198334716260433
        total_loss: -0.004557997919619083
        vf_explained_var: 0.001946568489074707
        vf_loss: 0.27755284309387207
    load_time_ms: 13540.8
    num_steps_sampled: 27360000
    num_steps_trained: 27360000
    sample_time_ms: 98993.758
    update_time_ms: 17.99
  iterations_since_restore: 125
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.174999999999997
    ram_util_percent: 11.1
  pid: 5668
  policy_reward_max:
    agent-0: 11.0
    agent-1: 6.0
    agent-2: 17.0
    agent-3: 17.0
    agent-4: 13.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.31
    agent-1: 1.68
    agent-2: 5.29
    agent-3: 4.65
    agent-4: 5.75
    agent-5: 3.38
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.656935636239787
    mean_inference_ms: 12.739059775028094
    mean_processing_ms: 57.401360379198834
  time_since_restore: 15725.684408664703
  time_this_iter_s: 123.40784192085266
  time_total_s: 38235.96765112877
  timestamp: 1637235648
  timesteps_since_restore: 12000000
  timesteps_this_iter: 96000
  timesteps_total: 27360000
  training_iteration: 285
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    285 |            38236 | 27360000 |    25.06 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 9
    apples_agent-0_mean: 2.89
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.92
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.46
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 2.96
    apples_agent-3_min: 0
    apples_agent-4_max: 17
    apples_agent-4_mean: 1.49
    apples_agent-4_min: 0
    apples_agent-5_max: 41
    apples_agent-5_mean: 2.88
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 100
    cleaning_beam_agent-0_mean: 51.82
    cleaning_beam_agent-0_min: 12
    cleaning_beam_agent-1_max: 306
    cleaning_beam_agent-1_mean: 192.36
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 121
    cleaning_beam_agent-2_mean: 32.67
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 96
    cleaning_beam_agent-3_mean: 46.49
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 139
    cleaning_beam_agent-4_mean: 45.35
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 58
    cleaning_beam_agent-5_mean: 21.32
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-42-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 56.0
  episode_reward_mean: 24.18
  episode_reward_min: -81.0
  episodes_this_iter: 96
  episodes_total: 27456
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12131.396
    learner:
      agent-0:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49506494402885437
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013427449157461524
        model: {}
        policy_loss: -0.0033377832733094692
        total_loss: -0.0041750152595341206
        vf_explained_var: 0.00016549229621887207
        vf_loss: 0.3408174216747284
      agent-1:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4814653694629669
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010100366780534387
        model: {}
        policy_loss: -0.0023757468443363905
        total_loss: -0.003211685223504901
        vf_explained_var: 0.012405633926391602
        vf_loss: 0.11444006860256195
      agent-2:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4627183973789215
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011153436498716474
        model: {}
        policy_loss: -0.002099980367347598
        total_loss: -0.002448846586048603
        vf_explained_var: 0.003572404384613037
        vf_loss: 4.655197620391846
      agent-3:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.61473548412323
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012408039765432477
        model: {}
        policy_loss: -0.002699823584407568
        total_loss: -0.003746295114979148
        vf_explained_var: -0.007815688848495483
        vf_loss: 0.35463839769363403
      agent-4:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6551084518432617
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015349704772233963
        model: {}
        policy_loss: -0.003656572662293911
        total_loss: -0.004743814468383789
        vf_explained_var: 0.021756857633590698
        vf_loss: 0.6574863195419312
      agent-5:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.874587893486023
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014749644324183464
        model: {}
        policy_loss: -0.003072310471907258
        total_loss: -0.004582697059959173
        vf_explained_var: -0.004569575190544128
        vf_loss: 0.2888827919960022
    load_time_ms: 13505.537
    num_steps_sampled: 27456000
    num_steps_trained: 27456000
    sample_time_ms: 99056.762
    update_time_ms: 17.983
  iterations_since_restore: 126
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.858426966292136
    ram_util_percent: 11.067977528089887
  pid: 5668
  policy_reward_max:
    agent-0: 10.0
    agent-1: 10.0
    agent-2: 15.0
    agent-3: 12.0
    agent-4: 19.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 4.45
    agent-1: 1.83
    agent-2: 4.2
    agent-3: 4.24
    agent-4: 6.35
    agent-5: 3.11
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -98.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.65209029343612
    mean_inference_ms: 12.738166667416763
    mean_processing_ms: 57.394706511617784
  time_since_restore: 15850.85819387436
  time_this_iter_s: 125.17378520965576
  time_total_s: 38361.141436338425
  timestamp: 1637235773
  timesteps_since_restore: 12096000
  timesteps_this_iter: 96000
  timesteps_total: 27456000
  training_iteration: 286
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    286 |          38361.1 | 27456000 |    24.18 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 15
    apples_agent-0_mean: 3.12
    apples_agent-0_min: 0
    apples_agent-1_max: 12
    apples_agent-1_mean: 1.26
    apples_agent-1_min: 0
    apples_agent-2_max: 25
    apples_agent-2_mean: 2.51
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 3.41
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.74
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 2.08
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 125
    cleaning_beam_agent-0_mean: 52.5
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 492
    cleaning_beam_agent-1_mean: 212.52
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 180
    cleaning_beam_agent-2_mean: 35.28
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 44.4
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 268
    cleaning_beam_agent-4_mean: 47.62
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 45
    cleaning_beam_agent-5_mean: 20.74
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-44-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 25.48
  episode_reward_min: -81.0
  episodes_this_iter: 96
  episodes_total: 27552
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12135.41
    learner:
      agent-0:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.506196141242981
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018380061956122518
        model: {}
        policy_loss: -0.0035972725600004196
        total_loss: -0.0044530415907502174
        vf_explained_var: 0.0006976276636123657
        vf_loss: 0.35136348009109497
      agent-1:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48355287313461304
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007913571898825467
        model: {}
        policy_loss: -0.0023243529722094536
        total_loss: -0.0031569357961416245
        vf_explained_var: 0.01802191138267517
        vf_loss: 0.1847129613161087
      agent-2:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46707743406295776
        entropy_coeff: 0.0017600000137463212
        kl: 0.001038032933138311
        model: {}
        policy_loss: -0.0032775052823126316
        total_loss: -0.00404711440205574
        vf_explained_var: 0.004381805658340454
        vf_loss: 0.5244765281677246
      agent-3:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5976013541221619
        entropy_coeff: 0.0017600000137463212
        kl: 0.001134388498030603
        model: {}
        policy_loss: -0.002697554649785161
        total_loss: -0.003700856352224946
        vf_explained_var: -0.0041405558586120605
        vf_loss: 0.4847581684589386
      agent-4:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.650152862071991
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019217500230297446
        model: {}
        policy_loss: -0.003417809959501028
        total_loss: -0.004379983060061932
        vf_explained_var: 0.010193750262260437
        vf_loss: 1.820949673652649
      agent-5:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8792919516563416
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015067335916683078
        model: {}
        policy_loss: -0.001837882911786437
        total_loss: -0.0032187544275075197
        vf_explained_var: -0.0008509159088134766
        vf_loss: 1.6668121814727783
    load_time_ms: 13493.685
    num_steps_sampled: 27552000
    num_steps_trained: 27552000
    sample_time_ms: 99070.985
    update_time_ms: 17.808
  iterations_since_restore: 127
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.00677966101695
    ram_util_percent: 11.083615819209038
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 11.0
    agent-2: 15.0
    agent-3: 13.0
    agent-4: 19.0
    agent-5: 15.0
  policy_reward_mean:
    agent-0: 4.61
    agent-1: 2.28
    agent-2: 4.84
    agent-3: 5.1
    agent-4: 5.74
    agent-5: 2.91
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -98.0
    agent-3: 0.0
    agent-4: -45.0
    agent-5: -49.0
  sampler_perf:
    mean_env_wait_ms: 22.64839311305532
    mean_inference_ms: 12.7374448984175
    mean_processing_ms: 57.38977417115627
  time_since_restore: 15975.073707818985
  time_this_iter_s: 124.21551394462585
  time_total_s: 38485.35695028305
  timestamp: 1637235897
  timesteps_since_restore: 12192000
  timesteps_this_iter: 96000
  timesteps_total: 27552000
  training_iteration: 287
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    287 |          38485.4 | 27552000 |    25.48 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 3.14
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.73
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 2.14
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.46
    apples_agent-3_min: 0
    apples_agent-4_max: 112
    apples_agent-4_mean: 2.8
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 1.59
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 88
    cleaning_beam_agent-0_mean: 47.82
    cleaning_beam_agent-0_min: 13
    cleaning_beam_agent-1_max: 312
    cleaning_beam_agent-1_mean: 199.1
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 152
    cleaning_beam_agent-2_mean: 33.83
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 45.67
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 206
    cleaning_beam_agent-4_mean: 48.42
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 62
    cleaning_beam_agent-5_mean: 22.35
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-47-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 25.08
  episode_reward_min: -31.0
  episodes_this_iter: 96
  episodes_total: 27648
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12144.799
    learner:
      agent-0:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49856793880462646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014029492158442736
        model: {}
        policy_loss: -0.0034492400009185076
        total_loss: -0.004299525171518326
        vf_explained_var: -0.0014227479696273804
        vf_loss: 0.2719392776489258
      agent-1:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4799208343029022
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010815294226631522
        model: {}
        policy_loss: -0.0024213611613959074
        total_loss: -0.003254134440794587
        vf_explained_var: 0.01582162082195282
        vf_loss: 0.1188560202717781
      agent-2:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46543169021606445
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010619556996971369
        model: {}
        policy_loss: -0.003335947170853615
        total_loss: -0.004098622128367424
        vf_explained_var: -0.000715985894203186
        vf_loss: 0.5648403763771057
      agent-3:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6186071634292603
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015205913223326206
        model: {}
        policy_loss: -0.002658760640770197
        total_loss: -0.003698557149618864
        vf_explained_var: -0.0014684796333312988
        vf_loss: 0.48951172828674316
      agent-4:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6523919105529785
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014997521648183465
        model: {}
        policy_loss: -0.0035451697185635567
        total_loss: -0.00463863555341959
        vf_explained_var: 0.03257189691066742
        vf_loss: 0.5474398136138916
      agent-5:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8854979276657104
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012302730465307832
        model: {}
        policy_loss: -0.002609637565910816
        total_loss: -0.0041430918499827385
        vf_explained_var: 0.0025956183671951294
        vf_loss: 0.2502182126045227
    load_time_ms: 13522.223
    num_steps_sampled: 27648000
    num_steps_trained: 27648000
    sample_time_ms: 98981.884
    update_time_ms: 17.865
  iterations_since_restore: 128
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.974719101123597
    ram_util_percent: 11.158426966292136
  pid: 5668
  policy_reward_max:
    agent-0: 11.0
    agent-1: 7.0
    agent-2: 18.0
    agent-3: 18.0
    agent-4: 16.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.14
    agent-1: 1.63
    agent-2: 4.93
    agent-3: 4.95
    agent-4: 6.17
    agent-5: 3.26
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -47.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.643973030683842
    mean_inference_ms: 12.736223695301042
    mean_processing_ms: 57.383654642945864
  time_since_restore: 16099.910233974457
  time_this_iter_s: 124.8365261554718
  time_total_s: 38610.19347643852
  timestamp: 1637236022
  timesteps_since_restore: 12288000
  timesteps_this_iter: 96000
  timesteps_total: 27648000
  training_iteration: 288
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    288 |          38610.2 | 27648000 |    25.08 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.4
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.94
    apples_agent-1_min: 0
    apples_agent-2_max: 28
    apples_agent-2_mean: 1.77
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 2.89
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.29
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 1.91
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 121
    cleaning_beam_agent-0_mean: 49.64
    cleaning_beam_agent-0_min: 10
    cleaning_beam_agent-1_max: 334
    cleaning_beam_agent-1_mean: 195.78
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 109
    cleaning_beam_agent-2_mean: 31.27
    cleaning_beam_agent-2_min: 8
    cleaning_beam_agent-3_max: 123
    cleaning_beam_agent-3_mean: 43.4
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 345
    cleaning_beam_agent-4_mean: 51.03
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 54
    cleaning_beam_agent-5_mean: 22.82
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-49-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 78.0
  episode_reward_mean: 24.78
  episode_reward_min: 6.0
  episodes_this_iter: 96
  episodes_total: 27744
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12144.611
    learner:
      agent-0:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4946320652961731
        entropy_coeff: 0.0017600000137463212
        kl: 0.001620129100047052
        model: {}
        policy_loss: -0.0035634199157357216
        total_loss: -0.004403118975460529
        vf_explained_var: -2.0384788513183594e-05
        vf_loss: 0.3085585832595825
      agent-1:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4828556478023529
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008606729679740965
        model: {}
        policy_loss: -0.0023892195895314217
        total_loss: -0.0032273977994918823
        vf_explained_var: 0.021109387278556824
        vf_loss: 0.11650623381137848
      agent-2:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45409876108169556
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011813491582870483
        model: {}
        policy_loss: -0.0037622381933033466
        total_loss: -0.004514290019869804
        vf_explained_var: -0.0006173700094223022
        vf_loss: 0.47163355350494385
      agent-3:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6082669496536255
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011942972196266055
        model: {}
        policy_loss: -0.0025949846021831036
        total_loss: -0.00362451933324337
        vf_explained_var: -0.008597016334533691
        vf_loss: 0.41017934679985046
      agent-4:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6590131521224976
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021843353752046824
        model: {}
        policy_loss: -0.003965472336858511
        total_loss: -0.005071504507213831
        vf_explained_var: 0.020853236317634583
        vf_loss: 0.5382660031318665
      agent-5:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8885892629623413
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014757760800421238
        model: {}
        policy_loss: -0.0027781319804489613
        total_loss: -0.004307177383452654
        vf_explained_var: 0.00117453932762146
        vf_loss: 0.3486945629119873
    load_time_ms: 13524.91
    num_steps_sampled: 27744000
    num_steps_trained: 27744000
    sample_time_ms: 98936.337
    update_time_ms: 17.889
  iterations_since_restore: 129
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.081355932203387
    ram_util_percent: 11.005649717514125
  pid: 5668
  policy_reward_max:
    agent-0: 18.0
    agent-1: 7.0
    agent-2: 23.0
    agent-3: 16.0
    agent-4: 16.0
    agent-5: 18.0
  policy_reward_mean:
    agent-0: 4.01
    agent-1: 1.72
    agent-2: 4.94
    agent-3: 4.47
    agent-4: 6.11
    agent-5: 3.53
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 1.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.640440062296243
    mean_inference_ms: 12.735378894750298
    mean_processing_ms: 57.37915946534418
  time_since_restore: 16224.411988019943
  time_this_iter_s: 124.50175404548645
  time_total_s: 38734.69523048401
  timestamp: 1637236147
  timesteps_since_restore: 12384000
  timesteps_this_iter: 96000
  timesteps_total: 27744000
  training_iteration: 289
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    289 |          38734.7 | 27744000 |    24.78 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 2.67
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 1.19
    apples_agent-1_min: 0
    apples_agent-2_max: 31
    apples_agent-2_mean: 2.8
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 2.97
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.4
    apples_agent-4_min: 0
    apples_agent-5_max: 63
    apples_agent-5_mean: 2.95
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 114
    cleaning_beam_agent-0_mean: 52.39
    cleaning_beam_agent-0_min: 6
    cleaning_beam_agent-1_max: 344
    cleaning_beam_agent-1_mean: 204.47
    cleaning_beam_agent-1_min: 77
    cleaning_beam_agent-2_max: 104
    cleaning_beam_agent-2_mean: 28.97
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 129
    cleaning_beam_agent-3_mean: 44.94
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 137
    cleaning_beam_agent-4_mean: 45.57
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 55
    cleaning_beam_agent-5_mean: 20.94
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 4
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-51-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 24.19
  episode_reward_min: -169.0
  episodes_this_iter: 96
  episodes_total: 27840
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12149.814
    learner:
      agent-0:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49963244795799255
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014909617602825165
        model: {}
        policy_loss: -0.003178499871864915
        total_loss: -0.004022602457553148
        vf_explained_var: 0.003447994589805603
        vf_loss: 0.3525054454803467
      agent-1:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47220784425735474
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008646408678032458
        model: {}
        policy_loss: -0.002383323386311531
        total_loss: -0.0031965472735464573
        vf_explained_var: 0.020011335611343384
        vf_loss: 0.17861995100975037
      agent-2:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45054131746292114
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011300027836114168
        model: {}
        policy_loss: -0.003660158021375537
        total_loss: -0.004396642558276653
        vf_explained_var: -0.0009746849536895752
        vf_loss: 0.564656138420105
      agent-3:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6139167547225952
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010536513291299343
        model: {}
        policy_loss: -0.0026121381670236588
        total_loss: -0.003656616434454918
        vf_explained_var: -0.006057798862457275
        vf_loss: 0.36016717553138733
      agent-4:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6469289064407349
        entropy_coeff: 0.0017600000137463212
        kl: 0.001800278201699257
        model: {}
        policy_loss: -0.0017246603965759277
        total_loss: -0.0007426640950143337
        vf_explained_var: 0.004555344581604004
        vf_loss: 21.20591163635254
      agent-5:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8724070191383362
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014435654738917947
        model: {}
        policy_loss: -0.00293459789827466
        total_loss: -0.004438949748873711
        vf_explained_var: 0.005775868892669678
        vf_loss: 0.3108617961406708
    load_time_ms: 13526.007
    num_steps_sampled: 27840000
    num_steps_trained: 27840000
    sample_time_ms: 98731.609
    update_time_ms: 17.345
  iterations_since_restore: 130
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.94632768361582
    ram_util_percent: 11.146892655367234
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 10.0
    agent-2: 18.0
    agent-3: 13.0
    agent-4: 15.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.11
    agent-1: 2.21
    agent-2: 5.46
    agent-3: 4.31
    agent-4: 4.5
    agent-5: 3.6
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -196.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.636065635425066
    mean_inference_ms: 12.733910163244342
    mean_processing_ms: 57.37381087886803
  time_since_restore: 16348.470131158829
  time_this_iter_s: 124.0581431388855
  time_total_s: 38858.753373622894
  timestamp: 1637236271
  timesteps_since_restore: 12480000
  timesteps_this_iter: 96000
  timesteps_total: 27840000
  training_iteration: 290
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    290 |          38858.8 | 27840000 |    24.19 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.51
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 0.74
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 2.36
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 3.07
    apples_agent-3_min: 0
    apples_agent-4_max: 67
    apples_agent-4_mean: 2.24
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 2.11
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 109
    cleaning_beam_agent-0_mean: 54.85
    cleaning_beam_agent-0_min: 18
    cleaning_beam_agent-1_max: 349
    cleaning_beam_agent-1_mean: 192.63
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 75
    cleaning_beam_agent-2_mean: 30.54
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 102
    cleaning_beam_agent-3_mean: 43.23
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 184
    cleaning_beam_agent-4_mean: 46.74
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 57
    cleaning_beam_agent-5_mean: 21.64
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-53-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 55.0
  episode_reward_mean: 25.85
  episode_reward_min: -25.0
  episodes_this_iter: 96
  episodes_total: 27936
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12149.782
    learner:
      agent-0:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5075486302375793
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009549173410050571
        model: {}
        policy_loss: -0.0025092805735766888
        total_loss: -0.0033304733224213123
        vf_explained_var: 0.00014741718769073486
        vf_loss: 0.7209177613258362
      agent-1:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4792245626449585
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010202232515439391
        model: {}
        policy_loss: -0.0024551148526370525
        total_loss: -0.0032864748500287533
        vf_explained_var: 0.02215038239955902
        vf_loss: 0.12068384885787964
      agent-2:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45250099897384644
        entropy_coeff: 0.0017600000137463212
        kl: 0.001459284219890833
        model: {}
        policy_loss: -0.003523886203765869
        total_loss: -0.004272080957889557
        vf_explained_var: 0.002243712544441223
        vf_loss: 0.4820370376110077
      agent-3:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6142706274986267
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010116748744621873
        model: {}
        policy_loss: -0.002824709750711918
        total_loss: -0.0038671069778501987
        vf_explained_var: 0.0012293308973312378
        vf_loss: 0.38720059394836426
      agent-4:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6565923690795898
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014743348583579063
        model: {}
        policy_loss: -0.00383967999368906
        total_loss: -0.0049311574548482895
        vf_explained_var: 0.013111382722854614
        vf_loss: 0.6412435173988342
      agent-5:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8716588020324707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018687298288568854
        model: {}
        policy_loss: -0.0029506634455174208
        total_loss: -0.004453897476196289
        vf_explained_var: -0.0013216286897659302
        vf_loss: 0.3088354468345642
    load_time_ms: 13522.24
    num_steps_sampled: 27936000
    num_steps_trained: 27936000
    sample_time_ms: 98609.494
    update_time_ms: 17.11
  iterations_since_restore: 131
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.038983050847456
    ram_util_percent: 11.010734463276837
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 14.0
    agent-3: 15.0
    agent-4: 16.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 3.81
    agent-1: 1.78
    agent-2: 5.7
    agent-3: 4.43
    agent-4: 6.52
    agent-5: 3.61
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.631912115146044
    mean_inference_ms: 12.733050108145074
    mean_processing_ms: 57.368513582221155
  time_since_restore: 16472.298833608627
  time_this_iter_s: 123.82870244979858
  time_total_s: 38982.58207607269
  timestamp: 1637236396
  timesteps_since_restore: 12576000
  timesteps_this_iter: 96000
  timesteps_total: 27936000
  training_iteration: 291
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    291 |          38982.6 | 27936000 |    25.85 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 3.5
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 1.05
    apples_agent-1_min: 0
    apples_agent-2_max: 17
    apples_agent-2_mean: 2.0
    apples_agent-2_min: 0
    apples_agent-3_max: 12
    apples_agent-3_mean: 3.37
    apples_agent-3_min: 0
    apples_agent-4_max: 43
    apples_agent-4_mean: 2.08
    apples_agent-4_min: 0
    apples_agent-5_max: 57
    apples_agent-5_mean: 3.34
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 128
    cleaning_beam_agent-0_mean: 55.63
    cleaning_beam_agent-0_min: 2
    cleaning_beam_agent-1_max: 468
    cleaning_beam_agent-1_mean: 209.97
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 104
    cleaning_beam_agent-2_mean: 34.29
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 125
    cleaning_beam_agent-3_mean: 48.5
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 150
    cleaning_beam_agent-4_mean: 49.15
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 55
    cleaning_beam_agent-5_mean: 20.84
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-55-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 90.0
  episode_reward_mean: 28.88
  episode_reward_min: 4.0
  episodes_this_iter: 96
  episodes_total: 28032
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12144.143
    learner:
      agent-0:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5097331404685974
        entropy_coeff: 0.0017600000137463212
        kl: 0.001384883071295917
        model: {}
        policy_loss: -0.0033792839385569096
        total_loss: -0.004234486725181341
        vf_explained_var: 0.004396259784698486
        vf_loss: 0.41928988695144653
      agent-1:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4743339419364929
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012819962576031685
        model: {}
        policy_loss: -0.002547023119404912
        total_loss: -0.0033668335527181625
        vf_explained_var: 0.022003144025802612
        vf_loss: 0.15015672147274017
      agent-2:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45423373579978943
        entropy_coeff: 0.0017600000137463212
        kl: 0.00155690498650074
        model: {}
        policy_loss: -0.0036945417523384094
        total_loss: -0.00443471921607852
        vf_explained_var: 0.003488585352897644
        vf_loss: 0.5927256345748901
      agent-3:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6196761131286621
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013981526717543602
        model: {}
        policy_loss: -0.0027187915984541178
        total_loss: -0.003758760867640376
        vf_explained_var: -0.001521378755569458
        vf_loss: 0.5066028237342834
      agent-4:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6417218446731567
        entropy_coeff: 0.0017600000137463212
        kl: 0.001519036595709622
        model: {}
        policy_loss: -0.0036733578890562057
        total_loss: -0.004726878367364407
        vf_explained_var: 0.014319494366645813
        vf_loss: 0.7590682506561279
      agent-5:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8797147870063782
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013888864777982235
        model: {}
        policy_loss: -0.0027366920839995146
        total_loss: -0.004242308903485537
        vf_explained_var: -0.000420302152633667
        vf_loss: 0.42676466703414917
    load_time_ms: 13538.053
    num_steps_sampled: 28032000
    num_steps_trained: 28032000
    sample_time_ms: 98656.63
    update_time_ms: 17.539
  iterations_since_restore: 132
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.865921787709496
    ram_util_percent: 11.149720670391062
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 11.0
    agent-2: 17.0
    agent-3: 17.0
    agent-4: 22.0
    agent-5: 23.0
  policy_reward_mean:
    agent-0: 4.79
    agent-1: 1.76
    agent-2: 5.85
    agent-3: 5.43
    agent-4: 6.86
    agent-5: 4.19
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: -1.0
  sampler_perf:
    mean_env_wait_ms: 22.62904952097525
    mean_inference_ms: 12.732153437298432
    mean_processing_ms: 57.36473626996265
  time_since_restore: 16597.5970993042
  time_this_iter_s: 125.2982656955719
  time_total_s: 39107.880341768265
  timestamp: 1637236521
  timesteps_since_restore: 12672000
  timesteps_this_iter: 96000
  timesteps_total: 28032000
  training_iteration: 292
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    292 |          39107.9 | 28032000 |    28.88 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 3.3
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.02
    apples_agent-1_min: 0
    apples_agent-2_max: 26
    apples_agent-2_mean: 2.52
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.55
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.3
    apples_agent-4_min: 0
    apples_agent-5_max: 43
    apples_agent-5_mean: 3.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 253
    cleaning_beam_agent-0_mean: 57.81
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 304
    cleaning_beam_agent-1_mean: 201.9
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 111
    cleaning_beam_agent-2_mean: 31.51
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 120
    cleaning_beam_agent-3_mean: 51.15
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 180
    cleaning_beam_agent-4_mean: 44.2
    cleaning_beam_agent-4_min: 10
    cleaning_beam_agent-5_max: 80
    cleaning_beam_agent-5_mean: 21.56
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-57-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 70.0
  episode_reward_mean: 26.76
  episode_reward_min: -30.0
  episodes_this_iter: 96
  episodes_total: 28128
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12154.831
    learner:
      agent-0:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5032433271408081
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006676081684418023
        model: {}
        policy_loss: -0.001663723960518837
        total_loss: -0.0023674415424466133
        vf_explained_var: 0.0005768686532974243
        vf_loss: 1.819913625717163
      agent-1:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4637124538421631
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008469359599985182
        model: {}
        policy_loss: -0.002329353941604495
        total_loss: -0.0031329402700066566
        vf_explained_var: 0.026413440704345703
        vf_loss: 0.12547080218791962
      agent-2:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45226413011550903
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014207076746970415
        model: {}
        policy_loss: -0.003265178529545665
        total_loss: -0.00399584136903286
        vf_explained_var: -0.0029835402965545654
        vf_loss: 0.6532464623451233
      agent-3:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6186688542366028
        entropy_coeff: 0.0017600000137463212
        kl: 0.001399365602992475
        model: {}
        policy_loss: -0.002718701958656311
        total_loss: -0.0037582560908049345
        vf_explained_var: 0.005013659596443176
        vf_loss: 0.49300506711006165
      agent-4:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6441466212272644
        entropy_coeff: 0.0017600000137463212
        kl: 0.001274188864044845
        model: {}
        policy_loss: -0.003615359775722027
        total_loss: -0.004691183101385832
        vf_explained_var: 0.015647903084754944
        vf_loss: 0.5787274837493896
      agent-5:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9024717211723328
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015843494329601526
        model: {}
        policy_loss: -0.0030026263557374477
        total_loss: -0.004559346009045839
        vf_explained_var: -0.005186676979064941
        vf_loss: 0.31633955240249634
    load_time_ms: 13538.519
    num_steps_sampled: 28128000
    num_steps_trained: 28128000
    sample_time_ms: 98690.365
    update_time_ms: 17.449
  iterations_since_restore: 133
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.924858757062147
    ram_util_percent: 11.075141242937852
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 9.0
    agent-2: 24.0
    agent-3: 15.0
    agent-4: 17.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 4.52
    agent-1: 1.75
    agent-2: 5.64
    agent-3: 5.06
    agent-4: 6.01
    agent-5: 3.78
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.62567589385395
    mean_inference_ms: 12.731413220503487
    mean_processing_ms: 57.3596049669137
  time_since_restore: 16722.400436639786
  time_this_iter_s: 124.80333733558655
  time_total_s: 39232.68367910385
  timestamp: 1637236646
  timesteps_since_restore: 12768000
  timesteps_this_iter: 96000
  timesteps_total: 28128000
  training_iteration: 293
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 18.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    293 |          39232.7 | 28128000 |    26.76 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 2.61
    apples_agent-0_min: 0
    apples_agent-1_max: 22
    apples_agent-1_mean: 1.19
    apples_agent-1_min: 0
    apples_agent-2_max: 64
    apples_agent-2_mean: 2.89
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 2.77
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.35
    apples_agent-4_min: 0
    apples_agent-5_max: 44
    apples_agent-5_mean: 2.53
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 148
    cleaning_beam_agent-0_mean: 53.24
    cleaning_beam_agent-0_min: 13
    cleaning_beam_agent-1_max: 290
    cleaning_beam_agent-1_mean: 197.17
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 86
    cleaning_beam_agent-2_mean: 27.22
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 51.2
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 191
    cleaning_beam_agent-4_mean: 49.46
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 65
    cleaning_beam_agent-5_mean: 19.83
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_06-59-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 24.64
  episode_reward_min: -30.0
  episodes_this_iter: 96
  episodes_total: 28224
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12152.345
    learner:
      agent-0:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49033576250076294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012492160312831402
        model: {}
        policy_loss: -0.003582854988053441
        total_loss: -0.004413449205458164
        vf_explained_var: -0.00015226006507873535
        vf_loss: 0.3239711821079254
      agent-1:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4624767303466797
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010091566946357489
        model: {}
        policy_loss: -0.0024886252358555794
        total_loss: -0.003157202620059252
        vf_explained_var: 0.006847992539405823
        vf_loss: 1.4538167715072632
      agent-2:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43963366746902466
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012618238106369972
        model: {}
        policy_loss: -0.0033882735297083855
        total_loss: -0.0041049811989068985
        vf_explained_var: 0.0008825510740280151
        vf_loss: 0.5704336762428284
      agent-3:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6204143166542053
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009917858988046646
        model: {}
        policy_loss: -0.0027553364634513855
        total_loss: -0.0038131317123770714
        vf_explained_var: -0.006121456623077393
        vf_loss: 0.34132492542266846
      agent-4:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6454147100448608
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015160086331889033
        model: {}
        policy_loss: -0.00377519428730011
        total_loss: -0.00486486591398716
        vf_explained_var: 0.010380670428276062
        vf_loss: 0.4626297354698181
      agent-5:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8539902567863464
        entropy_coeff: 0.0017600000137463212
        kl: 0.001228155568242073
        model: {}
        policy_loss: -0.0027407521847635508
        total_loss: -0.004217233974486589
        vf_explained_var: -0.004833489656448364
        vf_loss: 0.26541462540626526
    load_time_ms: 13541.473
    num_steps_sampled: 28224000
    num_steps_trained: 28224000
    sample_time_ms: 98708.826
    update_time_ms: 16.936
  iterations_since_restore: 134
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.074157303370786
    ram_util_percent: 11.141573033707866
  pid: 5668
  policy_reward_max:
    agent-0: 11.0
    agent-1: 8.0
    agent-2: 20.0
    agent-3: 17.0
    agent-4: 13.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.36
    agent-1: 1.36
    agent-2: 5.73
    agent-3: 4.03
    agent-4: 5.68
    agent-5: 3.48
  policy_reward_min:
    agent-0: 0.0
    agent-1: -49.0
    agent-2: 1.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.6225405991647
    mean_inference_ms: 12.73081265153328
    mean_processing_ms: 57.354142522204754
  time_since_restore: 16847.355656147003
  time_this_iter_s: 124.95521950721741
  time_total_s: 39357.63889861107
  timestamp: 1637236771
  timesteps_since_restore: 12864000
  timesteps_this_iter: 96000
  timesteps_total: 28224000
  training_iteration: 294
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    294 |          39357.6 | 28224000 |    24.64 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 2.59
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.71
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 2.19
    apples_agent-2_min: 0
    apples_agent-3_max: 34
    apples_agent-3_mean: 4.17
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 1.5
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 2.02
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 110
    cleaning_beam_agent-0_mean: 52.27
    cleaning_beam_agent-0_min: 15
    cleaning_beam_agent-1_max: 312
    cleaning_beam_agent-1_mean: 206.27
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 124
    cleaning_beam_agent-2_mean: 32.09
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 50.75
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 282
    cleaning_beam_agent-4_mean: 51.6
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 70
    cleaning_beam_agent-5_mean: 20.71
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-01-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 79.0
  episode_reward_mean: 25.8
  episode_reward_min: -27.0
  episodes_this_iter: 96
  episodes_total: 28320
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12150.891
    learner:
      agent-0:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5022402405738831
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012626753887161613
        model: {}
        policy_loss: -0.0033380435779690742
        total_loss: -0.004181480035185814
        vf_explained_var: -0.0035465359687805176
        vf_loss: 0.40508490800857544
      agent-1:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46780213713645935
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010232727508991957
        model: {}
        policy_loss: -0.002498428337275982
        total_loss: -0.0033089222852140665
        vf_explained_var: 0.005602821707725525
        vf_loss: 0.12838348746299744
      agent-2:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44624075293540955
        entropy_coeff: 0.0017600000137463212
        kl: 0.00117000553291291
        model: {}
        policy_loss: -0.0031703291460871696
        total_loss: -0.003907964564859867
        vf_explained_var: 0.0044769346714019775
        vf_loss: 0.4774755537509918
      agent-3:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6242613196372986
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012117376318201423
        model: {}
        policy_loss: -0.0024939305149018764
        total_loss: -0.003542608115822077
        vf_explained_var: -0.0007358789443969727
        vf_loss: 0.5002337694168091
      agent-4:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6548104286193848
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012884072493761778
        model: {}
        policy_loss: -0.003472820855677128
        total_loss: -0.004568550270050764
        vf_explained_var: 0.026303276419639587
        vf_loss: 0.5673525929450989
      agent-5:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.87867271900177
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012673859018832445
        model: {}
        policy_loss: -0.0028344960883259773
        total_loss: -0.004350587725639343
        vf_explained_var: 0.0028652548789978027
        vf_loss: 0.3037291467189789
    load_time_ms: 13542.644
    num_steps_sampled: 28320000
    num_steps_trained: 28320000
    sample_time_ms: 98754.083
    update_time_ms: 16.936
  iterations_since_restore: 135
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.494350282485875
    ram_util_percent: 11.130508474576269
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 8.0
    agent-2: 15.0
    agent-3: 24.0
    agent-4: 21.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.63
    agent-1: 1.79
    agent-2: 5.55
    agent-3: 5.23
    agent-4: 5.53
    agent-5: 3.07
  policy_reward_min:
    agent-0: 0.0
    agent-1: -1.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -41.0
    agent-5: -48.0
  sampler_perf:
    mean_env_wait_ms: 22.62036801586786
    mean_inference_ms: 12.72997469664026
    mean_processing_ms: 57.350326426463596
  time_since_restore: 16971.218581199646
  time_this_iter_s: 123.86292505264282
  time_total_s: 39481.50182366371
  timestamp: 1637236895
  timesteps_since_restore: 12960000
  timesteps_this_iter: 96000
  timesteps_total: 28320000
  training_iteration: 295
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    295 |          39481.5 | 28320000 |     25.8 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 2.97
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.83
    apples_agent-1_min: 0
    apples_agent-2_max: 35
    apples_agent-2_mean: 2.72
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 3.12
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 1.57
    apples_agent-4_min: 0
    apples_agent-5_max: 11
    apples_agent-5_mean: 2.29
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 150
    cleaning_beam_agent-0_mean: 52.8
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 271
    cleaning_beam_agent-1_mean: 196.42
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 86
    cleaning_beam_agent-2_mean: 26.35
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 116
    cleaning_beam_agent-3_mean: 50.83
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 122
    cleaning_beam_agent-4_mean: 44.14
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 66
    cleaning_beam_agent-5_mean: 20.58
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-03-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 70.0
  episode_reward_mean: 26.25
  episode_reward_min: -35.0
  episodes_this_iter: 96
  episodes_total: 28416
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12152.443
    learner:
      agent-0:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.506257176399231
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012311787577345967
        model: {}
        policy_loss: -0.0035696690902113914
        total_loss: -0.00442289374768734
        vf_explained_var: 0.000606536865234375
        vf_loss: 0.377895325422287
      agent-1:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4661619961261749
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006359544349834323
        model: {}
        policy_loss: -0.0013686607126146555
        total_loss: -0.0020548494067043066
        vf_explained_var: 0.004513636231422424
        vf_loss: 1.3425486087799072
      agent-2:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4350471496582031
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016476827440783381
        model: {}
        policy_loss: -0.0035076667554676533
        total_loss: -0.004216552712023258
        vf_explained_var: -0.006559014320373535
        vf_loss: 0.567988395690918
      agent-3:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6313165426254272
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012030943762511015
        model: {}
        policy_loss: -0.0026324568316340446
        total_loss: -0.0037034042179584503
        vf_explained_var: -0.0061199963092803955
        vf_loss: 0.4017025828361511
      agent-4:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6479008197784424
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023649376817047596
        model: {}
        policy_loss: -0.0040808552876114845
        total_loss: -0.005158529616892338
        vf_explained_var: 0.01753772795200348
        vf_loss: 0.6262998580932617
      agent-5:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8722548484802246
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010602334514260292
        model: {}
        policy_loss: -0.002441207878291607
        total_loss: -0.003946549259126186
        vf_explained_var: 0.01267966628074646
        vf_loss: 0.2982765734195709
    load_time_ms: 13568.13
    num_steps_sampled: 28416000
    num_steps_trained: 28416000
    sample_time_ms: 98731.319
    update_time_ms: 16.958
  iterations_since_restore: 136
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.946629213483146
    ram_util_percent: 11.079213483146066
  pid: 5668
  policy_reward_max:
    agent-0: 14.0
    agent-1: 11.0
    agent-2: 14.0
    agent-3: 14.0
    agent-4: 23.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.35
    agent-1: 1.26
    agent-2: 5.77
    agent-3: 4.79
    agent-4: 6.39
    agent-5: 3.69
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: 1.0
    agent-3: 0.0
    agent-4: 2.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.617314841146577
    mean_inference_ms: 12.728990569707781
    mean_processing_ms: 57.34677643643841
  time_since_restore: 17096.441731452942
  time_this_iter_s: 125.2231502532959
  time_total_s: 39606.72497391701
  timestamp: 1637237021
  timesteps_since_restore: 13056000
  timesteps_this_iter: 96000
  timesteps_total: 28416000
  training_iteration: 296
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    296 |          39606.7 | 28416000 |    26.25 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 3.11
    apples_agent-0_min: 0
    apples_agent-1_max: 19
    apples_agent-1_mean: 1.16
    apples_agent-1_min: 0
    apples_agent-2_max: 59
    apples_agent-2_mean: 2.87
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 3.39
    apples_agent-3_min: 0
    apples_agent-4_max: 5
    apples_agent-4_mean: 1.33
    apples_agent-4_min: 0
    apples_agent-5_max: 10
    apples_agent-5_mean: 2.15
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 132
    cleaning_beam_agent-0_mean: 52.98
    cleaning_beam_agent-0_min: 11
    cleaning_beam_agent-1_max: 304
    cleaning_beam_agent-1_mean: 195.28
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 86
    cleaning_beam_agent-2_mean: 26.86
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 118
    cleaning_beam_agent-3_mean: 52.47
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 135
    cleaning_beam_agent-4_mean: 52.22
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 47
    cleaning_beam_agent-5_mean: 19.03
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-05-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 61.0
  episode_reward_mean: 27.47
  episode_reward_min: 3.0
  episodes_this_iter: 96
  episodes_total: 28512
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12151.434
    learner:
      agent-0:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5000584125518799
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012728131841868162
        model: {}
        policy_loss: -0.0033597848378121853
        total_loss: -0.004205034580081701
        vf_explained_var: 0.007438004016876221
        vf_loss: 0.3485218286514282
      agent-1:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4602874517440796
        entropy_coeff: 0.0017600000137463212
        kl: 0.001207119901664555
        model: {}
        policy_loss: -0.0024616539012640715
        total_loss: -0.003258480690419674
        vf_explained_var: 0.020185112953186035
        vf_loss: 0.13285435736179352
      agent-2:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4389611482620239
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014614094980061054
        model: {}
        policy_loss: -0.0035533690825104713
        total_loss: -0.0042705354280769825
        vf_explained_var: -0.0017638802528381348
        vf_loss: 0.5540871620178223
      agent-3:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6232589483261108
        entropy_coeff: 0.0017600000137463212
        kl: 0.001208815141580999
        model: {}
        policy_loss: -0.00291270250454545
        total_loss: -0.003967445809394121
        vf_explained_var: -0.012306958436965942
        vf_loss: 0.4219299256801605
      agent-4:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.653499960899353
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016691768541932106
        model: {}
        policy_loss: -0.003700281959027052
        total_loss: -0.004792843479663134
        vf_explained_var: 0.02095387876033783
        vf_loss: 0.5760255455970764
      agent-5:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8737061023712158
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016979975625872612
        model: {}
        policy_loss: -0.002710571512579918
        total_loss: -0.004214318469166756
        vf_explained_var: -0.0015437304973602295
        vf_loss: 0.3397809863090515
    load_time_ms: 13596.25
    num_steps_sampled: 28512000
    num_steps_trained: 28512000
    sample_time_ms: 98663.276
    update_time_ms: 16.942
  iterations_since_restore: 137
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.095454545454547
    ram_util_percent: 11.153409090909092
  pid: 5668
  policy_reward_max:
    agent-0: 14.0
    agent-1: 7.0
    agent-2: 17.0
    agent-3: 14.0
    agent-4: 15.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.25
    agent-1: 1.98
    agent-2: 6.04
    agent-3: 4.75
    agent-4: 6.52
    agent-5: 3.93
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.614098811934767
    mean_inference_ms: 12.728104200445207
    mean_processing_ms: 57.34133285782149
  time_since_restore: 17220.271911859512
  time_this_iter_s: 123.83018040657043
  time_total_s: 39730.55515432358
  timestamp: 1637237144
  timesteps_since_restore: 13152000
  timesteps_this_iter: 96000
  timesteps_total: 28512000
  training_iteration: 297
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    297 |          39730.6 | 28512000 |    27.47 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 3.08
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.04
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.91
    apples_agent-2_min: 0
    apples_agent-3_max: 34
    apples_agent-3_mean: 3.09
    apples_agent-3_min: 0
    apples_agent-4_max: 21
    apples_agent-4_mean: 1.53
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 2.13
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 117
    cleaning_beam_agent-0_mean: 48.28
    cleaning_beam_agent-0_min: 10
    cleaning_beam_agent-1_max: 301
    cleaning_beam_agent-1_mean: 200.42
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 207
    cleaning_beam_agent-2_mean: 29.78
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 54.89
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 257
    cleaning_beam_agent-4_mean: 51.13
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 63
    cleaning_beam_agent-5_mean: 22.14
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-07-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 77.0
  episode_reward_mean: 24.64
  episode_reward_min: -36.0
  episodes_this_iter: 96
  episodes_total: 28608
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12137.915
    learner:
      agent-0:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48724812269210815
        entropy_coeff: 0.0017600000137463212
        kl: 0.001275139395147562
        model: {}
        policy_loss: -0.002987170359119773
        total_loss: -0.0037967925891280174
        vf_explained_var: 0.0040563493967056274
        vf_loss: 0.4793393015861511
      agent-1:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45514723658561707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009893729584291577
        model: {}
        policy_loss: -0.0026079192757606506
        total_loss: -0.003397685941308737
        vf_explained_var: 0.037091612815856934
        vf_loss: 0.11288180947303772
      agent-2:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4463580846786499
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017261177999898791
        model: {}
        policy_loss: -0.003710550256073475
        total_loss: -0.004452568478882313
        vf_explained_var: 0.0007876455783843994
        vf_loss: 0.4356991648674011
      agent-3:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6132350564002991
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018402461428195238
        model: {}
        policy_loss: -0.002669217064976692
        total_loss: -0.0037101246416568756
        vf_explained_var: -0.010264933109283447
        vf_loss: 0.383859246969223
      agent-4:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6453914642333984
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013829430099576712
        model: {}
        policy_loss: -0.003977705724537373
        total_loss: -0.005072304047644138
        vf_explained_var: 0.005341365933418274
        vf_loss: 0.41289860010147095
      agent-5:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8888572454452515
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017356014577671885
        model: {}
        policy_loss: -0.002826697425916791
        total_loss: -0.004362960811704397
        vf_explained_var: 0.003941729664802551
        vf_loss: 0.2812923789024353
    load_time_ms: 13580.917
    num_steps_sampled: 28608000
    num_steps_trained: 28608000
    sample_time_ms: 98746.676
    update_time_ms: 16.686
  iterations_since_restore: 138
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.109497206703914
    ram_util_percent: 11.095530726256984
  pid: 5668
  policy_reward_max:
    agent-0: 24.0
    agent-1: 7.0
    agent-2: 13.0
    agent-3: 17.0
    agent-4: 11.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.77
    agent-1: 1.81
    agent-2: 5.25
    agent-3: 4.39
    agent-4: 5.71
    agent-5: 3.71
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.61162137677713
    mean_inference_ms: 12.727724704602643
    mean_processing_ms: 57.33942286284424
  time_since_restore: 17345.608552217484
  time_this_iter_s: 125.33664035797119
  time_total_s: 39855.89179468155
  timestamp: 1637237270
  timesteps_since_restore: 13248000
  timesteps_this_iter: 96000
  timesteps_total: 28608000
  training_iteration: 298
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 18.3/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    298 |          39855.9 | 28608000 |    24.64 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 20
    apples_agent-0_mean: 3.11
    apples_agent-0_min: 0
    apples_agent-1_max: 10
    apples_agent-1_mean: 0.92
    apples_agent-1_min: 0
    apples_agent-2_max: 10
    apples_agent-2_mean: 2.16
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.62
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 1.77
    apples_agent-4_min: 0
    apples_agent-5_max: 21
    apples_agent-5_mean: 2.12
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 174
    cleaning_beam_agent-0_mean: 50.21
    cleaning_beam_agent-0_min: 14
    cleaning_beam_agent-1_max: 310
    cleaning_beam_agent-1_mean: 196.75
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 164
    cleaning_beam_agent-2_mean: 32.9
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 48.88
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 187
    cleaning_beam_agent-4_mean: 50.36
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 60
    cleaning_beam_agent-5_mean: 18.74
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-09-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 27.41
  episode_reward_min: -36.0
  episodes_this_iter: 96
  episodes_total: 28704
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12140.026
    learner:
      agent-0:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48330724239349365
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014249196974560618
        model: {}
        policy_loss: -0.0033084102906286716
        total_loss: -0.004116869531571865
        vf_explained_var: 0.007532373070716858
        vf_loss: 0.4216136336326599
      agent-1:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44926342368125916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008708547684364021
        model: {}
        policy_loss: -0.0024826866574585438
        total_loss: -0.0032589705660939217
        vf_explained_var: 0.024492919445037842
        vf_loss: 0.14422163367271423
      agent-2:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4492318034172058
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015022861771285534
        model: {}
        policy_loss: -0.0036126188933849335
        total_loss: -0.004340963438153267
        vf_explained_var: 0.007936283946037292
        vf_loss: 0.6230305433273315
      agent-3:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6085530519485474
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009211302967742085
        model: {}
        policy_loss: -0.002623598789796233
        total_loss: -0.0036460922565311193
        vf_explained_var: 0.0006583631038665771
        vf_loss: 0.485600084066391
      agent-4:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6483667492866516
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014268805971369147
        model: {}
        policy_loss: -0.0037051215767860413
        total_loss: -0.004791627638041973
        vf_explained_var: 0.015549927949905396
        vf_loss: 0.5462013483047485
      agent-5:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.870410680770874
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024178929161280394
        model: {}
        policy_loss: -0.0033988235518336296
        total_loss: -0.004899840801954269
        vf_explained_var: 0.009532526135444641
        vf_loss: 0.3090434968471527
    load_time_ms: 13574.549
    num_steps_sampled: 28704000
    num_steps_trained: 28704000
    sample_time_ms: 98737.159
    update_time_ms: 16.307
  iterations_since_restore: 139
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.9954802259887
    ram_util_percent: 11.080790960451976
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 10.0
    agent-2: 18.0
    agent-3: 16.0
    agent-4: 15.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.11
    agent-1: 1.96
    agent-2: 6.05
    agent-3: 5.13
    agent-4: 6.59
    agent-5: 3.57
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.607781511676812
    mean_inference_ms: 12.726933950386996
    mean_processing_ms: 57.33666450250658
  time_since_restore: 17470.009113550186
  time_this_iter_s: 124.40056133270264
  time_total_s: 39980.29235601425
  timestamp: 1637237394
  timesteps_since_restore: 13344000
  timesteps_this_iter: 96000
  timesteps_total: 28704000
  training_iteration: 299
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    299 |          39980.3 | 28704000 |    27.41 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 80
    apples_agent-0_mean: 4.41
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.12
    apples_agent-1_min: 0
    apples_agent-2_max: 14
    apples_agent-2_mean: 2.43
    apples_agent-2_min: 0
    apples_agent-3_max: 22
    apples_agent-3_mean: 3.49
    apples_agent-3_min: 0
    apples_agent-4_max: 30
    apples_agent-4_mean: 1.59
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 2.11
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 155
    cleaning_beam_agent-0_mean: 53.22
    cleaning_beam_agent-0_min: 14
    cleaning_beam_agent-1_max: 336
    cleaning_beam_agent-1_mean: 198.31
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 84
    cleaning_beam_agent-2_mean: 32.03
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 120
    cleaning_beam_agent-3_mean: 48.64
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 129
    cleaning_beam_agent-4_mean: 45.09
    cleaning_beam_agent-4_min: 3
    cleaning_beam_agent-5_max: 58
    cleaning_beam_agent-5_mean: 20.39
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-11-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 128.0
  episode_reward_mean: 26.6
  episode_reward_min: -30.0
  episodes_this_iter: 96
  episodes_total: 28800
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12134.444
    learner:
      agent-0:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49125364422798157
        entropy_coeff: 0.0017600000137463212
        kl: 0.001005368074402213
        model: {}
        policy_loss: -0.001967465737834573
        total_loss: -0.0025662763509899378
        vf_explained_var: 9.295344352722168e-05
        vf_loss: 2.6579842567443848
      agent-1:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4566449224948883
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008902682457119226
        model: {}
        policy_loss: -0.0024277884513139725
        total_loss: -0.0032113397028297186
        vf_explained_var: 0.021636858582496643
        vf_loss: 0.20143342018127441
      agent-2:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.449246883392334
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012190123088657856
        model: {}
        policy_loss: -0.003411273006349802
        total_loss: -0.004143288359045982
        vf_explained_var: 0.0016436278820037842
        vf_loss: 0.5865756869316101
      agent-3:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6082655191421509
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011846928391605616
        model: {}
        policy_loss: -0.0027105105109512806
        total_loss: -0.0037230285815894604
        vf_explained_var: 0.0011380910873413086
        vf_loss: 0.5802621841430664
      agent-4:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6597825288772583
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017482624389231205
        model: {}
        policy_loss: -0.003862846177071333
        total_loss: -0.004968979861587286
        vf_explained_var: 0.0021655261516571045
        vf_loss: 0.5508103370666504
      agent-5:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8819730281829834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013286576140671968
        model: {}
        policy_loss: -0.0028356914408504963
        total_loss: -0.0043474240228533745
        vf_explained_var: -0.0005609095096588135
        vf_loss: 0.4053734242916107
    load_time_ms: 13580.718
    num_steps_sampled: 28800000
    num_steps_trained: 28800000
    sample_time_ms: 98795.853
    update_time_ms: 16.411
  iterations_since_restore: 140
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.936158192090396
    ram_util_percent: 11.144067796610171
  pid: 5668
  policy_reward_max:
    agent-0: 28.0
    agent-1: 19.0
    agent-2: 14.0
    agent-3: 34.0
    agent-4: 17.0
    agent-5: 28.0
  policy_reward_mean:
    agent-0: 3.8
    agent-1: 2.22
    agent-2: 5.28
    agent-3: 5.11
    agent-4: 6.26
    agent-5: 3.93
  policy_reward_min:
    agent-0: -45.0
    agent-1: 0.0
    agent-2: -41.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.605093415336206
    mean_inference_ms: 12.725875546756631
    mean_processing_ms: 57.33076890598044
  time_since_restore: 17594.66450548172
  time_this_iter_s: 124.65539193153381
  time_total_s: 40104.947747945786
  timestamp: 1637237519
  timesteps_since_restore: 13440000
  timesteps_this_iter: 96000
  timesteps_total: 28800000
  training_iteration: 300
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    300 |          40104.9 | 28800000 |     26.6 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 2.77
    apples_agent-0_min: 0
    apples_agent-1_max: 3
    apples_agent-1_mean: 0.63
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 1.83
    apples_agent-2_min: 0
    apples_agent-3_max: 20
    apples_agent-3_mean: 3.31
    apples_agent-3_min: 0
    apples_agent-4_max: 9
    apples_agent-4_mean: 1.3
    apples_agent-4_min: 0
    apples_agent-5_max: 19
    apples_agent-5_mean: 2.4
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 107
    cleaning_beam_agent-0_mean: 52.34
    cleaning_beam_agent-0_min: 16
    cleaning_beam_agent-1_max: 298
    cleaning_beam_agent-1_mean: 190.53
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 87
    cleaning_beam_agent-2_mean: 28.61
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 142
    cleaning_beam_agent-3_mean: 50.48
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 124
    cleaning_beam_agent-4_mean: 45.16
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 71
    cleaning_beam_agent-5_mean: 22.98
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-14-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 60.0
  episode_reward_mean: 24.11
  episode_reward_min: 8.0
  episodes_this_iter: 96
  episodes_total: 28896
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12132.33
    learner:
      agent-0:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5038430094718933
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015349170425906777
        model: {}
        policy_loss: -0.0033736133482307196
        total_loss: -0.0042312233708798885
        vf_explained_var: 0.007861807942390442
        vf_loss: 0.29153746366500854
      agent-1:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46210604906082153
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013268382754176855
        model: {}
        policy_loss: -0.002475643064826727
        total_loss: -0.0032797264866530895
        vf_explained_var: 0.010385826230049133
        vf_loss: 0.0921928659081459
      agent-2:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4367937743663788
        entropy_coeff: 0.0017600000137463212
        kl: 0.001283344579860568
        model: {}
        policy_loss: -0.003610070561990142
        total_loss: -0.0043281977996230125
        vf_explained_var: -0.0034671127796173096
        vf_loss: 0.5062868595123291
      agent-3:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6077131628990173
        entropy_coeff: 0.0017600000137463212
        kl: 0.001788501744158566
        model: {}
        policy_loss: -0.002818132285028696
        total_loss: -0.0038423798978328705
        vf_explained_var: 0.004024624824523926
        vf_loss: 0.4532923102378845
      agent-4:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6525608897209167
        entropy_coeff: 0.0017600000137463212
        kl: 0.00140316691249609
        model: {}
        policy_loss: -0.003842813428491354
        total_loss: -0.004945439286530018
        vf_explained_var: 0.0107860267162323
        vf_loss: 0.4588316082954407
      agent-5:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8850719928741455
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015703443204984069
        model: {}
        policy_loss: -0.0029747802764177322
        total_loss: -0.0045056878589093685
        vf_explained_var: 0.011187031865119934
        vf_loss: 0.26818451285362244
    load_time_ms: 13573.152
    num_steps_sampled: 28896000
    num_steps_trained: 28896000
    sample_time_ms: 98835.73
    update_time_ms: 16.507
  iterations_since_restore: 141
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.89887640449438
    ram_util_percent: 10.999438202247191
  pid: 5668
  policy_reward_max:
    agent-0: 12.0
    agent-1: 6.0
    agent-2: 15.0
    agent-3: 21.0
    agent-4: 18.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 3.97
    agent-1: 1.48
    agent-2: 4.96
    agent-3: 4.59
    agent-4: 5.6
    agent-5: 3.51
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.60086267044127
    mean_inference_ms: 12.72481769745807
    mean_processing_ms: 57.326887019422855
  time_since_restore: 17718.79809165001
  time_this_iter_s: 124.13358616828918
  time_total_s: 40229.081334114075
  timestamp: 1637237644
  timesteps_since_restore: 13536000
  timesteps_this_iter: 96000
  timesteps_total: 28896000
  training_iteration: 301
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    301 |          40229.1 | 28896000 |    24.11 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 2.98
    apples_agent-0_min: 0
    apples_agent-1_max: 23
    apples_agent-1_mean: 1.16
    apples_agent-1_min: 0
    apples_agent-2_max: 11
    apples_agent-2_mean: 1.98
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 3.36
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 1.29
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 2.02
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 118
    cleaning_beam_agent-0_mean: 49.21
    cleaning_beam_agent-0_min: 8
    cleaning_beam_agent-1_max: 304
    cleaning_beam_agent-1_mean: 190.91
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 83
    cleaning_beam_agent-2_mean: 25.11
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 51.3
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 169
    cleaning_beam_agent-4_mean: 48.32
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 18.58
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-16-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 77.0
  episode_reward_mean: 25.53
  episode_reward_min: -23.0
  episodes_this_iter: 96
  episodes_total: 28992
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12136.851
    learner:
      agent-0:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4984045624732971
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014748615212738514
        model: {}
        policy_loss: -0.003463887609541416
        total_loss: -0.004301827400922775
        vf_explained_var: 0.0017078518867492676
        vf_loss: 0.39249467849731445
      agent-1:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4647010564804077
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007370861130766571
        model: {}
        policy_loss: -0.002243933267891407
        total_loss: -0.0030504828318953514
        vf_explained_var: 0.024556607007980347
        vf_loss: 0.11323864012956619
      agent-2:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42995384335517883
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008867772412486374
        model: {}
        policy_loss: -0.0030716927722096443
        total_loss: -0.0037761246785521507
        vf_explained_var: 0.001039847731590271
        vf_loss: 0.5228675603866577
      agent-3:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6128748655319214
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013348766369745135
        model: {}
        policy_loss: -0.002997795818373561
        total_loss: -0.004040979780256748
        vf_explained_var: -0.0028957724571228027
        vf_loss: 0.35474562644958496
      agent-4:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6527239680290222
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015477911802008748
        model: {}
        policy_loss: -0.003828961867839098
        total_loss: -0.004928592126816511
        vf_explained_var: 0.01442834734916687
        vf_loss: 0.49165332317352295
      agent-5:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8717067241668701
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022521731443703175
        model: {}
        policy_loss: -0.002325884997844696
        total_loss: -0.003697593230754137
        vf_explained_var: 0.00362241268157959
        vf_loss: 1.624981164932251
    load_time_ms: 13559.896
    num_steps_sampled: 28992000
    num_steps_trained: 28992000
    sample_time_ms: 98689.646
    update_time_ms: 16.284
  iterations_since_restore: 142
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.122727272727275
    ram_util_percent: 11.077272727272726
  pid: 5668
  policy_reward_max:
    agent-0: 16.0
    agent-1: 9.0
    agent-2: 20.0
    agent-3: 13.0
    agent-4: 16.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.57
    agent-1: 1.75
    agent-2: 5.27
    agent-3: 4.7
    agent-4: 5.85
    agent-5: 3.39
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: -47.0
  sampler_perf:
    mean_env_wait_ms: 22.596582239304542
    mean_inference_ms: 12.723374078537569
    mean_processing_ms: 57.32076436514052
  time_since_restore: 17842.544117689133
  time_this_iter_s: 123.74602603912354
  time_total_s: 40352.8273601532
  timestamp: 1637237768
  timesteps_since_restore: 13632000
  timesteps_this_iter: 96000
  timesteps_total: 28992000
  training_iteration: 302
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    302 |          40352.8 | 28992000 |    25.53 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 3.46
    apples_agent-0_min: 0
    apples_agent-1_max: 3
    apples_agent-1_mean: 0.9
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 2.94
    apples_agent-2_min: 0
    apples_agent-3_max: 34
    apples_agent-3_mean: 3.4
    apples_agent-3_min: 0
    apples_agent-4_max: 12
    apples_agent-4_mean: 1.47
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 2.08
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 119
    cleaning_beam_agent-0_mean: 51.51
    cleaning_beam_agent-0_min: 20
    cleaning_beam_agent-1_max: 305
    cleaning_beam_agent-1_mean: 190.22
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 97
    cleaning_beam_agent-2_mean: 28.02
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 137
    cleaning_beam_agent-3_mean: 52.16
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 101
    cleaning_beam_agent-4_mean: 49.92
    cleaning_beam_agent-4_min: 14
    cleaning_beam_agent-5_max: 51
    cleaning_beam_agent-5_mean: 20.04
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-18-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 57.0
  episode_reward_mean: 25.67
  episode_reward_min: 7.0
  episodes_this_iter: 96
  episodes_total: 29088
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12127.957
    learner:
      agent-0:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5075711011886597
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014947125455364585
        model: {}
        policy_loss: -0.003223750740289688
        total_loss: -0.004079238511621952
        vf_explained_var: 0.00374622642993927
        vf_loss: 0.3783777058124542
      agent-1:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45277103781700134
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013247347669675946
        model: {}
        policy_loss: -0.002311788033694029
        total_loss: -0.003096757922321558
        vf_explained_var: 0.023062527179718018
        vf_loss: 0.11904497444629669
      agent-2:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4407793879508972
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011079934192821383
        model: {}
        policy_loss: -0.0036111045628786087
        total_loss: -0.004337603226304054
        vf_explained_var: 0.0005038082599639893
        vf_loss: 0.49271267652511597
      agent-3:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6000372171401978
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012638585176318884
        model: {}
        policy_loss: -0.0028662055265158415
        total_loss: -0.003885002341121435
        vf_explained_var: -0.007422313094139099
        vf_loss: 0.37270328402519226
      agent-4:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6535667181015015
        entropy_coeff: 0.0017600000137463212
        kl: 0.001708582742139697
        model: {}
        policy_loss: -0.003883127123117447
        total_loss: -0.004984562750905752
        vf_explained_var: 0.01421096920967102
        vf_loss: 0.4884454607963562
      agent-5:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8795306086540222
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015835633967071772
        model: {}
        policy_loss: -0.0030131470412015915
        total_loss: -0.004536313004791737
        vf_explained_var: 0.0021421462297439575
        vf_loss: 0.24807099997997284
    load_time_ms: 13562.111
    num_steps_sampled: 29088000
    num_steps_trained: 29088000
    sample_time_ms: 98764.315
    update_time_ms: 16.199
  iterations_since_restore: 143
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.89832402234637
    ram_util_percent: 11.069832402234635
  pid: 5668
  policy_reward_max:
    agent-0: 14.0
    agent-1: 9.0
    agent-2: 17.0
    agent-3: 14.0
    agent-4: 21.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.38
    agent-1: 1.72
    agent-2: 5.46
    agent-3: 4.62
    agent-4: 6.06
    agent-5: 3.43
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.593586588481504
    mean_inference_ms: 12.722551878060893
    mean_processing_ms: 57.31907276669342
  time_since_restore: 17968.023512125015
  time_this_iter_s: 125.47939443588257
  time_total_s: 40478.30675458908
  timestamp: 1637237894
  timesteps_since_restore: 13728000
  timesteps_this_iter: 96000
  timesteps_total: 29088000
  training_iteration: 303
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    303 |          40478.3 | 29088000 |    25.67 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 3.3
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.93
    apples_agent-1_min: 0
    apples_agent-2_max: 15
    apples_agent-2_mean: 2.28
    apples_agent-2_min: 0
    apples_agent-3_max: 10
    apples_agent-3_mean: 3.39
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.64
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 2.17
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 192
    cleaning_beam_agent-0_mean: 51.79
    cleaning_beam_agent-0_min: 9
    cleaning_beam_agent-1_max: 299
    cleaning_beam_agent-1_mean: 194.12
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 99
    cleaning_beam_agent-2_mean: 25.86
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 49.8
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 200
    cleaning_beam_agent-4_mean: 49.64
    cleaning_beam_agent-4_min: 5
    cleaning_beam_agent-5_max: 46
    cleaning_beam_agent-5_mean: 19.35
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-20-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 69.0
  episode_reward_mean: 26.97
  episode_reward_min: 7.0
  episodes_this_iter: 96
  episodes_total: 29184
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12125.643
    learner:
      agent-0:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5156564712524414
        entropy_coeff: 0.0017600000137463212
        kl: 0.001690767938271165
        model: {}
        policy_loss: -0.003273776499554515
        total_loss: -0.004143039230257273
        vf_explained_var: 0.007453024387359619
        vf_loss: 0.3829481601715088
      agent-1:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4568725824356079
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007917536422610283
        model: {}
        policy_loss: -0.002058111596852541
        total_loss: -0.0028496189042925835
        vf_explained_var: 0.014137938618659973
        vf_loss: 0.12587973475456238
      agent-2:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42916545271873474
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012824679724872112
        model: {}
        policy_loss: -0.003332240507006645
        total_loss: -0.004032639786601067
        vf_explained_var: -0.006878092885017395
        vf_loss: 0.5493237376213074
      agent-3:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6121159791946411
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012473721290007234
        model: {}
        policy_loss: -0.0026917667128145695
        total_loss: -0.0037270495668053627
        vf_explained_var: -0.0012715458869934082
        vf_loss: 0.42039626836776733
      agent-4:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6625950336456299
        entropy_coeff: 0.0017600000137463212
        kl: 0.001525375060737133
        model: {}
        policy_loss: -0.0036626551300287247
        total_loss: -0.004771742038428783
        vf_explained_var: 0.019799336791038513
        vf_loss: 0.570803701877594
      agent-5:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8829520344734192
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015616404125466943
        model: {}
        policy_loss: -0.0027972678653895855
        total_loss: -0.004324633162468672
        vf_explained_var: 0.0002292841672897339
        vf_loss: 0.266310453414917
    load_time_ms: 13571.831
    num_steps_sampled: 29184000
    num_steps_trained: 29184000
    sample_time_ms: 98699.203
    update_time_ms: 16.313
  iterations_since_restore: 144
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.07457627118644
    ram_util_percent: 11.079096045197739
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 9.0
    agent-2: 19.0
    agent-3: 15.0
    agent-4: 16.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.29
    agent-1: 1.75
    agent-2: 5.7
    agent-3: 4.89
    agent-4: 6.57
    agent-5: 3.77
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.590314245739197
    mean_inference_ms: 12.722104480356501
    mean_processing_ms: 57.31551216354441
  time_since_restore: 18092.40586566925
  time_this_iter_s: 124.38235354423523
  time_total_s: 40602.689108133316
  timestamp: 1637238018
  timesteps_since_restore: 13824000
  timesteps_this_iter: 96000
  timesteps_total: 29184000
  training_iteration: 304
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.2/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    304 |          40602.7 | 29184000 |    26.97 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 2.81
    apples_agent-0_min: 0
    apples_agent-1_max: 6
    apples_agent-1_mean: 1.05
    apples_agent-1_min: 0
    apples_agent-2_max: 106
    apples_agent-2_mean: 4.26
    apples_agent-2_min: 0
    apples_agent-3_max: 13
    apples_agent-3_mean: 3.45
    apples_agent-3_min: 0
    apples_agent-4_max: 6
    apples_agent-4_mean: 1.11
    apples_agent-4_min: 0
    apples_agent-5_max: 8
    apples_agent-5_mean: 2.27
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 129
    cleaning_beam_agent-0_mean: 53.66
    cleaning_beam_agent-0_min: 2
    cleaning_beam_agent-1_max: 332
    cleaning_beam_agent-1_mean: 202.3
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 134
    cleaning_beam_agent-2_mean: 30.11
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 41.79
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 200
    cleaning_beam_agent-4_mean: 50.96
    cleaning_beam_agent-4_min: 15
    cleaning_beam_agent-5_max: 58
    cleaning_beam_agent-5_mean: 18.01
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-22-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 58.0
  episode_reward_mean: 28.23
  episode_reward_min: 12.0
  episodes_this_iter: 96
  episodes_total: 29280
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12124.331
    learner:
      agent-0:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5141708254814148
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010156134376302361
        model: {}
        policy_loss: -0.0032324939966201782
        total_loss: -0.004099864512681961
        vf_explained_var: -0.0015095770359039307
        vf_loss: 0.3756985664367676
      agent-1:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45740586519241333
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008903826237656176
        model: {}
        policy_loss: -0.0023243343457579613
        total_loss: -0.0031183334067463875
        vf_explained_var: 0.017567217350006104
        vf_loss: 0.11037229001522064
      agent-2:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4423455595970154
        entropy_coeff: 0.0017600000137463212
        kl: 0.001163708046078682
        model: {}
        policy_loss: -0.0032995729707181454
        total_loss: -0.004018791485577822
        vf_explained_var: 0.00792016088962555
        vf_loss: 0.5930810570716858
      agent-3:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5856118202209473
        entropy_coeff: 0.0017600000137463212
        kl: 0.000861206732224673
        model: {}
        policy_loss: -0.0022972479928284883
        total_loss: -0.0032838599290698767
        vf_explained_var: -0.0032629966735839844
        vf_loss: 0.44065403938293457
      agent-4:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6643017530441284
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015784850111231208
        model: {}
        policy_loss: -0.003938063979148865
        total_loss: -0.005045684054493904
        vf_explained_var: 0.011502698063850403
        vf_loss: 0.6155186891555786
      agent-5:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8668352365493774
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015619114274159074
        model: {}
        policy_loss: -0.002619100036099553
        total_loss: -0.00411280756816268
        vf_explained_var: 0.006406649947166443
        vf_loss: 0.3192322254180908
    load_time_ms: 13550.146
    num_steps_sampled: 29280000
    num_steps_trained: 29280000
    sample_time_ms: 98759.657
    update_time_ms: 16.43
  iterations_since_restore: 145
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.063841807909604
    ram_util_percent: 11.077401129943501
  pid: 5668
  policy_reward_max:
    agent-0: 17.0
    agent-1: 9.0
    agent-2: 17.0
    agent-3: 14.0
    agent-4: 21.0
    agent-5: 11.0
  policy_reward_mean:
    agent-0: 4.61
    agent-1: 1.71
    agent-2: 6.09
    agent-3: 4.79
    agent-4: 6.9
    agent-5: 4.13
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.587685029232897
    mean_inference_ms: 12.72145536713537
    mean_processing_ms: 57.31171987137262
  time_since_restore: 18216.6145734787
  time_this_iter_s: 124.20870780944824
  time_total_s: 40726.897815942764
  timestamp: 1637238143
  timesteps_since_restore: 13920000
  timesteps_this_iter: 96000
  timesteps_total: 29280000
  training_iteration: 305
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    305 |          40726.9 | 29280000 |    28.23 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 3.05
    apples_agent-0_min: 0
    apples_agent-1_max: 11
    apples_agent-1_mean: 0.86
    apples_agent-1_min: 0
    apples_agent-2_max: 20
    apples_agent-2_mean: 2.68
    apples_agent-2_min: 0
    apples_agent-3_max: 38
    apples_agent-3_mean: 4.15
    apples_agent-3_min: 0
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.58
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 2.5
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 170
    cleaning_beam_agent-0_mean: 50.58
    cleaning_beam_agent-0_min: 19
    cleaning_beam_agent-1_max: 329
    cleaning_beam_agent-1_mean: 197.99
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 94
    cleaning_beam_agent-2_mean: 25.53
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 45.62
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 140
    cleaning_beam_agent-4_mean: 45.55
    cleaning_beam_agent-4_min: 6
    cleaning_beam_agent-5_max: 50
    cleaning_beam_agent-5_mean: 17.79
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-24-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 69.0
  episode_reward_mean: 26.06
  episode_reward_min: -26.0
  episodes_this_iter: 96
  episodes_total: 29376
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12125.358
    learner:
      agent-0:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5176835060119629
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011887087021023035
        model: {}
        policy_loss: -0.003069065511226654
        total_loss: -0.0038593122735619545
        vf_explained_var: 0.006555885076522827
        vf_loss: 1.2087699174880981
      agent-1:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45626476407051086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008939469116739929
        model: {}
        policy_loss: -0.0024049337953329086
        total_loss: -0.0031985947862267494
        vf_explained_var: 0.013670101761817932
        vf_loss: 0.09367372840642929
      agent-2:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4230614900588989
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015883278101682663
        model: {}
        policy_loss: -0.003391626989468932
        total_loss: -0.004084775224328041
        vf_explained_var: -0.008829683065414429
        vf_loss: 0.5143917202949524
      agent-3:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5896599888801575
        entropy_coeff: 0.0017600000137463212
        kl: 0.001465833280235529
        model: {}
        policy_loss: -0.002846512943506241
        total_loss: -0.003840487450361252
        vf_explained_var: 0.0008505135774612427
        vf_loss: 0.4382542073726654
      agent-4:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6597483158111572
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012301317183300853
        model: {}
        policy_loss: -0.002670994494110346
        total_loss: -0.003646607743576169
        vf_explained_var: 0.014538079500198364
        vf_loss: 1.8554273843765259
      agent-5:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8728846311569214
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011419339571148157
        model: {}
        policy_loss: -0.002678348682820797
        total_loss: -0.004179792944341898
        vf_explained_var: 0.002446591854095459
        vf_loss: 0.3483349680900574
    load_time_ms: 13571.309
    num_steps_sampled: 29376000
    num_steps_trained: 29376000
    sample_time_ms: 98610.426
    update_time_ms: 16.298
  iterations_since_restore: 146
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.086931818181817
    ram_util_percent: 11.162500000000001
  pid: 5668
  policy_reward_max:
    agent-0: 17.0
    agent-1: 7.0
    agent-2: 15.0
    agent-3: 14.0
    agent-4: 20.0
    agent-5: 16.0
  policy_reward_mean:
    agent-0: 4.38
    agent-1: 1.43
    agent-2: 5.46
    agent-3: 4.89
    agent-4: 5.85
    agent-5: 4.05
  policy_reward_min:
    agent-0: -46.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: -40.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.58429648688949
    mean_inference_ms: 12.720702872026
    mean_processing_ms: 57.30602358956169
  time_since_restore: 18340.56700873375
  time_this_iter_s: 123.95243525505066
  time_total_s: 40850.850251197815
  timestamp: 1637238267
  timesteps_since_restore: 14016000
  timesteps_this_iter: 96000
  timesteps_total: 29376000
  training_iteration: 306
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    306 |          40850.9 | 29376000 |    26.06 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 11
    apples_agent-0_mean: 3.13
    apples_agent-0_min: 0
    apples_agent-1_max: 4
    apples_agent-1_mean: 0.89
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 1.95
    apples_agent-2_min: 0
    apples_agent-3_max: 11
    apples_agent-3_mean: 3.33
    apples_agent-3_min: 0
    apples_agent-4_max: 14
    apples_agent-4_mean: 1.43
    apples_agent-4_min: 0
    apples_agent-5_max: 28
    apples_agent-5_mean: 2.56
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 114
    cleaning_beam_agent-0_mean: 44.97
    cleaning_beam_agent-0_min: 9
    cleaning_beam_agent-1_max: 295
    cleaning_beam_agent-1_mean: 185.16
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 97
    cleaning_beam_agent-2_mean: 27.9
    cleaning_beam_agent-2_min: 5
    cleaning_beam_agent-3_max: 105
    cleaning_beam_agent-3_mean: 40.93
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 119
    cleaning_beam_agent-4_mean: 42.32
    cleaning_beam_agent-4_min: 12
    cleaning_beam_agent-5_max: 61
    cleaning_beam_agent-5_mean: 16.25
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-26-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 27.05
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 29472
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12127.051
    learner:
      agent-0:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4963096082210541
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013394266134127975
        model: {}
        policy_loss: -0.0034224670380353928
        total_loss: -0.00425716582685709
        vf_explained_var: 0.0021194517612457275
        vf_loss: 0.38805410265922546
      agent-1:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45447981357574463
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010381531901657581
        model: {}
        policy_loss: -0.0025443497579544783
        total_loss: -0.003327540121972561
        vf_explained_var: 0.0244857519865036
        vf_loss: 0.16695789992809296
      agent-2:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44146403670310974
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010849813697859645
        model: {}
        policy_loss: -0.003260022960603237
        total_loss: -0.003983284346759319
        vf_explained_var: -0.002373024821281433
        vf_loss: 0.5371454358100891
      agent-3:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5991849899291992
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008650781819596887
        model: {}
        policy_loss: -0.002630658447742462
        total_loss: -0.003641311079263687
        vf_explained_var: -0.0022858083248138428
        vf_loss: 0.43917086720466614
      agent-4:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6674696207046509
        entropy_coeff: 0.0017600000137463212
        kl: 0.001564874080941081
        model: {}
        policy_loss: -0.003502235049381852
        total_loss: -0.004618455655872822
        vf_explained_var: 0.021889150142669678
        vf_loss: 0.5852638483047485
      agent-5:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8791336417198181
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019806912168860435
        model: {}
        policy_loss: -0.0028165909461677074
        total_loss: -0.004330300260335207
        vf_explained_var: -0.0026660561561584473
        vf_loss: 0.33567315340042114
    load_time_ms: 13561.777
    num_steps_sampled: 29472000
    num_steps_trained: 29472000
    sample_time_ms: 98688.325
    update_time_ms: 16.38
  iterations_since_restore: 147
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.121348314606742
    ram_util_percent: 11.169101123595505
  pid: 5668
  policy_reward_max:
    agent-0: 15.0
    agent-1: 8.0
    agent-2: 21.0
    agent-3: 17.0
    agent-4: 22.0
    agent-5: 13.0
  policy_reward_mean:
    agent-0: 4.45
    agent-1: 2.1
    agent-2: 5.39
    agent-3: 4.76
    agent-4: 6.28
    agent-5: 4.07
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.581045149713553
    mean_inference_ms: 12.720087851608028
    mean_processing_ms: 57.30478720566001
  time_since_restore: 18465.08202981949
  time_this_iter_s: 124.51502108573914
  time_total_s: 40975.365272283554
  timestamp: 1637238391
  timesteps_since_restore: 14112000
  timesteps_this_iter: 96000
  timesteps_total: 29472000
  training_iteration: 307
  trial_id: '00000'
  
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
== Status ==
Memory usage on this node: 18.1/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    307 |          40975.4 | 29472000 |    27.05 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 3.35
    apples_agent-0_min: 0
    apples_agent-1_max: 8
    apples_agent-1_mean: 1.0
    apples_agent-1_min: 0
    apples_agent-2_max: 12
    apples_agent-2_mean: 2.32
    apples_agent-2_min: 0
    apples_agent-3_max: 14
    apples_agent-3_mean: 3.34
    apples_agent-3_min: 0
    apples_agent-4_max: 7
    apples_agent-4_mean: 1.38
    apples_agent-4_min: 0
    apples_agent-5_max: 22
    apples_agent-5_mean: 2.17
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 108
    cleaning_beam_agent-0_mean: 44.1
    cleaning_beam_agent-0_min: 12
    cleaning_beam_agent-1_max: 276
    cleaning_beam_agent-1_mean: 194.2
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 76
    cleaning_beam_agent-2_mean: 24.93
    cleaning_beam_agent-2_min: 6
    cleaning_beam_agent-3_max: 97
    cleaning_beam_agent-3_mean: 44.4
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 106
    cleaning_beam_agent-4_mean: 45.26
    cleaning_beam_agent-4_min: 4
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 18.87
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-28-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 68.0
  episode_reward_mean: 26.58
  episode_reward_min: 0.0
  episodes_this_iter: 96
  episodes_total: 29568
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12132.26
    learner:
      agent-0:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5054221749305725
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013587783323600888
        model: {}
        policy_loss: -0.0031771499197930098
        total_loss: -0.00402434729039669
        vf_explained_var: -0.0002090930938720703
        vf_loss: 0.4234432876110077
      agent-1:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46422678232192993
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012316456995904446
        model: {}
        policy_loss: -0.0024705249816179276
        total_loss: -0.0032764975912868977
        vf_explained_var: 0.027843579649925232
        vf_loss: 0.11066688597202301
      agent-2:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42422184348106384
        entropy_coeff: 0.0017600000137463212
        kl: 0.001018013572320342
        model: {}
        policy_loss: -0.0030448175966739655
        total_loss: -0.003727723378688097
        vf_explained_var: 0.004373326897621155
        vf_loss: 0.6372419595718384
      agent-3:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.600791335105896
        entropy_coeff: 0.0017600000137463212
        kl: 0.001390424557030201
        model: {}
        policy_loss: -0.0028120307251811028
        total_loss: -0.003832125337794423
        vf_explained_var: 0.000637739896774292
        vf_loss: 0.37299543619155884
      agent-4:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6694158911705017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020996350795030594
        model: {}
        policy_loss: -0.004250814206898212
        total_loss: -0.005381221882998943
        vf_explained_var: 0.03677649796009064
        vf_loss: 0.4776087999343872
      agent-5:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8965892195701599
        entropy_coeff: 0.0017600000137463212
        kl: 0.001717895152978599
        model: {}
        policy_loss: -0.0027066185139119625
        total_loss: -0.004251156002283096
        vf_explained_var: 0.0033130645751953125
        vf_loss: 0.33462250232696533
    load_time_ms: 13583.87
    num_steps_sampled: 29568000
    num_steps_trained: 29568000
    sample_time_ms: 98548.077
    update_time_ms: 16.196
  iterations_since_restore: 148
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.997740112994354
    ram_util_percent: 11.027683615819209
  pid: 5668
  policy_reward_max:
    agent-0: 18.0
    agent-1: 6.0
    agent-2: 18.0
    agent-3: 14.0
    agent-4: 17.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 4.81
    agent-1: 1.65
    agent-2: 5.61
    agent-3: 4.6
    agent-4: 5.92
    agent-5: 3.99
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: -42.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.577163801714175
    mean_inference_ms: 12.719145369284114
    mean_processing_ms: 57.29942194356006
  time_since_restore: 18589.32217693329
  time_this_iter_s: 124.24014711380005
  time_total_s: 41099.605419397354
  timestamp: 1637238516
  timesteps_since_restore: 14208000
  timesteps_this_iter: 96000
  timesteps_total: 29568000
  training_iteration: 308
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    308 |          41099.6 | 29568000 |    26.58 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 3.31
    apples_agent-0_min: 0
    apples_agent-1_max: 9
    apples_agent-1_mean: 1.03
    apples_agent-1_min: 0
    apples_agent-2_max: 34
    apples_agent-2_mean: 2.72
    apples_agent-2_min: 0
    apples_agent-3_max: 17
    apples_agent-3_mean: 3.63
    apples_agent-3_min: 0
    apples_agent-4_max: 10
    apples_agent-4_mean: 1.08
    apples_agent-4_min: 0
    apples_agent-5_max: 7
    apples_agent-5_mean: 2.19
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 80
    cleaning_beam_agent-0_mean: 39.12
    cleaning_beam_agent-0_min: 13
    cleaning_beam_agent-1_max: 280
    cleaning_beam_agent-1_mean: 192.75
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 116
    cleaning_beam_agent-2_mean: 26.69
    cleaning_beam_agent-2_min: 3
    cleaning_beam_agent-3_max: 101
    cleaning_beam_agent-3_mean: 40.13
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 129
    cleaning_beam_agent-4_mean: 48.11
    cleaning_beam_agent-4_min: 7
    cleaning_beam_agent-5_max: 52
    cleaning_beam_agent-5_mean: 18.02
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-30-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 68.0
  episode_reward_mean: 26.83
  episode_reward_min: 5.0
  episodes_this_iter: 96
  episodes_total: 29664
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12135.621
    learner:
      agent-0:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4979342222213745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015586630906909704
        model: {}
        policy_loss: -0.003497971687465906
        total_loss: -0.004338328260928392
        vf_explained_var: -0.005806565284729004
        vf_loss: 0.3600974678993225
      agent-1:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4539453089237213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011862419778481126
        model: {}
        policy_loss: -0.002307640388607979
        total_loss: -0.0030949851498007774
        vf_explained_var: 0.01863698661327362
        vf_loss: 0.11599130183458328
      agent-2:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4406048655509949
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011300162877887487
        model: {}
        policy_loss: -0.0033935322426259518
        total_loss: -0.004124005325138569
        vf_explained_var: -0.0028894245624542236
        vf_loss: 0.4499165713787079
      agent-3:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.578681230545044
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011189092183485627
        model: {}
        policy_loss: -0.002776241861283779
        total_loss: -0.0037525580264627934
        vf_explained_var: 0.0031628161668777466
        vf_loss: 0.4216310679912567
      agent-4:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6549856066703796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015211144927889109
        model: {}
        policy_loss: -0.0037384051829576492
        total_loss: -0.004838770721107721
        vf_explained_var: 0.02785380184650421
        vf_loss: 0.5240791440010071
      agent-5:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8969459533691406
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020046946592628956
        model: {}
        policy_loss: -0.0030287569388747215
        total_loss: -0.004577932879328728
        vf_explained_var: -0.002744346857070923
        vf_loss: 0.29450860619544983
    load_time_ms: 13578.788
    num_steps_sampled: 29664000
    num_steps_trained: 29664000
    sample_time_ms: 98612.281
    update_time_ms: 16.211
  iterations_since_restore: 149
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.407865168539324
    ram_util_percent: 11.135955056179771
  pid: 5668
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 14.0
    agent-3: 21.0
    agent-4: 14.0
    agent-5: 12.0
  policy_reward_mean:
    agent-0: 4.45
    agent-1: 1.93
    agent-2: 5.42
    agent-3: 4.92
    agent-4: 6.11
    agent-5: 4.0
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 1.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.574137260902184
    mean_inference_ms: 12.718520729796735
    mean_processing_ms: 57.29751952128272
  time_since_restore: 18714.313250541687
  time_this_iter_s: 124.99107360839844
  time_total_s: 41224.59649300575
  timestamp: 1637238641
  timesteps_since_restore: 14304000
  timesteps_this_iter: 96000
  timesteps_total: 29664000
  training_iteration: 309
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.6/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    309 |          41224.6 | 29664000 |    26.83 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 3.29
    apples_agent-0_min: 0
    apples_agent-1_max: 38
    apples_agent-1_mean: 1.82
    apples_agent-1_min: 0
    apples_agent-2_max: 35
    apples_agent-2_mean: 2.55
    apples_agent-2_min: 0
    apples_agent-3_max: 21
    apples_agent-3_mean: 3.51
    apples_agent-3_min: 0
    apples_agent-4_max: 93
    apples_agent-4_mean: 1.93
    apples_agent-4_min: 0
    apples_agent-5_max: 23
    apples_agent-5_mean: 2.49
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 89
    cleaning_beam_agent-0_mean: 42.75
    cleaning_beam_agent-0_min: 8
    cleaning_beam_agent-1_max: 286
    cleaning_beam_agent-1_mean: 187.99
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 107
    cleaning_beam_agent-2_mean: 25.23
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 41.6
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 162
    cleaning_beam_agent-4_mean: 46.33
    cleaning_beam_agent-4_min: 11
    cleaning_beam_agent-5_max: 76
    cleaning_beam_agent-5_mean: 18.9
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-32-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 76.0
  episode_reward_mean: 24.67
  episode_reward_min: -41.0
  episodes_this_iter: 96
  episodes_total: 29760
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12141.827
    learner:
      agent-0:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4946989119052887
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017234274419024587
        model: {}
        policy_loss: -0.003126099705696106
        total_loss: -0.0039598653092980385
        vf_explained_var: -0.0041366517543792725
        vf_loss: 0.3690485954284668
      agent-1:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45304396748542786
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007799056475050747
        model: {}
        policy_loss: -0.0021475558169186115
        total_loss: -0.0027983435429632664
        vf_explained_var: 0.008760228753089905
        vf_loss: 1.4656641483306885
      agent-2:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4270385205745697
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011373786255717278
        model: {}
        policy_loss: -0.0032506557181477547
        total_loss: -0.0039588650688529015
        vf_explained_var: 0.0008843094110488892
        vf_loss: 0.433780699968338
      agent-3:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5818513631820679
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012820924166589975
        model: {}
        policy_loss: -0.002313031814992428
        total_loss: -0.003162408946081996
        vf_explained_var: 0.004959389567375183
        vf_loss: 1.7468485832214355
      agent-4:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.666397750377655
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013520728098228574
        model: {}
        policy_loss: -0.003968049772083759
        total_loss: -0.005093447398394346
        vf_explained_var: 0.009334370493888855
        vf_loss: 0.47464466094970703
      agent-5:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8876773118972778
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015225366223603487
        model: {}
        policy_loss: -0.0028479155153036118
        total_loss: -0.00438421405851841
        vf_explained_var: -0.008959740400314331
        vf_loss: 0.2601208984851837
    load_time_ms: 13590.079
    num_steps_sampled: 29760000
    num_steps_trained: 29760000
    sample_time_ms: 98595.186
    update_time_ms: 16.197
  iterations_since_restore: 150
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.01412429378531
    ram_util_percent: 11.16497175141243
  pid: 5668
  policy_reward_max:
    agent-0: 13.0
    agent-1: 8.0
    agent-2: 17.0
    agent-3: 19.0
    agent-4: 15.0
    agent-5: 9.0
  policy_reward_mean:
    agent-0: 4.24
    agent-1: 1.45
    agent-2: 5.34
    agent-3: 4.15
    agent-4: 5.93
    agent-5: 3.56
  policy_reward_min:
    agent-0: 0.0
    agent-1: -50.0
    agent-2: 0.0
    agent-3: -49.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.57064859789434
    mean_inference_ms: 12.717782467771228
    mean_processing_ms: 57.29522721548427
  time_since_restore: 18839.005251169205
  time_this_iter_s: 124.6920006275177
  time_total_s: 41349.28849363327
  timestamp: 1637238766
  timesteps_since_restore: 14400000
  timesteps_this_iter: 96000
  timesteps_total: 29760000
  training_iteration: 310
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    310 |          41349.3 | 29760000 |    24.67 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 2.71
    apples_agent-0_min: 0
    apples_agent-1_max: 5
    apples_agent-1_mean: 1.05
    apples_agent-1_min: 0
    apples_agent-2_max: 34
    apples_agent-2_mean: 2.59
    apples_agent-2_min: 0
    apples_agent-3_max: 16
    apples_agent-3_mean: 3.32
    apples_agent-3_min: 0
    apples_agent-4_max: 8
    apples_agent-4_mean: 1.12
    apples_agent-4_min: 0
    apples_agent-5_max: 53
    apples_agent-5_mean: 3.11
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 97
    cleaning_beam_agent-0_mean: 45.97
    cleaning_beam_agent-0_min: 9
    cleaning_beam_agent-1_max: 279
    cleaning_beam_agent-1_mean: 192.68
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 69
    cleaning_beam_agent-2_mean: 25.6
    cleaning_beam_agent-2_min: 2
    cleaning_beam_agent-3_max: 192
    cleaning_beam_agent-3_mean: 42.91
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 260
    cleaning_beam_agent-4_mean: 48.93
    cleaning_beam_agent-4_min: 8
    cleaning_beam_agent-5_max: 76
    cleaning_beam_agent-5_mean: 17.43
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-34-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 75.0
  episode_reward_mean: 27.27
  episode_reward_min: 9.0
  episodes_this_iter: 96
  episodes_total: 29856
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12140.385
    learner:
      agent-0:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4946604073047638
        entropy_coeff: 0.0017600000137463212
        kl: 0.001477856538258493
        model: {}
        policy_loss: -0.00326257711276412
        total_loss: -0.004102876875549555
        vf_explained_var: 0.002781018614768982
        vf_loss: 0.30302634835243225
      agent-1:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45498713850975037
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009293704642914236
        model: {}
        policy_loss: -0.0023321891203522682
        total_loss: -0.0031201625242829323
        vf_explained_var: 0.033263593912124634
        vf_loss: 0.1280534565448761
      agent-2:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4277670979499817
        entropy_coeff: 0.0017600000137463212
        kl: 0.001110633835196495
        model: {}
        policy_loss: -0.0035725212655961514
        total_loss: -0.004271470941603184
        vf_explained_var: -0.00863945484161377
        vf_loss: 0.539217472076416
      agent-3:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5867489576339722
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014046067371964455
        model: {}
        policy_loss: -0.0025762326549738646
        total_loss: -0.0035673538222908974
        vf_explained_var: -0.007005453109741211
        vf_loss: 0.4155634343624115
      agent-4:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6727899312973022
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009330636821687222
        model: {}
        policy_loss: -0.003401655936613679
        total_loss: -0.004527823068201542
        vf_explained_var: 0.014898017048835754
        vf_loss: 0.5794063210487366
      agent-5:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8696909546852112
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022192769683897495
        model: {}
        policy_loss: -0.003242983017116785
        total_loss: -0.004744572099298239
        vf_explained_var: -0.0007077008485794067
        vf_loss: 0.2906568944454193
    load_time_ms: 13600.305
    num_steps_sampled: 29856000
    num_steps_trained: 29856000
    sample_time_ms: 98705.166
    update_time_ms: 16.201
  iterations_since_restore: 151
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.92722222222222
    ram_util_percent: 11.034999999999998
  pid: 5668
  policy_reward_max:
    agent-0: 12.0
    agent-1: 7.0
    agent-2: 17.0
    agent-3: 17.0
    agent-4: 27.0
    agent-5: 10.0
  policy_reward_mean:
    agent-0: 4.62
    agent-1: 1.92
    agent-2: 5.61
    agent-3: 4.81
    agent-4: 6.41
    agent-5: 3.9
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.568625882117257
    mean_inference_ms: 12.717420075612154
    mean_processing_ms: 57.293983532481846
  time_since_restore: 18964.286457538605
  time_this_iter_s: 125.28120636940002
  time_total_s: 41474.56970000267
  timestamp: 1637238892
  timesteps_since_restore: 14496000
  timesteps_this_iter: 96000
  timesteps_total: 29856000
  training_iteration: 311
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.5/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    311 |          41474.6 | 29856000 |    27.27 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


[2m[36m(pid=5668)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fa11de02518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 12
    apples_agent-0_mean: 3.57
    apples_agent-0_min: 0
    apples_agent-1_max: 7
    apples_agent-1_mean: 1.06
    apples_agent-1_min: 0
    apples_agent-2_max: 113
    apples_agent-2_mean: 3.71
    apples_agent-2_min: 0
    apples_agent-3_max: 18
    apples_agent-3_mean: 3.8
    apples_agent-3_min: 0
    apples_agent-4_max: 28
    apples_agent-4_mean: 1.88
    apples_agent-4_min: 0
    apples_agent-5_max: 15
    apples_agent-5_mean: 2.68
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 92
    cleaning_beam_agent-0_mean: 40.94
    cleaning_beam_agent-0_min: 12
    cleaning_beam_agent-1_max: 333
    cleaning_beam_agent-1_mean: 197.91
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 116
    cleaning_beam_agent-2_mean: 26.92
    cleaning_beam_agent-2_min: 4
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 41.07
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 138
    cleaning_beam_agent-4_mean: 49.18
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 53
    cleaning_beam_agent-5_mean: 16.9
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-36-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 89.0
  episode_reward_mean: 28.74
  episode_reward_min: -40.0
  episodes_this_iter: 96
  episodes_total: 29952
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12135.87
    learner:
      agent-0:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49287062883377075
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012208387488499284
        model: {}
        policy_loss: -0.002658185549080372
        total_loss: -0.003387708682566881
        vf_explained_var: 0.0035618245601654053
        vf_loss: 1.379331350326538
      agent-1:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46400946378707886
        entropy_coeff: 0.0017600000137463212
        kl: 0.001410957658663392
        model: {}
        policy_loss: -0.0024667815305292606
        total_loss: -0.0032688607461750507
        vf_explained_var: 0.024831265211105347
        vf_loss: 0.14579403400421143
      agent-2:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4281620681285858
        entropy_coeff: 0.0017600000137463212
        kl: 0.001103139016777277
        model: {}
        policy_loss: -0.003194021061062813
        total_loss: -0.0038747761864215136
        vf_explained_var: 0.001224234700202942
        vf_loss: 0.7280974388122559
      agent-3:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5814326405525208
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009460034780204296
        model: {}
        policy_loss: -0.002505386248230934
        total_loss: -0.00345922471024096
        vf_explained_var: 0.0010936856269836426
        vf_loss: 0.6948142647743225
      agent-4:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6502789258956909
        entropy_coeff: 0.0017600000137463212
        kl: 0.001470836577937007
        model: {}
        policy_loss: -0.003900212701410055
        total_loss: -0.004985782317817211
        vf_explained_var: 0.024058476090431213
        vf_loss: 0.5892072916030884
      agent-5:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.882644534111023
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023970629554241896
        model: {}
        policy_loss: -0.002883025910705328
        total_loss: -0.004394619259983301
        vf_explained_var: 0.001066550612449646
        vf_loss: 0.4186428487300873
    load_time_ms: 13608.211
    num_steps_sampled: 29952000
    num_steps_trained: 29952000
    sample_time_ms: 98762.359
    update_time_ms: 15.838
  iterations_since_restore: 152
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.092655367231643
    ram_util_percent: 11.150282485875707
  pid: 5668
  policy_reward_max:
    agent-0: 14.0
    agent-1: 8.0
    agent-2: 23.0
    agent-3: 26.0
    agent-4: 16.0
    agent-5: 15.0
  policy_reward_mean:
    agent-0: 4.52
    agent-1: 1.89
    agent-2: 6.38
    agent-3: 4.68
    agent-4: 6.89
    agent-5: 4.38
  policy_reward_min:
    agent-0: -49.0
    agent-1: 0.0
    agent-2: 1.0
    agent-3: -45.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.566360565689966
    mean_inference_ms: 12.716776565127965
    mean_processing_ms: 57.2912516746942
  time_since_restore: 19088.633930921555
  time_this_iter_s: 124.34747338294983
  time_total_s: 41598.91717338562
  timestamp: 1637239016
  timesteps_since_restore: 14592000
  timesteps_this_iter: 96000
  timesteps_total: 29952000
  training_iteration: 312
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 18.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.79 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc              |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.42:5668 |    312 |          41598.9 | 29952000 |    28.74 |
+--------------------------------------+----------+------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 3.08
    apples_agent-0_min: 0
    apples_agent-1_max: 28
    apples_agent-1_mean: 1.5
    apples_agent-1_min: 0
    apples_agent-2_max: 31
    apples_agent-2_mean: 2.49
    apples_agent-2_min: 0
    apples_agent-3_max: 15
    apples_agent-3_mean: 3.61
    apples_agent-3_min: 0
    apples_agent-4_max: 108
    apples_agent-4_mean: 2.23
    apples_agent-4_min: 0
    apples_agent-5_max: 23
    apples_agent-5_mean: 2.74
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 100
    cleaning_beam_agent-0_mean: 43.26
    cleaning_beam_agent-0_min: 14
    cleaning_beam_agent-1_max: 293
    cleaning_beam_agent-1_mean: 198.73
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 79
    cleaning_beam_agent-2_mean: 26.12
    cleaning_beam_agent-2_min: 1
    cleaning_beam_agent-3_max: 177
    cleaning_beam_agent-3_mean: 41.7
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 116
    cleaning_beam_agent-4_mean: 46.11
    cleaning_beam_agent-4_min: 9
    cleaning_beam_agent-5_max: 48
    cleaning_beam_agent-5_mean: 15.42
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-18_07-39-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 64.0
  episode_reward_mean: 28.57
  episode_reward_min: 8.0
  episodes_this_iter: 96
  episodes_total: 30048
  experiment_id: 7cf1a08f96194b599b8fb41ae21ffd0d
  experiment_tag: '0'
  hostname: gpu042
  info:
    grad_time_ms: 12136.357
    learner:
      agent-0:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4896002411842346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016607497818768024
        model: {}
        policy_loss: -0.003450736403465271
        total_loss: -0.004273613449186087
        vf_explained_var: 0.0012493729591369629
        vf_loss: 0.3881836235523224
      agent-1:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4568547010421753
        entropy_coeff: 0.0017600000137463212
        kl: 0.001087580225430429
        model: {}
        policy_loss: -0.0021231970749795437
        total_loss: -0.0029134913347661495
        vf_explained_var: 0.030708640813827515
        vf_loss: 0.1376972794532776
      agent-2:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4295237362384796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010737793054431677
        model: {}
        policy_loss: -0.0031664539128541946
        total_loss: -0.0038575679063796997
        vf_explained_var: -0.001848652958869934
        vf_loss: 0.6484804153442383
      agent-3:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5974791049957275
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008970366325229406
        model: {}
        policy_loss: -0.0028124796226620674
        total_loss: -0.0038181114941835403
        vf_explained_var: -0.0055290162563323975
        vf_loss: 0.45930802822113037
      agent-4:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6663693189620972
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019071849528700113
        model: {}
        policy_loss: -0.0040040528401732445
        total_loss: -0.0051251305267214775
        vf_explained_var: 0.021326303482055664
        vf_loss: 0.5173207521438599
      agent-5:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8795656561851501
        entropy_coeff: 0.0017600000137463212
        kl: 0.001823748112656176
        model: {}
        policy_loss: -0.002782384864985943
        total_loss: -0.004296993836760521
        vf_explained_var: 0.005120337009429932
        vf_loss: 0.33427008986473083
    load_time_ms: 13592.126
    num_steps_sampled: 30048000
    num_steps_trained: 30048000
    sample_time_ms: 98682.115
    update_time_ms: 15.863
  iterations_since_restore: 153
  node_ip: 172.17.8.42
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.070056497175138
    ram_util_percent: 11.085310734463274
  pid: 5668
  policy_reward_max:
    agent-0: 12.0
    agent-1: 9.0
    agent-2: 18.0
    agent-3: 15.0
    agent-4: 17.0
    agent-5: 14.0
  policy_reward_mean:
    agent-0: 4.84
    agent-1: 1.96
    agent-2: 6.19
    agent-3: 4.99
    agent-4: 6.32
    agent-5: 4.27
  policy_reward_min:
    agent-0: 0.0
    agent-1: 0.0
    agent-2: 0.0
    agent-3: 0.0
    agent-4: 0.0
    agent-5: 0.0
  sampler_perf:
    mean_env_wait_ms: 22.563379573122365
    mean_inference_ms: 12.71609315393258
    mean_processing_ms: 57.289120410273426
  time_since_restore: 19213.149778842926
  time_this_iter_s: 124.51584792137146
  time_total_s: 41723.43302130699
  timestamp: 1637239141
  timesteps_since_restore: 14688000
  timesteps_this_iter: 96000
  timesteps_total: 30048000
  training_iteration: 313
  trial_id: '00000'
  