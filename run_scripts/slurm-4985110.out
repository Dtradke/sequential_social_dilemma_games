>>> RESTORING FROM:  /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-15_17-00-45_bf59qjr/checkpoint_140
== Status ==
Memory usage on this node: 21.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+---------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |     80 |          12550.8 | 7680000 |   547.98 |
+--------------------------------------+----------+-------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m 2021-11-15 22:55:40,436	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=14340)[0m 2021-11-15 22:55:40,453	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=14340)[0m 2021-11-15 22:57:39,375	INFO trainable.py:180 -- _setup took 118.938 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=14340)[0m 2021-11-15 22:57:39,375	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=14340)[0m 2021-11-15 22:57:39,375	WARNING util.py:37 -- Install gputil for GPU system monitoring.
[2m[36m(pid=14340)[0m 2021-11-15 22:57:42,508	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=14340)[0m 2021-11-15 22:57:42,508	INFO trainable.py:423 -- Restored on 172.17.8.35 from checkpoint: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-15_17-00-45_bf59qjr/tmp7emsrra6restore_from_object/checkpoint-80
[2m[36m(pid=14340)[0m 2021-11-15 22:57:42,508	INFO trainable.py:430 -- Current state after restoring: {'_iteration': 80, '_timesteps_total': 7680000, '_time_total': 12550.816871404648, '_episodes_total': 7680}
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+---------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |     80 |          12550.8 | 7680000 |   547.98 |
+--------------------------------------+----------+-------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 6.895833333333333
    apples_agent-0_min: 0
    apples_agent-1_max: 154
    apples_agent-1_mean: 23.822916666666668
    apples_agent-1_min: 0
    apples_agent-2_max: 148
    apples_agent-2_mean: 14.3125
    apples_agent-2_min: 0
    apples_agent-3_max: 191
    apples_agent-3_mean: 96.02083333333333
    apples_agent-3_min: 17
    apples_agent-4_max: 57
    apples_agent-4_mean: 4.010416666666667
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 72.25
    apples_agent-5_min: 9
    cleaning_beam_agent-0_max: 460
    cleaning_beam_agent-0_mean: 298.2395833333333
    cleaning_beam_agent-0_min: 129
    cleaning_beam_agent-1_max: 532
    cleaning_beam_agent-1_mean: 272.8125
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 553
    cleaning_beam_agent-2_mean: 308.5416666666667
    cleaning_beam_agent-2_min: 26
    cleaning_beam_agent-3_max: 170
    cleaning_beam_agent-3_mean: 70.45833333333333
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 559
    cleaning_beam_agent-4_mean: 374.6041666666667
    cleaning_beam_agent-4_min: 196
    cleaning_beam_agent-5_max: 339
    cleaning_beam_agent-5_mean: 92.25
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 5
    fire_beam_agent-0_mean: 0.15625
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03125
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.010416666666666666
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.010416666666666666
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 29
    fire_beam_agent-5_mean: 0.6770833333333334
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-01-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 746.9999999999808
  episode_reward_mean: 540.6041666666674
  episode_reward_min: -264.9999999999983
  episodes_this_iter: 96
  episodes_total: 7776
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 24192.443
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 1.3283202648162842
        entropy_coeff: 0.0017600000137463212
        kl: 0.008476031944155693
        model: {}
        policy_loss: -0.01969384215772152
        total_loss: -0.018079152330756187
        vf_explained_var: 0.06296484172344208
        vf_loss: 22.57326316833496
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 1.201817512512207
        entropy_coeff: 0.0017600000137463212
        kl: 0.011828796938061714
        model: {}
        policy_loss: -0.026134110987186432
        total_loss: -0.02354559488594532
        vf_explained_var: 0.02761131525039673
        vf_loss: 23.37955093383789
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 1.1884686946868896
        entropy_coeff: 0.0017600000137463212
        kl: 0.010994712822139263
        model: {}
        policy_loss: -0.024338457733392715
        total_loss: -0.02205922082066536
        vf_explained_var: 0.09874622523784637
        vf_loss: 21.72002410888672
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 0.9701591730117798
        entropy_coeff: 0.0017600000137463212
        kl: 0.008781427517533302
        model: {}
        policy_loss: -0.019348561763763428
        total_loss: -0.01713288202881813
        vf_explained_var: 0.10444049537181854
        vf_loss: 21.66874885559082
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 1.1015218496322632
        entropy_coeff: 0.0017600000137463212
        kl: 0.012524912133812904
        model: {}
        policy_loss: -0.029770594090223312
        total_loss: -0.026938416063785553
        vf_explained_var: 0.05698651075363159
        vf_loss: 22.65872573852539
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 1.171739935874939
        entropy_coeff: 0.0017600000137463212
        kl: 0.011474035680294037
        model: {}
        policy_loss: -0.029577236622571945
        total_loss: -0.0273447223007679
        vf_explained_var: 0.1677737981081009
        vf_loss: 19.999706268310547
    load_time_ms: 60707.093
    num_steps_sampled: 7776000
    num_steps_trained: 7776000
    sample_time_ms: 124938.861
    update_time_ms: 4171.657
  iterations_since_restore: 1
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.527076923076923
    ram_util_percent: 16.38861538461538
  pid: 14340
  policy_reward_max:
    agent-0: 124.50000000000058
    agent-1: 124.50000000000058
    agent-2: 124.50000000000058
    agent-3: 124.50000000000058
    agent-4: 124.50000000000058
    agent-5: 124.50000000000058
  policy_reward_mean:
    agent-0: 90.10069444444468
    agent-1: 90.10069444444468
    agent-2: 90.10069444444468
    agent-3: 90.10069444444468
    agent-4: 90.10069444444468
    agent-5: 90.10069444444468
  policy_reward_min:
    agent-0: -44.16666666666612
    agent-1: -44.16666666666612
    agent-2: -44.16666666666612
    agent-3: -44.16666666666612
    agent-4: -44.16666666666612
    agent-5: -44.16666666666612
  sampler_perf:
    mean_env_wait_ms: 30.537948106631735
    mean_inference_ms: 15.35641670703412
    mean_processing_ms: 65.43282167616981
  time_since_restore: 217.86757516860962
  time_this_iter_s: 217.86757516860962
  time_total_s: 12768.684446573257
  timestamp: 1637035286
  timesteps_since_restore: 96000
  timesteps_this_iter: 96000
  timesteps_total: 7776000
  training_iteration: 81
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     81 |          12768.7 | 7776000 |  540.604 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 101
    apples_agent-0_mean: 7.2
    apples_agent-0_min: 0
    apples_agent-1_max: 131
    apples_agent-1_mean: 20.8
    apples_agent-1_min: 0
    apples_agent-2_max: 113
    apples_agent-2_mean: 11.65
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 89.01
    apples_agent-3_min: 25
    apples_agent-4_max: 85
    apples_agent-4_mean: 5.44
    apples_agent-4_min: 0
    apples_agent-5_max: 116
    apples_agent-5_mean: 77.63
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 323.34
    cleaning_beam_agent-0_min: 188
    cleaning_beam_agent-1_max: 544
    cleaning_beam_agent-1_mean: 271.05
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 672
    cleaning_beam_agent-2_mean: 345.74
    cleaning_beam_agent-2_min: 60
    cleaning_beam_agent-3_max: 229
    cleaning_beam_agent-3_mean: 68.46
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 562
    cleaning_beam_agent-4_mean: 351.39
    cleaning_beam_agent-4_min: 132
    cleaning_beam_agent-5_max: 284
    cleaning_beam_agent-5_mean: 84.77
    cleaning_beam_agent-5_min: 22
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 7
    fire_beam_agent-5_mean: 0.2
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-04-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 832.9999999999898
  episode_reward_mean: 578.2499999999997
  episode_reward_min: 130.00000000000085
  episodes_this_iter: 96
  episodes_total: 7872
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 18898.447
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 1.3281121253967285
        entropy_coeff: 0.0017600000137463212
        kl: 0.009633617475628853
        model: {}
        policy_loss: -0.02503718063235283
        total_loss: -0.023993147537112236
        vf_explained_var: 0.08526170253753662
        vf_loss: 14.547890663146973
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 1.2141344547271729
        entropy_coeff: 0.0017600000137463212
        kl: 0.012655933387577534
        model: {}
        policy_loss: -0.03074066899716854
        total_loss: -0.028853360563516617
        vf_explained_var: 0.06126981973648071
        vf_loss: 14.929967880249023
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 1.1631884574890137
        entropy_coeff: 0.0017600000137463212
        kl: 0.011608880013227463
        model: {}
        policy_loss: -0.027321726083755493
        total_loss: -0.025678623467683792
        vf_explained_var: 0.13928484916687012
        vf_loss: 13.68541431427002
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 0.9254370331764221
        entropy_coeff: 0.0017600000137463212
        kl: 0.009232066571712494
        model: {}
        policy_loss: -0.022939195856451988
        total_loss: -0.021458633244037628
        vf_explained_var: 0.20621289312839508
        vf_loss: 12.62918758392334
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 1.1188795566558838
        entropy_coeff: 0.0017600000137463212
        kl: 0.01363588497042656
        model: {}
        policy_loss: -0.035402823239564896
        total_loss: -0.03326427564024925
        vf_explained_var: 0.13214102387428284
        vf_loss: 13.805980682373047
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 1.148449420928955
        entropy_coeff: 0.0017600000137463212
        kl: 0.013314556330442429
        model: {}
        policy_loss: -0.03406217321753502
        total_loss: -0.032105594873428345
        vf_explained_var: 0.1740650236606598
        vf_loss: 13.14936637878418
    load_time_ms: 46281.612
    num_steps_sampled: 7872000
    num_steps_trained: 7872000
    sample_time_ms: 141159.27
    update_time_ms: 2159.234
  iterations_since_restore: 2
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.792150170648465
    ram_util_percent: 17.257337883959046
  pid: 14340
  policy_reward_max:
    agent-0: 138.83333333333354
    agent-1: 138.83333333333354
    agent-2: 138.83333333333354
    agent-3: 138.83333333333354
    agent-4: 138.83333333333354
    agent-5: 138.83333333333354
  policy_reward_mean:
    agent-0: 96.37500000000026
    agent-1: 96.37500000000026
    agent-2: 96.37500000000026
    agent-3: 96.37500000000026
    agent-4: 96.37500000000026
    agent-5: 96.37500000000026
  policy_reward_min:
    agent-0: 21.666666666666686
    agent-1: 21.666666666666686
    agent-2: 21.666666666666686
    agent-3: 21.666666666666686
    agent-4: 21.666666666666686
    agent-5: 21.666666666666686
  sampler_perf:
    mean_env_wait_ms: 30.56180863649486
    mean_inference_ms: 14.746792362733753
    mean_processing_ms: 65.05219688726798
  time_since_restore: 420.9886951446533
  time_this_iter_s: 203.1211199760437
  time_total_s: 12971.805566549301
  timestamp: 1637035492
  timesteps_since_restore: 192000
  timesteps_this_iter: 96000
  timesteps_total: 7872000
  training_iteration: 82
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     82 |          12971.8 | 7872000 |   578.25 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 3.47
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 23.91
    apples_agent-1_min: 0
    apples_agent-2_max: 243
    apples_agent-2_mean: 14.9
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 90.45
    apples_agent-3_min: 36
    apples_agent-4_max: 128
    apples_agent-4_mean: 8.41
    apples_agent-4_min: 0
    apples_agent-5_max: 132
    apples_agent-5_mean: 79.22
    apples_agent-5_min: 9
    cleaning_beam_agent-0_max: 486
    cleaning_beam_agent-0_mean: 355.45
    cleaning_beam_agent-0_min: 138
    cleaning_beam_agent-1_max: 503
    cleaning_beam_agent-1_mean: 248.09
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 605
    cleaning_beam_agent-2_mean: 323.07
    cleaning_beam_agent-2_min: 50
    cleaning_beam_agent-3_max: 180
    cleaning_beam_agent-3_mean: 82.14
    cleaning_beam_agent-3_min: 39
    cleaning_beam_agent-4_max: 540
    cleaning_beam_agent-4_mean: 354.14
    cleaning_beam_agent-4_min: 124
    cleaning_beam_agent-5_max: 218
    cleaning_beam_agent-5_mean: 82.58
    cleaning_beam_agent-5_min: 30
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 10
    fire_beam_agent-5_mean: 0.33
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-08-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 844.9999999999832
  episode_reward_mean: 593.5399999999989
  episode_reward_min: 169.99999999999932
  episodes_this_iter: 96
  episodes_total: 7968
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 17034.271
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 1.3368027210235596
        entropy_coeff: 0.0017600000137463212
        kl: 0.009749378077685833
        model: {}
        policy_loss: -0.024166874587535858
        total_loss: -0.023220539093017578
        vf_explained_var: 0.09518814086914062
        vf_loss: 13.492298126220703
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 1.2023684978485107
        entropy_coeff: 0.0017600000137463212
        kl: 0.01241432223469019
        model: {}
        policy_loss: -0.03169064223766327
        total_loss: -0.02986826002597809
        vf_explained_var: 0.024774551391601562
        vf_loss: 14.556861877441406
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 1.1584101915359497
        entropy_coeff: 0.0017600000137463212
        kl: 0.011963375844061375
        model: {}
        policy_loss: -0.025555990636348724
        total_loss: -0.023855971172451973
        vf_explained_var: 0.09813626110553741
        vf_loss: 13.461443901062012
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 0.9245859384536743
        entropy_coeff: 0.0017600000137463212
        kl: 0.009791220538318157
        model: {}
        policy_loss: -0.022528700530529022
        total_loss: -0.020968414843082428
        vf_explained_var: 0.17690111696720123
        vf_loss: 12.293144226074219
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 1.1091315746307373
        entropy_coeff: 0.0017600000137463212
        kl: 0.013667793944478035
        model: {}
        policy_loss: -0.0353303961455822
        total_loss: -0.03332101181149483
        vf_explained_var: 0.17735818028450012
        vf_loss: 12.278985977172852
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 1.1419233083724976
        entropy_coeff: 0.0017600000137463212
        kl: 0.013188977725803852
        model: {}
        policy_loss: -0.033150918781757355
        total_loss: -0.03127312660217285
        vf_explained_var: 0.1633133888244629
        vf_loss: 12.497785568237305
    load_time_ms: 42945.777
    num_steps_sampled: 7968000
    num_steps_trained: 7968000
    sample_time_ms: 145969.993
    update_time_ms: 1459.475
  iterations_since_restore: 3
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.867346938775512
    ram_util_percent: 17.185714285714287
  pid: 14340
  policy_reward_max:
    agent-0: 140.8333333333337
    agent-1: 140.8333333333337
    agent-2: 140.8333333333337
    agent-3: 140.8333333333337
    agent-4: 140.8333333333337
    agent-5: 140.8333333333337
  policy_reward_mean:
    agent-0: 98.92333333333362
    agent-1: 98.92333333333362
    agent-2: 98.92333333333362
    agent-3: 98.92333333333362
    agent-4: 98.92333333333362
    agent-5: 98.92333333333362
  policy_reward_min:
    agent-0: 28.333333333333385
    agent-1: 28.333333333333385
    agent-2: 28.333333333333385
    agent-3: 28.333333333333385
    agent-4: 28.333333333333385
    agent-5: 28.333333333333385
  sampler_perf:
    mean_env_wait_ms: 30.658222824155978
    mean_inference_ms: 14.59119765984908
    mean_processing_ms: 65.14440210904186
  time_since_restore: 626.3464381694794
  time_this_iter_s: 205.35774302482605
  time_total_s: 13177.163309574127
  timestamp: 1637035699
  timesteps_since_restore: 288000
  timesteps_this_iter: 96000
  timesteps_total: 7968000
  training_iteration: 83
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     83 |          13177.2 | 7968000 |   593.54 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 4.03
    apples_agent-0_min: 0
    apples_agent-1_max: 127
    apples_agent-1_mean: 22.29
    apples_agent-1_min: 0
    apples_agent-2_max: 194
    apples_agent-2_mean: 16.29
    apples_agent-2_min: 0
    apples_agent-3_max: 182
    apples_agent-3_mean: 95.39
    apples_agent-3_min: 40
    apples_agent-4_max: 63
    apples_agent-4_mean: 3.97
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 78.09
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 475
    cleaning_beam_agent-0_mean: 336.47
    cleaning_beam_agent-0_min: 202
    cleaning_beam_agent-1_max: 442
    cleaning_beam_agent-1_mean: 223.44
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 614
    cleaning_beam_agent-2_mean: 350.64
    cleaning_beam_agent-2_min: 53
    cleaning_beam_agent-3_max: 155
    cleaning_beam_agent-3_mean: 79.88
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 529
    cleaning_beam_agent-4_mean: 355.3
    cleaning_beam_agent-4_min: 178
    cleaning_beam_agent-5_max: 281
    cleaning_beam_agent-5_mean: 93.29
    cleaning_beam_agent-5_min: 32
    fire_beam_agent-0_max: 6
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-11-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 854.999999999984
  episode_reward_mean: 588.6300000000006
  episode_reward_min: 279.9999999999968
  episodes_this_iter: 96
  episodes_total: 8064
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 16115.837
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 1.3232595920562744
        entropy_coeff: 0.0017600000137463212
        kl: 0.010095841251313686
        model: {}
        policy_loss: -0.024926424026489258
        total_loss: -0.023991581052541733
        vf_explained_var: 0.0886654257774353
        vf_loss: 12.446111679077148
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 1.217033863067627
        entropy_coeff: 0.0017600000137463212
        kl: 0.013423656113445759
        model: {}
        policy_loss: -0.03219201788306236
        total_loss: -0.03036927618086338
        vf_explained_var: 0.061821699142456055
        vf_loss: 12.79990291595459
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 1.1204551458358765
        entropy_coeff: 0.0017600000137463212
        kl: 0.012358282692730427
        model: {}
        policy_loss: -0.026157252490520477
        total_loss: -0.024401569738984108
        vf_explained_var: 0.07893486320972443
        vf_loss: 12.560251235961914
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 0.9117593169212341
        entropy_coeff: 0.0017600000137463212
        kl: 0.009594925679266453
        model: {}
        policy_loss: -0.02163694240152836
        total_loss: -0.0201692096889019
        vf_explained_var: 0.15446558594703674
        vf_loss: 11.534405708312988
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 1.0920366048812866
        entropy_coeff: 0.0017600000137463212
        kl: 0.013981921598315239
        model: {}
        policy_loss: -0.03521332889795303
        total_loss: -0.033137015998363495
        vf_explained_var: 0.1200653463602066
        vf_loss: 12.019098281860352
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 1.1589046716690063
        entropy_coeff: 0.0017600000137463212
        kl: 0.012552037835121155
        model: {}
        policy_loss: -0.03328878805041313
        total_loss: -0.03168538957834244
        vf_explained_var: 0.17020343244075775
        vf_loss: 11.326591491699219
    load_time_ms: 41147.745
    num_steps_sampled: 8064000
    num_steps_trained: 8064000
    sample_time_ms: 148480.731
    update_time_ms: 1111.811
  iterations_since_restore: 4
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.65205479452055
    ram_util_percent: 17.10239726027397
  pid: 14340
  policy_reward_max:
    agent-0: 142.50000000000003
    agent-1: 142.50000000000003
    agent-2: 142.50000000000003
    agent-3: 142.50000000000003
    agent-4: 142.50000000000003
    agent-5: 142.50000000000003
  policy_reward_mean:
    agent-0: 98.10500000000026
    agent-1: 98.10500000000026
    agent-2: 98.10500000000026
    agent-3: 98.10500000000026
    agent-4: 98.10500000000026
    agent-5: 98.10500000000026
  policy_reward_min:
    agent-0: 46.666666666666615
    agent-1: 46.666666666666615
    agent-2: 46.666666666666615
    agent-3: 46.666666666666615
    agent-4: 46.666666666666615
    agent-5: 46.666666666666615
  sampler_perf:
    mean_env_wait_ms: 30.63464011420686
    mean_inference_ms: 14.442372615127065
    mean_processing_ms: 65.03411480235472
  time_since_restore: 831.6688032150269
  time_this_iter_s: 205.32236504554749
  time_total_s: 13382.485674619675
  timestamp: 1637035904
  timesteps_since_restore: 384000
  timesteps_this_iter: 96000
  timesteps_total: 8064000
  training_iteration: 84
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     84 |          13382.5 | 8064000 |   588.63 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 5.28
    apples_agent-0_min: 0
    apples_agent-1_max: 128
    apples_agent-1_mean: 22.63
    apples_agent-1_min: 0
    apples_agent-2_max: 114
    apples_agent-2_mean: 9.47
    apples_agent-2_min: 0
    apples_agent-3_max: 212
    apples_agent-3_mean: 88.1
    apples_agent-3_min: 25
    apples_agent-4_max: 104
    apples_agent-4_mean: 7.58
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 77.71
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 535
    cleaning_beam_agent-0_mean: 333.53
    cleaning_beam_agent-0_min: 212
    cleaning_beam_agent-1_max: 484
    cleaning_beam_agent-1_mean: 232.25
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 684
    cleaning_beam_agent-2_mean: 374.84
    cleaning_beam_agent-2_min: 131
    cleaning_beam_agent-3_max: 222
    cleaning_beam_agent-3_mean: 90.01
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 538
    cleaning_beam_agent-4_mean: 342.05
    cleaning_beam_agent-4_min: 129
    cleaning_beam_agent-5_max: 429
    cleaning_beam_agent-5_mean: 93.49
    cleaning_beam_agent-5_min: 30
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 18
    fire_beam_agent-5_mean: 0.2
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-15-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 795.999999999987
  episode_reward_mean: 556.2000000000011
  episode_reward_min: 199.99999999999875
  episodes_this_iter: 96
  episodes_total: 8160
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 15599.58
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 1.310241937637329
        entropy_coeff: 0.0017600000137463212
        kl: 0.01127336174249649
        model: {}
        policy_loss: -0.026563655585050583
        total_loss: -0.025248849764466286
        vf_explained_var: 0.05470278859138489
        vf_loss: 13.66159439086914
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 1.1982845067977905
        entropy_coeff: 0.0017600000137463212
        kl: 0.014755554497241974
        model: {}
        policy_loss: -0.03170142322778702
        total_loss: -0.029483351856470108
        vf_explained_var: 0.04762008786201477
        vf_loss: 13.759419441223145
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 1.1247515678405762
        entropy_coeff: 0.0017600000137463212
        kl: 0.01222684420645237
        model: {}
        policy_loss: -0.028319574892520905
        total_loss: -0.026562226936221123
        vf_explained_var: 0.10621744394302368
        vf_loss: 12.915471076965332
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 0.954042911529541
        entropy_coeff: 0.0017600000137463212
        kl: 0.010268238373100758
        model: {}
        policy_loss: -0.023726465180516243
        total_loss: -0.022202644497156143
        vf_explained_var: 0.20395450294017792
        vf_loss: 11.492849349975586
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 1.0969293117523193
        entropy_coeff: 0.0017600000137463212
        kl: 0.013846155256032944
        model: {}
        policy_loss: -0.03536270558834076
        total_loss: -0.03323905169963837
        vf_explained_var: 0.11082583665847778
        vf_loss: 12.850156784057617
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 1.1717705726623535
        entropy_coeff: 0.0017600000137463212
        kl: 0.01457605604082346
        model: {}
        policy_loss: -0.032463088631629944
        total_loss: -0.030414117500185966
        vf_explained_var: 0.17192892730236053
        vf_loss: 11.960698127746582
    load_time_ms: 41151.035
    num_steps_sampled: 8160000
    num_steps_trained: 8160000
    sample_time_ms: 148107.74
    update_time_ms: 899.903
  iterations_since_restore: 5
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.609722222222224
    ram_util_percent: 17.103125000000002
  pid: 14340
  policy_reward_max:
    agent-0: 132.66666666666706
    agent-1: 132.66666666666706
    agent-2: 132.66666666666706
    agent-3: 132.66666666666706
    agent-4: 132.66666666666706
    agent-5: 132.66666666666706
  policy_reward_mean:
    agent-0: 92.70000000000023
    agent-1: 92.70000000000023
    agent-2: 92.70000000000023
    agent-3: 92.70000000000023
    agent-4: 92.70000000000023
    agent-5: 92.70000000000023
  policy_reward_min:
    agent-0: 33.333333333333364
    agent-1: 33.333333333333364
    agent-2: 33.333333333333364
    agent-3: 33.333333333333364
    agent-4: 33.333333333333364
    agent-5: 33.333333333333364
  sampler_perf:
    mean_env_wait_ms: 30.670197572857163
    mean_inference_ms: 14.36825619425617
    mean_processing_ms: 65.06856366522223
  time_since_restore: 1033.1703479290009
  time_this_iter_s: 201.501544713974
  time_total_s: 13583.987219333649
  timestamp: 1637036106
  timesteps_since_restore: 480000
  timesteps_this_iter: 96000
  timesteps_total: 8160000
  training_iteration: 85
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     85 |            13584 | 8160000 |    556.2 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 65
    apples_agent-0_mean: 6.1
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 22.36
    apples_agent-1_min: 0
    apples_agent-2_max: 163
    apples_agent-2_mean: 12.31
    apples_agent-2_min: 0
    apples_agent-3_max: 169
    apples_agent-3_mean: 86.19
    apples_agent-3_min: 26
    apples_agent-4_max: 114
    apples_agent-4_mean: 4.74
    apples_agent-4_min: 0
    apples_agent-5_max: 127
    apples_agent-5_mean: 78.49
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 324.78
    cleaning_beam_agent-0_min: 142
    cleaning_beam_agent-1_max: 410
    cleaning_beam_agent-1_mean: 216.16
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 727
    cleaning_beam_agent-2_mean: 378.1
    cleaning_beam_agent-2_min: 143
    cleaning_beam_agent-3_max: 168
    cleaning_beam_agent-3_mean: 87.38
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 520
    cleaning_beam_agent-4_mean: 346.03
    cleaning_beam_agent-4_min: 175
    cleaning_beam_agent-5_max: 378
    cleaning_beam_agent-5_mean: 105.53
    cleaning_beam_agent-5_min: 35
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 9
    fire_beam_agent-5_mean: 0.12
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-18-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 781.9999999999811
  episode_reward_mean: 571.7400000000002
  episode_reward_min: 137.00000000000102
  episodes_this_iter: 96
  episodes_total: 8256
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 15211.142
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 1.3110851049423218
        entropy_coeff: 0.0017600000137463212
        kl: 0.01034073531627655
        model: {}
        policy_loss: -0.02608707919716835
        total_loss: -0.02508765086531639
        vf_explained_var: 0.12792648375034332
        vf_loss: 12.387886047363281
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 1.2323211431503296
        entropy_coeff: 0.0017600000137463212
        kl: 0.013352276757359505
        model: {}
        policy_loss: -0.031280361115932465
        total_loss: -0.029444795101881027
        vf_explained_var: 0.060442179441452026
        vf_loss: 13.340017318725586
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 1.134739875793457
        entropy_coeff: 0.0017600000137463212
        kl: 0.01316145807504654
        model: {}
        policy_loss: -0.02892819233238697
        total_loss: -0.027044333517551422
        vf_explained_var: 0.1205080896615982
        vf_loss: 12.48706340789795
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 0.9266349077224731
        entropy_coeff: 0.0017600000137463212
        kl: 0.010241338983178139
        model: {}
        policy_loss: -0.022792145609855652
        total_loss: -0.021216312423348427
        vf_explained_var: 0.18427333235740662
        vf_loss: 11.584442138671875
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 1.087985873222351
        entropy_coeff: 0.0017600000137463212
        kl: 0.013431533239781857
        model: {}
        policy_loss: -0.03246809169650078
        total_loss: -0.03044387325644493
        vf_explained_var: 0.11767785251140594
        vf_loss: 12.527693748474121
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 1.1539568901062012
        entropy_coeff: 0.0017600000137463212
        kl: 0.013579430989921093
        model: {}
        policy_loss: -0.03401051461696625
        total_loss: -0.03212574124336243
        vf_explained_var: 0.1552293449640274
        vf_loss: 11.998520851135254
    load_time_ms: 40744.618
    num_steps_sampled: 8256000
    num_steps_trained: 8256000
    sample_time_ms: 146926.714
    update_time_ms: 762.076
  iterations_since_restore: 6
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.465818181818182
    ram_util_percent: 16.999636363636363
  pid: 14340
  policy_reward_max:
    agent-0: 130.33333333333388
    agent-1: 130.33333333333388
    agent-2: 130.33333333333388
    agent-3: 130.33333333333388
    agent-4: 130.33333333333388
    agent-5: 130.33333333333388
  policy_reward_mean:
    agent-0: 95.29000000000025
    agent-1: 95.29000000000025
    agent-2: 95.29000000000025
    agent-3: 95.29000000000025
    agent-4: 95.29000000000025
    agent-5: 95.29000000000025
  policy_reward_min:
    agent-0: 22.833333333333343
    agent-1: 22.833333333333343
    agent-2: 22.833333333333343
    agent-3: 22.833333333333343
    agent-4: 22.833333333333343
    agent-5: 22.833333333333343
  sampler_perf:
    mean_env_wait_ms: 30.678144367420952
    mean_inference_ms: 14.32546513749564
    mean_processing_ms: 65.01858695029006
  time_since_restore: 1226.4227476119995
  time_this_iter_s: 193.25239968299866
  time_total_s: 13777.239619016647
  timestamp: 1637036299
  timesteps_since_restore: 576000
  timesteps_this_iter: 96000
  timesteps_total: 8256000
  training_iteration: 86
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     86 |          13777.2 | 8256000 |   571.74 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 65
    apples_agent-0_mean: 7.89
    apples_agent-0_min: 0
    apples_agent-1_max: 127
    apples_agent-1_mean: 23.79
    apples_agent-1_min: 0
    apples_agent-2_max: 86
    apples_agent-2_mean: 7.8
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 83.69
    apples_agent-3_min: 31
    apples_agent-4_max: 68
    apples_agent-4_mean: 6.13
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 75.52
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 539
    cleaning_beam_agent-0_mean: 315.63
    cleaning_beam_agent-0_min: 90
    cleaning_beam_agent-1_max: 407
    cleaning_beam_agent-1_mean: 233.31
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 632
    cleaning_beam_agent-2_mean: 387.38
    cleaning_beam_agent-2_min: 150
    cleaning_beam_agent-3_max: 147
    cleaning_beam_agent-3_mean: 79.89
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 523
    cleaning_beam_agent-4_mean: 351.16
    cleaning_beam_agent-4_min: 151
    cleaning_beam_agent-5_max: 387
    cleaning_beam_agent-5_mean: 103.44
    cleaning_beam_agent-5_min: 32
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 18
    fire_beam_agent-5_mean: 0.24
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-21-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 768.9999999999917
  episode_reward_mean: 554.5600000000012
  episode_reward_min: -14.999999999993376
  episodes_this_iter: 96
  episodes_total: 8352
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 14947.654
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 1.283984661102295
        entropy_coeff: 0.0017600000137463212
        kl: 0.010262561030685902
        model: {}
        policy_loss: -0.02595517411828041
        total_loss: -0.02450592815876007
        vf_explained_var: 0.04050932824611664
        vf_loss: 16.565486907958984
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 1.2237441539764404
        entropy_coeff: 0.0017600000137463212
        kl: 0.012134160846471786
        model: {}
        policy_loss: -0.029561230912804604
        total_loss: -0.027662096545100212
        vf_explained_var: 0.05884377658367157
        vf_loss: 16.260915756225586
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 1.1402071714401245
        entropy_coeff: 0.0017600000137463212
        kl: 0.011804696172475815
        model: {}
        policy_loss: -0.02758469060063362
        total_loss: -0.025582771748304367
        vf_explained_var: 0.04459992051124573
        vf_loss: 16.47742462158203
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 0.9253990054130554
        entropy_coeff: 0.0017600000137463212
        kl: 0.009143449366092682
        model: {}
        policy_loss: -0.020726807415485382
        total_loss: -0.019076773896813393
        vf_explained_var: 0.1648862659931183
        vf_loss: 14.500450134277344
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 1.1071290969848633
        entropy_coeff: 0.0017600000137463212
        kl: 0.013978068716824055
        model: {}
        policy_loss: -0.032897450029850006
        total_loss: -0.030516985803842545
        vf_explained_var: 0.11367000639438629
        vf_loss: 15.333969116210938
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 1.1550614833831787
        entropy_coeff: 0.0017600000137463212
        kl: 0.012415454722940922
        model: {}
        policy_loss: -0.03295270353555679
        total_loss: -0.031134724617004395
        vf_explained_var: 0.20750810205936432
        vf_loss: 13.677960395812988
    load_time_ms: 40983.326
    num_steps_sampled: 8352000
    num_steps_trained: 8352000
    sample_time_ms: 146227.28
    update_time_ms: 664.006
  iterations_since_restore: 7
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.58763250883392
    ram_util_percent: 16.99575971731449
  pid: 14340
  policy_reward_max:
    agent-0: 128.1666666666672
    agent-1: 128.1666666666672
    agent-2: 128.1666666666672
    agent-3: 128.1666666666672
    agent-4: 128.1666666666672
    agent-5: 128.1666666666672
  policy_reward_mean:
    agent-0: 92.4266666666669
    agent-1: 92.4266666666669
    agent-2: 92.4266666666669
    agent-3: 92.4266666666669
    agent-4: 92.4266666666669
    agent-5: 92.4266666666669
  policy_reward_min:
    agent-0: -2.500000000000087
    agent-1: -2.500000000000087
    agent-2: -2.500000000000087
    agent-3: -2.500000000000087
    agent-4: -2.500000000000087
    agent-5: -2.500000000000087
  sampler_perf:
    mean_env_wait_ms: 30.701820821909358
    mean_inference_ms: 14.288562437445623
    mean_processing_ms: 65.0013481183308
  time_since_restore: 1424.4533371925354
  time_this_iter_s: 198.0305895805359
  time_total_s: 13975.270208597183
  timestamp: 1637036498
  timesteps_since_restore: 672000
  timesteps_this_iter: 96000
  timesteps_total: 8352000
  training_iteration: 87
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     87 |          13975.3 | 8352000 |   554.56 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 5.39
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 25.19
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 8.36
    apples_agent-2_min: 0
    apples_agent-3_max: 152
    apples_agent-3_mean: 88.12
    apples_agent-3_min: 36
    apples_agent-4_max: 103
    apples_agent-4_mean: 4.19
    apples_agent-4_min: 0
    apples_agent-5_max: 143
    apples_agent-5_mean: 85.0
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 535
    cleaning_beam_agent-0_mean: 329.3
    cleaning_beam_agent-0_min: 117
    cleaning_beam_agent-1_max: 481
    cleaning_beam_agent-1_mean: 266.3
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 585
    cleaning_beam_agent-2_mean: 397.3
    cleaning_beam_agent-2_min: 163
    cleaning_beam_agent-3_max: 153
    cleaning_beam_agent-3_mean: 80.32
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 520
    cleaning_beam_agent-4_mean: 369.14
    cleaning_beam_agent-4_min: 150
    cleaning_beam_agent-5_max: 338
    cleaning_beam_agent-5_mean: 107.83
    cleaning_beam_agent-5_min: 42
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 6
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-24-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 800.9999999999851
  episode_reward_mean: 598.9299999999984
  episode_reward_min: 279.99999999999704
  episodes_this_iter: 96
  episodes_total: 8448
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 14724.278
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 1.2899361848831177
        entropy_coeff: 0.0017600000137463212
        kl: 0.011053280904889107
        model: {}
        policy_loss: -0.028398433700203896
        total_loss: -0.027230728417634964
        vf_explained_var: 0.09662632644176483
        vf_loss: 12.27332878112793
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 1.1955229043960571
        entropy_coeff: 0.0017600000137463212
        kl: 0.013970823958516121
        model: {}
        policy_loss: -0.033329036086797714
        total_loss: -0.03134062886238098
        vf_explained_var: 0.04425598680973053
        vf_loss: 12.983636856079102
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 1.1429102420806885
        entropy_coeff: 0.0017600000137463212
        kl: 0.01348838023841381
        model: {}
        policy_loss: -0.02963906154036522
        total_loss: -0.027724001556634903
        vf_explained_var: 0.0951274037361145
        vf_loss: 12.289042472839355
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 0.9054579734802246
        entropy_coeff: 0.0017600000137463212
        kl: 0.010034764185547829
        model: {}
        policy_loss: -0.023005761206150055
        total_loss: -0.021428808569908142
        vf_explained_var: 0.14459547400474548
        vf_loss: 11.636092185974121
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 1.0894675254821777
        entropy_coeff: 0.0017600000137463212
        kl: 0.013985645025968552
        model: {}
        policy_loss: -0.033203043043613434
        total_loss: -0.031071359291672707
        vf_explained_var: 0.07873959839344025
        vf_loss: 12.520159721374512
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 1.1254405975341797
        entropy_coeff: 0.0017600000137463212
        kl: 0.014955052174627781
        model: {}
        policy_loss: -0.03324887156486511
        total_loss: -0.031076399609446526
        vf_explained_var: 0.14568179845809937
        vf_loss: 11.622356414794922
    load_time_ms: 40320.732
    num_steps_sampled: 8448000
    num_steps_trained: 8448000
    sample_time_ms: 144675.962
    update_time_ms: 588.623
  iterations_since_restore: 8
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.89615384615385
    ram_util_percent: 16.88269230769231
  pid: 14340
  policy_reward_max:
    agent-0: 133.50000000000054
    agent-1: 133.50000000000054
    agent-2: 133.50000000000054
    agent-3: 133.50000000000054
    agent-4: 133.50000000000054
    agent-5: 133.50000000000054
  policy_reward_mean:
    agent-0: 99.82166666666693
    agent-1: 99.82166666666693
    agent-2: 99.82166666666693
    agent-3: 99.82166666666693
    agent-4: 99.82166666666693
    agent-5: 99.82166666666693
  policy_reward_min:
    agent-0: 46.66666666666653
    agent-1: 46.66666666666653
    agent-2: 46.66666666666653
    agent-3: 46.66666666666653
    agent-4: 46.66666666666653
    agent-5: 46.66666666666653
  sampler_perf:
    mean_env_wait_ms: 30.754214148908982
    mean_inference_ms: 14.25301700107344
    mean_processing_ms: 64.95038202647994
  time_since_restore: 1607.2767519950867
  time_this_iter_s: 182.82341480255127
  time_total_s: 14158.093623399734
  timestamp: 1637036681
  timesteps_since_restore: 768000
  timesteps_this_iter: 96000
  timesteps_total: 8448000
  training_iteration: 88
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     88 |          14158.1 | 8448000 |   598.93 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 5.37
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 19.15
    apples_agent-1_min: 0
    apples_agent-2_max: 91
    apples_agent-2_mean: 8.38
    apples_agent-2_min: 0
    apples_agent-3_max: 152
    apples_agent-3_mean: 90.83
    apples_agent-3_min: 30
    apples_agent-4_max: 186
    apples_agent-4_mean: 8.34
    apples_agent-4_min: 0
    apples_agent-5_max: 128
    apples_agent-5_mean: 82.23
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 536
    cleaning_beam_agent-0_mean: 317.7
    cleaning_beam_agent-0_min: 90
    cleaning_beam_agent-1_max: 503
    cleaning_beam_agent-1_mean: 259.68
    cleaning_beam_agent-1_min: 73
    cleaning_beam_agent-2_max: 680
    cleaning_beam_agent-2_mean: 407.12
    cleaning_beam_agent-2_min: 197
    cleaning_beam_agent-3_max: 177
    cleaning_beam_agent-3_mean: 85.65
    cleaning_beam_agent-3_min: 30
    cleaning_beam_agent-4_max: 523
    cleaning_beam_agent-4_mean: 359.65
    cleaning_beam_agent-4_min: 152
    cleaning_beam_agent-5_max: 321
    cleaning_beam_agent-5_mean: 109.72
    cleaning_beam_agent-5_min: 37
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-27-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 836.99999999998
  episode_reward_mean: 570.1800000000012
  episode_reward_min: 253.9999999999962
  episodes_this_iter: 96
  episodes_total: 8544
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 14589.356
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 1.2954710721969604
        entropy_coeff: 0.0017600000137463212
        kl: 0.011943875811994076
        model: {}
        policy_loss: -0.029368262737989426
        total_loss: -0.02797655761241913
        vf_explained_var: 0.075161412358284
        vf_loss: 12.82962417602539
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 1.2181578874588013
        entropy_coeff: 0.0017600000137463212
        kl: 0.01245865784585476
        model: {}
        policy_loss: -0.03037247434258461
        total_loss: -0.028681866824626923
        vf_explained_var: 0.03235180675983429
        vf_loss: 13.428333282470703
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 1.1296474933624268
        entropy_coeff: 0.0017600000137463212
        kl: 0.012453801929950714
        model: {}
        policy_loss: -0.029246266931295395
        total_loss: -0.0275201927870512
        vf_explained_var: 0.11810293793678284
        vf_loss: 12.23495864868164
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 0.9582919478416443
        entropy_coeff: 0.0017600000137463212
        kl: 0.010119949467480183
        model: {}
        policy_loss: -0.023226695135235786
        total_loss: -0.021781541407108307
        vf_explained_var: 0.2024478018283844
        vf_loss: 11.07756233215332
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 1.0690367221832275
        entropy_coeff: 0.0017600000137463212
        kl: 0.013649020344018936
        model: {}
        policy_loss: -0.03383493423461914
        total_loss: -0.03177698329091072
        vf_explained_var: 0.1284043937921524
        vf_loss: 12.096521377563477
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 1.1450929641723633
        entropy_coeff: 0.0017600000137463212
        kl: 0.014322232455015182
        model: {}
        policy_loss: -0.03338303044438362
        total_loss: -0.03139939159154892
        vf_explained_var: 0.18250279128551483
        vf_loss: 11.345532417297363
    load_time_ms: 38359.304
    num_steps_sampled: 8544000
    num_steps_trained: 8544000
    sample_time_ms: 143978.096
    update_time_ms: 530.193
  iterations_since_restore: 9
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.998800000000003
    ram_util_percent: 16.780000000000005
  pid: 14340
  policy_reward_max:
    agent-0: 139.50000000000026
    agent-1: 139.50000000000026
    agent-2: 139.50000000000026
    agent-3: 139.50000000000026
    agent-4: 139.50000000000026
    agent-5: 139.50000000000026
  policy_reward_mean:
    agent-0: 95.03000000000021
    agent-1: 95.03000000000021
    agent-2: 95.03000000000021
    agent-3: 95.03000000000021
    agent-4: 95.03000000000021
    agent-5: 95.03000000000021
  policy_reward_min:
    agent-0: 42.33333333333328
    agent-1: 42.33333333333328
    agent-2: 42.33333333333328
    agent-3: 42.33333333333328
    agent-4: 42.33333333333328
    agent-5: 42.33333333333328
  sampler_perf:
    mean_env_wait_ms: 30.803735639956013
    mean_inference_ms: 14.230043492197133
    mean_processing_ms: 64.96527027654018
  time_since_restore: 1782.0174629688263
  time_this_iter_s: 174.74071097373962
  time_total_s: 14332.834334373474
  timestamp: 1637036856
  timesteps_since_restore: 864000
  timesteps_this_iter: 96000
  timesteps_total: 8544000
  training_iteration: 89
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     89 |          14332.8 | 8544000 |   570.18 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 5.87
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 22.5
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 7.79
    apples_agent-2_min: 0
    apples_agent-3_max: 152
    apples_agent-3_mean: 85.71
    apples_agent-3_min: 31
    apples_agent-4_max: 73
    apples_agent-4_mean: 3.64
    apples_agent-4_min: 0
    apples_agent-5_max: 130
    apples_agent-5_mean: 82.06
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 513
    cleaning_beam_agent-0_mean: 351.32
    cleaning_beam_agent-0_min: 136
    cleaning_beam_agent-1_max: 452
    cleaning_beam_agent-1_mean: 247.45
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 613
    cleaning_beam_agent-2_mean: 382.64
    cleaning_beam_agent-2_min: 72
    cleaning_beam_agent-3_max: 189
    cleaning_beam_agent-3_mean: 80.87
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 542
    cleaning_beam_agent-4_mean: 361.96
    cleaning_beam_agent-4_min: 150
    cleaning_beam_agent-5_max: 261
    cleaning_beam_agent-5_mean: 106.22
    cleaning_beam_agent-5_min: 38
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 20
    fire_beam_agent-5_mean: 0.47
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-30-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 828.999999999989
  episode_reward_mean: 564.29
  episode_reward_min: 60.99999999999775
  episodes_this_iter: 96
  episodes_total: 8640
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 14480.858
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 1.2753057479858398
        entropy_coeff: 0.0017600000137463212
        kl: 0.011443548835814
        model: {}
        policy_loss: -0.028403325006365776
        total_loss: -0.02697303146123886
        vf_explained_var: 0.07127559185028076
        vf_loss: 13.861227035522461
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 1.2304792404174805
        entropy_coeff: 0.0017600000137463212
        kl: 0.01283445954322815
        model: {}
        policy_loss: -0.03073297254741192
        total_loss: -0.02889789268374443
        vf_explained_var: 0.038925155997276306
        vf_loss: 14.33834457397461
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 1.118001103401184
        entropy_coeff: 0.0017600000137463212
        kl: 0.012332982383668423
        model: {}
        policy_loss: -0.028830932453274727
        total_loss: -0.026993703097105026
        vf_explained_var: 0.10306812822818756
        vf_loss: 13.383130073547363
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 0.9546998739242554
        entropy_coeff: 0.0017600000137463212
        kl: 0.012774590402841568
        model: {}
        policy_loss: -0.022803647443652153
        total_loss: -0.020696431398391724
        vf_explained_var: 0.17419254779815674
        vf_loss: 12.325722694396973
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 1.0772677659988403
        entropy_coeff: 0.0017600000137463212
        kl: 0.013567585498094559
        model: {}
        policy_loss: -0.03540579974651337
        total_loss: -0.033295467495918274
        vf_explained_var: 0.13242881000041962
        vf_loss: 12.928001403808594
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 1.1502447128295898
        entropy_coeff: 0.0017600000137463212
        kl: 0.013015422970056534
        model: {}
        policy_loss: -0.03562360629439354
        total_loss: -0.03379657864570618
        vf_explained_var: 0.16412894427776337
        vf_loss: 12.483750343322754
    load_time_ms: 37941.421
    num_steps_sampled: 8640000
    num_steps_trained: 8640000
    sample_time_ms: 143380.736
    update_time_ms: 483.807
  iterations_since_restore: 10
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.15245283018868
    ram_util_percent: 16.792075471698112
  pid: 14340
  policy_reward_max:
    agent-0: 138.16666666666706
    agent-1: 138.16666666666706
    agent-2: 138.16666666666706
    agent-3: 138.16666666666706
    agent-4: 138.16666666666706
    agent-5: 138.16666666666706
  policy_reward_mean:
    agent-0: 94.04833333333362
    agent-1: 94.04833333333362
    agent-2: 94.04833333333362
    agent-3: 94.04833333333362
    agent-4: 94.04833333333362
    agent-5: 94.04833333333362
  policy_reward_min:
    agent-0: 10.166666666666702
    agent-1: 10.166666666666702
    agent-2: 10.166666666666702
    agent-3: 10.166666666666702
    agent-4: 10.166666666666702
    agent-5: 10.166666666666702
  sampler_perf:
    mean_env_wait_ms: 30.823150647157004
    mean_inference_ms: 14.233511673936608
    mean_processing_ms: 64.9740557582283
  time_since_restore: 1967.9309856891632
  time_this_iter_s: 185.9135227203369
  time_total_s: 14518.747857093811
  timestamp: 1637037042
  timesteps_since_restore: 960000
  timesteps_this_iter: 96000
  timesteps_total: 8640000
  training_iteration: 90
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     90 |          14518.7 | 8640000 |   564.29 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 8.1
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 17.62
    apples_agent-1_min: 0
    apples_agent-2_max: 48
    apples_agent-2_mean: 6.76
    apples_agent-2_min: 0
    apples_agent-3_max: 166
    apples_agent-3_mean: 91.55
    apples_agent-3_min: 27
    apples_agent-4_max: 84
    apples_agent-4_mean: 4.82
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 83.25
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 569
    cleaning_beam_agent-0_mean: 335.77
    cleaning_beam_agent-0_min: 136
    cleaning_beam_agent-1_max: 506
    cleaning_beam_agent-1_mean: 241.06
    cleaning_beam_agent-1_min: 85
    cleaning_beam_agent-2_max: 652
    cleaning_beam_agent-2_mean: 396.45
    cleaning_beam_agent-2_min: 124
    cleaning_beam_agent-3_max: 175
    cleaning_beam_agent-3_mean: 81.9
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 493
    cleaning_beam_agent-4_mean: 373.64
    cleaning_beam_agent-4_min: 180
    cleaning_beam_agent-5_max: 291
    cleaning_beam_agent-5_mean: 112.17
    cleaning_beam_agent-5_min: 35
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 6
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-33-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 808.9999999999706
  episode_reward_mean: 578.480000000001
  episode_reward_min: 194.999999999999
  episodes_this_iter: 96
  episodes_total: 8736
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13343.967
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 1.2586193084716797
        entropy_coeff: 0.0017600000137463212
        kl: 0.01145264320075512
        model: {}
        policy_loss: -0.028791191056370735
        total_loss: -0.02741885930299759
        vf_explained_var: 0.02467520534992218
        vf_loss: 12.969743728637695
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 1.2416833639144897
        entropy_coeff: 0.0017600000137463212
        kl: 0.012671352364122868
        model: {}
        policy_loss: -0.03089676797389984
        total_loss: -0.029276400804519653
        vf_explained_var: 0.043315574526786804
        vf_loss: 12.714568138122559
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 1.1141225099563599
        entropy_coeff: 0.0017600000137463212
        kl: 0.012099113315343857
        model: {}
        policy_loss: -0.02904898300766945
        total_loss: -0.027361758053302765
        vf_explained_var: 0.07642993330955505
        vf_loss: 12.282569885253906
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 0.9262056350708008
        entropy_coeff: 0.0017600000137463212
        kl: 0.010619113221764565
        model: {}
        policy_loss: -0.023891322314739227
        total_loss: -0.02227691560983658
        vf_explained_var: 0.15674833953380585
        vf_loss: 11.207009315490723
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 1.066015362739563
        entropy_coeff: 0.0017600000137463212
        kl: 0.014191423542797565
        model: {}
        policy_loss: -0.03646855056285858
        total_loss: -0.03431130200624466
        vf_explained_var: 0.10071548819541931
        vf_loss: 11.951486587524414
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 1.1277306079864502
        entropy_coeff: 0.0017600000137463212
        kl: 0.013214102014899254
        model: {}
        policy_loss: -0.035121235996484756
        total_loss: -0.03331693634390831
        vf_explained_var: 0.13777926564216614
        vf_loss: 11.462873458862305
    load_time_ms: 34094.512
    num_steps_sampled: 8736000
    num_steps_trained: 8736000
    sample_time_ms: 146611.491
    update_time_ms: 72.678
  iterations_since_restore: 11
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.687272727272727
    ram_util_percent: 16.606181818181817
  pid: 14340
  policy_reward_max:
    agent-0: 134.8333333333339
    agent-1: 134.8333333333339
    agent-2: 134.8333333333339
    agent-3: 134.8333333333339
    agent-4: 134.8333333333339
    agent-5: 134.8333333333339
  policy_reward_mean:
    agent-0: 96.41333333333358
    agent-1: 96.41333333333358
    agent-2: 96.41333333333358
    agent-3: 96.41333333333358
    agent-4: 96.41333333333358
    agent-5: 96.41333333333358
  policy_reward_min:
    agent-0: 32.50000000000003
    agent-1: 32.50000000000003
    agent-2: 32.50000000000003
    agent-3: 32.50000000000003
    agent-4: 32.50000000000003
    agent-5: 32.50000000000003
  sampler_perf:
    mean_env_wait_ms: 30.848826269791907
    mean_inference_ms: 14.216468634570019
    mean_processing_ms: 64.96970810732083
  time_since_restore: 2160.3807525634766
  time_this_iter_s: 192.44976687431335
  time_total_s: 14711.197623968124
  timestamp: 1637037234
  timesteps_since_restore: 1056000
  timesteps_this_iter: 96000
  timesteps_total: 8736000
  training_iteration: 91
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     91 |          14711.2 | 8736000 |   578.48 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 124
    apples_agent-0_mean: 7.65
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 20.69
    apples_agent-1_min: 0
    apples_agent-2_max: 118
    apples_agent-2_mean: 11.02
    apples_agent-2_min: 0
    apples_agent-3_max: 148
    apples_agent-3_mean: 86.38
    apples_agent-3_min: 36
    apples_agent-4_max: 40
    apples_agent-4_mean: 1.93
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 76.41
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 506
    cleaning_beam_agent-0_mean: 328.16
    cleaning_beam_agent-0_min: 136
    cleaning_beam_agent-1_max: 429
    cleaning_beam_agent-1_mean: 229.38
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 655
    cleaning_beam_agent-2_mean: 366.65
    cleaning_beam_agent-2_min: 144
    cleaning_beam_agent-3_max: 143
    cleaning_beam_agent-3_mean: 69.5
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 536
    cleaning_beam_agent-4_mean: 365.61
    cleaning_beam_agent-4_min: 75
    cleaning_beam_agent-5_max: 463
    cleaning_beam_agent-5_mean: 119.89
    cleaning_beam_agent-5_min: 28
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 5
    fire_beam_agent-5_mean: 0.16
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-36-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 808.9999999999706
  episode_reward_mean: 561.040000000001
  episode_reward_min: 168.9999999999988
  episodes_this_iter: 96
  episodes_total: 8832
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13275.081
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 1.2698949575424194
        entropy_coeff: 0.0017600000137463212
        kl: 0.011401689611375332
        model: {}
        policy_loss: -0.030528714880347252
        total_loss: -0.02918171137571335
        vf_explained_var: 0.07203471660614014
        vf_loss: 13.016857147216797
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 1.2296005487442017
        entropy_coeff: 0.0017600000137463212
        kl: 0.01331287156790495
        model: {}
        policy_loss: -0.0321759432554245
        total_loss: -0.030340757220983505
        vf_explained_var: 0.04678219556808472
        vf_loss: 13.367110252380371
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 1.1541805267333984
        entropy_coeff: 0.0017600000137463212
        kl: 0.015009674243628979
        model: {}
        policy_loss: -0.028356164693832397
        total_loss: -0.026148129254579544
        vf_explained_var: 0.1179865151643753
        vf_loss: 12.374524116516113
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 0.9259181022644043
        entropy_coeff: 0.0017600000137463212
        kl: 0.010137036442756653
        model: {}
        policy_loss: -0.024693742394447327
        total_loss: -0.023146098479628563
        vf_explained_var: 0.18093451857566833
        vf_loss: 11.498517990112305
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 1.063874363899231
        entropy_coeff: 0.0017600000137463212
        kl: 0.014978723600506783
        model: {}
        policy_loss: -0.036130473017692566
        total_loss: -0.03379577398300171
        vf_explained_var: 0.1359807848930359
        vf_loss: 12.11372184753418
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 1.135056734085083
        entropy_coeff: 0.0017600000137463212
        kl: 0.013568982481956482
        model: {}
        policy_loss: -0.035822562873363495
        total_loss: -0.03392874822020531
        vf_explained_var: 0.16103973984718323
        vf_loss: 11.777196884155273
    load_time_ms: 35010.559
    num_steps_sampled: 8832000
    num_steps_trained: 8832000
    sample_time_ms: 143860.741
    update_time_ms: 63.885
  iterations_since_restore: 12
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.93473282442748
    ram_util_percent: 16.781679389312973
  pid: 14340
  policy_reward_max:
    agent-0: 134.8333333333339
    agent-1: 134.8333333333339
    agent-2: 134.8333333333339
    agent-3: 134.8333333333339
    agent-4: 134.8333333333339
    agent-5: 134.8333333333339
  policy_reward_mean:
    agent-0: 93.5066666666669
    agent-1: 93.5066666666669
    agent-2: 93.5066666666669
    agent-3: 93.5066666666669
    agent-4: 93.5066666666669
    agent-5: 93.5066666666669
  policy_reward_min:
    agent-0: 28.166666666666725
    agent-1: 28.166666666666725
    agent-2: 28.166666666666725
    agent-3: 28.166666666666725
    agent-4: 28.166666666666725
    agent-5: 28.166666666666725
  sampler_perf:
    mean_env_wait_ms: 30.811390267487504
    mean_inference_ms: 14.236437524156617
    mean_processing_ms: 65.28459916707676
  time_since_restore: 2344.4185013771057
  time_this_iter_s: 184.03774881362915
  time_total_s: 14895.235372781754
  timestamp: 1637037419
  timesteps_since_restore: 1152000
  timesteps_this_iter: 96000
  timesteps_total: 8832000
  training_iteration: 92
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     92 |          14895.2 | 8832000 |   561.04 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 4.72
    apples_agent-0_min: 0
    apples_agent-1_max: 110
    apples_agent-1_mean: 20.34
    apples_agent-1_min: 0
    apples_agent-2_max: 172
    apples_agent-2_mean: 11.41
    apples_agent-2_min: 0
    apples_agent-3_max: 143
    apples_agent-3_mean: 83.93
    apples_agent-3_min: 23
    apples_agent-4_max: 98
    apples_agent-4_mean: 5.89
    apples_agent-4_min: 0
    apples_agent-5_max: 143
    apples_agent-5_mean: 74.66
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 504
    cleaning_beam_agent-0_mean: 319.61
    cleaning_beam_agent-0_min: 64
    cleaning_beam_agent-1_max: 426
    cleaning_beam_agent-1_mean: 233.11
    cleaning_beam_agent-1_min: 101
    cleaning_beam_agent-2_max: 655
    cleaning_beam_agent-2_mean: 355.55
    cleaning_beam_agent-2_min: 99
    cleaning_beam_agent-3_max: 136
    cleaning_beam_agent-3_mean: 75.87
    cleaning_beam_agent-3_min: 38
    cleaning_beam_agent-4_max: 568
    cleaning_beam_agent-4_mean: 354.88
    cleaning_beam_agent-4_min: 113
    cleaning_beam_agent-5_max: 448
    cleaning_beam_agent-5_mean: 106.74
    cleaning_beam_agent-5_min: 31
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-40-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 804.9999999999757
  episode_reward_mean: 544.080000000001
  episode_reward_min: 137.00000000000034
  episodes_this_iter: 96
  episodes_total: 8928
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13237.803
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 1.2457873821258545
        entropy_coeff: 0.0017600000137463212
        kl: 0.013748031109571457
        model: {}
        policy_loss: -0.02791638858616352
        total_loss: -0.025896288454532623
        vf_explained_var: 0.07977660000324249
        vf_loss: 14.630752563476562
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 1.2044713497161865
        entropy_coeff: 0.0017600000137463212
        kl: 0.013001042418181896
        model: {}
        policy_loss: -0.03238663077354431
        total_loss: -0.0303785540163517
        vf_explained_var: 0.03936344385147095
        vf_loss: 15.277405738830566
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 1.1394728422164917
        entropy_coeff: 0.0017600000137463212
        kl: 0.011651127599179745
        model: {}
        policy_loss: -0.028373466804623604
        total_loss: -0.02664116956293583
        vf_explained_var: 0.1146361380815506
        vf_loss: 14.075446128845215
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 0.9566432237625122
        entropy_coeff: 0.0017600000137463212
        kl: 0.011601682752370834
        model: {}
        policy_loss: -0.02579938992857933
        total_loss: -0.02394849807024002
        vf_explained_var: 0.23645657300949097
        vf_loss: 12.14244270324707
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 1.0862691402435303
        entropy_coeff: 0.0017600000137463212
        kl: 0.014282900840044022
        model: {}
        policy_loss: -0.037544310092926025
        total_loss: -0.03525543212890625
        vf_explained_var: 0.15437456965446472
        vf_loss: 13.441337585449219
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 1.1221861839294434
        entropy_coeff: 0.0017600000137463212
        kl: 0.01376133132725954
        model: {}
        policy_loss: -0.03474324941635132
        total_loss: -0.03274556249380112
        vf_explained_var: 0.23182076215744019
        vf_loss: 12.2046537399292
    load_time_ms: 34772.72
    num_steps_sampled: 8928000
    num_steps_trained: 8928000
    sample_time_ms: 142016.79
    update_time_ms: 64.726
  iterations_since_restore: 13
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.250763358778627
    ram_util_percent: 15.450000000000006
  pid: 14340
  policy_reward_max:
    agent-0: 134.16666666666706
    agent-1: 134.16666666666706
    agent-2: 134.16666666666706
    agent-3: 134.16666666666706
    agent-4: 134.16666666666706
    agent-5: 134.16666666666706
  policy_reward_mean:
    agent-0: 90.68000000000022
    agent-1: 90.68000000000022
    agent-2: 90.68000000000022
    agent-3: 90.68000000000022
    agent-4: 90.68000000000022
    agent-5: 90.68000000000022
  policy_reward_min:
    agent-0: 22.833333333333336
    agent-1: 22.833333333333336
    agent-2: 22.833333333333336
    agent-3: 22.833333333333336
    agent-4: 22.833333333333336
    agent-5: 22.833333333333336
  sampler_perf:
    mean_env_wait_ms: 30.80931825253505
    mean_inference_ms: 14.223937373921176
    mean_processing_ms: 65.31200965260749
  time_since_restore: 2528.5512251853943
  time_this_iter_s: 184.13272380828857
  time_total_s: 15079.368096590042
  timestamp: 1637037603
  timesteps_since_restore: 1248000
  timesteps_this_iter: 96000
  timesteps_total: 8928000
  training_iteration: 93
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     93 |          15079.4 | 8928000 |   544.08 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 94
    apples_agent-0_mean: 7.62
    apples_agent-0_min: 0
    apples_agent-1_max: 67
    apples_agent-1_mean: 18.26
    apples_agent-1_min: 0
    apples_agent-2_max: 103
    apples_agent-2_mean: 7.48
    apples_agent-2_min: 0
    apples_agent-3_max: 155
    apples_agent-3_mean: 88.4
    apples_agent-3_min: 32
    apples_agent-4_max: 99
    apples_agent-4_mean: 4.58
    apples_agent-4_min: 0
    apples_agent-5_max: 148
    apples_agent-5_mean: 76.17
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 546
    cleaning_beam_agent-0_mean: 325.45
    cleaning_beam_agent-0_min: 86
    cleaning_beam_agent-1_max: 532
    cleaning_beam_agent-1_mean: 263.32
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 633
    cleaning_beam_agent-2_mean: 349.62
    cleaning_beam_agent-2_min: 160
    cleaning_beam_agent-3_max: 176
    cleaning_beam_agent-3_mean: 78.13
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 568
    cleaning_beam_agent-4_mean: 383.34
    cleaning_beam_agent-4_min: 136
    cleaning_beam_agent-5_max: 307
    cleaning_beam_agent-5_mean: 93.26
    cleaning_beam_agent-5_min: 35
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-43-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 771.9999999999952
  episode_reward_mean: 563.6100000000009
  episode_reward_min: 185.99999999999835
  episodes_this_iter: 96
  episodes_total: 9024
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13192.624
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 1.263545036315918
        entropy_coeff: 0.0017600000137463212
        kl: 0.012269951403141022
        model: {}
        policy_loss: -0.030074533075094223
        total_loss: -0.028495337814092636
        vf_explained_var: 0.062379151582717896
        vf_loss: 13.490459442138672
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 1.2066681385040283
        entropy_coeff: 0.0017600000137463212
        kl: 0.013854937627911568
        model: {}
        policy_loss: -0.03372688218951225
        total_loss: -0.03171683847904205
        vf_explained_var: 0.053112417459487915
        vf_loss: 13.627900123596191
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 1.1484880447387695
        entropy_coeff: 0.0017600000137463212
        kl: 0.012847884558141232
        model: {}
        policy_loss: -0.029786642640829086
        total_loss: -0.027914542704820633
        vf_explained_var: 0.07979503273963928
        vf_loss: 13.238618850708008
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 0.9328407645225525
        entropy_coeff: 0.0017600000137463212
        kl: 0.010008263401687145
        model: {}
        policy_loss: -0.024002492427825928
        total_loss: -0.02247634157538414
        vf_explained_var: 0.19016854465007782
        vf_loss: 11.66301441192627
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 1.0641567707061768
        entropy_coeff: 0.0017600000137463212
        kl: 0.014105786569416523
        model: {}
        policy_loss: -0.037270743399858475
        total_loss: -0.0350639708340168
        vf_explained_var: 0.12529851496219635
        vf_loss: 12.585322380065918
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 1.118685245513916
        entropy_coeff: 0.0017600000137463212
        kl: 0.013694663532078266
        model: {}
        policy_loss: -0.035079456865787506
        total_loss: -0.03309342637658119
        vf_explained_var: 0.15503083169460297
        vf_loss: 12.159908294677734
    load_time_ms: 34618.83
    num_steps_sampled: 9024000
    num_steps_trained: 9024000
    sample_time_ms: 139890.373
    update_time_ms: 59.46
  iterations_since_restore: 14
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.46846153846154
    ram_util_percent: 15.368076923076925
  pid: 14340
  policy_reward_max:
    agent-0: 128.6666666666671
    agent-1: 128.6666666666671
    agent-2: 128.6666666666671
    agent-3: 128.6666666666671
    agent-4: 128.6666666666671
    agent-5: 128.6666666666671
  policy_reward_mean:
    agent-0: 93.9350000000002
    agent-1: 93.9350000000002
    agent-2: 93.9350000000002
    agent-3: 93.9350000000002
    agent-4: 93.9350000000002
    agent-5: 93.9350000000002
  policy_reward_min:
    agent-0: 31.00000000000008
    agent-1: 31.00000000000008
    agent-2: 31.00000000000008
    agent-3: 31.00000000000008
    agent-4: 31.00000000000008
    agent-5: 31.00000000000008
  sampler_perf:
    mean_env_wait_ms: 30.8133338047109
    mean_inference_ms: 14.211842360904852
    mean_processing_ms: 65.28319309267096
  time_since_restore: 2710.560577392578
  time_this_iter_s: 182.00935220718384
  time_total_s: 15261.377448797226
  timestamp: 1637037785
  timesteps_since_restore: 1344000
  timesteps_this_iter: 96000
  timesteps_total: 9024000
  training_iteration: 94
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     94 |          15261.4 | 9024000 |   563.61 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 6.36
    apples_agent-0_min: 0
    apples_agent-1_max: 147
    apples_agent-1_mean: 21.9
    apples_agent-1_min: 0
    apples_agent-2_max: 93
    apples_agent-2_mean: 9.26
    apples_agent-2_min: 0
    apples_agent-3_max: 169
    apples_agent-3_mean: 87.28
    apples_agent-3_min: 29
    apples_agent-4_max: 56
    apples_agent-4_mean: 3.05
    apples_agent-4_min: 0
    apples_agent-5_max: 167
    apples_agent-5_mean: 74.74
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 510
    cleaning_beam_agent-0_mean: 320.2
    cleaning_beam_agent-0_min: 150
    cleaning_beam_agent-1_max: 476
    cleaning_beam_agent-1_mean: 272.96
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 633
    cleaning_beam_agent-2_mean: 353.76
    cleaning_beam_agent-2_min: 165
    cleaning_beam_agent-3_max: 163
    cleaning_beam_agent-3_mean: 67.68
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 554
    cleaning_beam_agent-4_mean: 381.81
    cleaning_beam_agent-4_min: 138
    cleaning_beam_agent-5_max: 337
    cleaning_beam_agent-5_mean: 98.15
    cleaning_beam_agent-5_min: 30
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-46-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 832.9999999999886
  episode_reward_mean: 586.3199999999996
  episode_reward_min: 223.99999999999702
  episodes_this_iter: 96
  episodes_total: 9120
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13130.374
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 1.2801541090011597
        entropy_coeff: 0.0017600000137463212
        kl: 0.012051442638039589
        model: {}
        policy_loss: -0.02980084717273712
        total_loss: -0.028241850435733795
        vf_explained_var: 0.07889856398105621
        vf_loss: 14.017783164978027
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 1.1887495517730713
        entropy_coeff: 0.0017600000137463212
        kl: 0.013892102055251598
        model: {}
        policy_loss: -0.0318756103515625
        total_loss: -0.029703717678785324
        vf_explained_var: 0.024112090468406677
        vf_loss: 14.856714248657227
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 1.1561076641082764
        entropy_coeff: 0.0017600000137463212
        kl: 0.012688443064689636
        model: {}
        policy_loss: -0.030591264367103577
        total_loss: -0.028739409521222115
        vf_explained_var: 0.11256919801235199
        vf_loss: 13.489181518554688
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 0.8926591873168945
        entropy_coeff: 0.0017600000137463212
        kl: 0.010340956971049309
        model: {}
        policy_loss: -0.025078125298023224
        total_loss: -0.023323364555835724
        vf_explained_var: 0.17359158396720886
        vf_loss: 12.576507568359375
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 1.0552480220794678
        entropy_coeff: 0.0017600000137463212
        kl: 0.015132058411836624
        model: {}
        policy_loss: -0.035946186631917953
        total_loss: -0.03345117345452309
        vf_explained_var: 0.12834925949573517
        vf_loss: 13.258360862731934
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 1.081716775894165
        entropy_coeff: 0.0017600000137463212
        kl: 0.013957735151052475
        model: {}
        policy_loss: -0.03375104442238808
        total_loss: -0.031604550778865814
        vf_explained_var: 0.17238716781139374
        vf_loss: 12.5877103805542
    load_time_ms: 33975.45
    num_steps_sampled: 9120000
    num_steps_trained: 9120000
    sample_time_ms: 139103.662
    update_time_ms: 61.386
  iterations_since_restore: 15
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.651503759398498
    ram_util_percent: 15.367669172932333
  pid: 14340
  policy_reward_max:
    agent-0: 138.8333333333335
    agent-1: 138.8333333333335
    agent-2: 138.8333333333335
    agent-3: 138.8333333333335
    agent-4: 138.8333333333335
    agent-5: 138.8333333333335
  policy_reward_mean:
    agent-0: 97.72000000000025
    agent-1: 97.72000000000025
    agent-2: 97.72000000000025
    agent-3: 97.72000000000025
    agent-4: 97.72000000000025
    agent-5: 97.72000000000025
  policy_reward_min:
    agent-0: 37.33333333333335
    agent-1: 37.33333333333335
    agent-2: 37.33333333333335
    agent-3: 37.33333333333335
    agent-4: 37.33333333333335
    agent-5: 37.33333333333335
  sampler_perf:
    mean_env_wait_ms: 30.85627532799007
    mean_inference_ms: 14.234578579585213
    mean_processing_ms: 65.30652158387998
  time_since_restore: 2897.171751499176
  time_this_iter_s: 186.6111741065979
  time_total_s: 15447.988622903824
  timestamp: 1637037972
  timesteps_since_restore: 1440000
  timesteps_this_iter: 96000
  timesteps_total: 9120000
  training_iteration: 95
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     95 |            15448 | 9120000 |   586.32 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 6.69
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 15.44
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 9.23
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 93.04
    apples_agent-3_min: 45
    apples_agent-4_max: 89
    apples_agent-4_mean: 4.21
    apples_agent-4_min: 0
    apples_agent-5_max: 128
    apples_agent-5_mean: 77.39
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 551
    cleaning_beam_agent-0_mean: 325.65
    cleaning_beam_agent-0_min: 130
    cleaning_beam_agent-1_max: 488
    cleaning_beam_agent-1_mean: 246.87
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 619
    cleaning_beam_agent-2_mean: 357.41
    cleaning_beam_agent-2_min: 153
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 61.55
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 543
    cleaning_beam_agent-4_mean: 375.45
    cleaning_beam_agent-4_min: 198
    cleaning_beam_agent-5_max: 313
    cleaning_beam_agent-5_mean: 89.98
    cleaning_beam_agent-5_min: 25
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-49-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 801.9999999999766
  episode_reward_mean: 609.9799999999987
  episode_reward_min: 213.99999999999795
  episodes_this_iter: 96
  episodes_total: 9216
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13119.148
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 1.2436107397079468
        entropy_coeff: 0.0017600000137463212
        kl: 0.011810031719505787
        model: {}
        policy_loss: -0.0304399561136961
        total_loss: -0.02891184575855732
        vf_explained_var: 0.04796065390110016
        vf_loss: 13.548592567443848
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 1.2288103103637695
        entropy_coeff: 0.0017600000137463212
        kl: 0.01344117522239685
        model: {}
        policy_loss: -0.03240491822361946
        total_loss: -0.030512597411870956
        vf_explained_var: 0.03950554132461548
        vf_loss: 13.667901992797852
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 1.1504530906677246
        entropy_coeff: 0.0017600000137463212
        kl: 0.013701986521482468
        model: {}
        policy_loss: -0.03126032277941704
        total_loss: -0.0292634479701519
        vf_explained_var: 0.09911811351776123
        vf_loss: 12.81272029876709
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 0.8595253229141235
        entropy_coeff: 0.0017600000137463212
        kl: 0.01006871834397316
        model: {}
        policy_loss: -0.025015678256750107
        total_loss: -0.02339981310069561
        vf_explained_var: 0.21658958494663239
        vf_loss: 11.148873329162598
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 1.0564996004104614
        entropy_coeff: 0.0017600000137463212
        kl: 0.01365623064339161
        model: {}
        policy_loss: -0.03757484629750252
        total_loss: -0.035447508096694946
        vf_explained_var: 0.11692489683628082
        vf_loss: 12.555349349975586
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 1.0759912729263306
        entropy_coeff: 0.0017600000137463212
        kl: 0.013125456869602203
        model: {}
        policy_loss: -0.03504330664873123
        total_loss: -0.03310400992631912
        vf_explained_var: 0.1514519900083542
        vf_loss: 12.079575538635254
    load_time_ms: 31871.097
    num_steps_sampled: 9216000
    num_steps_trained: 9216000
    sample_time_ms: 138775.47
    update_time_ms: 60.849
  iterations_since_restore: 16
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.44041666666667
    ram_util_percent: 15.207500000000001
  pid: 14340
  policy_reward_max:
    agent-0: 133.6666666666672
    agent-1: 133.6666666666672
    agent-2: 133.6666666666672
    agent-3: 133.6666666666672
    agent-4: 133.6666666666672
    agent-5: 133.6666666666672
  policy_reward_mean:
    agent-0: 101.66333333333365
    agent-1: 101.66333333333365
    agent-2: 101.66333333333365
    agent-3: 101.66333333333365
    agent-4: 101.66333333333365
    agent-5: 101.66333333333365
  policy_reward_min:
    agent-0: 35.66666666666669
    agent-1: 35.66666666666669
    agent-2: 35.66666666666669
    agent-3: 35.66666666666669
    agent-4: 35.66666666666669
    agent-5: 35.66666666666669
  sampler_perf:
    mean_env_wait_ms: 30.86773909936123
    mean_inference_ms: 14.254504829166649
    mean_processing_ms: 65.30640380427947
  time_since_restore: 3065.8896794319153
  time_this_iter_s: 168.71792793273926
  time_total_s: 15616.706550836563
  timestamp: 1637038141
  timesteps_since_restore: 1536000
  timesteps_this_iter: 96000
  timesteps_total: 9216000
  training_iteration: 96
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     96 |          15616.7 | 9216000 |   609.98 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 4.85
    apples_agent-0_min: 0
    apples_agent-1_max: 247
    apples_agent-1_mean: 24.71
    apples_agent-1_min: 0
    apples_agent-2_max: 90
    apples_agent-2_mean: 9.3
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 82.98
    apples_agent-3_min: 24
    apples_agent-4_max: 110
    apples_agent-4_mean: 7.02
    apples_agent-4_min: 0
    apples_agent-5_max: 180
    apples_agent-5_mean: 77.27
    apples_agent-5_min: 19
    cleaning_beam_agent-0_max: 525
    cleaning_beam_agent-0_mean: 344.45
    cleaning_beam_agent-0_min: 134
    cleaning_beam_agent-1_max: 398
    cleaning_beam_agent-1_mean: 226.69
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 585
    cleaning_beam_agent-2_mean: 348.36
    cleaning_beam_agent-2_min: 96
    cleaning_beam_agent-3_max: 118
    cleaning_beam_agent-3_mean: 64.49
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 523
    cleaning_beam_agent-4_mean: 363.4
    cleaning_beam_agent-4_min: 202
    cleaning_beam_agent-5_max: 265
    cleaning_beam_agent-5_mean: 89.26
    cleaning_beam_agent-5_min: 24
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-51-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 795.9999999999901
  episode_reward_mean: 581.3399999999997
  episode_reward_min: 195.99999999999787
  episodes_this_iter: 96
  episodes_total: 9312
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13065.288
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 1.2327091693878174
        entropy_coeff: 0.0017600000137463212
        kl: 0.01194727048277855
        model: {}
        policy_loss: -0.02946215495467186
        total_loss: -0.02782902680337429
        vf_explained_var: 0.08944827318191528
        vf_loss: 14.132457733154297
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 1.1813538074493408
        entropy_coeff: 0.0017600000137463212
        kl: 0.01322484016418457
        model: {}
        policy_loss: -0.0322410985827446
        total_loss: -0.03017207235097885
        vf_explained_var: 0.032503724098205566
        vf_loss: 15.03244686126709
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 1.1551417112350464
        entropy_coeff: 0.0017600000137463212
        kl: 0.013941440731287003
        model: {}
        policy_loss: -0.028750471770763397
        total_loss: -0.026639368385076523
        vf_explained_var: 0.12672720849514008
        vf_loss: 13.558670043945312
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 0.8960338234901428
        entropy_coeff: 0.0017600000137463212
        kl: 0.010316023603081703
        model: {}
        policy_loss: -0.024567343294620514
        total_loss: -0.022872110828757286
        vf_explained_var: 0.22097596526145935
        vf_loss: 12.090507507324219
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 1.071366310119629
        entropy_coeff: 0.0017600000137463212
        kl: 0.015456176362931728
        model: {}
        policy_loss: -0.03782007098197937
        total_loss: -0.035256266593933105
        vf_explained_var: 0.1256335973739624
        vf_loss: 13.58169937133789
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 1.0861999988555908
        entropy_coeff: 0.0017600000137463212
        kl: 0.013077962212264538
        model: {}
        policy_loss: -0.0357566773891449
        total_loss: -0.033806852996349335
        vf_explained_var: 0.19757358729839325
        vf_loss: 12.459482192993164
    load_time_ms: 30286.343
    num_steps_sampled: 9312000
    num_steps_trained: 9312000
    sample_time_ms: 138022.192
    update_time_ms: 59.659
  iterations_since_restore: 17
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.576612903225808
    ram_util_percent: 15.211693548387094
  pid: 14340
  policy_reward_max:
    agent-0: 132.66666666666703
    agent-1: 132.66666666666703
    agent-2: 132.66666666666703
    agent-3: 132.66666666666703
    agent-4: 132.66666666666703
    agent-5: 132.66666666666703
  policy_reward_mean:
    agent-0: 96.89000000000027
    agent-1: 96.89000000000027
    agent-2: 96.89000000000027
    agent-3: 96.89000000000027
    agent-4: 96.89000000000027
    agent-5: 96.89000000000027
  policy_reward_min:
    agent-0: 32.666666666666735
    agent-1: 32.666666666666735
    agent-2: 32.666666666666735
    agent-3: 32.666666666666735
    agent-4: 32.666666666666735
    agent-5: 32.666666666666735
  sampler_perf:
    mean_env_wait_ms: 30.871688213619276
    mean_inference_ms: 14.270946402774264
    mean_processing_ms: 65.32077808586111
  time_since_restore: 3239.973793745041
  time_this_iter_s: 174.0841143131256
  time_total_s: 15790.790665149689
  timestamp: 1637038315
  timesteps_since_restore: 1632000
  timesteps_this_iter: 96000
  timesteps_total: 9312000
  training_iteration: 97
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     97 |          15790.8 | 9312000 |   581.34 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 5.34
    apples_agent-0_min: 0
    apples_agent-1_max: 171
    apples_agent-1_mean: 25.83
    apples_agent-1_min: 0
    apples_agent-2_max: 174
    apples_agent-2_mean: 12.45
    apples_agent-2_min: 0
    apples_agent-3_max: 148
    apples_agent-3_mean: 83.53
    apples_agent-3_min: 33
    apples_agent-4_max: 69
    apples_agent-4_mean: 3.09
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 79.08
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 361.29
    cleaning_beam_agent-0_min: 127
    cleaning_beam_agent-1_max: 442
    cleaning_beam_agent-1_mean: 233.69
    cleaning_beam_agent-1_min: 63
    cleaning_beam_agent-2_max: 575
    cleaning_beam_agent-2_mean: 326.48
    cleaning_beam_agent-2_min: 113
    cleaning_beam_agent-3_max: 118
    cleaning_beam_agent-3_mean: 62.14
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 569
    cleaning_beam_agent-4_mean: 366.85
    cleaning_beam_agent-4_min: 199
    cleaning_beam_agent-5_max: 379
    cleaning_beam_agent-5_mean: 95.47
    cleaning_beam_agent-5_min: 26
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-54-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 858.9999999999828
  episode_reward_mean: 599.7699999999992
  episode_reward_min: 246.99999999999608
  episodes_this_iter: 96
  episodes_total: 9408
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13050.604
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 1.231049656867981
        entropy_coeff: 0.0017600000137463212
        kl: 0.011581341736018658
        model: {}
        policy_loss: -0.029942218214273453
        total_loss: -0.02853509597480297
        vf_explained_var: 0.09200237691402435
        vf_loss: 12.575008392333984
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 1.1630611419677734
        entropy_coeff: 0.0017600000137463212
        kl: 0.014177394099533558
        model: {}
        policy_loss: -0.03231712058186531
        total_loss: -0.030163636431097984
        vf_explained_var: 0.0150052011013031
        vf_loss: 13.649941444396973
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 1.1551622152328491
        entropy_coeff: 0.0017600000137463212
        kl: 0.013196433894336224
        model: {}
        policy_loss: -0.026716265827417374
        total_loss: -0.024828139692544937
        vf_explained_var: 0.07445423305034637
        vf_loss: 12.819241523742676
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 0.859734296798706
        entropy_coeff: 0.0017600000137463212
        kl: 0.01051408052444458
        model: {}
        policy_loss: -0.023790687322616577
        total_loss: -0.022058088332414627
        vf_explained_var: 0.17465715110301971
        vf_loss: 11.42915153503418
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 1.0749757289886475
        entropy_coeff: 0.0017600000137463212
        kl: 0.01700945571064949
        model: {}
        policy_loss: -0.034997910261154175
        total_loss: -0.032268889248371124
        vf_explained_var: 0.11963050067424774
        vf_loss: 12.190910339355469
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 1.0909069776535034
        entropy_coeff: 0.0017600000137463212
        kl: 0.01400246936827898
        model: {}
        policy_loss: -0.03413757309317589
        total_loss: -0.03204407915472984
        vf_explained_var: 0.12430991232395172
        vf_loss: 12.129986763000488
    load_time_ms: 29641.193
    num_steps_sampled: 9408000
    num_steps_trained: 9408000
    sample_time_ms: 138135.284
    update_time_ms: 58.628
  iterations_since_restore: 18
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.618181818181817
    ram_util_percent: 15.253754940711458
  pid: 14340
  policy_reward_max:
    agent-0: 143.16666666666677
    agent-1: 143.16666666666677
    agent-2: 143.16666666666677
    agent-3: 143.16666666666677
    agent-4: 143.16666666666677
    agent-5: 143.16666666666677
  policy_reward_mean:
    agent-0: 99.96166666666696
    agent-1: 99.96166666666696
    agent-2: 99.96166666666696
    agent-3: 99.96166666666696
    agent-4: 99.96166666666696
    agent-5: 99.96166666666696
  policy_reward_min:
    agent-0: 41.16666666666667
    agent-1: 41.16666666666667
    agent-2: 41.16666666666667
    agent-3: 41.16666666666667
    agent-4: 41.16666666666667
    agent-5: 41.16666666666667
  sampler_perf:
    mean_env_wait_ms: 30.88007933384795
    mean_inference_ms: 14.286558397083018
    mean_processing_ms: 65.3441394893238
  time_since_restore: 3417.3360986709595
  time_this_iter_s: 177.36230492591858
  time_total_s: 15968.152970075607
  timestamp: 1637038493
  timesteps_since_restore: 1728000
  timesteps_this_iter: 96000
  timesteps_total: 9408000
  training_iteration: 98
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     98 |          15968.2 | 9408000 |   599.77 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 4.84
    apples_agent-0_min: 0
    apples_agent-1_max: 271
    apples_agent-1_mean: 22.23
    apples_agent-1_min: 0
    apples_agent-2_max: 103
    apples_agent-2_mean: 9.74
    apples_agent-2_min: 0
    apples_agent-3_max: 140
    apples_agent-3_mean: 86.42
    apples_agent-3_min: 23
    apples_agent-4_max: 90
    apples_agent-4_mean: 6.11
    apples_agent-4_min: 0
    apples_agent-5_max: 161
    apples_agent-5_mean: 75.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 563
    cleaning_beam_agent-0_mean: 368.26
    cleaning_beam_agent-0_min: 149
    cleaning_beam_agent-1_max: 465
    cleaning_beam_agent-1_mean: 227.89
    cleaning_beam_agent-1_min: 55
    cleaning_beam_agent-2_max: 555
    cleaning_beam_agent-2_mean: 313.63
    cleaning_beam_agent-2_min: 28
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 67.3
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 560
    cleaning_beam_agent-4_mean: 364.15
    cleaning_beam_agent-4_min: 128
    cleaning_beam_agent-5_max: 389
    cleaning_beam_agent-5_mean: 113.53
    cleaning_beam_agent-5_min: 31
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.05
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-57-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 802.9999999999898
  episode_reward_mean: 578.9999999999997
  episode_reward_min: 194.99999999999915
  episodes_this_iter: 96
  episodes_total: 9504
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12969.845
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 1.2201462984085083
        entropy_coeff: 0.0017600000137463212
        kl: 0.011651742272078991
        model: {}
        policy_loss: -0.029916809871792793
        total_loss: -0.028447095304727554
        vf_explained_var: 0.07739070057868958
        vf_loss: 12.868246078491211
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 1.20046067237854
        entropy_coeff: 0.0017600000137463212
        kl: 0.013310586102306843
        model: {}
        policy_loss: -0.03153539076447487
        total_loss: -0.029649676755070686
        vf_explained_var: 0.042278096079826355
        vf_loss: 13.36413288116455
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 1.1550450325012207
        entropy_coeff: 0.0017600000137463212
        kl: 0.012320443987846375
        model: {}
        policy_loss: -0.0289533119648695
        total_loss: -0.02728169411420822
        vf_explained_var: 0.11025060713291168
        vf_loss: 12.404094696044922
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 0.8628053069114685
        entropy_coeff: 0.0017600000137463212
        kl: 0.010217620059847832
        model: {}
        policy_loss: -0.025606993585824966
        total_loss: -0.023951606824994087
        vf_explained_var: 0.1895277351140976
        vf_loss: 11.304025650024414
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 1.0790973901748657
        entropy_coeff: 0.0017600000137463212
        kl: 0.013743027113378048
        model: {}
        policy_loss: -0.03729088231921196
        total_loss: -0.03523341193795204
        vf_explained_var: 0.1327575147151947
        vf_loss: 12.08076286315918
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 1.0822423696517944
        entropy_coeff: 0.0017600000137463212
        kl: 0.014080807566642761
        model: {}
        policy_loss: -0.03682178258895874
        total_loss: -0.03471067547798157
        vf_explained_var: 0.1394602656364441
        vf_loss: 11.99693489074707
    load_time_ms: 29212.499
    num_steps_sampled: 9504000
    num_steps_trained: 9504000
    sample_time_ms: 137497.925
    update_time_ms: 57.711
  iterations_since_restore: 19
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.92413793103448
    ram_util_percent: 15.108189655172414
  pid: 14340
  policy_reward_max:
    agent-0: 133.83333333333348
    agent-1: 133.83333333333348
    agent-2: 133.83333333333348
    agent-3: 133.83333333333348
    agent-4: 133.83333333333348
    agent-5: 133.83333333333348
  policy_reward_mean:
    agent-0: 96.50000000000027
    agent-1: 96.50000000000027
    agent-2: 96.50000000000027
    agent-3: 96.50000000000027
    agent-4: 96.50000000000027
    agent-5: 96.50000000000027
  policy_reward_min:
    agent-0: 32.50000000000004
    agent-1: 32.50000000000004
    agent-2: 32.50000000000004
    agent-3: 32.50000000000004
    agent-4: 32.50000000000004
    agent-5: 32.50000000000004
  sampler_perf:
    mean_env_wait_ms: 30.88706870632101
    mean_inference_ms: 14.302108371038731
    mean_processing_ms: 65.3417343191481
  time_since_restore: 3580.576472043991
  time_this_iter_s: 163.24037337303162
  time_total_s: 16131.393343448639
  timestamp: 1637038656
  timesteps_since_restore: 1824000
  timesteps_this_iter: 96000
  timesteps_total: 9504000
  training_iteration: 99
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |     99 |          16131.4 | 9504000 |      579 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 2.78
    apples_agent-0_min: 0
    apples_agent-1_max: 124
    apples_agent-1_mean: 24.57
    apples_agent-1_min: 0
    apples_agent-2_max: 368
    apples_agent-2_mean: 16.22
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 83.69
    apples_agent-3_min: 25
    apples_agent-4_max: 68
    apples_agent-4_mean: 4.03
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 78.66
    apples_agent-5_min: 12
    cleaning_beam_agent-0_max: 583
    cleaning_beam_agent-0_mean: 367.04
    cleaning_beam_agent-0_min: 125
    cleaning_beam_agent-1_max: 397
    cleaning_beam_agent-1_mean: 224.91
    cleaning_beam_agent-1_min: 54
    cleaning_beam_agent-2_max: 521
    cleaning_beam_agent-2_mean: 310.47
    cleaning_beam_agent-2_min: 55
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 67.86
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 508
    cleaning_beam_agent-4_mean: 356.59
    cleaning_beam_agent-4_min: 135
    cleaning_beam_agent-5_max: 327
    cleaning_beam_agent-5_mean: 110.39
    cleaning_beam_agent-5_min: 29
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-00-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 849.9999999999916
  episode_reward_mean: 587.5299999999992
  episode_reward_min: 147.0000000000001
  episodes_this_iter: 96
  episodes_total: 9600
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12902.255
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 1.23261296749115
        entropy_coeff: 0.0017600000137463212
        kl: 0.011956276372075081
        model: {}
        policy_loss: -0.029375800862908363
        total_loss: -0.02778497338294983
        vf_explained_var: 0.10550053417682648
        vf_loss: 13.689735412597656
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 1.1650934219360352
        entropy_coeff: 0.0017600000137463212
        kl: 0.013277892023324966
        model: {}
        policy_loss: -0.03297433257102966
        total_loss: -0.030862348154187202
        vf_explained_var: 0.015961214900016785
        vf_loss: 15.069747924804688
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 1.133309006690979
        entropy_coeff: 0.0017600000137463212
        kl: 0.012079470790922642
        model: {}
        policy_loss: -0.029021061956882477
        total_loss: -0.027260223403573036
        vf_explained_var: 0.12443536520004272
        vf_loss: 13.395649909973145
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 0.8511108160018921
        entropy_coeff: 0.0017600000137463212
        kl: 0.010283894836902618
        model: {}
        policy_loss: -0.02614428475499153
        total_loss: -0.02440764755010605
        vf_explained_var: 0.2306116223335266
        vf_loss: 11.778107643127441
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 1.0792125463485718
        entropy_coeff: 0.0017600000137463212
        kl: 0.01468149945139885
        model: {}
        policy_loss: -0.037502869963645935
        total_loss: -0.035143136978149414
        vf_explained_var: 0.1359480321407318
        vf_loss: 13.228422164916992
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 1.086299180984497
        entropy_coeff: 0.0017600000137463212
        kl: 0.014006417244672775
        model: {}
        policy_loss: -0.03499344736337662
        total_loss: -0.03284500539302826
        vf_explained_var: 0.177444726228714
        vf_loss: 12.590460777282715
    load_time_ms: 27599.161
    num_steps_sampled: 9600000
    num_steps_trained: 9600000
    sample_time_ms: 137002.749
    update_time_ms: 54.572
  iterations_since_restore: 20
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.029059829059833
    ram_util_percent: 15.052991452991451
  pid: 14340
  policy_reward_max:
    agent-0: 141.6666666666669
    agent-1: 141.6666666666669
    agent-2: 141.6666666666669
    agent-3: 141.6666666666669
    agent-4: 141.6666666666669
    agent-5: 141.6666666666669
  policy_reward_mean:
    agent-0: 97.92166666666695
    agent-1: 97.92166666666695
    agent-2: 97.92166666666695
    agent-3: 97.92166666666695
    agent-4: 97.92166666666695
    agent-5: 97.92166666666695
  policy_reward_min:
    agent-0: 24.500000000000025
    agent-1: 24.500000000000025
    agent-2: 24.500000000000025
    agent-3: 24.500000000000025
    agent-4: 24.500000000000025
    agent-5: 24.500000000000025
  sampler_perf:
    mean_env_wait_ms: 30.886243984115882
    mean_inference_ms: 14.314613036110837
    mean_processing_ms: 65.34707977050138
  time_since_restore: 3744.6412060260773
  time_this_iter_s: 164.06473398208618
  time_total_s: 16295.458077430725
  timestamp: 1637038820
  timesteps_since_restore: 1920000
  timesteps_this_iter: 96000
  timesteps_total: 9600000
  training_iteration: 100
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    100 |          16295.5 | 9600000 |   587.53 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 2.67
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 23.62
    apples_agent-1_min: 0
    apples_agent-2_max: 202
    apples_agent-2_mean: 11.12
    apples_agent-2_min: 0
    apples_agent-3_max: 139
    apples_agent-3_mean: 87.02
    apples_agent-3_min: 42
    apples_agent-4_max: 74
    apples_agent-4_mean: 4.13
    apples_agent-4_min: 0
    apples_agent-5_max: 154
    apples_agent-5_mean: 76.94
    apples_agent-5_min: 12
    cleaning_beam_agent-0_max: 567
    cleaning_beam_agent-0_mean: 370.67
    cleaning_beam_agent-0_min: 211
    cleaning_beam_agent-1_max: 486
    cleaning_beam_agent-1_mean: 229.04
    cleaning_beam_agent-1_min: 70
    cleaning_beam_agent-2_max: 525
    cleaning_beam_agent-2_mean: 300.23
    cleaning_beam_agent-2_min: 56
    cleaning_beam_agent-3_max: 149
    cleaning_beam_agent-3_mean: 62.51
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 530
    cleaning_beam_agent-4_mean: 364.53
    cleaning_beam_agent-4_min: 151
    cleaning_beam_agent-5_max: 258
    cleaning_beam_agent-5_mean: 90.16
    cleaning_beam_agent-5_min: 27
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-03-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 862.9999999999794
  episode_reward_mean: 609.7399999999981
  episode_reward_min: 236.99999999999596
  episodes_this_iter: 96
  episodes_total: 9696
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12911.234
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 1.2481622695922852
        entropy_coeff: 0.0017600000137463212
        kl: 0.012035315856337547
        model: {}
        policy_loss: -0.028815705329179764
        total_loss: -0.02717294916510582
        vf_explained_var: 0.07530030608177185
        vf_loss: 14.324607849121094
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 1.1605700254440308
        entropy_coeff: 0.0017600000137463212
        kl: 0.013402036391198635
        model: {}
        policy_loss: -0.03164365142583847
        total_loss: -0.029507799074053764
        vf_explained_var: 0.032584697008132935
        vf_loss: 14.980457305908203
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 1.164298176765442
        entropy_coeff: 0.0017600000137463212
        kl: 0.012421551160514355
        model: {}
        policy_loss: -0.029301881790161133
        total_loss: -0.02751345932483673
        vf_explained_var: 0.12610988318920135
        vf_loss: 13.532768249511719
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 0.8233579397201538
        entropy_coeff: 0.0017600000137463212
        kl: 0.009714556857943535
        model: {}
        policy_loss: -0.02349174953997135
        total_loss: -0.02178407460451126
        vf_explained_var: 0.2166237235069275
        vf_loss: 12.138720512390137
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 1.075173258781433
        entropy_coeff: 0.0017600000137463212
        kl: 0.014345920644700527
        model: {}
        policy_loss: -0.038135357201099396
        total_loss: -0.03576739877462387
        vf_explained_var: 0.1019914299249649
        vf_loss: 13.91077995300293
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 1.0632907152175903
        entropy_coeff: 0.0017600000137463212
        kl: 0.013610518537461758
        model: {}
        policy_loss: -0.03613944724202156
        total_loss: -0.03401641175150871
        vf_explained_var: 0.17909404635429382
        vf_loss: 12.723228454589844
    load_time_ms: 27798.215
    num_steps_sampled: 9696000
    num_steps_trained: 9696000
    sample_time_ms: 134189.188
    update_time_ms: 50.041
  iterations_since_restore: 21
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.53598326359833
    ram_util_percent: 15.015062761506277
  pid: 14340
  policy_reward_max:
    agent-0: 143.83333333333317
    agent-1: 143.83333333333317
    agent-2: 143.83333333333317
    agent-3: 143.83333333333317
    agent-4: 143.83333333333317
    agent-5: 143.83333333333317
  policy_reward_mean:
    agent-0: 101.62333333333366
    agent-1: 101.62333333333366
    agent-2: 101.62333333333366
    agent-3: 101.62333333333366
    agent-4: 101.62333333333366
    agent-5: 101.62333333333366
  policy_reward_min:
    agent-0: 39.49999999999994
    agent-1: 39.49999999999994
    agent-2: 39.49999999999994
    agent-3: 39.49999999999994
    agent-4: 39.49999999999994
    agent-5: 39.49999999999994
  sampler_perf:
    mean_env_wait_ms: 30.88735731828841
    mean_inference_ms: 14.325120108468523
    mean_processing_ms: 65.34911540015975
  time_since_restore: 3911.0964601039886
  time_this_iter_s: 166.45525407791138
  time_total_s: 16461.913331508636
  timestamp: 1637038988
  timesteps_since_restore: 2016000
  timesteps_this_iter: 96000
  timesteps_total: 9696000
  training_iteration: 101
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    101 |          16461.9 | 9696000 |   609.74 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 76
    apples_agent-0_mean: 2.8
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 20.81
    apples_agent-1_min: 0
    apples_agent-2_max: 147
    apples_agent-2_mean: 10.85
    apples_agent-2_min: 0
    apples_agent-3_max: 184
    apples_agent-3_mean: 88.31
    apples_agent-3_min: 40
    apples_agent-4_max: 52
    apples_agent-4_mean: 2.17
    apples_agent-4_min: 0
    apples_agent-5_max: 137
    apples_agent-5_mean: 82.25
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 535
    cleaning_beam_agent-0_mean: 385.76
    cleaning_beam_agent-0_min: 200
    cleaning_beam_agent-1_max: 379
    cleaning_beam_agent-1_mean: 233.25
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 604
    cleaning_beam_agent-2_mean: 307.42
    cleaning_beam_agent-2_min: 110
    cleaning_beam_agent-3_max: 189
    cleaning_beam_agent-3_mean: 70.46
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 538
    cleaning_beam_agent-4_mean: 381.35
    cleaning_beam_agent-4_min: 159
    cleaning_beam_agent-5_max: 417
    cleaning_beam_agent-5_mean: 91.9
    cleaning_beam_agent-5_min: 28
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-05-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 854.9999999999859
  episode_reward_mean: 632.6799999999965
  episode_reward_min: 230.99999999999608
  episodes_this_iter: 96
  episodes_total: 9792
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12911.023
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 1.2382234334945679
        entropy_coeff: 0.0017600000137463212
        kl: 0.012042643502354622
        model: {}
        policy_loss: -0.028942327946424484
        total_loss: -0.0272645503282547
        vf_explained_var: 0.05637536942958832
        vf_loss: 14.485238075256348
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 1.1943604946136475
        entropy_coeff: 0.0017600000137463212
        kl: 0.013315977528691292
        model: {}
        policy_loss: -0.031543832272291183
        total_loss: -0.029514508321881294
        vf_explained_var: 0.0438232421875
        vf_loss: 14.682008743286133
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 1.1517689228057861
        entropy_coeff: 0.0017600000137463212
        kl: 0.013439206406474113
        model: {}
        policy_loss: -0.028860781341791153
        total_loss: -0.02687239646911621
        vf_explained_var: 0.13482895493507385
        vf_loss: 13.27655029296875
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 0.7891468405723572
        entropy_coeff: 0.0017600000137463212
        kl: 0.009849192574620247
        model: {}
        policy_loss: -0.024767518043518066
        total_loss: -0.02295006811618805
        vf_explained_var: 0.1936463564634323
        vf_loss: 12.365118026733398
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 1.0668834447860718
        entropy_coeff: 0.0017600000137463212
        kl: 0.014570878818631172
        model: {}
        policy_loss: -0.03632625937461853
        total_loss: -0.03395850956439972
        vf_explained_var: 0.13196244835853577
        vf_loss: 13.31287670135498
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 1.0458319187164307
        entropy_coeff: 0.0017600000137463212
        kl: 0.013420294038951397
        model: {}
        policy_loss: -0.03461797907948494
        total_loss: -0.0324922576546669
        vf_explained_var: 0.1640661060810089
        vf_loss: 12.82324504852295
    load_time_ms: 25208.429
    num_steps_sampled: 9792000
    num_steps_trained: 9792000
    sample_time_ms: 134774.296
    update_time_ms: 46.111
  iterations_since_restore: 22
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.891880341880345
    ram_util_percent: 14.877350427350427
  pid: 14340
  policy_reward_max:
    agent-0: 142.50000000000017
    agent-1: 142.50000000000017
    agent-2: 142.50000000000017
    agent-3: 142.50000000000017
    agent-4: 142.50000000000017
    agent-5: 142.50000000000017
  policy_reward_mean:
    agent-0: 105.44666666666699
    agent-1: 105.44666666666699
    agent-2: 105.44666666666699
    agent-3: 105.44666666666699
    agent-4: 105.44666666666699
    agent-5: 105.44666666666699
  policy_reward_min:
    agent-0: 38.49999999999999
    agent-1: 38.49999999999999
    agent-2: 38.49999999999999
    agent-3: 38.49999999999999
    agent-4: 38.49999999999999
    agent-5: 38.49999999999999
  sampler_perf:
    mean_env_wait_ms: 30.896228962011527
    mean_inference_ms: 14.333669395859392
    mean_processing_ms: 65.36331742173341
  time_since_restore: 4074.939600467682
  time_this_iter_s: 163.84314036369324
  time_total_s: 16625.75647187233
  timestamp: 1637039152
  timesteps_since_restore: 2112000
  timesteps_this_iter: 96000
  timesteps_total: 9792000
  training_iteration: 102
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    102 |          16625.8 | 9792000 |   632.68 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 126
    apples_agent-0_mean: 3.67
    apples_agent-0_min: 0
    apples_agent-1_max: 386
    apples_agent-1_mean: 21.09
    apples_agent-1_min: 0
    apples_agent-2_max: 173
    apples_agent-2_mean: 14.19
    apples_agent-2_min: 0
    apples_agent-3_max: 164
    apples_agent-3_mean: 96.05
    apples_agent-3_min: 41
    apples_agent-4_max: 70
    apples_agent-4_mean: 4.13
    apples_agent-4_min: 0
    apples_agent-5_max: 205
    apples_agent-5_mean: 81.82
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 520
    cleaning_beam_agent-0_mean: 391.52
    cleaning_beam_agent-0_min: 172
    cleaning_beam_agent-1_max: 510
    cleaning_beam_agent-1_mean: 266.75
    cleaning_beam_agent-1_min: 70
    cleaning_beam_agent-2_max: 572
    cleaning_beam_agent-2_mean: 317.6
    cleaning_beam_agent-2_min: 92
    cleaning_beam_agent-3_max: 151
    cleaning_beam_agent-3_mean: 68.76
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 555
    cleaning_beam_agent-4_mean: 366.57
    cleaning_beam_agent-4_min: 180
    cleaning_beam_agent-5_max: 292
    cleaning_beam_agent-5_mean: 98.59
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-08-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 862.9999999999759
  episode_reward_mean: 617.009999999998
  episode_reward_min: 229.99999999999812
  episodes_this_iter: 96
  episodes_total: 9888
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12896.437
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 1.2342532873153687
        entropy_coeff: 0.0017600000137463212
        kl: 0.010891443118453026
        model: {}
        policy_loss: -0.029291464015841484
        total_loss: -0.027753498405218124
        vf_explained_var: 0.009549945592880249
        vf_loss: 15.319643020629883
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 1.1635725498199463
        entropy_coeff: 0.0017600000137463212
        kl: 0.013090671971440315
        model: {}
        policy_loss: -0.03253168612718582
        total_loss: -0.030414095148444176
        vf_explained_var: 0.0006681233644485474
        vf_loss: 15.473443984985352
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 1.1346428394317627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0122141744941473
        model: {}
        policy_loss: -0.030497722327709198
        total_loss: -0.028726249933242798
        vf_explained_var: 0.14278669655323029
        vf_loss: 13.25606918334961
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 0.8035193681716919
        entropy_coeff: 0.0017600000137463212
        kl: 0.010224517434835434
        model: {}
        policy_loss: -0.024854959920048714
        total_loss: -0.022978950291872025
        vf_explained_var: 0.19403821229934692
        vf_loss: 12.453007698059082
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 1.067700743675232
        entropy_coeff: 0.0017600000137463212
        kl: 0.01407729834318161
        model: {}
        policy_loss: -0.03660477697849274
        total_loss: -0.03428555279970169
        vf_explained_var: 0.1071406900882721
        vf_loss: 13.829161643981934
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 1.0559828281402588
        entropy_coeff: 0.0017600000137463212
        kl: 0.013138328678905964
        model: {}
        policy_loss: -0.0354183092713356
        total_loss: -0.033367812633514404
        vf_explained_var: 0.1708856225013733
        vf_loss: 12.813614845275879
    load_time_ms: 23397.702
    num_steps_sampled: 9888000
    num_steps_trained: 9888000
    sample_time_ms: 134025.711
    update_time_ms: 70.062
  iterations_since_restore: 23
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.36035242290749
    ram_util_percent: 14.877533039647574
  pid: 14340
  policy_reward_max:
    agent-0: 143.8333333333333
    agent-1: 143.8333333333333
    agent-2: 143.8333333333333
    agent-3: 143.8333333333333
    agent-4: 143.8333333333333
    agent-5: 143.8333333333333
  policy_reward_mean:
    agent-0: 102.83500000000032
    agent-1: 102.83500000000032
    agent-2: 102.83500000000032
    agent-3: 102.83500000000032
    agent-4: 102.83500000000032
    agent-5: 102.83500000000032
  policy_reward_min:
    agent-0: 38.3333333333333
    agent-1: 38.3333333333333
    agent-2: 38.3333333333333
    agent-3: 38.3333333333333
    agent-4: 38.3333333333333
    agent-5: 38.3333333333333
  sampler_perf:
    mean_env_wait_ms: 30.913189530019235
    mean_inference_ms: 14.343313064387937
    mean_processing_ms: 65.37230119441868
  time_since_restore: 4233.962933301926
  time_this_iter_s: 159.02333283424377
  time_total_s: 16784.779804706573
  timestamp: 1637039312
  timesteps_since_restore: 2208000
  timesteps_this_iter: 96000
  timesteps_total: 9888000
  training_iteration: 103
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    103 |          16784.8 | 9888000 |   617.01 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 235
    apples_agent-0_mean: 6.0
    apples_agent-0_min: 0
    apples_agent-1_max: 315
    apples_agent-1_mean: 23.45
    apples_agent-1_min: 0
    apples_agent-2_max: 220
    apples_agent-2_mean: 15.08
    apples_agent-2_min: 0
    apples_agent-3_max: 175
    apples_agent-3_mean: 90.52
    apples_agent-3_min: 32
    apples_agent-4_max: 70
    apples_agent-4_mean: 5.83
    apples_agent-4_min: 0
    apples_agent-5_max: 147
    apples_agent-5_mean: 78.79
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 512
    cleaning_beam_agent-0_mean: 370.98
    cleaning_beam_agent-0_min: 196
    cleaning_beam_agent-1_max: 520
    cleaning_beam_agent-1_mean: 264.43
    cleaning_beam_agent-1_min: 37
    cleaning_beam_agent-2_max: 503
    cleaning_beam_agent-2_mean: 316.12
    cleaning_beam_agent-2_min: 85
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 64.44
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 519
    cleaning_beam_agent-4_mean: 365.58
    cleaning_beam_agent-4_min: 119
    cleaning_beam_agent-5_max: 330
    cleaning_beam_agent-5_mean: 104.7
    cleaning_beam_agent-5_min: 39
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-11-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 844.999999999979
  episode_reward_mean: 612.2299999999982
  episode_reward_min: 265.99999999999625
  episodes_this_iter: 96
  episodes_total: 9984
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12885.334
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 1.248113989830017
        entropy_coeff: 0.0017600000137463212
        kl: 0.011026730760931969
        model: {}
        policy_loss: -0.02895691990852356
        total_loss: -0.027533065527677536
        vf_explained_var: 0.029683679342269897
        vf_loss: 14.151853561401367
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 1.1717126369476318
        entropy_coeff: 0.0017600000137463212
        kl: 0.013815924525260925
        model: {}
        policy_loss: -0.032735489308834076
        total_loss: -0.03062652237713337
        vf_explained_var: 0.03451727330684662
        vf_loss: 14.079950332641602
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 1.1601777076721191
        entropy_coeff: 0.0017600000137463212
        kl: 0.012605156749486923
        model: {}
        policy_loss: -0.030830610543489456
        total_loss: -0.028998766094446182
        vf_explained_var: 0.07224339246749878
        vf_loss: 13.52724552154541
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 0.8004350662231445
        entropy_coeff: 0.0017600000137463212
        kl: 0.01099916361272335
        model: {}
        policy_loss: -0.023308847099542618
        total_loss: -0.021341310814023018
        vf_explained_var: 0.19414572417736053
        vf_loss: 11.764671325683594
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 1.0748145580291748
        entropy_coeff: 0.0017600000137463212
        kl: 0.01359906792640686
        model: {}
        policy_loss: -0.035537682473659515
        total_loss: -0.03345487639307976
        vf_explained_var: 0.13985489308834076
        vf_loss: 12.546709060668945
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 1.064832091331482
        entropy_coeff: 0.0017600000137463212
        kl: 0.013232354074716568
        model: {}
        policy_loss: -0.03532834351062775
        total_loss: -0.03333296999335289
        vf_explained_var: 0.16157886385917664
        vf_loss: 12.23009967803955
    load_time_ms: 21587.372
    num_steps_sampled: 9984000
    num_steps_trained: 9984000
    sample_time_ms: 133312.845
    update_time_ms: 70.49
  iterations_since_restore: 24
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.35178571428571
    ram_util_percent: 14.864285714285714
  pid: 14340
  policy_reward_max:
    agent-0: 140.83333333333348
    agent-1: 140.83333333333348
    agent-2: 140.83333333333348
    agent-3: 140.83333333333348
    agent-4: 140.83333333333348
    agent-5: 140.83333333333348
  policy_reward_mean:
    agent-0: 102.03833333333365
    agent-1: 102.03833333333365
    agent-2: 102.03833333333365
    agent-3: 102.03833333333365
    agent-4: 102.03833333333365
    agent-5: 102.03833333333365
  policy_reward_min:
    agent-0: 44.333333333333286
    agent-1: 44.333333333333286
    agent-2: 44.333333333333286
    agent-3: 44.333333333333286
    agent-4: 44.333333333333286
    agent-5: 44.333333333333286
  sampler_perf:
    mean_env_wait_ms: 30.91993229070811
    mean_inference_ms: 14.351178785949683
    mean_processing_ms: 65.43499022848395
  time_since_restore: 4390.636916875839
  time_this_iter_s: 156.67398357391357
  time_total_s: 16941.453788280487
  timestamp: 1637039469
  timesteps_since_restore: 2304000
  timesteps_this_iter: 96000
  timesteps_total: 9984000
  training_iteration: 104
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    104 |          16941.5 | 9984000 |   612.23 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 235
    apples_agent-0_mean: 5.62
    apples_agent-0_min: 0
    apples_agent-1_max: 123
    apples_agent-1_mean: 20.61
    apples_agent-1_min: 0
    apples_agent-2_max: 178
    apples_agent-2_mean: 13.76
    apples_agent-2_min: 0
    apples_agent-3_max: 188
    apples_agent-3_mean: 93.09
    apples_agent-3_min: 37
    apples_agent-4_max: 100
    apples_agent-4_mean: 7.51
    apples_agent-4_min: 0
    apples_agent-5_max: 147
    apples_agent-5_mean: 74.84
    apples_agent-5_min: 7
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 369.3
    cleaning_beam_agent-0_min: 228
    cleaning_beam_agent-1_max: 448
    cleaning_beam_agent-1_mean: 256.16
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 672
    cleaning_beam_agent-2_mean: 348.57
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 58.12
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 588
    cleaning_beam_agent-4_mean: 362.01
    cleaning_beam_agent-4_min: 203
    cleaning_beam_agent-5_max: 406
    cleaning_beam_agent-5_mean: 110.31
    cleaning_beam_agent-5_min: 24
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-14-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 885.999999999978
  episode_reward_mean: 616.329999999998
  episode_reward_min: 261.9999999999949
  episodes_this_iter: 96
  episodes_total: 10080
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12883.139
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 1.245090365409851
        entropy_coeff: 0.0017600000137463212
        kl: 0.011772556230425835
        model: {}
        policy_loss: -0.029860906302928925
        total_loss: -0.02830486372113228
        vf_explained_var: 0.051114097237586975
        vf_loss: 13.928942680358887
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 1.1805754899978638
        entropy_coeff: 0.0017600000137463212
        kl: 0.012982621788978577
        model: {}
        policy_loss: -0.032649390399456024
        total_loss: -0.030674738809466362
        vf_explained_var: 0.007841944694519043
        vf_loss: 14.559422492980957
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 1.1372692584991455
        entropy_coeff: 0.0017600000137463212
        kl: 0.01250014454126358
        model: {}
        policy_loss: -0.02931186929345131
        total_loss: -0.027493169531226158
        vf_explained_var: 0.0999453216791153
        vf_loss: 13.202667236328125
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 0.8058589696884155
        entropy_coeff: 0.0017600000137463212
        kl: 0.01023163739591837
        model: {}
        policy_loss: -0.023771723732352257
        total_loss: -0.021991368383169174
        vf_explained_var: 0.21527878940105438
        vf_loss: 11.523406028747559
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 1.0845820903778076
        entropy_coeff: 0.0017600000137463212
        kl: 0.014095475897192955
        model: {}
        policy_loss: -0.03724485635757446
        total_loss: -0.03502523899078369
        vf_explained_var: 0.10826645791530609
        vf_loss: 13.093832015991211
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 1.0531271696090698
        entropy_coeff: 0.0017600000137463212
        kl: 0.01368140708655119
        model: {}
        policy_loss: -0.03548969328403473
        total_loss: -0.03335630148649216
        vf_explained_var: 0.14714084565639496
        vf_loss: 12.506093978881836
    load_time_ms: 21292.135
    num_steps_sampled: 10080000
    num_steps_trained: 10080000
    sample_time_ms: 132395.419
    update_time_ms: 73.045
  iterations_since_restore: 25
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.070281124497992
    ram_util_percent: 15.108433734939757
  pid: 14340
  policy_reward_max:
    agent-0: 147.66666666666688
    agent-1: 147.66666666666688
    agent-2: 147.66666666666688
    agent-3: 147.66666666666688
    agent-4: 147.66666666666688
    agent-5: 147.66666666666688
  policy_reward_mean:
    agent-0: 102.72166666666698
    agent-1: 102.72166666666698
    agent-2: 102.72166666666698
    agent-3: 102.72166666666698
    agent-4: 102.72166666666698
    agent-5: 102.72166666666698
  policy_reward_min:
    agent-0: 43.666666666666586
    agent-1: 43.666666666666586
    agent-2: 43.666666666666586
    agent-3: 43.666666666666586
    agent-4: 43.666666666666586
    agent-5: 43.666666666666586
  sampler_perf:
    mean_env_wait_ms: 30.930515902015337
    mean_inference_ms: 14.359607464720904
    mean_processing_ms: 65.44989167376926
  time_since_restore: 4565.087835073471
  time_this_iter_s: 174.45091819763184
  time_total_s: 17115.90470647812
  timestamp: 1637039644
  timesteps_since_restore: 2400000
  timesteps_this_iter: 96000
  timesteps_total: 10080000
  training_iteration: 105
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    105 |          17115.9 | 10080000 |   616.33 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 73
    apples_agent-0_mean: 3.44
    apples_agent-0_min: 0
    apples_agent-1_max: 174
    apples_agent-1_mean: 26.14
    apples_agent-1_min: 0
    apples_agent-2_max: 99
    apples_agent-2_mean: 11.87
    apples_agent-2_min: 0
    apples_agent-3_max: 188
    apples_agent-3_mean: 93.78
    apples_agent-3_min: 37
    apples_agent-4_max: 30
    apples_agent-4_mean: 3.24
    apples_agent-4_min: 0
    apples_agent-5_max: 135
    apples_agent-5_mean: 77.01
    apples_agent-5_min: 28
    cleaning_beam_agent-0_max: 502
    cleaning_beam_agent-0_mean: 377.28
    cleaning_beam_agent-0_min: 192
    cleaning_beam_agent-1_max: 542
    cleaning_beam_agent-1_mean: 263.37
    cleaning_beam_agent-1_min: 62
    cleaning_beam_agent-2_max: 562
    cleaning_beam_agent-2_mean: 341.82
    cleaning_beam_agent-2_min: 107
    cleaning_beam_agent-3_max: 104
    cleaning_beam_agent-3_mean: 51.03
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 513
    cleaning_beam_agent-4_mean: 368.0
    cleaning_beam_agent-4_min: 180
    cleaning_beam_agent-5_max: 344
    cleaning_beam_agent-5_mean: 104.86
    cleaning_beam_agent-5_min: 35
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-17-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 845.9999999999825
  episode_reward_mean: 626.8999999999983
  episode_reward_min: 336.000000000003
  episodes_this_iter: 96
  episodes_total: 10176
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12865.376
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 1.2302660942077637
        entropy_coeff: 0.0017600000137463212
        kl: 0.013167091645300388
        model: {}
        policy_loss: -0.028749896213412285
        total_loss: -0.027028486132621765
        vf_explained_var: 0.0479796826839447
        vf_loss: 12.532588958740234
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 1.1400443315505981
        entropy_coeff: 0.0017600000137463212
        kl: 0.014255525544285774
        model: {}
        policy_loss: -0.03151620179414749
        total_loss: -0.02941114827990532
        vf_explained_var: 0.04178802669048309
        vf_loss: 12.604268074035645
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 1.1257827281951904
        entropy_coeff: 0.0017600000137463212
        kl: 0.012868721038103104
        model: {}
        policy_loss: -0.031813882291316986
        total_loss: -0.030011307448148727
        vf_explained_var: 0.07904571294784546
        vf_loss: 12.102102279663086
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 0.7964876890182495
        entropy_coeff: 0.0017600000137463212
        kl: 0.009546401910483837
        model: {}
        policy_loss: -0.022606026381254196
        total_loss: -0.020988181233406067
        vf_explained_var: 0.15605869889259338
        vf_loss: 11.103864669799805
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 1.0666226148605347
        entropy_coeff: 0.0017600000137463212
        kl: 0.01356072910130024
        model: {}
        policy_loss: -0.035606466233730316
        total_loss: -0.033572494983673096
        vf_explained_var: 0.08854223787784576
        vf_loss: 11.990819931030273
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 1.0377352237701416
        entropy_coeff: 0.0017600000137463212
        kl: 0.013399654999375343
        model: {}
        policy_loss: -0.034193217754364014
        total_loss: -0.03218681737780571
        vf_explained_var: 0.12369155883789062
        vf_loss: 11.52883529663086
    load_time_ms: 22716.038
    num_steps_sampled: 10176000
    num_steps_trained: 10176000
    sample_time_ms: 132004.745
    update_time_ms: 71.414
  iterations_since_restore: 26
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.62705882352941
    ram_util_percent: 15.091764705882351
  pid: 14340
  policy_reward_max:
    agent-0: 141.00000000000045
    agent-1: 141.00000000000045
    agent-2: 141.00000000000045
    agent-3: 141.00000000000045
    agent-4: 141.00000000000045
    agent-5: 141.00000000000045
  policy_reward_mean:
    agent-0: 104.48333333333365
    agent-1: 104.48333333333365
    agent-2: 104.48333333333365
    agent-3: 104.48333333333365
    agent-4: 104.48333333333365
    agent-5: 104.48333333333365
  policy_reward_min:
    agent-0: 55.999999999999815
    agent-1: 55.999999999999815
    agent-2: 55.999999999999815
    agent-3: 55.999999999999815
    agent-4: 55.999999999999815
    agent-5: 55.999999999999815
  sampler_perf:
    mean_env_wait_ms: 30.94732329187713
    mean_inference_ms: 14.366428581769567
    mean_processing_ms: 65.46621394270377
  time_since_restore: 4743.936633348465
  time_this_iter_s: 178.8487982749939
  time_total_s: 17294.753504753113
  timestamp: 1637039823
  timesteps_since_restore: 2496000
  timesteps_this_iter: 96000
  timesteps_total: 10176000
  training_iteration: 106
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    106 |          17294.8 | 10176000 |    626.9 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 3.06
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 22.45
    apples_agent-1_min: 0
    apples_agent-2_max: 220
    apples_agent-2_mean: 19.45
    apples_agent-2_min: 0
    apples_agent-3_max: 188
    apples_agent-3_mean: 88.16
    apples_agent-3_min: 29
    apples_agent-4_max: 95
    apples_agent-4_mean: 4.3
    apples_agent-4_min: 0
    apples_agent-5_max: 128
    apples_agent-5_mean: 75.93
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 511
    cleaning_beam_agent-0_mean: 358.33
    cleaning_beam_agent-0_min: 180
    cleaning_beam_agent-1_max: 555
    cleaning_beam_agent-1_mean: 283.8
    cleaning_beam_agent-1_min: 72
    cleaning_beam_agent-2_max: 605
    cleaning_beam_agent-2_mean: 351.85
    cleaning_beam_agent-2_min: 128
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 56.0
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 529
    cleaning_beam_agent-4_mean: 363.6
    cleaning_beam_agent-4_min: 142
    cleaning_beam_agent-5_max: 444
    cleaning_beam_agent-5_mean: 111.49
    cleaning_beam_agent-5_min: 30
    fire_beam_agent-0_max: 5
    fire_beam_agent-0_mean: 0.07
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 10
    fire_beam_agent-5_mean: 0.15
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-20-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 849.9999999999822
  episode_reward_mean: 619.6399999999961
  episode_reward_min: 179.99999999999872
  episodes_this_iter: 96
  episodes_total: 10272
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12863.268
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 1.2505992650985718
        entropy_coeff: 0.0017600000137463212
        kl: 0.011687899008393288
        model: {}
        policy_loss: -0.030643993988633156
        total_loss: -0.029016640037298203
        vf_explained_var: 0.047673583030700684
        vf_loss: 14.90829849243164
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 1.1500040292739868
        entropy_coeff: 0.0017600000137463212
        kl: 0.013863232918083668
        model: {}
        policy_loss: -0.03320089355111122
        total_loss: -0.030942346900701523
        vf_explained_var: 0.035669997334480286
        vf_loss: 15.099069595336914
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 1.1155178546905518
        entropy_coeff: 0.0017600000137463212
        kl: 0.012513682246208191
        model: {}
        policy_loss: -0.031235795468091965
        total_loss: -0.029313484206795692
        vf_explained_var: 0.1169716864824295
        vf_loss: 13.828926086425781
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 0.7835979461669922
        entropy_coeff: 0.0017600000137463212
        kl: 0.010448554530739784
        model: {}
        policy_loss: -0.023954037576913834
        total_loss: -0.02200150489807129
        vf_explained_var: 0.20674532651901245
        vf_loss: 12.41952896118164
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 1.0760948657989502
        entropy_coeff: 0.0017600000137463212
        kl: 0.014441633597016335
        model: {}
        policy_loss: -0.037138406187295914
        total_loss: -0.03472242131829262
        vf_explained_var: 0.09216445684432983
        vf_loss: 14.215858459472656
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 1.0472819805145264
        entropy_coeff: 0.0017600000137463212
        kl: 0.013705993071198463
        model: {}
        policy_loss: -0.03541984409093857
        total_loss: -0.03313668817281723
        vf_explained_var: 0.11576664447784424
        vf_loss: 13.851755142211914
    load_time_ms: 23460.105
    num_steps_sampled: 10272000
    num_steps_trained: 10272000
    sample_time_ms: 131672.487
    update_time_ms: 66.893
  iterations_since_restore: 27
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.32992125984252
    ram_util_percent: 15.072834645669294
  pid: 14340
  policy_reward_max:
    agent-0: 141.66666666666706
    agent-1: 141.66666666666706
    agent-2: 141.66666666666706
    agent-3: 141.66666666666706
    agent-4: 141.66666666666706
    agent-5: 141.66666666666706
  policy_reward_mean:
    agent-0: 103.27333333333365
    agent-1: 103.27333333333365
    agent-2: 103.27333333333365
    agent-3: 103.27333333333365
    agent-4: 103.27333333333365
    agent-5: 103.27333333333365
  policy_reward_min:
    agent-0: 30.00000000000007
    agent-1: 30.00000000000007
    agent-2: 30.00000000000007
    agent-3: 30.00000000000007
    agent-4: 30.00000000000007
    agent-5: 30.00000000000007
  sampler_perf:
    mean_env_wait_ms: 30.955611381096304
    mean_inference_ms: 14.373933454756964
    mean_processing_ms: 65.46241426999399
  time_since_restore: 4922.039511442184
  time_this_iter_s: 178.10287809371948
  time_total_s: 17472.856382846832
  timestamp: 1637040002
  timesteps_since_restore: 2592000
  timesteps_this_iter: 96000
  timesteps_total: 10272000
  training_iteration: 107
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    107 |          17472.9 | 10272000 |   619.64 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 2.91
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 24.52
    apples_agent-1_min: 0
    apples_agent-2_max: 283
    apples_agent-2_mean: 13.3
    apples_agent-2_min: 0
    apples_agent-3_max: 138
    apples_agent-3_mean: 84.51
    apples_agent-3_min: 25
    apples_agent-4_max: 158
    apples_agent-4_mean: 5.45
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 77.84
    apples_agent-5_min: 7
    cleaning_beam_agent-0_max: 436
    cleaning_beam_agent-0_mean: 348.1
    cleaning_beam_agent-0_min: 101
    cleaning_beam_agent-1_max: 499
    cleaning_beam_agent-1_mean: 264.4
    cleaning_beam_agent-1_min: 60
    cleaning_beam_agent-2_max: 580
    cleaning_beam_agent-2_mean: 361.32
    cleaning_beam_agent-2_min: 109
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 58.89
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 573
    cleaning_beam_agent-4_mean: 375.72
    cleaning_beam_agent-4_min: 219
    cleaning_beam_agent-5_max: 354
    cleaning_beam_agent-5_mean: 105.76
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-22-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 891.9999999999849
  episode_reward_mean: 618.6299999999974
  episode_reward_min: 244.99999999999775
  episodes_this_iter: 96
  episodes_total: 10368
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12849.387
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 1.257590651512146
        entropy_coeff: 0.0017600000137463212
        kl: 0.011067061685025692
        model: {}
        policy_loss: -0.029463432729244232
        total_loss: -0.027971208095550537
        vf_explained_var: 0.08705039322376251
        vf_loss: 14.92176342010498
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 1.1490614414215088
        entropy_coeff: 0.0017600000137463212
        kl: 0.013783941976726055
        model: {}
        policy_loss: -0.03467336297035217
        total_loss: -0.032314594835042953
        vf_explained_var: 0.006598830223083496
        vf_loss: 16.243257522583008
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 1.1267812252044678
        entropy_coeff: 0.0017600000137463212
        kl: 0.01325441338121891
        model: {}
        policy_loss: -0.03165655583143234
        total_loss: -0.02947058528661728
        vf_explained_var: 0.07178179919719696
        vf_loss: 15.18220043182373
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 0.8073139786720276
        entropy_coeff: 0.0017600000137463212
        kl: 0.01010892167687416
        model: {}
        policy_loss: -0.026115084066987038
        total_loss: -0.024289026856422424
        vf_explained_var: 0.25008440017700195
        vf_loss: 12.25143814086914
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 1.0712926387786865
        entropy_coeff: 0.0017600000137463212
        kl: 0.01461711060255766
        model: {}
        policy_loss: -0.03805593028664589
        total_loss: -0.03564430773258209
        vf_explained_var: 0.15989527106285095
        vf_loss: 13.736763000488281
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 1.0283135175704956
        entropy_coeff: 0.0017600000137463212
        kl: 0.013567543588578701
        model: {}
        policy_loss: -0.035713233053684235
        total_loss: -0.033487528562545776
        vf_explained_var: 0.1921832412481308
        vf_loss: 13.220291137695312
    load_time_ms: 22275.496
    num_steps_sampled: 10368000
    num_steps_trained: 10368000
    sample_time_ms: 131425.04
    update_time_ms: 67.503
  iterations_since_restore: 28
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.954310344827583
    ram_util_percent: 14.911206896551722
  pid: 14340
  policy_reward_max:
    agent-0: 148.66666666666674
    agent-1: 148.66666666666674
    agent-2: 148.66666666666674
    agent-3: 148.66666666666674
    agent-4: 148.66666666666674
    agent-5: 148.66666666666674
  policy_reward_mean:
    agent-0: 103.10500000000029
    agent-1: 103.10500000000029
    agent-2: 103.10500000000029
    agent-3: 103.10500000000029
    agent-4: 103.10500000000029
    agent-5: 103.10500000000029
  policy_reward_min:
    agent-0: 40.83333333333338
    agent-1: 40.83333333333338
    agent-2: 40.83333333333338
    agent-3: 40.83333333333338
    agent-4: 40.83333333333338
    agent-5: 40.83333333333338
  sampler_perf:
    mean_env_wait_ms: 30.960113938258424
    mean_inference_ms: 14.38099859047487
    mean_processing_ms: 65.46067289749772
  time_since_restore: 5084.897031545639
  time_this_iter_s: 162.8575201034546
  time_total_s: 17635.713902950287
  timestamp: 1637040165
  timesteps_since_restore: 2688000
  timesteps_this_iter: 96000
  timesteps_total: 10368000
  training_iteration: 108
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    108 |          17635.7 | 10368000 |   618.63 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 78
    apples_agent-0_mean: 5.66
    apples_agent-0_min: 0
    apples_agent-1_max: 168
    apples_agent-1_mean: 29.01
    apples_agent-1_min: 0
    apples_agent-2_max: 164
    apples_agent-2_mean: 13.89
    apples_agent-2_min: 0
    apples_agent-3_max: 229
    apples_agent-3_mean: 90.43
    apples_agent-3_min: 36
    apples_agent-4_max: 70
    apples_agent-4_mean: 4.58
    apples_agent-4_min: 0
    apples_agent-5_max: 147
    apples_agent-5_mean: 76.52
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 471
    cleaning_beam_agent-0_mean: 348.28
    cleaning_beam_agent-0_min: 191
    cleaning_beam_agent-1_max: 429
    cleaning_beam_agent-1_mean: 245.96
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 642
    cleaning_beam_agent-2_mean: 361.28
    cleaning_beam_agent-2_min: 159
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 60.39
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 526
    cleaning_beam_agent-4_mean: 368.31
    cleaning_beam_agent-4_min: 153
    cleaning_beam_agent-5_max: 393
    cleaning_beam_agent-5_mean: 99.81
    cleaning_beam_agent-5_min: 33
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 12
    fire_beam_agent-5_mean: 0.14
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-25-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 870.9999999999775
  episode_reward_mean: 618.6699999999973
  episode_reward_min: 269.9999999999966
  episodes_this_iter: 96
  episodes_total: 10464
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12865.675
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 1.2312359809875488
        entropy_coeff: 0.0017600000137463212
        kl: 0.011212002485990524
        model: {}
        policy_loss: -0.030591696500778198
        total_loss: -0.029013214632868767
        vf_explained_var: 0.045536622405052185
        vf_loss: 15.030539512634277
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 1.1514078378677368
        entropy_coeff: 0.0017600000137463212
        kl: 0.013840502128005028
        model: {}
        policy_loss: -0.032647088170051575
        total_loss: -0.030390648171305656
        vf_explained_var: 0.03890807926654816
        vf_loss: 15.148168563842773
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 1.1126387119293213
        entropy_coeff: 0.0017600000137463212
        kl: 0.013234773650765419
        model: {}
        policy_loss: -0.03165192902088165
        total_loss: -0.029522627592086792
        vf_explained_var: 0.08584345877170563
        vf_loss: 14.4058837890625
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 0.8009914755821228
        entropy_coeff: 0.0017600000137463212
        kl: 0.009408836252987385
        model: {}
        policy_loss: -0.024920552968978882
        total_loss: -0.023240074515342712
        vf_explained_var: 0.23300278186798096
        vf_loss: 12.0845308303833
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 1.0754432678222656
        entropy_coeff: 0.0017600000137463212
        kl: 0.014180026948451996
        model: {}
        policy_loss: -0.03751947730779648
        total_loss: -0.03524196892976761
        vf_explained_var: 0.15312859416007996
        vf_loss: 13.34288215637207
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 1.0271596908569336
        entropy_coeff: 0.0017600000137463212
        kl: 0.01392524503171444
        model: {}
        policy_loss: -0.03640317916870117
        total_loss: -0.034079600125551224
        vf_explained_var: 0.14599139988422394
        vf_loss: 13.46332836151123
    load_time_ms: 23423.988
    num_steps_sampled: 10464000
    num_steps_trained: 10464000
    sample_time_ms: 131075.101
    update_time_ms: 67.92
  iterations_since_restore: 29
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.14672131147541
    ram_util_percent: 15.06762295081967
  pid: 14340
  policy_reward_max:
    agent-0: 145.1666666666669
    agent-1: 145.1666666666669
    agent-2: 145.1666666666669
    agent-3: 145.1666666666669
    agent-4: 145.1666666666669
    agent-5: 145.1666666666669
  policy_reward_mean:
    agent-0: 103.111666666667
    agent-1: 103.111666666667
    agent-2: 103.111666666667
    agent-3: 103.111666666667
    agent-4: 103.111666666667
    agent-5: 103.111666666667
  policy_reward_min:
    agent-0: 44.99999999999995
    agent-1: 44.99999999999995
    agent-2: 44.99999999999995
    agent-3: 44.99999999999995
    agent-4: 44.99999999999995
    agent-5: 44.99999999999995
  sampler_perf:
    mean_env_wait_ms: 30.96511094490187
    mean_inference_ms: 14.383716122331416
    mean_processing_ms: 65.4624923907482
  time_since_restore: 5256.2828958034515
  time_this_iter_s: 171.3858642578125
  time_total_s: 17807.0997672081
  timestamp: 1637040336
  timesteps_since_restore: 2784000
  timesteps_this_iter: 96000
  timesteps_total: 10464000
  training_iteration: 109
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    109 |          17807.1 | 10464000 |   618.67 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 86
    apples_agent-0_mean: 4.68
    apples_agent-0_min: 0
    apples_agent-1_max: 133
    apples_agent-1_mean: 20.65
    apples_agent-1_min: 0
    apples_agent-2_max: 175
    apples_agent-2_mean: 14.46
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 85.37
    apples_agent-3_min: 42
    apples_agent-4_max: 45
    apples_agent-4_mean: 2.11
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 78.93
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 449
    cleaning_beam_agent-0_mean: 332.24
    cleaning_beam_agent-0_min: 128
    cleaning_beam_agent-1_max: 489
    cleaning_beam_agent-1_mean: 265.56
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 624
    cleaning_beam_agent-2_mean: 360.94
    cleaning_beam_agent-2_min: 126
    cleaning_beam_agent-3_max: 166
    cleaning_beam_agent-3_mean: 58.37
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 545
    cleaning_beam_agent-4_mean: 364.97
    cleaning_beam_agent-4_min: 157
    cleaning_beam_agent-5_max: 501
    cleaning_beam_agent-5_mean: 100.06
    cleaning_beam_agent-5_min: 28
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-28-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 844.9999999999777
  episode_reward_mean: 635.8399999999965
  episode_reward_min: 303.0000000000014
  episodes_this_iter: 96
  episodes_total: 10560
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12868.203
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 1.2581785917282104
        entropy_coeff: 0.0017600000137463212
        kl: 0.011460398323833942
        model: {}
        policy_loss: -0.029255058616399765
        total_loss: -0.02772079035639763
        vf_explained_var: 0.05098870396614075
        vf_loss: 14.565783500671387
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 1.1649386882781982
        entropy_coeff: 0.0017600000137463212
        kl: 0.014415718615055084
        model: {}
        policy_loss: -0.03367658331990242
        total_loss: -0.031353600323200226
        vf_explained_var: 0.029211610555648804
        vf_loss: 14.901299476623535
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 1.1253385543823242
        entropy_coeff: 0.0017600000137463212
        kl: 0.013411372900009155
        model: {}
        policy_loss: -0.03100966289639473
        total_loss: -0.028939269483089447
        vf_explained_var: 0.10965016484260559
        vf_loss: 13.687141418457031
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 0.7771163582801819
        entropy_coeff: 0.0017600000137463212
        kl: 0.009675554931163788
        model: {}
        policy_loss: -0.02468622289597988
        total_loss: -0.022893372923135757
        vf_explained_var: 0.2021341621875763
        vf_loss: 12.25466251373291
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 1.0639053583145142
        entropy_coeff: 0.0017600000137463212
        kl: 0.015023755840957165
        model: {}
        policy_loss: -0.0366378016769886
        total_loss: -0.03410346060991287
        vf_explained_var: 0.08668692409992218
        vf_loss: 14.020599365234375
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 0.9955530166625977
        entropy_coeff: 0.0017600000137463212
        kl: 0.013462393544614315
        model: {}
        policy_loss: -0.03351480886340141
        total_loss: -0.03129423409700394
        vf_explained_var: 0.16504256427288055
        vf_loss: 12.802647590637207
    load_time_ms: 23160.672
    num_steps_sampled: 10560000
    num_steps_trained: 10560000
    sample_time_ms: 131081.595
    update_time_ms: 69.946
  iterations_since_restore: 30
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.19608695652174
    ram_util_percent: 14.869999999999996
  pid: 14340
  policy_reward_max:
    agent-0: 140.83333333333326
    agent-1: 140.83333333333326
    agent-2: 140.83333333333326
    agent-3: 140.83333333333326
    agent-4: 140.83333333333326
    agent-5: 140.83333333333326
  policy_reward_mean:
    agent-0: 105.97333333333364
    agent-1: 105.97333333333364
    agent-2: 105.97333333333364
    agent-3: 105.97333333333364
    agent-4: 105.97333333333364
    agent-5: 105.97333333333364
  policy_reward_min:
    agent-0: 50.49999999999995
    agent-1: 50.49999999999995
    agent-2: 50.49999999999995
    agent-3: 50.49999999999995
    agent-4: 50.49999999999995
    agent-5: 50.49999999999995
  sampler_perf:
    mean_env_wait_ms: 30.970010532606675
    mean_inference_ms: 14.390365274244786
    mean_processing_ms: 65.46917919473833
  time_since_restore: 5417.8252720832825
  time_this_iter_s: 161.54237627983093
  time_total_s: 17968.64214348793
  timestamp: 1637040498
  timesteps_since_restore: 2880000
  timesteps_this_iter: 96000
  timesteps_total: 10560000
  training_iteration: 110
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    110 |          17968.6 | 10560000 |   635.84 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 69
    apples_agent-0_mean: 4.03
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 25.19
    apples_agent-1_min: 0
    apples_agent-2_max: 254
    apples_agent-2_mean: 18.21
    apples_agent-2_min: 0
    apples_agent-3_max: 175
    apples_agent-3_mean: 87.13
    apples_agent-3_min: 27
    apples_agent-4_max: 109
    apples_agent-4_mean: 8.5
    apples_agent-4_min: 0
    apples_agent-5_max: 130
    apples_agent-5_mean: 73.69
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 480
    cleaning_beam_agent-0_mean: 327.55
    cleaning_beam_agent-0_min: 164
    cleaning_beam_agent-1_max: 545
    cleaning_beam_agent-1_mean: 264.17
    cleaning_beam_agent-1_min: 67
    cleaning_beam_agent-2_max: 567
    cleaning_beam_agent-2_mean: 344.84
    cleaning_beam_agent-2_min: 73
    cleaning_beam_agent-3_max: 146
    cleaning_beam_agent-3_mean: 76.35
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 504
    cleaning_beam_agent-4_mean: 342.96
    cleaning_beam_agent-4_min: 159
    cleaning_beam_agent-5_max: 261
    cleaning_beam_agent-5_mean: 97.32
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-31-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 867.9999999999749
  episode_reward_mean: 588.6499999999987
  episode_reward_min: 162.9999999999999
  episodes_this_iter: 96
  episodes_total: 10656
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12980.311
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 1.2560927867889404
        entropy_coeff: 0.0017600000137463212
        kl: 0.01168401911854744
        model: {}
        policy_loss: -0.03159511089324951
        total_loss: -0.02976888418197632
        vf_explained_var: 0.06295250356197357
        vf_loss: 17.001422882080078
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 1.1313612461090088
        entropy_coeff: 0.0017600000137463212
        kl: 0.013387477025389671
        model: {}
        policy_loss: -0.03367532417178154
        total_loss: -0.03123745322227478
        vf_explained_var: 0.03519919514656067
        vf_loss: 17.515670776367188
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 1.1057343482971191
        entropy_coeff: 0.0017600000137463212
        kl: 0.012840183451771736
        model: {}
        policy_loss: -0.031157249584794044
        total_loss: -0.028929181396961212
        vf_explained_var: 0.11490818858146667
        vf_loss: 16.061227798461914
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 0.8432798385620117
        entropy_coeff: 0.0017600000137463212
        kl: 0.011442942544817924
        model: {}
        policy_loss: -0.025117799639701843
        total_loss: -0.023027237504720688
        vf_explained_var: 0.29092496633529663
        vf_loss: 12.861446380615234
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 1.0774284601211548
        entropy_coeff: 0.0017600000137463212
        kl: 0.01459189597517252
        model: {}
        policy_loss: -0.03841928392648697
        total_loss: -0.035857193171978
        vf_explained_var: 0.15172433853149414
        vf_loss: 15.399846076965332
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 1.0254226922988892
        entropy_coeff: 0.0017600000137463212
        kl: 0.013330614194273949
        model: {}
        policy_loss: -0.03525184094905853
        total_loss: -0.0329706072807312
        vf_explained_var: 0.21747766435146332
        vf_loss: 14.198555946350098
    load_time_ms: 22471.874
    num_steps_sampled: 10656000
    num_steps_trained: 10656000
    sample_time_ms: 131119.355
    update_time_ms: 83.219
  iterations_since_restore: 31
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.219047619047622
    ram_util_percent: 14.877489177489176
  pid: 14340
  policy_reward_max:
    agent-0: 144.66666666666666
    agent-1: 144.66666666666666
    agent-2: 144.66666666666666
    agent-3: 144.66666666666666
    agent-4: 144.66666666666666
    agent-5: 144.66666666666666
  policy_reward_mean:
    agent-0: 98.10833333333359
    agent-1: 98.10833333333359
    agent-2: 98.10833333333359
    agent-3: 98.10833333333359
    agent-4: 98.10833333333359
    agent-5: 98.10833333333359
  policy_reward_min:
    agent-0: 27.16666666666671
    agent-1: 27.16666666666671
    agent-2: 27.16666666666671
    agent-3: 27.16666666666671
    agent-4: 27.16666666666671
    agent-5: 27.16666666666671
  sampler_perf:
    mean_env_wait_ms: 30.971732272860823
    mean_inference_ms: 14.393443051608665
    mean_processing_ms: 65.47309092716486
  time_since_restore: 5578.925313234329
  time_this_iter_s: 161.10004115104675
  time_total_s: 18129.742184638977
  timestamp: 1637040660
  timesteps_since_restore: 2976000
  timesteps_this_iter: 96000
  timesteps_total: 10656000
  training_iteration: 111
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    111 |          18129.7 | 10656000 |   588.65 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 4.11
    apples_agent-0_min: 0
    apples_agent-1_max: 160
    apples_agent-1_mean: 26.36
    apples_agent-1_min: 0
    apples_agent-2_max: 333
    apples_agent-2_mean: 22.04
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 93.27
    apples_agent-3_min: 0
    apples_agent-4_max: 69
    apples_agent-4_mean: 4.54
    apples_agent-4_min: 0
    apples_agent-5_max: 124
    apples_agent-5_mean: 73.52
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 453
    cleaning_beam_agent-0_mean: 333.78
    cleaning_beam_agent-0_min: 123
    cleaning_beam_agent-1_max: 463
    cleaning_beam_agent-1_mean: 273.27
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 567
    cleaning_beam_agent-2_mean: 332.62
    cleaning_beam_agent-2_min: 87
    cleaning_beam_agent-3_max: 162
    cleaning_beam_agent-3_mean: 59.98
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 510
    cleaning_beam_agent-4_mean: 355.36
    cleaning_beam_agent-4_min: 197
    cleaning_beam_agent-5_max: 314
    cleaning_beam_agent-5_mean: 105.41
    cleaning_beam_agent-5_min: 33
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-33-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 907.9999999999887
  episode_reward_mean: 627.2499999999967
  episode_reward_min: 150.9999999999999
  episodes_this_iter: 96
  episodes_total: 10752
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12981.17
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 1.2421287298202515
        entropy_coeff: 0.0017600000137463212
        kl: 0.011588966473937035
        model: {}
        policy_loss: -0.03037228435277939
        total_loss: -0.028695140033960342
        vf_explained_var: 0.08876997232437134
        vf_loss: 15.454977035522461
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 1.133678674697876
        entropy_coeff: 0.0017600000137463212
        kl: 0.01364266686141491
        model: {}
        policy_loss: -0.03303784877061844
        total_loss: -0.030666399747133255
        vf_explained_var: 0.03458091616630554
        vf_loss: 16.381893157958984
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 1.120143175125122
        entropy_coeff: 0.0017600000137463212
        kl: 0.013231776654720306
        model: {}
        policy_loss: -0.032384999096393585
        total_loss: -0.030141891911625862
        vf_explained_var: 0.07513535022735596
        vf_loss: 15.682031631469727
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 0.8037541508674622
        entropy_coeff: 0.0017600000137463212
        kl: 0.010036170482635498
        model: {}
        policy_loss: -0.02394375391304493
        total_loss: -0.022122511640191078
        vf_explained_var: 0.2754984200000763
        vf_loss: 12.286169052124023
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 1.069143533706665
        entropy_coeff: 0.0017600000137463212
        kl: 0.014343454502522945
        model: {}
        policy_loss: -0.037563715130090714
        total_loss: -0.035101160407066345
        vf_explained_var: 0.13009758293628693
        vf_loss: 14.755581855773926
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 1.0095477104187012
        entropy_coeff: 0.0017600000137463212
        kl: 0.013887858018279076
        model: {}
        policy_loss: -0.03395157307386398
        total_loss: -0.031551823019981384
        vf_explained_var: 0.17483648657798767
        vf_loss: 13.989822387695312
    load_time_ms: 24077.491
    num_steps_sampled: 10752000
    num_steps_trained: 10752000
    sample_time_ms: 130558.978
    update_time_ms: 86.81
  iterations_since_restore: 32
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.87710843373494
    ram_util_percent: 15.062248995983936
  pid: 14340
  policy_reward_max:
    agent-0: 151.33333333333312
    agent-1: 151.33333333333312
    agent-2: 151.33333333333312
    agent-3: 151.33333333333312
    agent-4: 151.33333333333312
    agent-5: 151.33333333333312
  policy_reward_mean:
    agent-0: 104.54166666666697
    agent-1: 104.54166666666697
    agent-2: 104.54166666666697
    agent-3: 104.54166666666697
    agent-4: 104.54166666666697
    agent-5: 104.54166666666697
  policy_reward_min:
    agent-0: 25.1666666666667
    agent-1: 25.1666666666667
    agent-2: 25.1666666666667
    agent-3: 25.1666666666667
    agent-4: 25.1666666666667
    agent-5: 25.1666666666667
  sampler_perf:
    mean_env_wait_ms: 30.972711939301607
    mean_inference_ms: 14.39645022051295
    mean_processing_ms: 65.47746236972807
  time_since_restore: 5753.289582967758
  time_this_iter_s: 174.36426973342896
  time_total_s: 18304.106454372406
  timestamp: 1637040835
  timesteps_since_restore: 3072000
  timesteps_this_iter: 96000
  timesteps_total: 10752000
  training_iteration: 112
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    112 |          18304.1 | 10752000 |   627.25 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 2.25
    apples_agent-0_min: 0
    apples_agent-1_max: 194
    apples_agent-1_mean: 24.46
    apples_agent-1_min: 0
    apples_agent-2_max: 271
    apples_agent-2_mean: 17.9
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 94.19
    apples_agent-3_min: 25
    apples_agent-4_max: 69
    apples_agent-4_mean: 6.84
    apples_agent-4_min: 0
    apples_agent-5_max: 129
    apples_agent-5_mean: 76.09
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 544
    cleaning_beam_agent-0_mean: 329.03
    cleaning_beam_agent-0_min: 144
    cleaning_beam_agent-1_max: 531
    cleaning_beam_agent-1_mean: 306.02
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 702
    cleaning_beam_agent-2_mean: 307.26
    cleaning_beam_agent-2_min: 57
    cleaning_beam_agent-3_max: 159
    cleaning_beam_agent-3_mean: 64.94
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 594
    cleaning_beam_agent-4_mean: 362.32
    cleaning_beam_agent-4_min: 165
    cleaning_beam_agent-5_max: 312
    cleaning_beam_agent-5_mean: 102.98
    cleaning_beam_agent-5_min: 26
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-36-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 853.9999999999903
  episode_reward_mean: 621.4499999999971
  episode_reward_min: 215.99999999999807
  episodes_this_iter: 96
  episodes_total: 10848
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13084.607
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 1.2419602870941162
        entropy_coeff: 0.0017600000137463212
        kl: 0.010982414707541466
        model: {}
        policy_loss: -0.03063822351396084
        total_loss: -0.029091615229845047
        vf_explained_var: 0.06765308976173401
        vf_loss: 15.35975170135498
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 1.1079020500183105
        entropy_coeff: 0.0017600000137463212
        kl: 0.01322443038225174
        model: {}
        policy_loss: -0.032555025070905685
        total_loss: -0.030275825411081314
        vf_explained_var: 0.03865760564804077
        vf_loss: 15.842220306396484
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 1.1291509866714478
        entropy_coeff: 0.0017600000137463212
        kl: 0.013503288850188255
        model: {}
        policy_loss: -0.034364428371191025
        total_loss: -0.03209847956895828
        vf_explained_var: 0.058122649788856506
        vf_loss: 15.52601146697998
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 0.8085110783576965
        entropy_coeff: 0.0017600000137463212
        kl: 0.010014230385422707
        model: {}
        policy_loss: -0.02501576580107212
        total_loss: -0.02323022112250328
        vf_explained_var: 0.2680128216743469
        vf_loss: 12.056816101074219
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 1.0716040134429932
        entropy_coeff: 0.0017600000137463212
        kl: 0.014689115807414055
        model: {}
        policy_loss: -0.03776480257511139
        total_loss: -0.035352952778339386
        vf_explained_var: 0.17472684383392334
        vf_loss: 13.600532531738281
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 1.0029083490371704
        entropy_coeff: 0.0017600000137463212
        kl: 0.013786865398287773
        model: {}
        policy_loss: -0.03484990447759628
        total_loss: -0.03248346596956253
        vf_explained_var: 0.1674996018409729
        vf_loss: 13.741852760314941
    load_time_ms: 24718.716
    num_steps_sampled: 10848000
    num_steps_trained: 10848000
    sample_time_ms: 131553.201
    update_time_ms: 63.508
  iterations_since_restore: 33
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.920171673819745
    ram_util_percent: 14.969527896995707
  pid: 14340
  policy_reward_max:
    agent-0: 142.33333333333343
    agent-1: 142.33333333333343
    agent-2: 142.33333333333343
    agent-3: 142.33333333333343
    agent-4: 142.33333333333343
    agent-5: 142.33333333333343
  policy_reward_mean:
    agent-0: 103.57500000000032
    agent-1: 103.57500000000032
    agent-2: 103.57500000000032
    agent-3: 103.57500000000032
    agent-4: 103.57500000000032
    agent-5: 103.57500000000032
  policy_reward_min:
    agent-0: 36.00000000000007
    agent-1: 36.00000000000007
    agent-2: 36.00000000000007
    agent-3: 36.00000000000007
    agent-4: 36.00000000000007
    agent-5: 36.00000000000007
  sampler_perf:
    mean_env_wait_ms: 30.97656382945668
    mean_inference_ms: 14.4003137949699
    mean_processing_ms: 65.48534716372697
  time_since_restore: 5929.147756576538
  time_this_iter_s: 175.8581736087799
  time_total_s: 18479.964627981186
  timestamp: 1637041011
  timesteps_since_restore: 3168000
  timesteps_this_iter: 96000
  timesteps_total: 10848000
  training_iteration: 113
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    113 |            18480 | 10848000 |   621.45 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 5.2
    apples_agent-0_min: 0
    apples_agent-1_max: 133
    apples_agent-1_mean: 20.94
    apples_agent-1_min: 0
    apples_agent-2_max: 232
    apples_agent-2_mean: 22.35
    apples_agent-2_min: 0
    apples_agent-3_max: 214
    apples_agent-3_mean: 93.03
    apples_agent-3_min: 23
    apples_agent-4_max: 92
    apples_agent-4_mean: 5.85
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 77.42
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 483
    cleaning_beam_agent-0_mean: 344.65
    cleaning_beam_agent-0_min: 141
    cleaning_beam_agent-1_max: 510
    cleaning_beam_agent-1_mean: 297.75
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 500
    cleaning_beam_agent-2_mean: 276.09
    cleaning_beam_agent-2_min: 122
    cleaning_beam_agent-3_max: 200
    cleaning_beam_agent-3_mean: 71.08
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 513
    cleaning_beam_agent-4_mean: 367.9
    cleaning_beam_agent-4_min: 163
    cleaning_beam_agent-5_max: 284
    cleaning_beam_agent-5_mean: 109.7
    cleaning_beam_agent-5_min: 32
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-39-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 846.9999999999854
  episode_reward_mean: 606.3999999999979
  episode_reward_min: 275.9999999999974
  episodes_this_iter: 96
  episodes_total: 10944
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13241.094
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 1.2315765619277954
        entropy_coeff: 0.0017600000137463212
        kl: 0.011018568649888039
        model: {}
        policy_loss: -0.028860270977020264
        total_loss: -0.02725953608751297
        vf_explained_var: 0.05081009864807129
        vf_loss: 15.645973205566406
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 1.1168702840805054
        entropy_coeff: 0.0017600000137463212
        kl: 0.013116509653627872
        model: {}
        policy_loss: -0.033784352242946625
        total_loss: -0.031562380492687225
        vf_explained_var: 0.05187349021434784
        vf_loss: 15.643640518188477
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 1.151125192642212
        entropy_coeff: 0.0017600000137463212
        kl: 0.01349643338471651
        model: {}
        policy_loss: -0.03394673019647598
        total_loss: -0.031781554222106934
        vf_explained_var: 0.0953933447599411
        vf_loss: 14.91866683959961
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 0.7990999221801758
        entropy_coeff: 0.0017600000137463212
        kl: 0.008877317421138287
        model: {}
        policy_loss: -0.023556826636195183
        total_loss: -0.021953033283352852
        vf_explained_var: 0.25107133388519287
        vf_loss: 12.347454071044922
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 1.0493443012237549
        entropy_coeff: 0.0017600000137463212
        kl: 0.014714126475155354
        model: {}
        policy_loss: -0.03824567794799805
        total_loss: -0.03570747748017311
        vf_explained_var: 0.12535244226455688
        vf_loss: 14.422191619873047
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 1.0093415975570679
        entropy_coeff: 0.0017600000137463212
        kl: 0.013610207475721836
        model: {}
        policy_loss: -0.035611845552921295
        total_loss: -0.03332849219441414
        vf_explained_var: 0.189005047082901
        vf_loss: 13.377557754516602
    load_time_ms: 25115.343
    num_steps_sampled: 10944000
    num_steps_trained: 10944000
    sample_time_ms: 131954.617
    update_time_ms: 63.046
  iterations_since_restore: 34
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.434599156118143
    ram_util_percent: 14.899578059071729
  pid: 14340
  policy_reward_max:
    agent-0: 141.1666666666668
    agent-1: 141.1666666666668
    agent-2: 141.1666666666668
    agent-3: 141.1666666666668
    agent-4: 141.1666666666668
    agent-5: 141.1666666666668
  policy_reward_mean:
    agent-0: 101.06666666666698
    agent-1: 101.06666666666698
    agent-2: 101.06666666666698
    agent-3: 101.06666666666698
    agent-4: 101.06666666666698
    agent-5: 101.06666666666698
  policy_reward_min:
    agent-0: 45.999999999999886
    agent-1: 45.999999999999886
    agent-2: 45.999999999999886
    agent-3: 45.999999999999886
    agent-4: 45.999999999999886
    agent-5: 45.999999999999886
  sampler_perf:
    mean_env_wait_ms: 30.97984862153551
    mean_inference_ms: 14.404051685053835
    mean_processing_ms: 65.48888003729378
  time_since_restore: 6095.305799245834
  time_this_iter_s: 166.15804266929626
  time_total_s: 18646.122670650482
  timestamp: 1637041177
  timesteps_since_restore: 3264000
  timesteps_this_iter: 96000
  timesteps_total: 10944000
  training_iteration: 114
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    114 |          18646.1 | 10944000 |    606.4 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 2.41
    apples_agent-0_min: 0
    apples_agent-1_max: 181
    apples_agent-1_mean: 24.1
    apples_agent-1_min: 0
    apples_agent-2_max: 218
    apples_agent-2_mean: 16.97
    apples_agent-2_min: 0
    apples_agent-3_max: 174
    apples_agent-3_mean: 101.18
    apples_agent-3_min: 19
    apples_agent-4_max: 32
    apples_agent-4_mean: 2.04
    apples_agent-4_min: 0
    apples_agent-5_max: 147
    apples_agent-5_mean: 77.23
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 457
    cleaning_beam_agent-0_mean: 332.12
    cleaning_beam_agent-0_min: 197
    cleaning_beam_agent-1_max: 515
    cleaning_beam_agent-1_mean: 274.61
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 481
    cleaning_beam_agent-2_mean: 296.89
    cleaning_beam_agent-2_min: 93
    cleaning_beam_agent-3_max: 205
    cleaning_beam_agent-3_mean: 70.23
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 519
    cleaning_beam_agent-4_mean: 366.47
    cleaning_beam_agent-4_min: 192
    cleaning_beam_agent-5_max: 418
    cleaning_beam_agent-5_mean: 111.4
    cleaning_beam_agent-5_min: 34
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-42-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 853.9999999999882
  episode_reward_mean: 640.8199999999973
  episode_reward_min: 217.9999999999962
  episodes_this_iter: 96
  episodes_total: 11040
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13248.113
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 1.239343523979187
        entropy_coeff: 0.0017600000137463212
        kl: 0.011408194899559021
        model: {}
        policy_loss: -0.030287915840744972
        total_loss: -0.028668854385614395
        vf_explained_var: 0.0360824316740036
        vf_loss: 15.186622619628906
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 1.1319916248321533
        entropy_coeff: 0.0017600000137463212
        kl: 0.01401037722826004
        model: {}
        policy_loss: -0.03351800516247749
        total_loss: -0.031194917857646942
        vf_explained_var: 0.03977639973163605
        vf_loss: 15.133161544799805
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 1.1472809314727783
        entropy_coeff: 0.0017600000137463212
        kl: 0.013354163616895676
        model: {}
        policy_loss: -0.032558150589466095
        total_loss: -0.030492626130580902
        vf_explained_var: 0.10228830575942993
        vf_loss: 14.139053344726562
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 0.778244137763977
        entropy_coeff: 0.0017600000137463212
        kl: 0.008831850253045559
        model: {}
        policy_loss: -0.022403709590435028
        total_loss: -0.020714331418275833
        vf_explained_var: 0.17867408692836761
        vf_loss: 12.92713737487793
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 1.0554122924804688
        entropy_coeff: 0.0017600000137463212
        kl: 0.01440026517957449
        model: {}
        policy_loss: -0.03658834844827652
        total_loss: -0.034210313111543655
        vf_explained_var: 0.1396641731262207
        vf_loss: 13.555093765258789
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 0.9929351806640625
        entropy_coeff: 0.0017600000137463212
        kl: 0.013129610568284988
        model: {}
        policy_loss: -0.03463968634605408
        total_loss: -0.0324455164372921
        vf_explained_var: 0.16487973928451538
        vf_loss: 13.158086776733398
    load_time_ms: 25407.93
    num_steps_sampled: 11040000
    num_steps_trained: 11040000
    sample_time_ms: 131796.073
    update_time_ms: 58.013
  iterations_since_restore: 35
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.6648
    ram_util_percent: 15.0892
  pid: 14340
  policy_reward_max:
    agent-0: 142.3333333333336
    agent-1: 142.3333333333336
    agent-2: 142.3333333333336
    agent-3: 142.3333333333336
    agent-4: 142.3333333333336
    agent-5: 142.3333333333336
  policy_reward_mean:
    agent-0: 106.80333333333365
    agent-1: 106.80333333333365
    agent-2: 106.80333333333365
    agent-3: 106.80333333333365
    agent-4: 106.80333333333365
    agent-5: 106.80333333333365
  policy_reward_min:
    agent-0: 36.33333333333336
    agent-1: 36.33333333333336
    agent-2: 36.33333333333336
    agent-3: 36.33333333333336
    agent-4: 36.33333333333336
    agent-5: 36.33333333333336
  sampler_perf:
    mean_env_wait_ms: 30.97762906592677
    mean_inference_ms: 14.453913817802182
    mean_processing_ms: 65.5324243106249
  time_since_restore: 6271.11315369606
  time_this_iter_s: 175.80735445022583
  time_total_s: 18821.930025100708
  timestamp: 1637041353
  timesteps_since_restore: 3360000
  timesteps_this_iter: 96000
  timesteps_total: 11040000
  training_iteration: 115
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    115 |          18821.9 | 11040000 |   640.82 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 3.48
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 18.98
    apples_agent-1_min: 0
    apples_agent-2_max: 216
    apples_agent-2_mean: 13.83
    apples_agent-2_min: 0
    apples_agent-3_max: 228
    apples_agent-3_mean: 92.04
    apples_agent-3_min: 4
    apples_agent-4_max: 76
    apples_agent-4_mean: 7.23
    apples_agent-4_min: 0
    apples_agent-5_max: 133
    apples_agent-5_mean: 74.91
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 444
    cleaning_beam_agent-0_mean: 321.36
    cleaning_beam_agent-0_min: 204
    cleaning_beam_agent-1_max: 551
    cleaning_beam_agent-1_mean: 271.62
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 530
    cleaning_beam_agent-2_mean: 276.92
    cleaning_beam_agent-2_min: 91
    cleaning_beam_agent-3_max: 269
    cleaning_beam_agent-3_mean: 87.86
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 501
    cleaning_beam_agent-4_mean: 333.94
    cleaning_beam_agent-4_min: 132
    cleaning_beam_agent-5_max: 351
    cleaning_beam_agent-5_mean: 108.91
    cleaning_beam_agent-5_min: 25
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-45-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 849.9999999999897
  episode_reward_mean: 591.5199999999995
  episode_reward_min: 192.99999999999798
  episodes_this_iter: 96
  episodes_total: 11136
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13232.056
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 1.2610034942626953
        entropy_coeff: 0.0017600000137463212
        kl: 0.01149536482989788
        model: {}
        policy_loss: -0.03163636103272438
        total_loss: -0.030142974108457565
        vf_explained_var: 0.05672098696231842
        vf_loss: 14.136784553527832
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 1.1497790813446045
        entropy_coeff: 0.0017600000137463212
        kl: 0.013219163753092289
        model: {}
        policy_loss: -0.03302879258990288
        total_loss: -0.030988354235887527
        vf_explained_var: 0.05243842303752899
        vf_loss: 14.202224731445312
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 1.1666285991668701
        entropy_coeff: 0.0017600000137463212
        kl: 0.013130368664860725
        model: {}
        policy_loss: -0.033534176647663116
        total_loss: -0.03154018521308899
        vf_explained_var: 0.052789390087127686
        vf_loss: 14.211852073669434
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 0.8331945538520813
        entropy_coeff: 0.0017600000137463212
        kl: 0.01196127850562334
        model: {}
        policy_loss: -0.02316291816532612
        total_loss: -0.02106485143303871
        vf_explained_var: 0.21776647865772247
        vf_loss: 11.722359657287598
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 1.069704532623291
        entropy_coeff: 0.0017600000137463212
        kl: 0.014866875484585762
        model: {}
        policy_loss: -0.03764622285962105
        total_loss: -0.03529798984527588
        vf_explained_var: 0.16273349523544312
        vf_loss: 12.5753812789917
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 0.9994478821754456
        entropy_coeff: 0.0017600000137463212
        kl: 0.0139458067715168
        model: {}
        policy_loss: -0.03481370955705643
        total_loss: -0.03256673365831375
        vf_explained_var: 0.18855921924114227
        vf_loss: 12.168436050415039
    load_time_ms: 25466.727
    num_steps_sampled: 11136000
    num_steps_trained: 11136000
    sample_time_ms: 131645.178
    update_time_ms: 58.889
  iterations_since_restore: 36
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.60708661417323
    ram_util_percent: 15.090551181102363
  pid: 14340
  policy_reward_max:
    agent-0: 141.66666666666677
    agent-1: 141.66666666666677
    agent-2: 141.66666666666677
    agent-3: 141.66666666666677
    agent-4: 141.66666666666677
    agent-5: 141.66666666666677
  policy_reward_mean:
    agent-0: 98.5866666666669
    agent-1: 98.5866666666669
    agent-2: 98.5866666666669
    agent-3: 98.5866666666669
    agent-4: 98.5866666666669
    agent-5: 98.5866666666669
  policy_reward_min:
    agent-0: 32.166666666666735
    agent-1: 32.166666666666735
    agent-2: 32.166666666666735
    agent-3: 32.166666666666735
    agent-4: 32.166666666666735
    agent-5: 32.166666666666735
  sampler_perf:
    mean_env_wait_ms: 30.969367752633033
    mean_inference_ms: 14.456953276378377
    mean_processing_ms: 65.54808324745022
  time_since_restore: 6448.935518741608
  time_this_iter_s: 177.82236504554749
  time_total_s: 18999.752390146255
  timestamp: 1637041531
  timesteps_since_restore: 3456000
  timesteps_this_iter: 96000
  timesteps_total: 11136000
  training_iteration: 116
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    116 |          18999.8 | 11136000 |   591.52 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 4.93
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 21.0
    apples_agent-1_min: 0
    apples_agent-2_max: 267
    apples_agent-2_mean: 12.94
    apples_agent-2_min: 0
    apples_agent-3_max: 143
    apples_agent-3_mean: 89.4
    apples_agent-3_min: 24
    apples_agent-4_max: 58
    apples_agent-4_mean: 3.11
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 76.05
    apples_agent-5_min: 4
    cleaning_beam_agent-0_max: 426
    cleaning_beam_agent-0_mean: 312.04
    cleaning_beam_agent-0_min: 197
    cleaning_beam_agent-1_max: 468
    cleaning_beam_agent-1_mean: 278.48
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 498
    cleaning_beam_agent-2_mean: 287.98
    cleaning_beam_agent-2_min: 42
    cleaning_beam_agent-3_max: 205
    cleaning_beam_agent-3_mean: 81.58
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 478
    cleaning_beam_agent-4_mean: 339.49
    cleaning_beam_agent-4_min: 170
    cleaning_beam_agent-5_max: 436
    cleaning_beam_agent-5_mean: 120.06
    cleaning_beam_agent-5_min: 25
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-48-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 849.9999999999744
  episode_reward_mean: 600.809999999998
  episode_reward_min: 192.99999999999798
  episodes_this_iter: 96
  episodes_total: 11232
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13242.0
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 1.2576823234558105
        entropy_coeff: 0.0017600000137463212
        kl: 0.011941008269786835
        model: {}
        policy_loss: -0.03130805864930153
        total_loss: -0.029685985296964645
        vf_explained_var: 0.09853848814964294
        vf_loss: 14.473939895629883
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 1.156746745109558
        entropy_coeff: 0.0017600000137463212
        kl: 0.014097229577600956
        model: {}
        policy_loss: -0.0342416949570179
        total_loss: -0.0319131575524807
        vf_explained_var: 0.03740224242210388
        vf_loss: 15.449677467346191
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 1.1577805280685425
        entropy_coeff: 0.0017600000137463212
        kl: 0.012765445746481419
        model: {}
        policy_loss: -0.03295210376381874
        total_loss: -0.03098319098353386
        vf_explained_var: 0.09475253522396088
        vf_loss: 14.53520679473877
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 0.8184208869934082
        entropy_coeff: 0.0017600000137463212
        kl: 0.010408424772322178
        model: {}
        policy_loss: -0.024690302088856697
        total_loss: -0.022764474153518677
        vf_explained_var: 0.19968512654304504
        vf_loss: 12.84560775756836
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 1.060211181640625
        entropy_coeff: 0.0017600000137463212
        kl: 0.013743524439632893
        model: {}
        policy_loss: -0.037628479301929474
        total_loss: -0.03527607396245003
        vf_explained_var: 0.08395934104919434
        vf_loss: 14.696728706359863
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 0.9861080646514893
        entropy_coeff: 0.0017600000137463212
        kl: 0.016712984070181847
        model: {}
        policy_loss: -0.03181451931595802
        total_loss: -0.028939176350831985
        vf_explained_var: 0.209473118185997
        vf_loss: 12.6829833984375
    load_time_ms: 23576.566
    num_steps_sampled: 11232000
    num_steps_trained: 11232000
    sample_time_ms: 131515.741
    update_time_ms: 62.743
  iterations_since_restore: 37
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.46160714285714
    ram_util_percent: 14.874107142857143
  pid: 14340
  policy_reward_max:
    agent-0: 141.66666666666703
    agent-1: 141.66666666666703
    agent-2: 141.66666666666703
    agent-3: 141.66666666666703
    agent-4: 141.66666666666703
    agent-5: 141.66666666666703
  policy_reward_mean:
    agent-0: 100.1350000000003
    agent-1: 100.1350000000003
    agent-2: 100.1350000000003
    agent-3: 100.1350000000003
    agent-4: 100.1350000000003
    agent-5: 100.1350000000003
  policy_reward_min:
    agent-0: 32.166666666666735
    agent-1: 32.166666666666735
    agent-2: 32.166666666666735
    agent-3: 32.166666666666735
    agent-4: 32.166666666666735
    agent-5: 32.166666666666735
  sampler_perf:
    mean_env_wait_ms: 30.96252033839501
    mean_inference_ms: 14.4592374882163
    mean_processing_ms: 65.54592911916102
  time_since_restore: 6606.973767757416
  time_this_iter_s: 158.0382490158081
  time_total_s: 19157.790639162064
  timestamp: 1637041689
  timesteps_since_restore: 3552000
  timesteps_this_iter: 96000
  timesteps_total: 11232000
  training_iteration: 117
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    117 |          19157.8 | 11232000 |   600.81 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 4.89
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 20.59
    apples_agent-1_min: 0
    apples_agent-2_max: 136
    apples_agent-2_mean: 12.72
    apples_agent-2_min: 0
    apples_agent-3_max: 152
    apples_agent-3_mean: 94.31
    apples_agent-3_min: 30
    apples_agent-4_max: 83
    apples_agent-4_mean: 5.66
    apples_agent-4_min: 0
    apples_agent-5_max: 133
    apples_agent-5_mean: 79.05
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 416
    cleaning_beam_agent-0_mean: 304.95
    cleaning_beam_agent-0_min: 160
    cleaning_beam_agent-1_max: 484
    cleaning_beam_agent-1_mean: 275.4
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 453
    cleaning_beam_agent-2_mean: 272.74
    cleaning_beam_agent-2_min: 94
    cleaning_beam_agent-3_max: 229
    cleaning_beam_agent-3_mean: 92.25
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 335.8
    cleaning_beam_agent-4_min: 152
    cleaning_beam_agent-5_max: 361
    cleaning_beam_agent-5_mean: 115.98
    cleaning_beam_agent-5_min: 38
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-51-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 849.9999999999744
  episode_reward_mean: 602.1199999999983
  episode_reward_min: 275.9999999999988
  episodes_this_iter: 96
  episodes_total: 11328
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13247.205
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 1.2485491037368774
        entropy_coeff: 0.0017600000137463212
        kl: 0.01253288984298706
        model: {}
        policy_loss: -0.03138509765267372
        total_loss: -0.02971292845904827
        vf_explained_var: 0.080641970038414
        vf_loss: 13.630330085754395
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 1.1632466316223145
        entropy_coeff: 0.0017600000137463212
        kl: 0.013810173608362675
        model: {}
        policy_loss: -0.03148166835308075
        total_loss: -0.029340296983718872
        vf_explained_var: 0.0380341112613678
        vf_loss: 14.26646614074707
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 1.1577705144882202
        entropy_coeff: 0.0017600000137463212
        kl: 0.014810379594564438
        model: {}
        policy_loss: -0.03193223476409912
        total_loss: -0.02961142733693123
        vf_explained_var: 0.0591578483581543
        vf_loss: 13.964062690734863
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 0.8396581411361694
        entropy_coeff: 0.0017600000137463212
        kl: 0.010685757733881474
        model: {}
        policy_loss: -0.02561311610043049
        total_loss: -0.023754026740789413
        vf_explained_var: 0.19062407314777374
        vf_loss: 11.997368812561035
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 1.060677170753479
        entropy_coeff: 0.0017600000137463212
        kl: 0.014772485010325909
        model: {}
        policy_loss: -0.038376834243535995
        total_loss: -0.03599211573600769
        vf_explained_var: 0.12461671233177185
        vf_loss: 12.970101356506348
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 0.9973865747451782
        entropy_coeff: 0.0017600000137463212
        kl: 0.013730943202972412
        model: {}
        policy_loss: -0.03444401174783707
        total_loss: -0.03221887722611427
        vf_explained_var: 0.16710998117923737
        vf_loss: 12.343433380126953
    load_time_ms: 24814.354
    num_steps_sampled: 11328000
    num_steps_trained: 11328000
    sample_time_ms: 130968.682
    update_time_ms: 61.224
  iterations_since_restore: 38
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.78971193415638
    ram_util_percent: 15.062551440329216
  pid: 14340
  policy_reward_max:
    agent-0: 141.66666666666703
    agent-1: 141.66666666666703
    agent-2: 141.66666666666703
    agent-3: 141.66666666666703
    agent-4: 141.66666666666703
    agent-5: 141.66666666666703
  policy_reward_mean:
    agent-0: 100.35333333333361
    agent-1: 100.35333333333361
    agent-2: 100.35333333333361
    agent-3: 100.35333333333361
    agent-4: 100.35333333333361
    agent-5: 100.35333333333361
  policy_reward_min:
    agent-0: 45.999999999999964
    agent-1: 45.999999999999964
    agent-2: 45.999999999999964
    agent-3: 45.999999999999964
    agent-4: 45.999999999999964
    agent-5: 45.999999999999964
  sampler_perf:
    mean_env_wait_ms: 30.950562629955986
    mean_inference_ms: 14.461084770742026
    mean_processing_ms: 65.54327609342991
  time_since_restore: 6776.778106212616
  time_this_iter_s: 169.8043384552002
  time_total_s: 19327.594977617264
  timestamp: 1637041860
  timesteps_since_restore: 3648000
  timesteps_this_iter: 96000
  timesteps_total: 11328000
  training_iteration: 118
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    118 |          19327.6 | 11328000 |   602.12 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 5.67
    apples_agent-0_min: 0
    apples_agent-1_max: 74
    apples_agent-1_mean: 17.6
    apples_agent-1_min: 0
    apples_agent-2_max: 197
    apples_agent-2_mean: 19.68
    apples_agent-2_min: 0
    apples_agent-3_max: 179
    apples_agent-3_mean: 88.82
    apples_agent-3_min: 19
    apples_agent-4_max: 83
    apples_agent-4_mean: 3.8
    apples_agent-4_min: 0
    apples_agent-5_max: 161
    apples_agent-5_mean: 77.84
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 434
    cleaning_beam_agent-0_mean: 291.36
    cleaning_beam_agent-0_min: 155
    cleaning_beam_agent-1_max: 461
    cleaning_beam_agent-1_mean: 277.24
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 467
    cleaning_beam_agent-2_mean: 265.1
    cleaning_beam_agent-2_min: 101
    cleaning_beam_agent-3_max: 196
    cleaning_beam_agent-3_mean: 78.13
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 438
    cleaning_beam_agent-4_mean: 324.77
    cleaning_beam_agent-4_min: 188
    cleaning_beam_agent-5_max: 485
    cleaning_beam_agent-5_mean: 121.76
    cleaning_beam_agent-5_min: 27
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-53-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 829.9999999999791
  episode_reward_mean: 569.7499999999997
  episode_reward_min: 158.99999999999906
  episodes_this_iter: 96
  episodes_total: 11424
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13247.822
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 1.2486106157302856
        entropy_coeff: 0.0017600000137463212
        kl: 0.011967318132519722
        model: {}
        policy_loss: -0.030461691319942474
        total_loss: -0.02879677340388298
        vf_explained_var: 0.1091519445180893
        vf_loss: 14.690091133117676
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 1.159654140472412
        entropy_coeff: 0.0017600000137463212
        kl: 0.013385122641921043
        model: {}
        policy_loss: -0.03137694671750069
        total_loss: -0.02912794053554535
        vf_explained_var: 0.022282034158706665
        vf_loss: 16.129749298095703
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 1.159277081489563
        entropy_coeff: 0.0017600000137463212
        kl: 0.013310651294887066
        model: {}
        policy_loss: -0.03343893587589264
        total_loss: -0.0313132107257843
        vf_explained_var: 0.08865334093570709
        vf_loss: 15.039236068725586
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 0.8382532596588135
        entropy_coeff: 0.0017600000137463212
        kl: 0.010976708494126797
        model: {}
        policy_loss: -0.02602459117770195
        total_loss: -0.02408650331199169
        vf_explained_var: 0.262081503868103
        vf_loss: 12.180767059326172
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 1.0685423612594604
        entropy_coeff: 0.0017600000137463212
        kl: 0.015192897990345955
        model: {}
        policy_loss: -0.03937821835279465
        total_loss: -0.03676270321011543
        vf_explained_var: 0.11660663783550262
        vf_loss: 14.57565975189209
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 1.0109299421310425
        entropy_coeff: 0.0017600000137463212
        kl: 0.013404128141701221
        model: {}
        policy_loss: -0.0354682132601738
        total_loss: -0.03318363055586815
        vf_explained_var: 0.16196998953819275
        vf_loss: 13.829898834228516
    load_time_ms: 25150.461
    num_steps_sampled: 11424000
    num_steps_trained: 11424000
    sample_time_ms: 131422.689
    update_time_ms: 58.599
  iterations_since_restore: 39
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.0703125
    ram_util_percent: 15.101171874999999
  pid: 14340
  policy_reward_max:
    agent-0: 138.33333333333368
    agent-1: 138.33333333333368
    agent-2: 138.33333333333368
    agent-3: 138.33333333333368
    agent-4: 138.33333333333368
    agent-5: 138.33333333333368
  policy_reward_mean:
    agent-0: 94.95833333333357
    agent-1: 94.95833333333357
    agent-2: 94.95833333333357
    agent-3: 94.95833333333357
    agent-4: 94.95833333333357
    agent-5: 94.95833333333357
  policy_reward_min:
    agent-0: 26.50000000000006
    agent-1: 26.50000000000006
    agent-2: 26.50000000000006
    agent-3: 26.50000000000006
    agent-4: 26.50000000000006
    agent-5: 26.50000000000006
  sampler_perf:
    mean_env_wait_ms: 30.93580297463296
    mean_inference_ms: 14.463197174011395
    mean_processing_ms: 65.53831383070631
  time_since_restore: 6956.070123672485
  time_this_iter_s: 179.29201745986938
  time_total_s: 19506.886995077133
  timestamp: 1637042039
  timesteps_since_restore: 3744000
  timesteps_this_iter: 96000
  timesteps_total: 11424000
  training_iteration: 119
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    119 |          19506.9 | 11424000 |   569.75 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 4.65
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 18.11
    apples_agent-1_min: 0
    apples_agent-2_max: 296
    apples_agent-2_mean: 19.18
    apples_agent-2_min: 0
    apples_agent-3_max: 151
    apples_agent-3_mean: 91.69
    apples_agent-3_min: 31
    apples_agent-4_max: 66
    apples_agent-4_mean: 3.71
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 82.57
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 392
    cleaning_beam_agent-0_mean: 294.43
    cleaning_beam_agent-0_min: 198
    cleaning_beam_agent-1_max: 509
    cleaning_beam_agent-1_mean: 277.55
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 532
    cleaning_beam_agent-2_mean: 270.57
    cleaning_beam_agent-2_min: 70
    cleaning_beam_agent-3_max: 178
    cleaning_beam_agent-3_mean: 83.99
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 536
    cleaning_beam_agent-4_mean: 326.59
    cleaning_beam_agent-4_min: 147
    cleaning_beam_agent-5_max: 482
    cleaning_beam_agent-5_mean: 116.51
    cleaning_beam_agent-5_min: 32
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-56-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 869.999999999974
  episode_reward_mean: 606.2599999999984
  episode_reward_min: 306.9999999999999
  episodes_this_iter: 96
  episodes_total: 11520
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13250.155
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 1.2542922496795654
        entropy_coeff: 0.0017600000137463212
        kl: 0.01193271018564701
        model: {}
        policy_loss: -0.031162992119789124
        total_loss: -0.029570505023002625
        vf_explained_var: 0.11281584203243256
        vf_loss: 14.135016441345215
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 1.1260581016540527
        entropy_coeff: 0.0017600000137463212
        kl: 0.01358359307050705
        model: {}
        policy_loss: -0.03205837309360504
        total_loss: -0.02980804815888405
        vf_explained_var: 0.049151286482810974
        vf_loss: 15.154620170593262
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 1.1564898490905762
        entropy_coeff: 0.0017600000137463212
        kl: 0.01388031430542469
        model: {}
        policy_loss: -0.03506976738572121
        total_loss: -0.03286571800708771
        vf_explained_var: 0.0823570191860199
        vf_loss: 14.63412857055664
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 0.8021584153175354
        entropy_coeff: 0.0017600000137463212
        kl: 0.009852913208305836
        model: {}
        policy_loss: -0.024857059121131897
        total_loss: -0.023028124123811722
        vf_explained_var: 0.20349127054214478
        vf_loss: 12.70154857635498
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 1.058178424835205
        entropy_coeff: 0.0017600000137463212
        kl: 0.014195386320352554
        model: {}
        policy_loss: -0.038662370294332504
        total_loss: -0.03625008463859558
        vf_explained_var: 0.09931391477584839
        vf_loss: 14.355968475341797
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 0.9850763082504272
        entropy_coeff: 0.0017600000137463212
        kl: 0.013578228652477264
        model: {}
        policy_loss: -0.03390226513147354
        total_loss: -0.03162536025047302
        vf_explained_var: 0.18778477609157562
        vf_loss: 12.949893951416016
    load_time_ms: 26664.689
    num_steps_sampled: 11520000
    num_steps_trained: 11520000
    sample_time_ms: 131595.077
    update_time_ms: 60.744
  iterations_since_restore: 40
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.991338582677166
    ram_util_percent: 15.078346456692913
  pid: 14340
  policy_reward_max:
    agent-0: 145.00000000000006
    agent-1: 145.00000000000006
    agent-2: 145.00000000000006
    agent-3: 145.00000000000006
    agent-4: 145.00000000000006
    agent-5: 145.00000000000006
  policy_reward_mean:
    agent-0: 101.0433333333336
    agent-1: 101.0433333333336
    agent-2: 101.0433333333336
    agent-3: 101.0433333333336
    agent-4: 101.0433333333336
    agent-5: 101.0433333333336
  policy_reward_min:
    agent-0: 51.16666666666657
    agent-1: 51.16666666666657
    agent-2: 51.16666666666657
    agent-3: 51.16666666666657
    agent-4: 51.16666666666657
    agent-5: 51.16666666666657
  sampler_perf:
    mean_env_wait_ms: 30.924702253085982
    mean_inference_ms: 14.463739768826876
    mean_processing_ms: 65.53671993619392
  time_since_restore: 7134.502790689468
  time_this_iter_s: 178.43266701698303
  time_total_s: 19685.319662094116
  timestamp: 1637042218
  timesteps_since_restore: 3840000
  timesteps_this_iter: 96000
  timesteps_total: 11520000
  training_iteration: 120
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    120 |          19685.3 | 11520000 |   606.26 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 7.48
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 18.28
    apples_agent-1_min: 0
    apples_agent-2_max: 272
    apples_agent-2_mean: 18.38
    apples_agent-2_min: 0
    apples_agent-3_max: 151
    apples_agent-3_mean: 88.57
    apples_agent-3_min: 25
    apples_agent-4_max: 90
    apples_agent-4_mean: 5.73
    apples_agent-4_min: 0
    apples_agent-5_max: 229
    apples_agent-5_mean: 77.82
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 376
    cleaning_beam_agent-0_mean: 271.26
    cleaning_beam_agent-0_min: 166
    cleaning_beam_agent-1_max: 455
    cleaning_beam_agent-1_mean: 272.6
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 490
    cleaning_beam_agent-2_mean: 293.6
    cleaning_beam_agent-2_min: 103
    cleaning_beam_agent-3_max: 187
    cleaning_beam_agent-3_mean: 83.66
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 454
    cleaning_beam_agent-4_mean: 321.11
    cleaning_beam_agent-4_min: 116
    cleaning_beam_agent-5_max: 482
    cleaning_beam_agent-5_mean: 126.93
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-59-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 846.999999999991
  episode_reward_mean: 577.6099999999994
  episode_reward_min: 141.00000000000054
  episodes_this_iter: 96
  episodes_total: 11616
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13127.656
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 1.2634975910186768
        entropy_coeff: 0.0017600000137463212
        kl: 0.012294473126530647
        model: {}
        policy_loss: -0.03237587958574295
        total_loss: -0.030651148408651352
        vf_explained_var: 0.09124988317489624
        vf_loss: 14.8959321975708
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 1.1580469608306885
        entropy_coeff: 0.0017600000137463212
        kl: 0.013644018210470676
        model: {}
        policy_loss: -0.032395556569099426
        total_loss: -0.030121002346277237
        vf_explained_var: 0.03375843167304993
        vf_loss: 15.839189529418945
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 1.1466377973556519
        entropy_coeff: 0.0017600000137463212
        kl: 0.01322650071233511
        model: {}
        policy_loss: -0.03373418375849724
        total_loss: -0.03157780319452286
        vf_explained_var: 0.0677582174539566
        vf_loss: 15.291650772094727
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 0.8362049460411072
        entropy_coeff: 0.0017600000137463212
        kl: 0.01063683070242405
        model: {}
        policy_loss: -0.026531675830483437
        total_loss: -0.0246223546564579
        vf_explained_var: 0.23500759899616241
        vf_loss: 12.536725044250488
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 1.0616004467010498
        entropy_coeff: 0.0017600000137463212
        kl: 0.01449427381157875
        model: {}
        policy_loss: -0.0392155759036541
        total_loss: -0.03673085197806358
        vf_explained_var: 0.11372031271457672
        vf_loss: 14.542842864990234
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 0.9915103316307068
        entropy_coeff: 0.0017600000137463212
        kl: 0.012772362679243088
        model: {}
        policy_loss: -0.03533721715211868
        total_loss: -0.03320716321468353
        vf_explained_var: 0.19472528994083405
        vf_loss: 13.206425666809082
    load_time_ms: 28142.029
    num_steps_sampled: 11616000
    num_steps_trained: 11616000
    sample_time_ms: 131632.564
    update_time_ms: 51.771
  iterations_since_restore: 41
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.784337349397592
    ram_util_percent: 15.089959839357428
  pid: 14340
  policy_reward_max:
    agent-0: 141.16666666666708
    agent-1: 141.16666666666708
    agent-2: 141.16666666666708
    agent-3: 141.16666666666708
    agent-4: 141.16666666666708
    agent-5: 141.16666666666708
  policy_reward_mean:
    agent-0: 96.26833333333357
    agent-1: 96.26833333333357
    agent-2: 96.26833333333357
    agent-3: 96.26833333333357
    agent-4: 96.26833333333357
    agent-5: 96.26833333333357
  policy_reward_min:
    agent-0: 23.50000000000002
    agent-1: 23.50000000000002
    agent-2: 23.50000000000002
    agent-3: 23.50000000000002
    agent-4: 23.50000000000002
    agent-5: 23.50000000000002
  sampler_perf:
    mean_env_wait_ms: 30.917141652571804
    mean_inference_ms: 14.465782204682323
    mean_processing_ms: 65.53509810676499
  time_since_restore: 7309.4402594566345
  time_this_iter_s: 174.93746876716614
  time_total_s: 19860.257130861282
  timestamp: 1637042393
  timesteps_since_restore: 3936000
  timesteps_this_iter: 96000
  timesteps_total: 11616000
  training_iteration: 121
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    121 |          19860.3 | 11616000 |   577.61 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 101
    apples_agent-0_mean: 8.84
    apples_agent-0_min: 0
    apples_agent-1_max: 115
    apples_agent-1_mean: 13.09
    apples_agent-1_min: 0
    apples_agent-2_max: 147
    apples_agent-2_mean: 8.22
    apples_agent-2_min: 0
    apples_agent-3_max: 153
    apples_agent-3_mean: 94.18
    apples_agent-3_min: 16
    apples_agent-4_max: 84
    apples_agent-4_mean: 4.86
    apples_agent-4_min: 0
    apples_agent-5_max: 227
    apples_agent-5_mean: 82.74
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 392
    cleaning_beam_agent-0_mean: 270.77
    cleaning_beam_agent-0_min: 127
    cleaning_beam_agent-1_max: 519
    cleaning_beam_agent-1_mean: 264.97
    cleaning_beam_agent-1_min: 64
    cleaning_beam_agent-2_max: 477
    cleaning_beam_agent-2_mean: 296.02
    cleaning_beam_agent-2_min: 114
    cleaning_beam_agent-3_max: 174
    cleaning_beam_agent-3_mean: 76.99
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 493
    cleaning_beam_agent-4_mean: 324.79
    cleaning_beam_agent-4_min: 164
    cleaning_beam_agent-5_max: 367
    cleaning_beam_agent-5_mean: 118.31
    cleaning_beam_agent-5_min: 30
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 7
    fire_beam_agent-5_mean: 0.09
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-02-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 837.999999999984
  episode_reward_mean: 597.0699999999981
  episode_reward_min: 189.0000000000006
  episodes_this_iter: 96
  episodes_total: 11712
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13127.906
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 1.2518010139465332
        entropy_coeff: 0.0017600000137463212
        kl: 0.011560432612895966
        model: {}
        policy_loss: -0.03235679119825363
        total_loss: -0.030665434896945953
        vf_explained_var: 0.046112701296806335
        vf_loss: 15.824393272399902
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 1.1231335401535034
        entropy_coeff: 0.0017600000137463212
        kl: 0.01290101744234562
        model: {}
        policy_loss: -0.031224584206938744
        total_loss: -0.029014073312282562
        vf_explained_var: 0.031137391924858093
        vf_loss: 16.070232391357422
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 1.1663851737976074
        entropy_coeff: 0.0017600000137463212
        kl: 0.013469679281115532
        model: {}
        policy_loss: -0.03318428620696068
        total_loss: -0.03103078156709671
        vf_explained_var: 0.08807803690433502
        vf_loss: 15.124082565307617
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 0.8187975883483887
        entropy_coeff: 0.0017600000137463212
        kl: 0.010048316791653633
        model: {}
        policy_loss: -0.025408808141946793
        total_loss: -0.023474333807826042
        vf_explained_var: 0.17722360789775848
        vf_loss: 13.658946990966797
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 1.0595895051956177
        entropy_coeff: 0.0017600000137463212
        kl: 0.014457454904913902
        model: {}
        policy_loss: -0.03653785586357117
        total_loss: -0.034040529280900955
        vf_explained_var: 0.11363671720027924
        vf_loss: 14.707111358642578
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 0.9782674312591553
        entropy_coeff: 0.0017600000137463212
        kl: 0.012788025662302971
        model: {}
        policy_loss: -0.03336787968873978
        total_loss: -0.031094837933778763
        vf_explained_var: 0.13327550888061523
        vf_loss: 14.37185001373291
    load_time_ms: 27084.88
    num_steps_sampled: 11712000
    num_steps_trained: 11712000
    sample_time_ms: 131899.15
    update_time_ms: 51.813
  iterations_since_restore: 42
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.666666666666668
    ram_util_percent: 14.93502109704641
  pid: 14340
  policy_reward_max:
    agent-0: 139.66666666666688
    agent-1: 139.66666666666688
    agent-2: 139.66666666666688
    agent-3: 139.66666666666688
    agent-4: 139.66666666666688
    agent-5: 139.66666666666688
  policy_reward_mean:
    agent-0: 99.51166666666695
    agent-1: 99.51166666666695
    agent-2: 99.51166666666695
    agent-3: 99.51166666666695
    agent-4: 99.51166666666695
    agent-5: 99.51166666666695
  policy_reward_min:
    agent-0: 31.5
    agent-1: 31.5
    agent-2: 31.5
    agent-3: 31.5
    agent-4: 31.5
    agent-5: 31.5
  sampler_perf:
    mean_env_wait_ms: 30.906600663370686
    mean_inference_ms: 14.467121801986163
    mean_processing_ms: 65.53826818584172
  time_since_restore: 7475.894131183624
  time_this_iter_s: 166.45387172698975
  time_total_s: 20026.711002588272
  timestamp: 1637042560
  timesteps_since_restore: 4032000
  timesteps_this_iter: 96000
  timesteps_total: 11712000
  training_iteration: 122
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    122 |          20026.7 | 11712000 |   597.07 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 5.13
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 19.12
    apples_agent-1_min: 0
    apples_agent-2_max: 172
    apples_agent-2_mean: 17.84
    apples_agent-2_min: 0
    apples_agent-3_max: 155
    apples_agent-3_mean: 85.82
    apples_agent-3_min: 12
    apples_agent-4_max: 75
    apples_agent-4_mean: 4.09
    apples_agent-4_min: 0
    apples_agent-5_max: 227
    apples_agent-5_mean: 85.44
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 344
    cleaning_beam_agent-0_mean: 278.04
    cleaning_beam_agent-0_min: 105
    cleaning_beam_agent-1_max: 420
    cleaning_beam_agent-1_mean: 242.09
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 509
    cleaning_beam_agent-2_mean: 266.43
    cleaning_beam_agent-2_min: 89
    cleaning_beam_agent-3_max: 296
    cleaning_beam_agent-3_mean: 78.66
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 435
    cleaning_beam_agent-4_mean: 311.48
    cleaning_beam_agent-4_min: 142
    cleaning_beam_agent-5_max: 371
    cleaning_beam_agent-5_mean: 107.76
    cleaning_beam_agent-5_min: 27
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-05-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 792.9999999999911
  episode_reward_mean: 565.3799999999991
  episode_reward_min: 189.9999999999979
  episodes_this_iter: 96
  episodes_total: 11808
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13039.217
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 1.2718861103057861
        entropy_coeff: 0.0017600000137463212
        kl: 0.011919817887246609
        model: {}
        policy_loss: -0.03190775215625763
        total_loss: -0.030272167176008224
        vf_explained_var: 0.07197074592113495
        vf_loss: 14.901381492614746
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 1.15886652469635
        entropy_coeff: 0.0017600000137463212
        kl: 0.013282770290970802
        model: {}
        policy_loss: -0.033104557543992996
        total_loss: -0.03095434233546257
        vf_explained_var: 0.045351311564445496
        vf_loss: 15.332653999328613
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 1.144829511642456
        entropy_coeff: 0.0017600000137463212
        kl: 0.013817256316542625
        model: {}
        policy_loss: -0.03495458886027336
        total_loss: -0.03278007358312607
        vf_explained_var: 0.11194218695163727
        vf_loss: 14.259621620178223
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 0.8527579307556152
        entropy_coeff: 0.0017600000137463212
        kl: 0.010942436754703522
        model: {}
        policy_loss: -0.026917632669210434
        total_loss: -0.02504121884703636
        vf_explained_var: 0.259049654006958
        vf_loss: 11.8878173828125
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 1.0567753314971924
        entropy_coeff: 0.0017600000137463212
        kl: 0.01422915980219841
        model: {}
        policy_loss: -0.0386149026453495
        total_loss: -0.03618112951517105
        vf_explained_var: 0.09887124598026276
        vf_loss: 14.478670120239258
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 0.9933134317398071
        entropy_coeff: 0.0017600000137463212
        kl: 0.01290705893188715
        model: {}
        policy_loss: -0.034021370112895966
        total_loss: -0.031913887709379196
        vf_explained_var: 0.20681142807006836
        vf_loss: 12.74305534362793
    load_time_ms: 27678.737
    num_steps_sampled: 11808000
    num_steps_trained: 11808000
    sample_time_ms: 130842.168
    update_time_ms: 50.318
  iterations_since_restore: 43
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.11522633744856
    ram_util_percent: 15.032098765432098
  pid: 14340
  policy_reward_max:
    agent-0: 132.16666666666694
    agent-1: 132.16666666666694
    agent-2: 132.16666666666694
    agent-3: 132.16666666666694
    agent-4: 132.16666666666694
    agent-5: 132.16666666666694
  policy_reward_mean:
    agent-0: 94.23000000000026
    agent-1: 94.23000000000026
    agent-2: 94.23000000000026
    agent-3: 94.23000000000026
    agent-4: 94.23000000000026
    agent-5: 94.23000000000026
  policy_reward_min:
    agent-0: 31.66666666666675
    agent-1: 31.66666666666675
    agent-2: 31.66666666666675
    agent-3: 31.66666666666675
    agent-4: 31.66666666666675
    agent-5: 31.66666666666675
  sampler_perf:
    mean_env_wait_ms: 30.888259561893836
    mean_inference_ms: 14.469877350313544
    mean_processing_ms: 65.53717304042573
  time_since_restore: 7646.137065649033
  time_this_iter_s: 170.24293446540833
  time_total_s: 20196.95393705368
  timestamp: 1637042730
  timesteps_since_restore: 4128000
  timesteps_this_iter: 96000
  timesteps_total: 11808000
  training_iteration: 123
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    123 |            20197 | 11808000 |   565.38 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 72
    apples_agent-0_mean: 5.59
    apples_agent-0_min: 0
    apples_agent-1_max: 144
    apples_agent-1_mean: 15.46
    apples_agent-1_min: 0
    apples_agent-2_max: 192
    apples_agent-2_mean: 10.93
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 87.87
    apples_agent-3_min: 25
    apples_agent-4_max: 84
    apples_agent-4_mean: 5.26
    apples_agent-4_min: 0
    apples_agent-5_max: 161
    apples_agent-5_mean: 79.29
    apples_agent-5_min: 3
    cleaning_beam_agent-0_max: 415
    cleaning_beam_agent-0_mean: 294.31
    cleaning_beam_agent-0_min: 174
    cleaning_beam_agent-1_max: 438
    cleaning_beam_agent-1_mean: 250.06
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 474
    cleaning_beam_agent-2_mean: 277.0
    cleaning_beam_agent-2_min: 88
    cleaning_beam_agent-3_max: 327
    cleaning_beam_agent-3_mean: 79.1
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 490
    cleaning_beam_agent-4_mean: 309.61
    cleaning_beam_agent-4_min: 157
    cleaning_beam_agent-5_max: 443
    cleaning_beam_agent-5_mean: 105.08
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-08-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 804.9999999999875
  episode_reward_mean: 554.1200000000013
  episode_reward_min: 237.9999999999974
  episodes_this_iter: 96
  episodes_total: 11904
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12885.983
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 1.2526659965515137
        entropy_coeff: 0.0017600000137463212
        kl: 0.012387143447995186
        model: {}
        policy_loss: -0.031079798936843872
        total_loss: -0.029427826404571533
        vf_explained_var: 0.07541058957576752
        vf_loss: 13.792357444763184
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 1.153613567352295
        entropy_coeff: 0.0017600000137463212
        kl: 0.01357643585652113
        model: {}
        policy_loss: -0.032794684171676636
        total_loss: -0.030713357031345367
        vf_explained_var: 0.06363503634929657
        vf_loss: 13.963937759399414
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 1.1518267393112183
        entropy_coeff: 0.0017600000137463212
        kl: 0.013095675967633724
        model: {}
        policy_loss: -0.034135427325963974
        total_loss: -0.0321408212184906
        vf_explained_var: 0.060358837246894836
        vf_loss: 14.026851654052734
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 0.8437807559967041
        entropy_coeff: 0.0017600000137463212
        kl: 0.01040883082896471
        model: {}
        policy_loss: -0.026422565802931786
        total_loss: -0.02464997209608555
        vf_explained_var: 0.2112656682729721
        vf_loss: 11.758846282958984
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 1.0452954769134521
        entropy_coeff: 0.0017600000137463212
        kl: 0.013658594340085983
        model: {}
        policy_loss: -0.03852609172463417
        total_loss: -0.03633761405944824
        vf_explained_var: 0.13202181458473206
        vf_loss: 12.964799880981445
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 0.9877290725708008
        entropy_coeff: 0.0017600000137463212
        kl: 0.013788895681500435
        model: {}
        policy_loss: -0.03448840603232384
        total_loss: -0.0322967953979969
        vf_explained_var: 0.2144422084093094
        vf_loss: 11.722296714782715
    load_time_ms: 28916.315
    num_steps_sampled: 11904000
    num_steps_trained: 11904000
    sample_time_ms: 130605.621
    update_time_ms: 54.154
  iterations_since_restore: 44
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.61847389558233
    ram_util_percent: 15.101606425702812
  pid: 14340
  policy_reward_max:
    agent-0: 134.16666666666694
    agent-1: 134.16666666666694
    agent-2: 134.16666666666694
    agent-3: 134.16666666666694
    agent-4: 134.16666666666694
    agent-5: 134.16666666666694
  policy_reward_mean:
    agent-0: 92.35333333333354
    agent-1: 92.35333333333354
    agent-2: 92.35333333333354
    agent-3: 92.35333333333354
    agent-4: 92.35333333333354
    agent-5: 92.35333333333354
  policy_reward_min:
    agent-0: 39.666666666666714
    agent-1: 39.666666666666714
    agent-2: 39.666666666666714
    agent-3: 39.666666666666714
    agent-4: 39.666666666666714
    agent-5: 39.666666666666714
  sampler_perf:
    mean_env_wait_ms: 30.873376094657782
    mean_inference_ms: 14.472696406693885
    mean_processing_ms: 65.53431008882849
  time_since_restore: 7820.860335826874
  time_this_iter_s: 174.7232701778412
  time_total_s: 20371.67720723152
  timestamp: 1637042905
  timesteps_since_restore: 4224000
  timesteps_this_iter: 96000
  timesteps_total: 11904000
  training_iteration: 124
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    124 |          20371.7 | 11904000 |   554.12 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 4.24
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 18.66
    apples_agent-1_min: 0
    apples_agent-2_max: 168
    apples_agent-2_mean: 9.4
    apples_agent-2_min: 0
    apples_agent-3_max: 203
    apples_agent-3_mean: 89.06
    apples_agent-3_min: 0
    apples_agent-4_max: 121
    apples_agent-4_mean: 6.01
    apples_agent-4_min: 0
    apples_agent-5_max: 193
    apples_agent-5_mean: 81.94
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 403
    cleaning_beam_agent-0_mean: 288.21
    cleaning_beam_agent-0_min: 131
    cleaning_beam_agent-1_max: 412
    cleaning_beam_agent-1_mean: 248.84
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 441
    cleaning_beam_agent-2_mean: 277.72
    cleaning_beam_agent-2_min: 130
    cleaning_beam_agent-3_max: 207
    cleaning_beam_agent-3_mean: 76.65
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 421
    cleaning_beam_agent-4_mean: 312.77
    cleaning_beam_agent-4_min: 114
    cleaning_beam_agent-5_max: 457
    cleaning_beam_agent-5_mean: 104.44
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-11-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 854.9999999999852
  episode_reward_mean: 586.689999999999
  episode_reward_min: 115.00000000000095
  episodes_this_iter: 96
  episodes_total: 12000
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12879.933
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 1.2446317672729492
        entropy_coeff: 0.0017600000137463212
        kl: 0.011470325291156769
        model: {}
        policy_loss: -0.031257182359695435
        total_loss: -0.02963137812912464
        vf_explained_var: 0.055021271109580994
        vf_loss: 15.222898483276367
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 1.173926591873169
        entropy_coeff: 0.0017600000137463212
        kl: 0.013298598118126392
        model: {}
        policy_loss: -0.033009957522153854
        total_loss: -0.030894631519913673
        vf_explained_var: 0.05484585464000702
        vf_loss: 15.217166900634766
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 1.1607170104980469
        entropy_coeff: 0.0017600000137463212
        kl: 0.012715311720967293
        model: {}
        policy_loss: -0.03413860499858856
        total_loss: -0.03218253701925278
        vf_explained_var: 0.09595248103141785
        vf_loss: 14.558696746826172
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 0.8286975622177124
        entropy_coeff: 0.0017600000137463212
        kl: 0.010130362585186958
        model: {}
        policy_loss: -0.02711339108645916
        total_loss: -0.02527696080505848
        vf_explained_var: 0.21189013123512268
        vf_loss: 12.688645362854004
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 1.0476094484329224
        entropy_coeff: 0.0017600000137463212
        kl: 0.01456554513424635
        model: {}
        policy_loss: -0.03958355262875557
        total_loss: -0.0371188186109066
        vf_explained_var: 0.13347941637039185
        vf_loss: 13.954120635986328
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 0.971254289150238
        entropy_coeff: 0.0017600000137463212
        kl: 0.012709341011941433
        model: {}
        policy_loss: -0.03309417515993118
        total_loss: -0.030981626361608505
        vf_explained_var: 0.20467540621757507
        vf_loss: 12.800858497619629
    load_time_ms: 28705.426
    num_steps_sampled: 12000000
    num_steps_trained: 12000000
    sample_time_ms: 130679.476
    update_time_ms: 53.378
  iterations_since_restore: 45
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.704032258064515
    ram_util_percent: 15.083870967741936
  pid: 14340
  policy_reward_max:
    agent-0: 142.50000000000048
    agent-1: 142.50000000000048
    agent-2: 142.50000000000048
    agent-3: 142.50000000000048
    agent-4: 142.50000000000048
    agent-5: 142.50000000000048
  policy_reward_mean:
    agent-0: 97.78166666666694
    agent-1: 97.78166666666694
    agent-2: 97.78166666666694
    agent-3: 97.78166666666694
    agent-4: 97.78166666666694
    agent-5: 97.78166666666694
  policy_reward_min:
    agent-0: 19.16666666666666
    agent-1: 19.16666666666666
    agent-2: 19.16666666666666
    agent-3: 19.16666666666666
    agent-4: 19.16666666666666
    agent-5: 19.16666666666666
  sampler_perf:
    mean_env_wait_ms: 30.860721565129623
    mean_inference_ms: 14.474497912868367
    mean_processing_ms: 65.53989704658018
  time_since_restore: 7995.222168207169
  time_this_iter_s: 174.3618323802948
  time_total_s: 20546.039039611816
  timestamp: 1637043080
  timesteps_since_restore: 4320000
  timesteps_this_iter: 96000
  timesteps_total: 12000000
  training_iteration: 125
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    125 |            20546 | 12000000 |   586.69 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 4.79
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 20.94
    apples_agent-1_min: 0
    apples_agent-2_max: 216
    apples_agent-2_mean: 20.12
    apples_agent-2_min: 0
    apples_agent-3_max: 178
    apples_agent-3_mean: 88.81
    apples_agent-3_min: 35
    apples_agent-4_max: 79
    apples_agent-4_mean: 3.03
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 80.94
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 386
    cleaning_beam_agent-0_mean: 290.91
    cleaning_beam_agent-0_min: 136
    cleaning_beam_agent-1_max: 394
    cleaning_beam_agent-1_mean: 247.11
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 473
    cleaning_beam_agent-2_mean: 283.28
    cleaning_beam_agent-2_min: 112
    cleaning_beam_agent-3_max: 266
    cleaning_beam_agent-3_mean: 83.5
    cleaning_beam_agent-3_min: 30
    cleaning_beam_agent-4_max: 407
    cleaning_beam_agent-4_mean: 314.25
    cleaning_beam_agent-4_min: 167
    cleaning_beam_agent-5_max: 403
    cleaning_beam_agent-5_mean: 98.21
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-14-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 884.9999999999751
  episode_reward_mean: 591.9599999999996
  episode_reward_min: 235.9999999999965
  episodes_this_iter: 96
  episodes_total: 12096
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12885.409
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 1.2354799509048462
        entropy_coeff: 0.0017600000137463212
        kl: 0.011462775990366936
        model: {}
        policy_loss: -0.0315716527402401
        total_loss: -0.030033117160201073
        vf_explained_var: 0.08240845799446106
        vf_loss: 14.20425033569336
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 1.1780424118041992
        entropy_coeff: 0.0017600000137463212
        kl: 0.015693755820393562
        model: {}
        policy_loss: -0.029522638767957687
        total_loss: -0.026997044682502747
        vf_explained_var: 0.05638338625431061
        vf_loss: 14.601968765258789
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 1.1453263759613037
        entropy_coeff: 0.0017600000137463212
        kl: 0.012711599469184875
        model: {}
        policy_loss: -0.03390280902385712
        total_loss: -0.0319967120885849
        vf_explained_var: 0.10836216807365417
        vf_loss: 13.795491218566895
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 0.8469277024269104
        entropy_coeff: 0.0017600000137463212
        kl: 0.011057024821639061
        model: {}
        policy_loss: -0.025738723576068878
        total_loss: -0.023829294368624687
        vf_explained_var: 0.23177304863929749
        vf_loss: 11.886157035827637
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 1.0343685150146484
        entropy_coeff: 0.0017600000137463212
        kl: 0.013815086334943771
        model: {}
        policy_loss: -0.03764437511563301
        total_loss: -0.03527392819523811
        vf_explained_var: 0.07784873247146606
        vf_loss: 14.279147148132324
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 0.9747249484062195
        entropy_coeff: 0.0017600000137463212
        kl: 0.013151498511433601
        model: {}
        policy_loss: -0.03141035884618759
        total_loss: -0.029246404767036438
        vf_explained_var: 0.1930067241191864
        vf_loss: 12.491684913635254
    load_time_ms: 28736.631
    num_steps_sampled: 12096000
    num_steps_trained: 12096000
    sample_time_ms: 130505.618
    update_time_ms: 51.991
  iterations_since_restore: 46
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.25793650793651
    ram_util_percent: 15.099206349206348
  pid: 14340
  policy_reward_max:
    agent-0: 147.50000000000006
    agent-1: 147.50000000000006
    agent-2: 147.50000000000006
    agent-3: 147.50000000000006
    agent-4: 147.50000000000006
    agent-5: 147.50000000000006
  policy_reward_mean:
    agent-0: 98.66000000000025
    agent-1: 98.66000000000025
    agent-2: 98.66000000000025
    agent-3: 98.66000000000025
    agent-4: 98.66000000000025
    agent-5: 98.66000000000025
  policy_reward_min:
    agent-0: 39.333333333333286
    agent-1: 39.333333333333286
    agent-2: 39.333333333333286
    agent-3: 39.333333333333286
    agent-4: 39.333333333333286
    agent-5: 39.333333333333286
  sampler_perf:
    mean_env_wait_ms: 30.850234399256937
    mean_inference_ms: 14.476096559581372
    mean_processing_ms: 65.54332028314192
  time_since_restore: 8171.607713699341
  time_this_iter_s: 176.38554549217224
  time_total_s: 20722.42458510399
  timestamp: 1637043256
  timesteps_since_restore: 4416000
  timesteps_this_iter: 96000
  timesteps_total: 12096000
  training_iteration: 126
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    126 |          20722.4 | 12096000 |   591.96 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 5.74
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 21.14
    apples_agent-1_min: 0
    apples_agent-2_max: 206
    apples_agent-2_mean: 10.25
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 89.08
    apples_agent-3_min: 11
    apples_agent-4_max: 58
    apples_agent-4_mean: 3.34
    apples_agent-4_min: 0
    apples_agent-5_max: 158
    apples_agent-5_mean: 77.44
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 386
    cleaning_beam_agent-0_mean: 278.24
    cleaning_beam_agent-0_min: 75
    cleaning_beam_agent-1_max: 434
    cleaning_beam_agent-1_mean: 249.32
    cleaning_beam_agent-1_min: 89
    cleaning_beam_agent-2_max: 532
    cleaning_beam_agent-2_mean: 302.49
    cleaning_beam_agent-2_min: 147
    cleaning_beam_agent-3_max: 220
    cleaning_beam_agent-3_mean: 84.66
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 452
    cleaning_beam_agent-4_mean: 314.99
    cleaning_beam_agent-4_min: 180
    cleaning_beam_agent-5_max: 523
    cleaning_beam_agent-5_mean: 122.76
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-17-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 847.9999999999978
  episode_reward_mean: 577.4299999999993
  episode_reward_min: 117.00000000000108
  episodes_this_iter: 96
  episodes_total: 12192
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12892.0
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 1.220355749130249
        entropy_coeff: 0.0017600000137463212
        kl: 0.011926795355975628
        model: {}
        policy_loss: -0.03226582333445549
        total_loss: -0.030568784102797508
        vf_explained_var: 0.09323589503765106
        vf_loss: 14.595029830932617
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 1.1468818187713623
        entropy_coeff: 0.0017600000137463212
        kl: 0.01386121567338705
        model: {}
        policy_loss: -0.03328157588839531
        total_loss: -0.03099689818918705
        vf_explained_var: 0.04898543655872345
        vf_loss: 15.309465408325195
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 1.1676034927368164
        entropy_coeff: 0.0017600000137463212
        kl: 0.01328559871762991
        model: {}
        policy_loss: -0.03489035367965698
        total_loss: -0.032835498452186584
        vf_explained_var: 0.09723278880119324
        vf_loss: 14.527172088623047
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 0.8268217444419861
        entropy_coeff: 0.0017600000137463212
        kl: 0.0103113679215312
        model: {}
        policy_loss: -0.026227319613099098
        total_loss: -0.02435031160712242
        vf_explained_var: 0.21011480689048767
        vf_loss: 12.699380874633789
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 1.0342520475387573
        entropy_coeff: 0.0017600000137463212
        kl: 0.014260556548833847
        model: {}
        policy_loss: -0.03712984174489975
        total_loss: -0.03466898202896118
        vf_explained_var: 0.1118391752243042
        vf_loss: 14.29031753540039
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 0.9853739738464355
        entropy_coeff: 0.0017600000137463212
        kl: 0.01303237397223711
        model: {}
        policy_loss: -0.03331778571009636
        total_loss: -0.031115107238292694
        vf_explained_var: 0.1726885288953781
        vf_loss: 13.304619789123535
    load_time_ms: 30287.291
    num_steps_sampled: 12192000
    num_steps_trained: 12192000
    sample_time_ms: 130912.253
    update_time_ms: 49.406
  iterations_since_restore: 47
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.414624505928852
    ram_util_percent: 15.0498023715415
  pid: 14340
  policy_reward_max:
    agent-0: 141.3333333333334
    agent-1: 141.3333333333334
    agent-2: 141.3333333333334
    agent-3: 141.3333333333334
    agent-4: 141.3333333333334
    agent-5: 141.3333333333334
  policy_reward_mean:
    agent-0: 96.2383333333336
    agent-1: 96.2383333333336
    agent-2: 96.2383333333336
    agent-3: 96.2383333333336
    agent-4: 96.2383333333336
    agent-5: 96.2383333333336
  policy_reward_min:
    agent-0: 19.500000000000004
    agent-1: 19.500000000000004
    agent-2: 19.500000000000004
    agent-3: 19.500000000000004
    agent-4: 19.500000000000004
    agent-5: 19.500000000000004
  sampler_perf:
    mean_env_wait_ms: 30.843242638107064
    mean_inference_ms: 14.4796623999833
    mean_processing_ms: 65.58772645491295
  time_since_restore: 8349.26326584816
  time_this_iter_s: 177.65555214881897
  time_total_s: 20900.080137252808
  timestamp: 1637043434
  timesteps_since_restore: 4512000
  timesteps_this_iter: 96000
  timesteps_total: 12192000
  training_iteration: 127
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    127 |          20900.1 | 12192000 |   577.43 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 5.54
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 19.92
    apples_agent-1_min: 0
    apples_agent-2_max: 426
    apples_agent-2_mean: 18.45
    apples_agent-2_min: 0
    apples_agent-3_max: 185
    apples_agent-3_mean: 87.82
    apples_agent-3_min: 0
    apples_agent-4_max: 59
    apples_agent-4_mean: 1.74
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 84.46
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 392
    cleaning_beam_agent-0_mean: 278.22
    cleaning_beam_agent-0_min: 124
    cleaning_beam_agent-1_max: 491
    cleaning_beam_agent-1_mean: 261.1
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 490
    cleaning_beam_agent-2_mean: 287.9
    cleaning_beam_agent-2_min: 61
    cleaning_beam_agent-3_max: 295
    cleaning_beam_agent-3_mean: 82.82
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 428
    cleaning_beam_agent-4_mean: 316.27
    cleaning_beam_agent-4_min: 157
    cleaning_beam_agent-5_max: 377
    cleaning_beam_agent-5_mean: 97.9
    cleaning_beam_agent-5_min: 24
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-19-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 868.9999999999759
  episode_reward_mean: 596.0099999999984
  episode_reward_min: 220.9999999999963
  episodes_this_iter: 96
  episodes_total: 12288
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12971.159
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 1.2098623514175415
        entropy_coeff: 0.0017600000137463212
        kl: 0.011427498422563076
        model: {}
        policy_loss: -0.031729940325021744
        total_loss: -0.030183659866452217
        vf_explained_var: 0.1325414478778839
        vf_loss: 13.901365280151367
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 1.1491456031799316
        entropy_coeff: 0.0017600000137463212
        kl: 0.01276613213121891
        model: {}
        policy_loss: -0.033300090581178665
        total_loss: -0.031208690255880356
        vf_explained_var: 0.026487916707992554
        vf_loss: 15.606704711914062
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 1.1592884063720703
        entropy_coeff: 0.0017600000137463212
        kl: 0.012972541153430939
        model: {}
        policy_loss: -0.034192007035017014
        total_loss: -0.03214860334992409
        vf_explained_var: 0.07108414173126221
        vf_loss: 14.892454147338867
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 0.8083599805831909
        entropy_coeff: 0.0017600000137463212
        kl: 0.009394284337759018
        model: {}
        policy_loss: -0.025578413158655167
        total_loss: -0.023820634931325912
        vf_explained_var: 0.18762525916099548
        vf_loss: 13.016356468200684
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 1.042978048324585
        entropy_coeff: 0.0017600000137463212
        kl: 0.015318067744374275
        model: {}
        policy_loss: -0.03590347245335579
        total_loss: -0.03321908414363861
        vf_explained_var: 0.09111446142196655
        vf_loss: 14.564115524291992
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 0.9753284454345703
        entropy_coeff: 0.0017600000137463212
        kl: 0.0127636743709445
        model: {}
        policy_loss: -0.03335200622677803
        total_loss: -0.031220868229866028
        vf_explained_var: 0.19239364564418793
        vf_loss: 12.949836730957031
    load_time_ms: 28870.421
    num_steps_sampled: 12288000
    num_steps_trained: 12288000
    sample_time_ms: 131540.251
    update_time_ms: 51.137
  iterations_since_restore: 48
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.893534482758618
    ram_util_percent: 14.887931034482758
  pid: 14340
  policy_reward_max:
    agent-0: 144.83333333333366
    agent-1: 144.83333333333366
    agent-2: 144.83333333333366
    agent-3: 144.83333333333366
    agent-4: 144.83333333333366
    agent-5: 144.83333333333366
  policy_reward_mean:
    agent-0: 99.3350000000003
    agent-1: 99.3350000000003
    agent-2: 99.3350000000003
    agent-3: 99.3350000000003
    agent-4: 99.3350000000003
    agent-5: 99.3350000000003
  policy_reward_min:
    agent-0: 36.833333333333336
    agent-1: 36.833333333333336
    agent-2: 36.833333333333336
    agent-3: 36.833333333333336
    agent-4: 36.833333333333336
    agent-5: 36.833333333333336
  sampler_perf:
    mean_env_wait_ms: 30.829020207322273
    mean_inference_ms: 14.482106245747724
    mean_processing_ms: 65.58768002941277
  time_since_restore: 8512.038457155228
  time_this_iter_s: 162.77519130706787
  time_total_s: 21062.855328559875
  timestamp: 1637043597
  timesteps_since_restore: 4608000
  timesteps_this_iter: 96000
  timesteps_total: 12288000
  training_iteration: 128
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    128 |          21062.9 | 12288000 |   596.01 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 4.36
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 19.69
    apples_agent-1_min: 0
    apples_agent-2_max: 154
    apples_agent-2_mean: 9.9
    apples_agent-2_min: 0
    apples_agent-3_max: 161
    apples_agent-3_mean: 86.56
    apples_agent-3_min: 33
    apples_agent-4_max: 58
    apples_agent-4_mean: 3.3
    apples_agent-4_min: 0
    apples_agent-5_max: 175
    apples_agent-5_mean: 84.29
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 426
    cleaning_beam_agent-0_mean: 292.5
    cleaning_beam_agent-0_min: 65
    cleaning_beam_agent-1_max: 495
    cleaning_beam_agent-1_mean: 260.5
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 639
    cleaning_beam_agent-2_mean: 302.35
    cleaning_beam_agent-2_min: 93
    cleaning_beam_agent-3_max: 183
    cleaning_beam_agent-3_mean: 80.18
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 493
    cleaning_beam_agent-4_mean: 313.3
    cleaning_beam_agent-4_min: 166
    cleaning_beam_agent-5_max: 385
    cleaning_beam_agent-5_mean: 100.12
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-22-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 858.999999999971
  episode_reward_mean: 606.2099999999982
  episode_reward_min: 276.9999999999994
  episodes_this_iter: 96
  episodes_total: 12384
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13081.952
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 1.2056524753570557
        entropy_coeff: 0.0017600000137463212
        kl: 0.011124972254037857
        model: {}
        policy_loss: -0.03178571164608002
        total_loss: -0.03018595650792122
        vf_explained_var: 0.049106284976005554
        vf_loss: 14.967061996459961
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 1.138410210609436
        entropy_coeff: 0.0017600000137463212
        kl: 0.013156870380043983
        model: {}
        policy_loss: -0.03397251293063164
        total_loss: -0.03179609403014183
        vf_explained_var: 0.0160750150680542
        vf_loss: 15.486446380615234
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 1.1638755798339844
        entropy_coeff: 0.0017600000137463212
        kl: 0.01381352636963129
        model: {}
        policy_loss: -0.03376368433237076
        total_loss: -0.031621165573596954
        vf_explained_var: 0.09242631494998932
        vf_loss: 14.282353401184082
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 0.8120149374008179
        entropy_coeff: 0.0017600000137463212
        kl: 0.010186917148530483
        model: {}
        policy_loss: -0.02633550949394703
        total_loss: -0.024473486468195915
        vf_explained_var: 0.20310954749584198
        vf_loss: 12.537872314453125
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 1.0511021614074707
        entropy_coeff: 0.0017600000137463212
        kl: 0.01438022032380104
        model: {}
        policy_loss: -0.03731047362089157
        total_loss: -0.034862201660871506
        vf_explained_var: 0.09658434987068176
        vf_loss: 14.22168254852295
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 0.9662798643112183
        entropy_coeff: 0.0017600000137463212
        kl: 0.012226402759552002
        model: {}
        policy_loss: -0.03218517079949379
        total_loss: -0.030133435502648354
        vf_explained_var: 0.16952307522296906
        vf_loss: 13.071057319641113
    load_time_ms: 27120.612
    num_steps_sampled: 12384000
    num_steps_trained: 12384000
    sample_time_ms: 130931.682
    update_time_ms: 60.004
  iterations_since_restore: 49
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.493750000000002
    ram_util_percent: 14.81026785714286
  pid: 14340
  policy_reward_max:
    agent-0: 143.1666666666667
    agent-1: 143.1666666666667
    agent-2: 143.1666666666667
    agent-3: 143.1666666666667
    agent-4: 143.1666666666667
    agent-5: 143.1666666666667
  policy_reward_mean:
    agent-0: 101.0350000000003
    agent-1: 101.0350000000003
    agent-2: 101.0350000000003
    agent-3: 101.0350000000003
    agent-4: 101.0350000000003
    agent-5: 101.0350000000003
  policy_reward_min:
    agent-0: 46.166666666666615
    agent-1: 46.166666666666615
    agent-2: 46.166666666666615
    agent-3: 46.166666666666615
    agent-4: 46.166666666666615
    agent-5: 46.166666666666615
  sampler_perf:
    mean_env_wait_ms: 30.82319840820425
    mean_inference_ms: 14.483076040611895
    mean_processing_ms: 65.58633604528377
  time_since_restore: 8668.912608861923
  time_this_iter_s: 156.87415170669556
  time_total_s: 21219.72948026657
  timestamp: 1637043754
  timesteps_since_restore: 4704000
  timesteps_this_iter: 96000
  timesteps_total: 12384000
  training_iteration: 129
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    129 |          21219.7 | 12384000 |   606.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 68
    apples_agent-0_mean: 6.31
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 19.06
    apples_agent-1_min: 0
    apples_agent-2_max: 196
    apples_agent-2_mean: 9.72
    apples_agent-2_min: 0
    apples_agent-3_max: 163
    apples_agent-3_mean: 81.26
    apples_agent-3_min: 0
    apples_agent-4_max: 82
    apples_agent-4_mean: 4.47
    apples_agent-4_min: 0
    apples_agent-5_max: 167
    apples_agent-5_mean: 81.23
    apples_agent-5_min: 14
    cleaning_beam_agent-0_max: 387
    cleaning_beam_agent-0_mean: 274.78
    cleaning_beam_agent-0_min: 84
    cleaning_beam_agent-1_max: 441
    cleaning_beam_agent-1_mean: 253.27
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 512
    cleaning_beam_agent-2_mean: 301.79
    cleaning_beam_agent-2_min: 109
    cleaning_beam_agent-3_max: 172
    cleaning_beam_agent-3_mean: 83.26
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 466
    cleaning_beam_agent-4_mean: 305.43
    cleaning_beam_agent-4_min: 120
    cleaning_beam_agent-5_max: 412
    cleaning_beam_agent-5_mean: 100.7
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-25-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 913.9999999999828
  episode_reward_mean: 591.2399999999991
  episode_reward_min: 189.99999999999815
  episodes_this_iter: 96
  episodes_total: 12480
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13153.428
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 1.2017557621002197
        entropy_coeff: 0.0017600000137463212
        kl: 0.011833655647933483
        model: {}
        policy_loss: -0.032245200127363205
        total_loss: -0.03054303303360939
        vf_explained_var: 0.1290494203567505
        vf_loss: 14.505255699157715
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 1.1508536338806152
        entropy_coeff: 0.0017600000137463212
        kl: 0.013242965564131737
        model: {}
        policy_loss: -0.033719077706336975
        total_loss: -0.031485989689826965
        vf_explained_var: 0.03385092318058014
        vf_loss: 16.099950790405273
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 1.1680583953857422
        entropy_coeff: 0.0017600000137463212
        kl: 0.01292913407087326
        model: {}
        policy_loss: -0.033811867237091064
        total_loss: -0.03172560781240463
        vf_explained_var: 0.06617322564125061
        vf_loss: 15.562166213989258
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 0.8270362615585327
        entropy_coeff: 0.0017600000137463212
        kl: 0.010772498324513435
        model: {}
        policy_loss: -0.026699472218751907
        total_loss: -0.0247107595205307
        vf_explained_var: 0.2251310646533966
        vf_loss: 12.89794921875
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 1.0559269189834595
        entropy_coeff: 0.0017600000137463212
        kl: 0.013221174478530884
        model: {}
        policy_loss: -0.036881156265735626
        total_loss: -0.03458110988140106
        vf_explained_var: 0.09052731096744537
        vf_loss: 15.142477989196777
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 0.9801017642021179
        entropy_coeff: 0.0017600000137463212
        kl: 0.012659847736358643
        model: {}
        policy_loss: -0.03249674290418625
        total_loss: -0.030380649492144585
        vf_explained_var: 0.2139468640089035
        vf_loss: 13.091005325317383
    load_time_ms: 25622.11
    num_steps_sampled: 12480000
    num_steps_trained: 12480000
    sample_time_ms: 130142.34
    update_time_ms: 68.062
  iterations_since_restore: 50
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.502232142857142
    ram_util_percent: 14.868303571428571
  pid: 14340
  policy_reward_max:
    agent-0: 152.3333333333333
    agent-1: 152.3333333333333
    agent-2: 152.3333333333333
    agent-3: 152.3333333333333
    agent-4: 152.3333333333333
    agent-5: 152.3333333333333
  policy_reward_mean:
    agent-0: 98.54000000000025
    agent-1: 98.54000000000025
    agent-2: 98.54000000000025
    agent-3: 98.54000000000025
    agent-4: 98.54000000000025
    agent-5: 98.54000000000025
  policy_reward_min:
    agent-0: 31.666666666666735
    agent-1: 31.666666666666735
    agent-2: 31.666666666666735
    agent-3: 31.666666666666735
    agent-4: 31.666666666666735
    agent-5: 31.666666666666735
  sampler_perf:
    mean_env_wait_ms: 30.812771965033306
    mean_inference_ms: 14.483471463505097
    mean_processing_ms: 65.58829385052339
  time_since_restore: 8825.30963730812
  time_this_iter_s: 156.3970284461975
  time_total_s: 21376.12650871277
  timestamp: 1637043911
  timesteps_since_restore: 4800000
  timesteps_this_iter: 96000
  timesteps_total: 12480000
  training_iteration: 130
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    130 |          21376.1 | 12480000 |   591.24 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 81
    apples_agent-0_mean: 5.85
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 20.54
    apples_agent-1_min: 0
    apples_agent-2_max: 187
    apples_agent-2_mean: 8.34
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 84.04
    apples_agent-3_min: 37
    apples_agent-4_max: 77
    apples_agent-4_mean: 3.3
    apples_agent-4_min: 0
    apples_agent-5_max: 164
    apples_agent-5_mean: 82.22
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 360
    cleaning_beam_agent-0_mean: 281.62
    cleaning_beam_agent-0_min: 119
    cleaning_beam_agent-1_max: 402
    cleaning_beam_agent-1_mean: 253.31
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 489
    cleaning_beam_agent-2_mean: 302.81
    cleaning_beam_agent-2_min: 102
    cleaning_beam_agent-3_max: 163
    cleaning_beam_agent-3_mean: 74.59
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 444
    cleaning_beam_agent-4_mean: 304.47
    cleaning_beam_agent-4_min: 153
    cleaning_beam_agent-5_max: 360
    cleaning_beam_agent-5_mean: 99.61
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-27-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 826.9999999999768
  episode_reward_mean: 601.1799999999982
  episode_reward_min: 236.99999999999824
  episodes_this_iter: 96
  episodes_total: 12576
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13205.593
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 1.214630365371704
        entropy_coeff: 0.0017600000137463212
        kl: 0.012131880968809128
        model: {}
        policy_loss: -0.033420346677303314
        total_loss: -0.031690195202827454
        vf_explained_var: 0.09538456797599792
        vf_loss: 14.415252685546875
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 1.139617919921875
        entropy_coeff: 0.0017600000137463212
        kl: 0.012852279469370842
        model: {}
        policy_loss: -0.032731495797634125
        total_loss: -0.030641840770840645
        vf_explained_var: 0.04310426115989685
        vf_loss: 15.249307632446289
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 1.1605901718139648
        entropy_coeff: 0.0017600000137463212
        kl: 0.012807881459593773
        model: {}
        policy_loss: -0.03329624608159065
        total_loss: -0.031332992017269135
        vf_explained_var: 0.09368611872196198
        vf_loss: 14.443159103393555
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 0.7925151586532593
        entropy_coeff: 0.0017600000137463212
        kl: 0.010733364149928093
        model: {}
        policy_loss: -0.02492867410182953
        total_loss: -0.02289690636098385
        vf_explained_var: 0.19685101509094238
        vf_loss: 12.799223899841309
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 1.0570871829986572
        entropy_coeff: 0.0017600000137463212
        kl: 0.014205768704414368
        model: {}
        policy_loss: -0.03713902086019516
        total_loss: -0.03474164009094238
        vf_explained_var: 0.11056700348854065
        vf_loss: 14.166983604431152
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 0.9619712233543396
        entropy_coeff: 0.0017600000137463212
        kl: 0.011854659765958786
        model: {}
        policy_loss: -0.031264569610357285
        total_loss: -0.029282432049512863
        vf_explained_var: 0.18170617520809174
        vf_loss: 13.04274845123291
    load_time_ms: 23938.42
    num_steps_sampled: 12576000
    num_steps_trained: 12576000
    sample_time_ms: 129934.443
    update_time_ms: 74.01
  iterations_since_restore: 51
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.5237668161435
    ram_util_percent: 14.842152466367715
  pid: 14340
  policy_reward_max:
    agent-0: 137.8333333333334
    agent-1: 137.8333333333334
    agent-2: 137.8333333333334
    agent-3: 137.8333333333334
    agent-4: 137.8333333333334
    agent-5: 137.8333333333334
  policy_reward_mean:
    agent-0: 100.19666666666696
    agent-1: 100.19666666666696
    agent-2: 100.19666666666696
    agent-3: 100.19666666666696
    agent-4: 100.19666666666696
    agent-5: 100.19666666666696
  policy_reward_min:
    agent-0: 39.500000000000036
    agent-1: 39.500000000000036
    agent-2: 39.500000000000036
    agent-3: 39.500000000000036
    agent-4: 39.500000000000036
    agent-5: 39.500000000000036
  sampler_perf:
    mean_env_wait_ms: 30.802895405477617
    mean_inference_ms: 14.485033666042883
    mean_processing_ms: 65.5914039348248
  time_since_restore: 8982.032283067703
  time_this_iter_s: 156.72264575958252
  time_total_s: 21532.84915447235
  timestamp: 1637044069
  timesteps_since_restore: 4896000
  timesteps_this_iter: 96000
  timesteps_total: 12576000
  training_iteration: 131
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    131 |          21532.8 | 12576000 |   601.18 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 3.89
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 22.64
    apples_agent-1_min: 0
    apples_agent-2_max: 121
    apples_agent-2_mean: 9.07
    apples_agent-2_min: 0
    apples_agent-3_max: 138
    apples_agent-3_mean: 80.61
    apples_agent-3_min: 0
    apples_agent-4_max: 57
    apples_agent-4_mean: 3.16
    apples_agent-4_min: 0
    apples_agent-5_max: 144
    apples_agent-5_mean: 82.56
    apples_agent-5_min: 3
    cleaning_beam_agent-0_max: 394
    cleaning_beam_agent-0_mean: 296.24
    cleaning_beam_agent-0_min: 131
    cleaning_beam_agent-1_max: 463
    cleaning_beam_agent-1_mean: 242.91
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 470
    cleaning_beam_agent-2_mean: 286.51
    cleaning_beam_agent-2_min: 100
    cleaning_beam_agent-3_max: 203
    cleaning_beam_agent-3_mean: 73.8
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 462
    cleaning_beam_agent-4_mean: 302.8
    cleaning_beam_agent-4_min: 144
    cleaning_beam_agent-5_max: 343
    cleaning_beam_agent-5_mean: 90.28
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-30-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 826.9999999999768
  episode_reward_mean: 587.2299999999999
  episode_reward_min: 241.99999999999602
  episodes_this_iter: 96
  episodes_total: 12672
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13230.924
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 1.184108018875122
        entropy_coeff: 0.0017600000137463212
        kl: 0.011508836410939693
        model: {}
        policy_loss: -0.0317043662071228
        total_loss: -0.03006383776664734
        vf_explained_var: 0.06672075390815735
        vf_loss: 14.227865219116211
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 1.1434969902038574
        entropy_coeff: 0.0017600000137463212
        kl: 0.012977814301848412
        model: {}
        policy_loss: -0.03344862163066864
        total_loss: -0.031391654163599014
        vf_explained_var: 0.032590776681900024
        vf_loss: 14.73958969116211
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 1.1805588006973267
        entropy_coeff: 0.0017600000137463212
        kl: 0.013075754977762699
        model: {}
        policy_loss: -0.03474870324134827
        total_loss: -0.032809652388095856
        vf_explained_var: 0.0799519270658493
        vf_loss: 14.016834259033203
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 0.8206152319908142
        entropy_coeff: 0.0017600000137463212
        kl: 0.010228448547422886
        model: {}
        policy_loss: -0.026590367779135704
        total_loss: -0.024717826396226883
        vf_explained_var: 0.16573362052440643
        vf_loss: 12.711336135864258
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 1.0517884492874146
        entropy_coeff: 0.0017600000137463212
        kl: 0.014668583869934082
        model: {}
        policy_loss: -0.035365231335163116
        total_loss: -0.032910965383052826
        vf_explained_var: 0.0998479574918747
        vf_loss: 13.716979026794434
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 0.9679533243179321
        entropy_coeff: 0.0017600000137463212
        kl: 0.012125791981816292
        model: {}
        policy_loss: -0.031870871782302856
        total_loss: -0.029908552765846252
        vf_explained_var: 0.18527168035507202
        vf_loss: 12.407617568969727
    load_time_ms: 23439.075
    num_steps_sampled: 12672000
    num_steps_trained: 12672000
    sample_time_ms: 129280.124
    update_time_ms: 80.891
  iterations_since_restore: 52
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.772072072072074
    ram_util_percent: 14.801351351351352
  pid: 14340
  policy_reward_max:
    agent-0: 137.8333333333334
    agent-1: 137.8333333333334
    agent-2: 137.8333333333334
    agent-3: 137.8333333333334
    agent-4: 137.8333333333334
    agent-5: 137.8333333333334
  policy_reward_mean:
    agent-0: 97.87166666666691
    agent-1: 97.87166666666691
    agent-2: 97.87166666666691
    agent-3: 97.87166666666691
    agent-4: 97.87166666666691
    agent-5: 97.87166666666691
  policy_reward_min:
    agent-0: 40.3333333333333
    agent-1: 40.3333333333333
    agent-2: 40.3333333333333
    agent-3: 40.3333333333333
    agent-4: 40.3333333333333
    agent-5: 40.3333333333333
  sampler_perf:
    mean_env_wait_ms: 30.79220888061986
    mean_inference_ms: 14.486519089912585
    mean_processing_ms: 65.59062254423668
  time_since_restore: 9137.262605905533
  time_this_iter_s: 155.2303228378296
  time_total_s: 21688.07947731018
  timestamp: 1637044225
  timesteps_since_restore: 4992000
  timesteps_this_iter: 96000
  timesteps_total: 12672000
  training_iteration: 132
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    132 |          21688.1 | 12672000 |   587.23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 6.05
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 22.96
    apples_agent-1_min: 0
    apples_agent-2_max: 184
    apples_agent-2_mean: 13.13
    apples_agent-2_min: 0
    apples_agent-3_max: 218
    apples_agent-3_mean: 87.5
    apples_agent-3_min: 18
    apples_agent-4_max: 48
    apples_agent-4_mean: 2.2
    apples_agent-4_min: 0
    apples_agent-5_max: 225
    apples_agent-5_mean: 82.63
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 432
    cleaning_beam_agent-0_mean: 299.05
    cleaning_beam_agent-0_min: 123
    cleaning_beam_agent-1_max: 406
    cleaning_beam_agent-1_mean: 235.99
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 500
    cleaning_beam_agent-2_mean: 299.8
    cleaning_beam_agent-2_min: 90
    cleaning_beam_agent-3_max: 157
    cleaning_beam_agent-3_mean: 67.22
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 453
    cleaning_beam_agent-4_mean: 310.01
    cleaning_beam_agent-4_min: 140
    cleaning_beam_agent-5_max: 508
    cleaning_beam_agent-5_mean: 94.45
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-33-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 804.9999999999824
  episode_reward_mean: 590.7899999999988
  episode_reward_min: 150.00000000000034
  episodes_this_iter: 96
  episodes_total: 12768
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13219.722
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 1.1601229906082153
        entropy_coeff: 0.0017600000137463212
        kl: 0.011904498562216759
        model: {}
        policy_loss: -0.03169915825128555
        total_loss: -0.02989933080971241
        vf_explained_var: 0.10607537627220154
        vf_loss: 14.607451438903809
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 1.146745204925537
        entropy_coeff: 0.0017600000137463212
        kl: 0.012779748067259789
        model: {}
        policy_loss: -0.034146420657634735
        total_loss: -0.032031308859586716
        vf_explained_var: 0.034515902400016785
        vf_loss: 15.77436351776123
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 1.1500887870788574
        entropy_coeff: 0.0017600000137463212
        kl: 0.012953082099556923
        model: {}
        policy_loss: -0.03391505032777786
        total_loss: -0.03188054636120796
        vf_explained_var: 0.10153236985206604
        vf_loss: 14.680464744567871
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 0.7911255359649658
        entropy_coeff: 0.0017600000137463212
        kl: 0.010975324548780918
        model: {}
        policy_loss: -0.025599002838134766
        total_loss: -0.023533452302217484
        vf_explained_var: 0.22711901366710663
        vf_loss: 12.628662109375
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 1.0576823949813843
        entropy_coeff: 0.0017600000137463212
        kl: 0.014841771684587002
        model: {}
        policy_loss: -0.03742961958050728
        total_loss: -0.0348765030503273
        vf_explained_var: 0.11551795899868011
        vf_loss: 14.462782859802246
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 0.9613354206085205
        entropy_coeff: 0.0017600000137463212
        kl: 0.01172060426324606
        model: {}
        policy_loss: -0.030961595475673676
        total_loss: -0.0289512537419796
        vf_explained_var: 0.1688636988401413
        vf_loss: 13.581710815429688
    load_time_ms: 23054.846
    num_steps_sampled: 12768000
    num_steps_trained: 12768000
    sample_time_ms: 129092.015
    update_time_ms: 83.117
  iterations_since_restore: 53
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.332780082987554
    ram_util_percent: 14.900000000000002
  pid: 14340
  policy_reward_max:
    agent-0: 134.1666666666668
    agent-1: 134.1666666666668
    agent-2: 134.1666666666668
    agent-3: 134.1666666666668
    agent-4: 134.1666666666668
    agent-5: 134.1666666666668
  policy_reward_mean:
    agent-0: 98.46500000000029
    agent-1: 98.46500000000029
    agent-2: 98.46500000000029
    agent-3: 98.46500000000029
    agent-4: 98.46500000000029
    agent-5: 98.46500000000029
  policy_reward_min:
    agent-0: 25.000000000000018
    agent-1: 25.000000000000018
    agent-2: 25.000000000000018
    agent-3: 25.000000000000018
    agent-4: 25.000000000000018
    agent-5: 25.000000000000018
  sampler_perf:
    mean_env_wait_ms: 30.781532828444544
    mean_inference_ms: 14.486730586983837
    mean_processing_ms: 65.58903277182014
  time_since_restore: 9301.732592344284
  time_this_iter_s: 164.46998643875122
  time_total_s: 21852.549463748932
  timestamp: 1637044394
  timesteps_since_restore: 5088000
  timesteps_this_iter: 96000
  timesteps_total: 12768000
  training_iteration: 133
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    133 |          21852.5 | 12768000 |   590.79 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 5.2
    apples_agent-0_min: 0
    apples_agent-1_max: 129
    apples_agent-1_mean: 23.53
    apples_agent-1_min: 0
    apples_agent-2_max: 128
    apples_agent-2_mean: 12.06
    apples_agent-2_min: 0
    apples_agent-3_max: 201
    apples_agent-3_mean: 85.54
    apples_agent-3_min: 23
    apples_agent-4_max: 46
    apples_agent-4_mean: 2.61
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 83.32
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 414
    cleaning_beam_agent-0_mean: 311.38
    cleaning_beam_agent-0_min: 139
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 223.97
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 516
    cleaning_beam_agent-2_mean: 294.45
    cleaning_beam_agent-2_min: 85
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 62.26
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 456
    cleaning_beam_agent-4_mean: 322.04
    cleaning_beam_agent-4_min: 145
    cleaning_beam_agent-5_max: 669
    cleaning_beam_agent-5_mean: 105.2
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-35-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 890.999999999994
  episode_reward_mean: 607.5599999999978
  episode_reward_min: 191.99999999999807
  episodes_this_iter: 96
  episodes_total: 12864
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13288.345
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 1.1558029651641846
        entropy_coeff: 0.0017600000137463212
        kl: 0.011510947719216347
        model: {}
        policy_loss: -0.03281405568122864
        total_loss: -0.03099679946899414
        vf_explained_var: 0.11016811430454254
        vf_loss: 15.492783546447754
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 1.1386260986328125
        entropy_coeff: 0.0017600000137463212
        kl: 0.013260314241051674
        model: {}
        policy_loss: -0.03393549099564552
        total_loss: -0.031615495681762695
        vf_explained_var: 0.04003863036632538
        vf_loss: 16.71915626525879
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 1.151322841644287
        entropy_coeff: 0.0017600000137463212
        kl: 0.01332014612853527
        model: {}
        policy_loss: -0.033783141523599625
        total_loss: -0.031595759093761444
        vf_explained_var: 0.10972759127616882
        vf_loss: 15.496809005737305
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 0.7781788110733032
        entropy_coeff: 0.0017600000137463212
        kl: 0.011039668694138527
        model: {}
        policy_loss: -0.025312373414635658
        total_loss: -0.02314571850001812
        vf_explained_var: 0.23686781525611877
        vf_loss: 13.283191680908203
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 1.0514578819274902
        entropy_coeff: 0.0017600000137463212
        kl: 0.014158515259623528
        model: {}
        policy_loss: -0.03709826618432999
        total_loss: -0.03462580591440201
        vf_explained_var: 0.14370432496070862
        vf_loss: 14.913257598876953
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 0.9632771015167236
        entropy_coeff: 0.0017600000137463212
        kl: 0.011800428852438927
        model: {}
        policy_loss: -0.03155708312988281
        total_loss: -0.02952696941792965
        vf_explained_var: 0.21537944674491882
        vf_loss: 13.653987884521484
    load_time_ms: 21358.934
    num_steps_sampled: 12864000
    num_steps_trained: 12864000
    sample_time_ms: 129214.817
    update_time_ms: 83.66
  iterations_since_restore: 54
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.31145374449339
    ram_util_percent: 14.785903083700443
  pid: 14340
  policy_reward_max:
    agent-0: 148.49999999999997
    agent-1: 148.49999999999997
    agent-2: 148.49999999999997
    agent-3: 148.49999999999997
    agent-4: 148.49999999999997
    agent-5: 148.49999999999997
  policy_reward_mean:
    agent-0: 101.26000000000028
    agent-1: 101.26000000000028
    agent-2: 101.26000000000028
    agent-3: 101.26000000000028
    agent-4: 101.26000000000028
    agent-5: 101.26000000000028
  policy_reward_min:
    agent-0: 32.00000000000004
    agent-1: 32.00000000000004
    agent-2: 32.00000000000004
    agent-3: 32.00000000000004
    agent-4: 32.00000000000004
    agent-5: 32.00000000000004
  sampler_perf:
    mean_env_wait_ms: 30.77419107754266
    mean_inference_ms: 14.487006730743879
    mean_processing_ms: 65.59404003684654
  time_since_restore: 9461.41754603386
  time_this_iter_s: 159.6849536895752
  time_total_s: 22012.234417438507
  timestamp: 1637044554
  timesteps_since_restore: 5184000
  timesteps_this_iter: 96000
  timesteps_total: 12864000
  training_iteration: 134
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    134 |          22012.2 | 12864000 |   607.56 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 4.5
    apples_agent-0_min: 0
    apples_agent-1_max: 158
    apples_agent-1_mean: 24.11
    apples_agent-1_min: 0
    apples_agent-2_max: 301
    apples_agent-2_mean: 16.07
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 87.69
    apples_agent-3_min: 35
    apples_agent-4_max: 61
    apples_agent-4_mean: 3.06
    apples_agent-4_min: 0
    apples_agent-5_max: 144
    apples_agent-5_mean: 80.67
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 391
    cleaning_beam_agent-0_mean: 319.69
    cleaning_beam_agent-0_min: 162
    cleaning_beam_agent-1_max: 416
    cleaning_beam_agent-1_mean: 238.4
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 492
    cleaning_beam_agent-2_mean: 304.61
    cleaning_beam_agent-2_min: 88
    cleaning_beam_agent-3_max: 135
    cleaning_beam_agent-3_mean: 64.13
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 414
    cleaning_beam_agent-4_mean: 316.21
    cleaning_beam_agent-4_min: 109
    cleaning_beam_agent-5_max: 548
    cleaning_beam_agent-5_mean: 102.46
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-38-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 891.9999999999781
  episode_reward_mean: 609.7399999999978
  episode_reward_min: 174.999999999998
  episodes_this_iter: 96
  episodes_total: 12960
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13320.809
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 1.154017686843872
        entropy_coeff: 0.0017600000137463212
        kl: 0.01215231604874134
        model: {}
        policy_loss: -0.03216635063290596
        total_loss: -0.030262097716331482
        vf_explained_var: 0.1390065848827362
        vf_loss: 15.048623085021973
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 1.1112887859344482
        entropy_coeff: 0.0017600000137463212
        kl: 0.013705451041460037
        model: {}
        policy_loss: -0.03449104726314545
        total_loss: -0.03206499293446541
        vf_explained_var: 0.06146152317523956
        vf_loss: 16.408321380615234
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 1.1407091617584229
        entropy_coeff: 0.0017600000137463212
        kl: 0.013299967162311077
        model: {}
        policy_loss: -0.03319127857685089
        total_loss: -0.030979663133621216
        vf_explained_var: 0.10805067420005798
        vf_loss: 15.592708587646484
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 0.7739424109458923
        entropy_coeff: 0.0017600000137463212
        kl: 0.009850757196545601
        model: {}
        policy_loss: -0.025835487991571426
        total_loss: -0.023879501968622208
        vf_explained_var: 0.2283060997724533
        vf_loss: 13.479718208312988
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 1.0417364835739136
        entropy_coeff: 0.0017600000137463212
        kl: 0.013385606929659843
        model: {}
        policy_loss: -0.03592388331890106
        total_loss: -0.03355826064944267
        vf_explained_var: 0.1295890063047409
        vf_loss: 15.219517707824707
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 0.9467533826828003
        entropy_coeff: 0.0017600000137463212
        kl: 0.011994739063084126
        model: {}
        policy_loss: -0.031660426408052444
        total_loss: -0.029522297903895378
        vf_explained_var: 0.19662168622016907
        vf_loss: 14.054667472839355
    load_time_ms: 19649.58
    num_steps_sampled: 12960000
    num_steps_trained: 12960000
    sample_time_ms: 129088.626
    update_time_ms: 81.401
  iterations_since_restore: 55
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.290582959641256
    ram_util_percent: 14.786098654708521
  pid: 14340
  policy_reward_max:
    agent-0: 148.66666666666666
    agent-1: 148.66666666666666
    agent-2: 148.66666666666666
    agent-3: 148.66666666666666
    agent-4: 148.66666666666666
    agent-5: 148.66666666666666
  policy_reward_mean:
    agent-0: 101.62333333333362
    agent-1: 101.62333333333362
    agent-2: 101.62333333333362
    agent-3: 101.62333333333362
    agent-4: 101.62333333333362
    agent-5: 101.62333333333362
  policy_reward_min:
    agent-0: 29.166666666666742
    agent-1: 29.166666666666742
    agent-2: 29.166666666666742
    agent-3: 29.166666666666742
    agent-4: 29.166666666666742
    agent-5: 29.166666666666742
  sampler_perf:
    mean_env_wait_ms: 30.766910084352943
    mean_inference_ms: 14.48754924849341
    mean_processing_ms: 65.59973279545305
  time_since_restore: 9617.695897340775
  time_this_iter_s: 156.27835130691528
  time_total_s: 22168.512768745422
  timestamp: 1637044711
  timesteps_since_restore: 5280000
  timesteps_this_iter: 96000
  timesteps_total: 12960000
  training_iteration: 135
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    135 |          22168.5 | 12960000 |   609.74 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 80
    apples_agent-0_mean: 6.77
    apples_agent-0_min: 0
    apples_agent-1_max: 176
    apples_agent-1_mean: 22.24
    apples_agent-1_min: 0
    apples_agent-2_max: 276
    apples_agent-2_mean: 15.14
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 81.75
    apples_agent-3_min: 0
    apples_agent-4_max: 65
    apples_agent-4_mean: 3.29
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 81.33
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 416
    cleaning_beam_agent-0_mean: 309.14
    cleaning_beam_agent-0_min: 147
    cleaning_beam_agent-1_max: 462
    cleaning_beam_agent-1_mean: 240.82
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 440
    cleaning_beam_agent-2_mean: 301.99
    cleaning_beam_agent-2_min: 51
    cleaning_beam_agent-3_max: 185
    cleaning_beam_agent-3_mean: 63.39
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 434
    cleaning_beam_agent-4_mean: 329.57
    cleaning_beam_agent-4_min: 130
    cleaning_beam_agent-5_max: 434
    cleaning_beam_agent-5_mean: 107.92
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-41-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 825.9999999999898
  episode_reward_mean: 600.3799999999987
  episode_reward_min: 242.99999999999724
  episodes_this_iter: 96
  episodes_total: 13056
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13372.969
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 1.1377110481262207
        entropy_coeff: 0.0017600000137463212
        kl: 0.011731638573110104
        model: {}
        policy_loss: -0.030664877966046333
        total_loss: -0.028840677812695503
        vf_explained_var: 0.08856761455535889
        vf_loss: 14.802427291870117
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 1.1248414516448975
        entropy_coeff: 0.0017600000137463212
        kl: 0.014044329524040222
        model: {}
        policy_loss: -0.03577113524079323
        total_loss: -0.03340914845466614
        vf_explained_var: 0.05586083233356476
        vf_loss: 15.328386306762695
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 1.147868275642395
        entropy_coeff: 0.0017600000137463212
        kl: 0.013113366439938545
        model: {}
        policy_loss: -0.034052055329084396
        total_loss: -0.03197772055864334
        vf_explained_var: 0.09399707615375519
        vf_loss: 14.719114303588867
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 0.7786492109298706
        entropy_coeff: 0.0017600000137463212
        kl: 0.009935006499290466
        model: {}
        policy_loss: -0.025054916739463806
        total_loss: -0.02317756414413452
        vf_explained_var: 0.22366070747375488
        vf_loss: 12.607728004455566
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 1.0420030355453491
        entropy_coeff: 0.0017600000137463212
        kl: 0.013153835199773312
        model: {}
        policy_loss: -0.03549356758594513
        total_loss: -0.033284444361925125
        vf_explained_var: 0.13132986426353455
        vf_loss: 14.122819900512695
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 0.935634195804596
        entropy_coeff: 0.0017600000137463212
        kl: 0.011798780411481857
        model: {}
        policy_loss: -0.03130406141281128
        total_loss: -0.029221106320619583
        vf_explained_var: 0.15751533210277557
        vf_loss: 13.699126243591309
    load_time_ms: 17901.605
    num_steps_sampled: 13056000
    num_steps_trained: 13056000
    sample_time_ms: 128852.454
    update_time_ms: 102.588
  iterations_since_restore: 56
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.274222222222225
    ram_util_percent: 14.797333333333336
  pid: 14340
  policy_reward_max:
    agent-0: 137.66666666666697
    agent-1: 137.66666666666697
    agent-2: 137.66666666666697
    agent-3: 137.66666666666697
    agent-4: 137.66666666666697
    agent-5: 137.66666666666697
  policy_reward_mean:
    agent-0: 100.06333333333362
    agent-1: 100.06333333333362
    agent-2: 100.06333333333362
    agent-3: 100.06333333333362
    agent-4: 100.06333333333362
    agent-5: 100.06333333333362
  policy_reward_min:
    agent-0: 40.5
    agent-1: 40.5
    agent-2: 40.5
    agent-3: 40.5
    agent-4: 40.5
    agent-5: 40.5
  sampler_perf:
    mean_env_wait_ms: 30.76173710139236
    mean_inference_ms: 14.48938358324745
    mean_processing_ms: 65.60578767304166
  time_since_restore: 9775.00171470642
  time_this_iter_s: 157.30581736564636
  time_total_s: 22325.81858611107
  timestamp: 1637044868
  timesteps_since_restore: 5376000
  timesteps_this_iter: 96000
  timesteps_total: 13056000
  training_iteration: 136
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    136 |          22325.8 | 13056000 |   600.38 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 5.57
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 19.91
    apples_agent-1_min: 0
    apples_agent-2_max: 157
    apples_agent-2_mean: 17.17
    apples_agent-2_min: 0
    apples_agent-3_max: 164
    apples_agent-3_mean: 82.09
    apples_agent-3_min: 0
    apples_agent-4_max: 67
    apples_agent-4_mean: 3.47
    apples_agent-4_min: 0
    apples_agent-5_max: 181
    apples_agent-5_mean: 85.31
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 399
    cleaning_beam_agent-0_mean: 301.3
    cleaning_beam_agent-0_min: 176
    cleaning_beam_agent-1_max: 414
    cleaning_beam_agent-1_mean: 245.45
    cleaning_beam_agent-1_min: 100
    cleaning_beam_agent-2_max: 435
    cleaning_beam_agent-2_mean: 288.15
    cleaning_beam_agent-2_min: 118
    cleaning_beam_agent-3_max: 170
    cleaning_beam_agent-3_mean: 62.18
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 534
    cleaning_beam_agent-4_mean: 322.48
    cleaning_beam_agent-4_min: 124
    cleaning_beam_agent-5_max: 288
    cleaning_beam_agent-5_mean: 105.35
    cleaning_beam_agent-5_min: 31
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-43-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 870.9999999999914
  episode_reward_mean: 593.5799999999984
  episode_reward_min: 215.9999999999968
  episodes_this_iter: 96
  episodes_total: 13152
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13401.599
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 1.1479555368423462
        entropy_coeff: 0.0017600000137463212
        kl: 0.012194609269499779
        model: {}
        policy_loss: -0.03299134224653244
        total_loss: -0.031150713562965393
        vf_explained_var: 0.130813866853714
        vf_loss: 14.22105884552002
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 1.1463088989257812
        entropy_coeff: 0.0017600000137463212
        kl: 0.013370735570788383
        model: {}
        policy_loss: -0.03439060598611832
        total_loss: -0.03214717656373978
        vf_explained_var: 0.03168453276157379
        vf_loss: 15.86783218383789
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 1.170918583869934
        entropy_coeff: 0.0017600000137463212
        kl: 0.012293223291635513
        model: {}
        policy_loss: -0.03325089439749718
        total_loss: -0.031276457011699677
        vf_explained_var: 0.03756755590438843
        vf_loss: 15.76607608795166
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 0.7869376540184021
        entropy_coeff: 0.0017600000137463212
        kl: 0.009906664490699768
        model: {}
        policy_loss: -0.02673906832933426
        total_loss: -0.02489730715751648
        vf_explained_var: 0.23808227479457855
        vf_loss: 12.454387664794922
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 1.0475269556045532
        entropy_coeff: 0.0017600000137463212
        kl: 0.014361525885760784
        model: {}
        policy_loss: -0.036526378244161606
        total_loss: -0.03406539559364319
        vf_explained_var: 0.12437348067760468
        vf_loss: 14.323298454284668
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 0.9608160257339478
        entropy_coeff: 0.0017600000137463212
        kl: 0.011646288447082043
        model: {}
        policy_loss: -0.03230765089392662
        total_loss: -0.03032197803258896
        vf_explained_var: 0.1762084811925888
        vf_loss: 13.47451400756836
    load_time_ms: 16356.684
    num_steps_sampled: 13152000
    num_steps_trained: 13152000
    sample_time_ms: 128255.802
    update_time_ms: 117.985
  iterations_since_restore: 57
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.474553571428572
    ram_util_percent: 14.75089285714286
  pid: 14340
  policy_reward_max:
    agent-0: 145.16666666666717
    agent-1: 145.16666666666717
    agent-2: 145.16666666666717
    agent-3: 145.16666666666717
    agent-4: 145.16666666666717
    agent-5: 145.16666666666717
  policy_reward_mean:
    agent-0: 98.93000000000025
    agent-1: 98.93000000000025
    agent-2: 98.93000000000025
    agent-3: 98.93000000000025
    agent-4: 98.93000000000025
    agent-5: 98.93000000000025
  policy_reward_min:
    agent-0: 36.00000000000001
    agent-1: 36.00000000000001
    agent-2: 36.00000000000001
    agent-3: 36.00000000000001
    agent-4: 36.00000000000001
    agent-5: 36.00000000000001
  sampler_perf:
    mean_env_wait_ms: 30.754778760076263
    mean_inference_ms: 14.490531488749914
    mean_processing_ms: 65.60878701841665
  time_since_restore: 9931.71082854271
  time_this_iter_s: 156.70911383628845
  time_total_s: 22482.527699947357
  timestamp: 1637045026
  timesteps_since_restore: 5472000
  timesteps_this_iter: 96000
  timesteps_total: 13152000
  training_iteration: 137
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    137 |          22482.5 | 13152000 |   593.58 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 5.4
    apples_agent-0_min: 0
    apples_agent-1_max: 140
    apples_agent-1_mean: 18.5
    apples_agent-1_min: 0
    apples_agent-2_max: 177
    apples_agent-2_mean: 16.82
    apples_agent-2_min: 0
    apples_agent-3_max: 145
    apples_agent-3_mean: 86.03
    apples_agent-3_min: 25
    apples_agent-4_max: 58
    apples_agent-4_mean: 5.03
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 84.67
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 428
    cleaning_beam_agent-0_mean: 299.99
    cleaning_beam_agent-0_min: 142
    cleaning_beam_agent-1_max: 434
    cleaning_beam_agent-1_mean: 237.3
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 685
    cleaning_beam_agent-2_mean: 295.92
    cleaning_beam_agent-2_min: 83
    cleaning_beam_agent-3_max: 170
    cleaning_beam_agent-3_mean: 65.83
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 526
    cleaning_beam_agent-4_mean: 328.07
    cleaning_beam_agent-4_min: 148
    cleaning_beam_agent-5_max: 240
    cleaning_beam_agent-5_mean: 84.39
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-46-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 867.9999999999812
  episode_reward_mean: 608.4099999999986
  episode_reward_min: 198.9999999999983
  episodes_this_iter: 96
  episodes_total: 13248
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13318.736
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 1.1388037204742432
        entropy_coeff: 0.0017600000137463212
        kl: 0.010824980214238167
        model: {}
        policy_loss: -0.030698448419570923
        total_loss: -0.02896992862224579
        vf_explained_var: 0.11603206396102905
        vf_loss: 15.678199768066406
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 1.1277780532836914
        entropy_coeff: 0.0017600000137463212
        kl: 0.013757145963609219
        model: {}
        policy_loss: -0.034394703805446625
        total_loss: -0.03192203491926193
        vf_explained_var: 0.03753556311130524
        vf_loss: 17.061302185058594
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 1.1459800004959106
        entropy_coeff: 0.0017600000137463212
        kl: 0.013673379085958004
        model: {}
        policy_loss: -0.033261772245168686
        total_loss: -0.031025394797325134
        vf_explained_var: 0.14365264773368835
        vf_loss: 15.186275482177734
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 0.7844303846359253
        entropy_coeff: 0.0017600000137463212
        kl: 0.0094605078920722
        model: {}
        policy_loss: -0.02593112736940384
        total_loss: -0.024097256362438202
        vf_explained_var: 0.25411757826805115
        vf_loss: 13.22365665435791
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 1.0314977169036865
        entropy_coeff: 0.0017600000137463212
        kl: 0.013444550335407257
        model: {}
        policy_loss: -0.036371421068906784
        total_loss: -0.03387045860290527
        vf_explained_var: 0.0819946676492691
        vf_loss: 16.27492332458496
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 0.9509544372558594
        entropy_coeff: 0.0017600000137463212
        kl: 0.01178261823952198
        model: {}
        policy_loss: -0.03179144486784935
        total_loss: -0.029739346355199814
        vf_explained_var: 0.2286318987607956
        vf_loss: 13.692544937133789
    load_time_ms: 18092.411
    num_steps_sampled: 13248000
    num_steps_trained: 13248000
    sample_time_ms: 127615.074
    update_time_ms: 113.754
  iterations_since_restore: 58
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.646031746031742
    ram_util_percent: 15.041666666666664
  pid: 14340
  policy_reward_max:
    agent-0: 144.66666666666697
    agent-1: 144.66666666666697
    agent-2: 144.66666666666697
    agent-3: 144.66666666666697
    agent-4: 144.66666666666697
    agent-5: 144.66666666666697
  policy_reward_mean:
    agent-0: 101.40166666666696
    agent-1: 101.40166666666696
    agent-2: 101.40166666666696
    agent-3: 101.40166666666696
    agent-4: 101.40166666666696
    agent-5: 101.40166666666696
  policy_reward_min:
    agent-0: 33.1666666666667
    agent-1: 33.1666666666667
    agent-2: 33.1666666666667
    agent-3: 33.1666666666667
    agent-4: 33.1666666666667
    agent-5: 33.1666666666667
  sampler_perf:
    mean_env_wait_ms: 30.74720328533964
    mean_inference_ms: 14.520092254290516
    mean_processing_ms: 65.6438321446976
  time_since_restore: 10104.511409044266
  time_this_iter_s: 172.8005805015564
  time_total_s: 22655.328280448914
  timestamp: 1637045203
  timesteps_since_restore: 5568000
  timesteps_this_iter: 96000
  timesteps_total: 13248000
  training_iteration: 138
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    138 |          22655.3 | 13248000 |   608.41 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 169
    apples_agent-0_mean: 5.18
    apples_agent-0_min: 0
    apples_agent-1_max: 122
    apples_agent-1_mean: 26.12
    apples_agent-1_min: 0
    apples_agent-2_max: 163
    apples_agent-2_mean: 16.57
    apples_agent-2_min: 0
    apples_agent-3_max: 226
    apples_agent-3_mean: 87.49
    apples_agent-3_min: 27
    apples_agent-4_max: 107
    apples_agent-4_mean: 5.13
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 80.37
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 401
    cleaning_beam_agent-0_mean: 298.13
    cleaning_beam_agent-0_min: 132
    cleaning_beam_agent-1_max: 448
    cleaning_beam_agent-1_mean: 230.29
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 451
    cleaning_beam_agent-2_mean: 281.96
    cleaning_beam_agent-2_min: 83
    cleaning_beam_agent-3_max: 160
    cleaning_beam_agent-3_mean: 57.9
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 511
    cleaning_beam_agent-4_mean: 333.61
    cleaning_beam_agent-4_min: 113
    cleaning_beam_agent-5_max: 478
    cleaning_beam_agent-5_mean: 112.67
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-49-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 851.9999999999924
  episode_reward_mean: 591.2099999999994
  episode_reward_min: 206.9999999999974
  episodes_this_iter: 96
  episodes_total: 13344
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13253.192
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 1.1550383567810059
        entropy_coeff: 0.0017600000137463212
        kl: 0.011799226514995098
        model: {}
        policy_loss: -0.03197773918509483
        total_loss: -0.030111711472272873
        vf_explained_var: 0.04323956370353699
        vf_loss: 15.390466690063477
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 1.135698676109314
        entropy_coeff: 0.0017600000137463212
        kl: 0.012934510596096516
        model: {}
        policy_loss: -0.034031253308057785
        total_loss: -0.031869806349277496
        vf_explained_var: 0.0238569974899292
        vf_loss: 15.733807563781738
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 1.155470609664917
        entropy_coeff: 0.0017600000137463212
        kl: 0.012365026399493217
        model: {}
        policy_loss: -0.03249640390276909
        total_loss: -0.030599301680922508
        vf_explained_var: 0.09494343400001526
        vf_loss: 14.577301025390625
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 0.7638462781906128
        entropy_coeff: 0.0017600000137463212
        kl: 0.009296268224716187
        model: {}
        policy_loss: -0.02443566918373108
        total_loss: -0.022644806653261185
        vf_explained_var: 0.2067154347896576
        vf_loss: 12.759801864624023
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 1.030959129333496
        entropy_coeff: 0.0017600000137463212
        kl: 0.012989960610866547
        model: {}
        policy_loss: -0.03586694598197937
        total_loss: -0.03368019312620163
        vf_explained_var: 0.12824632227420807
        vf_loss: 14.03246784210205
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 0.9802085161209106
        entropy_coeff: 0.0017600000137463212
        kl: 0.01215789932757616
        model: {}
        policy_loss: -0.03204602748155594
        total_loss: -0.03004038706421852
        vf_explained_var: 0.19276341795921326
        vf_loss: 12.992270469665527
    load_time_ms: 18049.006
    num_steps_sampled: 13344000
    num_steps_trained: 13344000
    sample_time_ms: 128047.807
    update_time_ms: 103.794
  iterations_since_restore: 59
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.141228070175433
    ram_util_percent: 14.798684210526318
  pid: 14340
  policy_reward_max:
    agent-0: 142.00000000000034
    agent-1: 142.00000000000034
    agent-2: 142.00000000000034
    agent-3: 142.00000000000034
    agent-4: 142.00000000000034
    agent-5: 142.00000000000034
  policy_reward_mean:
    agent-0: 98.53500000000024
    agent-1: 98.53500000000024
    agent-2: 98.53500000000024
    agent-3: 98.53500000000024
    agent-4: 98.53500000000024
    agent-5: 98.53500000000024
  policy_reward_min:
    agent-0: 34.50000000000003
    agent-1: 34.50000000000003
    agent-2: 34.50000000000003
    agent-3: 34.50000000000003
    agent-4: 34.50000000000003
    agent-5: 34.50000000000003
  sampler_perf:
    mean_env_wait_ms: 30.735151648444923
    mean_inference_ms: 14.525785441639448
    mean_processing_ms: 65.63726948113592
  time_since_restore: 10264.545167684555
  time_this_iter_s: 160.0337586402893
  time_total_s: 22815.362039089203
  timestamp: 1637045363
  timesteps_since_restore: 5664000
  timesteps_this_iter: 96000
  timesteps_total: 13344000
  training_iteration: 139
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    139 |          22815.4 | 13344000 |   591.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 5.22
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 19.7
    apples_agent-1_min: 0
    apples_agent-2_max: 82
    apples_agent-2_mean: 10.83
    apples_agent-2_min: 0
    apples_agent-3_max: 155
    apples_agent-3_mean: 78.55
    apples_agent-3_min: 18
    apples_agent-4_max: 61
    apples_agent-4_mean: 3.28
    apples_agent-4_min: 0
    apples_agent-5_max: 143
    apples_agent-5_mean: 77.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 388
    cleaning_beam_agent-0_mean: 298.02
    cleaning_beam_agent-0_min: 138
    cleaning_beam_agent-1_max: 397
    cleaning_beam_agent-1_mean: 239.14
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 461
    cleaning_beam_agent-2_mean: 291.95
    cleaning_beam_agent-2_min: 123
    cleaning_beam_agent-3_max: 158
    cleaning_beam_agent-3_mean: 64.15
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 525
    cleaning_beam_agent-4_mean: 327.49
    cleaning_beam_agent-4_min: 192
    cleaning_beam_agent-5_max: 388
    cleaning_beam_agent-5_mean: 96.11
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-52-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 815.9999999999911
  episode_reward_mean: 588.9699999999992
  episode_reward_min: 248.999999999995
  episodes_this_iter: 96
  episodes_total: 13440
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13181.6
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 1.158484935760498
        entropy_coeff: 0.0017600000137463212
        kl: 0.011305835098028183
        model: {}
        policy_loss: -0.03117360547184944
        total_loss: -0.029470913112163544
        vf_explained_var: 0.08333395421504974
        vf_loss: 14.804548263549805
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 1.1095069646835327
        entropy_coeff: 0.0017600000137463212
        kl: 0.013827661983668804
        model: {}
        policy_loss: -0.03325437754392624
        total_loss: -0.030893798917531967
        vf_explained_var: 0.04264642298221588
        vf_loss: 15.477741241455078
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 1.1532485485076904
        entropy_coeff: 0.0017600000137463212
        kl: 0.012506721541285515
        model: {}
        policy_loss: -0.033029939979314804
        total_loss: -0.03109952248632908
        vf_explained_var: 0.09713253378868103
        vf_loss: 14.587900161743164
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 0.789769172668457
        entropy_coeff: 0.0017600000137463212
        kl: 0.009696908295154572
        model: {}
        policy_loss: -0.02628651075065136
        total_loss: -0.024442462250590324
        vf_explained_var: 0.19835887849330902
        vf_loss: 12.946599006652832
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 1.019953727722168
        entropy_coeff: 0.0017600000137463212
        kl: 0.013520966283977032
        model: {}
        policy_loss: -0.035044848918914795
        total_loss: -0.032686665654182434
        vf_explained_var: 0.10276007652282715
        vf_loss: 14.491032600402832
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 0.9564708471298218
        entropy_coeff: 0.0017600000137463212
        kl: 0.011388582177460194
        model: {}
        policy_loss: -0.032253142446279526
        total_loss: -0.030366968363523483
        vf_explained_var: 0.20059119164943695
        vf_loss: 12.918463706970215
    load_time_ms: 18050.449
    num_steps_sampled: 13440000
    num_steps_trained: 13440000
    sample_time_ms: 128128.228
    update_time_ms: 115.781
  iterations_since_restore: 60
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.365333333333332
    ram_util_percent: 14.841333333333335
  pid: 14340
  policy_reward_max:
    agent-0: 136.00000000000009
    agent-1: 136.00000000000009
    agent-2: 136.00000000000009
    agent-3: 136.00000000000009
    agent-4: 136.00000000000009
    agent-5: 136.00000000000009
  policy_reward_mean:
    agent-0: 98.16166666666692
    agent-1: 98.16166666666692
    agent-2: 98.16166666666692
    agent-3: 98.16166666666692
    agent-4: 98.16166666666692
    agent-5: 98.16166666666692
  policy_reward_min:
    agent-0: 41.49999999999993
    agent-1: 41.49999999999993
    agent-2: 41.49999999999993
    agent-3: 41.49999999999993
    agent-4: 41.49999999999993
    agent-5: 41.49999999999993
  sampler_perf:
    mean_env_wait_ms: 30.72599050799763
    mean_inference_ms: 14.525282336880592
    mean_processing_ms: 65.63787724661583
  time_since_restore: 10421.15418291092
  time_this_iter_s: 156.60901522636414
  time_total_s: 22971.971054315567
  timestamp: 1637045521
  timesteps_since_restore: 5760000
  timesteps_this_iter: 96000
  timesteps_total: 13440000
  training_iteration: 140
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    140 |            22972 | 13440000 |   588.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 140
    apples_agent-0_mean: 8.67
    apples_agent-0_min: 0
    apples_agent-1_max: 143
    apples_agent-1_mean: 25.9
    apples_agent-1_min: 0
    apples_agent-2_max: 65
    apples_agent-2_mean: 6.22
    apples_agent-2_min: 0
    apples_agent-3_max: 149
    apples_agent-3_mean: 73.2
    apples_agent-3_min: 8
    apples_agent-4_max: 60
    apples_agent-4_mean: 5.14
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 86.63
    apples_agent-5_min: 26
    cleaning_beam_agent-0_max: 418
    cleaning_beam_agent-0_mean: 291.85
    cleaning_beam_agent-0_min: 110
    cleaning_beam_agent-1_max: 427
    cleaning_beam_agent-1_mean: 237.56
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 504
    cleaning_beam_agent-2_mean: 311.82
    cleaning_beam_agent-2_min: 56
    cleaning_beam_agent-3_max: 125
    cleaning_beam_agent-3_mean: 66.56
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 465
    cleaning_beam_agent-4_mean: 320.23
    cleaning_beam_agent-4_min: 126
    cleaning_beam_agent-5_max: 414
    cleaning_beam_agent-5_mean: 95.17
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-54-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 869.9999999999884
  episode_reward_mean: 593.5499999999988
  episode_reward_min: 204.9999999999975
  episodes_this_iter: 96
  episodes_total: 13536
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13129.111
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 1.1365091800689697
        entropy_coeff: 0.0017600000137463212
        kl: 0.011564183980226517
        model: {}
        policy_loss: -0.03128344565629959
        total_loss: -0.029414452612400055
        vf_explained_var: 0.08228226006031036
        vf_loss: 15.564085006713867
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 1.1226612329483032
        entropy_coeff: 0.0017600000137463212
        kl: 0.013248944655060768
        model: {}
        policy_loss: -0.03477143496274948
        total_loss: -0.032445766031742096
        vf_explained_var: 0.02593831717967987
        vf_loss: 16.517621994018555
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 1.1395001411437988
        entropy_coeff: 0.0017600000137463212
        kl: 0.011925648897886276
        model: {}
        policy_loss: -0.03118879348039627
        total_loss: -0.029255615547299385
        vf_explained_var: 0.08430209755897522
        vf_loss: 15.535642623901367
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 0.7789993286132812
        entropy_coeff: 0.0017600000137463212
        kl: 0.010189246386289597
        model: {}
        policy_loss: -0.025563212111592293
        total_loss: -0.023597434163093567
        vf_explained_var: 0.2342635542154312
        vf_loss: 12.989636421203613
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 1.036251425743103
        entropy_coeff: 0.0017600000137463212
        kl: 0.013477683998644352
        model: {}
        policy_loss: -0.036646440625190735
        total_loss: -0.034287527203559875
        vf_explained_var: 0.12296386063098907
        vf_loss: 14.871756553649902
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 0.9569816589355469
        entropy_coeff: 0.0017600000137463212
        kl: 0.011870541609823704
        model: {}
        policy_loss: -0.030959218740463257
        total_loss: -0.028902243822813034
        vf_explained_var: 0.19345363974571228
        vf_loss: 13.671560287475586
    load_time_ms: 18087.321
    num_steps_sampled: 13536000
    num_steps_trained: 13536000
    sample_time_ms: 128072.813
    update_time_ms: 115.49
  iterations_since_restore: 61
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.529017857142858
    ram_util_percent: 14.826339285714287
  pid: 14340
  policy_reward_max:
    agent-0: 145.00000000000026
    agent-1: 145.00000000000026
    agent-2: 145.00000000000026
    agent-3: 145.00000000000026
    agent-4: 145.00000000000026
    agent-5: 145.00000000000026
  policy_reward_mean:
    agent-0: 98.92500000000022
    agent-1: 98.92500000000022
    agent-2: 98.92500000000022
    agent-3: 98.92500000000022
    agent-4: 98.92500000000022
    agent-5: 98.92500000000022
  policy_reward_min:
    agent-0: 34.166666666666714
    agent-1: 34.166666666666714
    agent-2: 34.166666666666714
    agent-3: 34.166666666666714
    agent-4: 34.166666666666714
    agent-5: 34.166666666666714
  sampler_perf:
    mean_env_wait_ms: 30.71879567682172
    mean_inference_ms: 14.525579950042395
    mean_processing_ms: 65.6417815494931
  time_since_restore: 10577.069492578506
  time_this_iter_s: 155.91530966758728
  time_total_s: 23127.886363983154
  timestamp: 1637045679
  timesteps_since_restore: 5856000
  timesteps_this_iter: 96000
  timesteps_total: 13536000
  training_iteration: 141
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    141 |          23127.9 | 13536000 |   593.55 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 140
    apples_agent-0_mean: 7.69
    apples_agent-0_min: 0
    apples_agent-1_max: 131
    apples_agent-1_mean: 25.54
    apples_agent-1_min: 0
    apples_agent-2_max: 147
    apples_agent-2_mean: 6.58
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 77.83
    apples_agent-3_min: 24
    apples_agent-4_max: 103
    apples_agent-4_mean: 3.58
    apples_agent-4_min: 0
    apples_agent-5_max: 186
    apples_agent-5_mean: 87.96
    apples_agent-5_min: 12
    cleaning_beam_agent-0_max: 400
    cleaning_beam_agent-0_mean: 290.09
    cleaning_beam_agent-0_min: 126
    cleaning_beam_agent-1_max: 342
    cleaning_beam_agent-1_mean: 226.27
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 528
    cleaning_beam_agent-2_mean: 306.27
    cleaning_beam_agent-2_min: 100
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 64.35
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 460
    cleaning_beam_agent-4_mean: 332.21
    cleaning_beam_agent-4_min: 162
    cleaning_beam_agent-5_max: 298
    cleaning_beam_agent-5_mean: 85.78
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-57-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 935.9999999999782
  episode_reward_mean: 606.4699999999979
  episode_reward_min: 232.99999999999835
  episodes_this_iter: 96
  episodes_total: 13632
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13130.335
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 1.1598930358886719
        entropy_coeff: 0.0017600000137463212
        kl: 0.010820596478879452
        model: {}
        policy_loss: -0.032044339925050735
        total_loss: -0.030486436560750008
        vf_explained_var: 0.16048680245876312
        vf_loss: 14.352002143859863
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 1.0861222743988037
        entropy_coeff: 0.0017600000137463212
        kl: 0.012909379787743092
        model: {}
        policy_loss: -0.03432484343647957
        total_loss: -0.03200358897447586
        vf_explained_var: 0.034684985876083374
        vf_loss: 16.509510040283203
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 1.1424142122268677
        entropy_coeff: 0.0017600000137463212
        kl: 0.012088156305253506
        model: {}
        policy_loss: -0.031380899250507355
        total_loss: -0.029440168291330338
        vf_explained_var: 0.10192665457725525
        vf_loss: 15.33747673034668
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 0.7615418434143066
        entropy_coeff: 0.0017600000137463212
        kl: 0.009475002065300941
        model: {}
        policy_loss: -0.026317166164517403
        total_loss: -0.024446725845336914
        vf_explained_var: 0.23074452579021454
        vf_loss: 13.157543182373047
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 1.0152995586395264
        entropy_coeff: 0.0017600000137463212
        kl: 0.01312797050923109
        model: {}
        policy_loss: -0.03466244414448738
        total_loss: -0.03235800936818123
        vf_explained_var: 0.1427866816520691
        vf_loss: 14.657721519470215
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 0.9606660008430481
        entropy_coeff: 0.0017600000137463212
        kl: 0.01208785455673933
        model: {}
        policy_loss: -0.03135097026824951
        total_loss: -0.029283151030540466
        vf_explained_var: 0.2159685492515564
        vf_loss: 13.410232543945312
    load_time_ms: 18079.603
    num_steps_sampled: 13632000
    num_steps_trained: 13632000
    sample_time_ms: 128124.348
    update_time_ms: 107.644
  iterations_since_restore: 62
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.570720720720715
    ram_util_percent: 14.81171171171171
  pid: 14340
  policy_reward_max:
    agent-0: 155.99999999999966
    agent-1: 155.99999999999966
    agent-2: 155.99999999999966
    agent-3: 155.99999999999966
    agent-4: 155.99999999999966
    agent-5: 155.99999999999966
  policy_reward_mean:
    agent-0: 101.07833333333362
    agent-1: 101.07833333333362
    agent-2: 101.07833333333362
    agent-3: 101.07833333333362
    agent-4: 101.07833333333362
    agent-5: 101.07833333333362
  policy_reward_min:
    agent-0: 38.8333333333333
    agent-1: 38.8333333333333
    agent-2: 38.8333333333333
    agent-3: 38.8333333333333
    agent-4: 38.8333333333333
    agent-5: 38.8333333333333
  sampler_perf:
    mean_env_wait_ms: 30.710562475949295
    mean_inference_ms: 14.52584075776432
    mean_processing_ms: 65.63811511534912
  time_since_restore: 10732.690356731415
  time_this_iter_s: 155.62086415290833
  time_total_s: 23283.507228136063
  timestamp: 1637045835
  timesteps_since_restore: 5952000
  timesteps_this_iter: 96000
  timesteps_total: 13632000
  training_iteration: 142
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    142 |          23283.5 | 13632000 |   606.47 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 4.03
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 26.6
    apples_agent-1_min: 0
    apples_agent-2_max: 163
    apples_agent-2_mean: 15.36
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 80.49
    apples_agent-3_min: 18
    apples_agent-4_max: 16
    apples_agent-4_mean: 0.79
    apples_agent-4_min: 0
    apples_agent-5_max: 170
    apples_agent-5_mean: 86.52
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 453
    cleaning_beam_agent-0_mean: 301.04
    cleaning_beam_agent-0_min: 134
    cleaning_beam_agent-1_max: 391
    cleaning_beam_agent-1_mean: 227.08
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 506
    cleaning_beam_agent-2_mean: 290.04
    cleaning_beam_agent-2_min: 55
    cleaning_beam_agent-3_max: 192
    cleaning_beam_agent-3_mean: 63.17
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 455
    cleaning_beam_agent-4_mean: 341.33
    cleaning_beam_agent-4_min: 223
    cleaning_beam_agent-5_max: 368
    cleaning_beam_agent-5_mean: 91.67
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-00-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 935.9999999999782
  episode_reward_mean: 596.5999999999995
  episode_reward_min: 250.9999999999956
  episodes_this_iter: 96
  episodes_total: 13728
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13137.2
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 1.1656825542449951
        entropy_coeff: 0.0017600000137463212
        kl: 0.011458159424364567
        model: {}
        policy_loss: -0.03164694085717201
        total_loss: -0.029992859810590744
        vf_explained_var: 0.08995719254016876
        vf_loss: 14.140510559082031
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 1.0993245840072632
        entropy_coeff: 0.0017600000137463212
        kl: 0.01284370943903923
        model: {}
        policy_loss: -0.033016324043273926
        total_loss: -0.03086433932185173
        vf_explained_var: 0.023635253310203552
        vf_loss: 15.180508613586426
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 1.1181622743606567
        entropy_coeff: 0.0017600000137463212
        kl: 0.012102754786610603
        model: {}
        policy_loss: -0.03171723708510399
        total_loss: -0.02990604005753994
        vf_explained_var: 0.12552419304847717
        vf_loss: 13.586137771606445
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 0.7720609307289124
        entropy_coeff: 0.0017600000137463212
        kl: 0.008969057351350784
        model: {}
        policy_loss: -0.02505282126367092
        total_loss: -0.023420238867402077
        vf_explained_var: 0.22919893264770508
        vf_loss: 11.975959777832031
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 1.0132431983947754
        entropy_coeff: 0.0017600000137463212
        kl: 0.012451558373868465
        model: {}
        policy_loss: -0.0345931351184845
        total_loss: -0.032454848289489746
        vf_explained_var: 0.07848674058914185
        vf_loss: 14.312845230102539
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 0.9798970818519592
        entropy_coeff: 0.0017600000137463212
        kl: 0.01270197331905365
        model: {}
        policy_loss: -0.032302375882864
        total_loss: -0.030220773071050644
        vf_explained_var: 0.18502217531204224
        vf_loss: 12.65829086303711
    load_time_ms: 18306.192
    num_steps_sampled: 13728000
    num_steps_trained: 13728000
    sample_time_ms: 128074.813
    update_time_ms: 113.677
  iterations_since_restore: 63
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.471848739495794
    ram_util_percent: 14.952941176470587
  pid: 14340
  policy_reward_max:
    agent-0: 155.99999999999966
    agent-1: 155.99999999999966
    agent-2: 155.99999999999966
    agent-3: 155.99999999999966
    agent-4: 155.99999999999966
    agent-5: 155.99999999999966
  policy_reward_mean:
    agent-0: 99.43333333333358
    agent-1: 99.43333333333358
    agent-2: 99.43333333333358
    agent-3: 99.43333333333358
    agent-4: 99.43333333333358
    agent-5: 99.43333333333358
  policy_reward_min:
    agent-0: 41.83333333333326
    agent-1: 41.83333333333326
    agent-2: 41.83333333333326
    agent-3: 41.83333333333326
    agent-4: 41.83333333333326
    agent-5: 41.83333333333326
  sampler_perf:
    mean_env_wait_ms: 30.703988356017188
    mean_inference_ms: 14.526295345373997
    mean_processing_ms: 65.6365400227706
  time_since_restore: 10899.051627397537
  time_this_iter_s: 166.36127066612244
  time_total_s: 23449.868498802185
  timestamp: 1637046002
  timesteps_since_restore: 6048000
  timesteps_this_iter: 96000
  timesteps_total: 13728000
  training_iteration: 143
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    143 |          23449.9 | 13728000 |    596.6 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 5.77
    apples_agent-0_min: 0
    apples_agent-1_max: 111
    apples_agent-1_mean: 20.5
    apples_agent-1_min: 0
    apples_agent-2_max: 107
    apples_agent-2_mean: 12.98
    apples_agent-2_min: 0
    apples_agent-3_max: 146
    apples_agent-3_mean: 76.13
    apples_agent-3_min: 0
    apples_agent-4_max: 74
    apples_agent-4_mean: 3.03
    apples_agent-4_min: 0
    apples_agent-5_max: 147
    apples_agent-5_mean: 83.72
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 427
    cleaning_beam_agent-0_mean: 294.58
    cleaning_beam_agent-0_min: 64
    cleaning_beam_agent-1_max: 394
    cleaning_beam_agent-1_mean: 236.14
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 506
    cleaning_beam_agent-2_mean: 294.58
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 129
    cleaning_beam_agent-3_mean: 60.01
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 448
    cleaning_beam_agent-4_mean: 321.93
    cleaning_beam_agent-4_min: 146
    cleaning_beam_agent-5_max: 456
    cleaning_beam_agent-5_mean: 100.35
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-02-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 800.9999999999885
  episode_reward_mean: 569.4099999999984
  episode_reward_min: 181.9999999999988
  episodes_this_iter: 96
  episodes_total: 13824
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13102.888
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 1.161278247833252
        entropy_coeff: 0.0017600000137463212
        kl: 0.01087084412574768
        model: {}
        policy_loss: -0.031068474054336548
        total_loss: -0.029408015310764313
        vf_explained_var: 0.1142587810754776
        vf_loss: 15.301421165466309
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 1.1065926551818848
        entropy_coeff: 0.0017600000137463212
        kl: 0.012534781359136105
        model: {}
        policy_loss: -0.03362303599715233
        total_loss: -0.031384143978357315
        vf_explained_var: 0.02859736979007721
        vf_loss: 16.795377731323242
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 1.1428401470184326
        entropy_coeff: 0.0017600000137463212
        kl: 0.012153634801506996
        model: {}
        policy_loss: -0.030766796320676804
        total_loss: -0.028821980580687523
        vf_explained_var: 0.11870549619197845
        vf_loss: 15.254840850830078
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 0.7640722990036011
        entropy_coeff: 0.0017600000137463212
        kl: 0.009510127827525139
        model: {}
        policy_loss: -0.0261242538690567
        total_loss: -0.024304712191224098
        vf_explained_var: 0.2699171006679535
        vf_loss: 12.622823715209961
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 1.0229523181915283
        entropy_coeff: 0.0017600000137463212
        kl: 0.01319996826350689
        model: {}
        policy_loss: -0.034958548843860626
        total_loss: -0.03257662057876587
        vf_explained_var: 0.1079876720905304
        vf_loss: 15.423336029052734
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 0.9942585229873657
        entropy_coeff: 0.0017600000137463212
        kl: 0.012227105908095837
        model: {}
        policy_loss: -0.032758679240942
        total_loss: -0.030696479603648186
        vf_explained_var: 0.20892944931983948
        vf_loss: 13.666702270507812
    load_time_ms: 19830.792
    num_steps_sampled: 13824000
    num_steps_trained: 13824000
    sample_time_ms: 127885.746
    update_time_ms: 113.7
  iterations_since_restore: 64
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.003252032520326
    ram_util_percent: 15.047154471544713
  pid: 14340
  policy_reward_max:
    agent-0: 133.5000000000002
    agent-1: 133.5000000000002
    agent-2: 133.5000000000002
    agent-3: 133.5000000000002
    agent-4: 133.5000000000002
    agent-5: 133.5000000000002
  policy_reward_mean:
    agent-0: 94.9016666666669
    agent-1: 94.9016666666669
    agent-2: 94.9016666666669
    agent-3: 94.9016666666669
    agent-4: 94.9016666666669
    agent-5: 94.9016666666669
  policy_reward_min:
    agent-0: 30.333333333333393
    agent-1: 30.333333333333393
    agent-2: 30.333333333333393
    agent-3: 30.333333333333393
    agent-4: 30.333333333333393
    agent-5: 30.333333333333393
  sampler_perf:
    mean_env_wait_ms: 30.69587326637631
    mean_inference_ms: 14.527190647333462
    mean_processing_ms: 65.6328002992017
  time_since_restore: 11071.69800400734
  time_this_iter_s: 172.64637660980225
  time_total_s: 23622.514875411987
  timestamp: 1637046175
  timesteps_since_restore: 6144000
  timesteps_this_iter: 96000
  timesteps_total: 13824000
  training_iteration: 144
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    144 |          23622.5 | 13824000 |   569.41 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 5.25
    apples_agent-0_min: 0
    apples_agent-1_max: 167
    apples_agent-1_mean: 25.42
    apples_agent-1_min: 0
    apples_agent-2_max: 109
    apples_agent-2_mean: 8.63
    apples_agent-2_min: 0
    apples_agent-3_max: 150
    apples_agent-3_mean: 85.53
    apples_agent-3_min: 14
    apples_agent-4_max: 57
    apples_agent-4_mean: 2.86
    apples_agent-4_min: 0
    apples_agent-5_max: 157
    apples_agent-5_mean: 87.05
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 415
    cleaning_beam_agent-0_mean: 302.03
    cleaning_beam_agent-0_min: 142
    cleaning_beam_agent-1_max: 463
    cleaning_beam_agent-1_mean: 222.61
    cleaning_beam_agent-1_min: 77
    cleaning_beam_agent-2_max: 484
    cleaning_beam_agent-2_mean: 305.28
    cleaning_beam_agent-2_min: 121
    cleaning_beam_agent-3_max: 225
    cleaning_beam_agent-3_mean: 64.82
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 520
    cleaning_beam_agent-4_mean: 350.54
    cleaning_beam_agent-4_min: 207
    cleaning_beam_agent-5_max: 382
    cleaning_beam_agent-5_mean: 88.48
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-05-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 867.9999999999882
  episode_reward_mean: 617.8899999999974
  episode_reward_min: 194.99999999999895
  episodes_this_iter: 96
  episodes_total: 13920
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13135.221
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 1.1605781316757202
        entropy_coeff: 0.0017600000137463212
        kl: 0.011193611659109592
        model: {}
        policy_loss: -0.03114725649356842
        total_loss: -0.029414739459753036
        vf_explained_var: 0.07934896647930145
        vf_loss: 15.364139556884766
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 1.0907198190689087
        entropy_coeff: 0.0017600000137463212
        kl: 0.013206427916884422
        model: {}
        policy_loss: -0.03254737704992294
        total_loss: -0.03019525110721588
        vf_explained_var: 0.023574337363243103
        vf_loss: 16.305023193359375
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 1.137448787689209
        entropy_coeff: 0.0017600000137463212
        kl: 0.012112576514482498
        model: {}
        policy_loss: -0.03144150972366333
        total_loss: -0.029597634449601173
        vf_explained_var: 0.14767317473888397
        vf_loss: 14.232712745666504
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 0.7347261309623718
        entropy_coeff: 0.0017600000137463212
        kl: 0.009173664264380932
        model: {}
        policy_loss: -0.023171819746494293
        total_loss: -0.02139071188867092
        vf_explained_var: 0.2574504613876343
        vf_loss: 12.39492416381836
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 1.002268671989441
        entropy_coeff: 0.0017600000137463212
        kl: 0.012172437272965908
        model: {}
        policy_loss: -0.03355742245912552
        total_loss: -0.03137899190187454
        vf_explained_var: 0.0966230183839798
        vf_loss: 15.079365730285645
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 1.0005109310150146
        entropy_coeff: 0.0017600000137463212
        kl: 0.012204520404338837
        model: {}
        policy_loss: -0.03218456730246544
        total_loss: -0.030122658237814903
        vf_explained_var: 0.17199178040027618
        vf_loss: 13.819046020507812
    load_time_ms: 19828.042
    num_steps_sampled: 13920000
    num_steps_trained: 13920000
    sample_time_ms: 127893.123
    update_time_ms: 118.101
  iterations_since_restore: 65
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.514798206278023
    ram_util_percent: 14.864125560538113
  pid: 14340
  policy_reward_max:
    agent-0: 144.6666666666667
    agent-1: 144.6666666666667
    agent-2: 144.6666666666667
    agent-3: 144.6666666666667
    agent-4: 144.6666666666667
    agent-5: 144.6666666666667
  policy_reward_mean:
    agent-0: 102.98166666666697
    agent-1: 102.98166666666697
    agent-2: 102.98166666666697
    agent-3: 102.98166666666697
    agent-4: 102.98166666666697
    agent-5: 102.98166666666697
  policy_reward_min:
    agent-0: 32.50000000000006
    agent-1: 32.50000000000006
    agent-2: 32.50000000000006
    agent-3: 32.50000000000006
    agent-4: 32.50000000000006
    agent-5: 32.50000000000006
  sampler_perf:
    mean_env_wait_ms: 30.690519529034976
    mean_inference_ms: 14.528175516434725
    mean_processing_ms: 65.62909953179067
  time_since_restore: 11228.395698547363
  time_this_iter_s: 156.6976945400238
  time_total_s: 23779.21256995201
  timestamp: 1637046332
  timesteps_since_restore: 6240000
  timesteps_this_iter: 96000
  timesteps_total: 13920000
  training_iteration: 145
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    145 |          23779.2 | 13920000 |   617.89 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 86
    apples_agent-0_mean: 6.84
    apples_agent-0_min: 0
    apples_agent-1_max: 126
    apples_agent-1_mean: 25.24
    apples_agent-1_min: 0
    apples_agent-2_max: 327
    apples_agent-2_mean: 13.37
    apples_agent-2_min: 0
    apples_agent-3_max: 207
    apples_agent-3_mean: 84.65
    apples_agent-3_min: 19
    apples_agent-4_max: 35
    apples_agent-4_mean: 1.31
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 85.07
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 467
    cleaning_beam_agent-0_mean: 303.54
    cleaning_beam_agent-0_min: 80
    cleaning_beam_agent-1_max: 385
    cleaning_beam_agent-1_mean: 214.65
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 491
    cleaning_beam_agent-2_mean: 287.99
    cleaning_beam_agent-2_min: 103
    cleaning_beam_agent-3_max: 161
    cleaning_beam_agent-3_mean: 63.74
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 445
    cleaning_beam_agent-4_mean: 328.64
    cleaning_beam_agent-4_min: 151
    cleaning_beam_agent-5_max: 382
    cleaning_beam_agent-5_mean: 96.16
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-08-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 821.9999999999943
  episode_reward_mean: 594.7399999999996
  episode_reward_min: 270.99999999999824
  episodes_this_iter: 96
  episodes_total: 14016
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13078.329
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 1.155715823173523
        entropy_coeff: 0.0017600000137463212
        kl: 0.010523764416575432
        model: {}
        policy_loss: -0.031060419976711273
        total_loss: -0.02954287827014923
        vf_explained_var: 0.08760002255439758
        vf_loss: 14.468490600585938
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 1.102493405342102
        entropy_coeff: 0.0017600000137463212
        kl: 0.012456430122256279
        model: {}
        policy_loss: -0.03301997482776642
        total_loss: -0.030959995463490486
        vf_explained_var: 0.04897259175777435
        vf_loss: 15.090777397155762
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 1.1363143920898438
        entropy_coeff: 0.0017600000137463212
        kl: 0.011331845074892044
        model: {}
        policy_loss: -0.0303952619433403
        total_loss: -0.028665049001574516
        vf_explained_var: 0.07702319324016571
        vf_loss: 14.637569427490234
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 0.7482632398605347
        entropy_coeff: 0.0017600000137463212
        kl: 0.009093238972127438
        model: {}
        policy_loss: -0.024759482592344284
        total_loss: -0.023036431521177292
        vf_explained_var: 0.22864103317260742
        vf_loss: 12.213396072387695
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 1.022608995437622
        entropy_coeff: 0.0017600000137463212
        kl: 0.012470467947423458
        model: {}
        policy_loss: -0.03454416245222092
        total_loss: -0.03243746981024742
        vf_explained_var: 0.10899029672145844
        vf_loss: 14.12393569946289
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 0.9974221587181091
        entropy_coeff: 0.0017600000137463212
        kl: 0.012427790090441704
        model: {}
        policy_loss: -0.0324275977909565
        total_loss: -0.030384384095668793
        vf_explained_var: 0.17205263674259186
        vf_loss: 13.131189346313477
    load_time_ms: 19869.443
    num_steps_sampled: 14016000
    num_steps_trained: 14016000
    sample_time_ms: 127902.805
    update_time_ms: 95.484
  iterations_since_restore: 66
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.260267857142853
    ram_util_percent: 14.804017857142858
  pid: 14340
  policy_reward_max:
    agent-0: 137.00000000000026
    agent-1: 137.00000000000026
    agent-2: 137.00000000000026
    agent-3: 137.00000000000026
    agent-4: 137.00000000000026
    agent-5: 137.00000000000026
  policy_reward_mean:
    agent-0: 99.12333333333362
    agent-1: 99.12333333333362
    agent-2: 99.12333333333362
    agent-3: 99.12333333333362
    agent-4: 99.12333333333362
    agent-5: 99.12333333333362
  policy_reward_min:
    agent-0: 45.16666666666659
    agent-1: 45.16666666666659
    agent-2: 45.16666666666659
    agent-3: 45.16666666666659
    agent-4: 45.16666666666659
    agent-5: 45.16666666666659
  sampler_perf:
    mean_env_wait_ms: 30.682780717100613
    mean_inference_ms: 14.527932291391016
    mean_processing_ms: 65.63131677128831
  time_since_restore: 11385.435916423798
  time_this_iter_s: 157.04021787643433
  time_total_s: 23936.252787828445
  timestamp: 1637046489
  timesteps_since_restore: 6336000
  timesteps_this_iter: 96000
  timesteps_total: 14016000
  training_iteration: 146
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    146 |          23936.3 | 14016000 |   594.74 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 97
    apples_agent-0_mean: 6.51
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 26.03
    apples_agent-1_min: 0
    apples_agent-2_max: 207
    apples_agent-2_mean: 15.96
    apples_agent-2_min: 0
    apples_agent-3_max: 173
    apples_agent-3_mean: 80.32
    apples_agent-3_min: 10
    apples_agent-4_max: 46
    apples_agent-4_mean: 2.7
    apples_agent-4_min: 0
    apples_agent-5_max: 169
    apples_agent-5_mean: 83.52
    apples_agent-5_min: 11
    cleaning_beam_agent-0_max: 529
    cleaning_beam_agent-0_mean: 329.21
    cleaning_beam_agent-0_min: 91
    cleaning_beam_agent-1_max: 406
    cleaning_beam_agent-1_mean: 229.58
    cleaning_beam_agent-1_min: 77
    cleaning_beam_agent-2_max: 542
    cleaning_beam_agent-2_mean: 296.08
    cleaning_beam_agent-2_min: 53
    cleaning_beam_agent-3_max: 168
    cleaning_beam_agent-3_mean: 66.21
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 475
    cleaning_beam_agent-4_mean: 327.22
    cleaning_beam_agent-4_min: 120
    cleaning_beam_agent-5_max: 487
    cleaning_beam_agent-5_mean: 108.09
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.07
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-10-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 809.9999999999742
  episode_reward_mean: 592.4399999999987
  episode_reward_min: 72.9999999999999
  episodes_this_iter: 96
  episodes_total: 14112
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13069.166
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 1.1382458209991455
        entropy_coeff: 0.0017600000137463212
        kl: 0.010342716239392757
        model: {}
        policy_loss: -0.02990739420056343
        total_loss: -0.02834605798125267
        vf_explained_var: 0.11124347150325775
        vf_loss: 14.961052894592285
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 1.102461576461792
        entropy_coeff: 0.0017600000137463212
        kl: 0.012622693553566933
        model: {}
        policy_loss: -0.0323653444647789
        total_loss: -0.030170265585184097
        vf_explained_var: 0.04289546608924866
        vf_loss: 16.108732223510742
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 1.1278566122055054
        entropy_coeff: 0.0017600000137463212
        kl: 0.012257697992026806
        model: {}
        policy_loss: -0.030544113367795944
        total_loss: -0.028616521507501602
        vf_explained_var: 0.13138964772224426
        vf_loss: 14.61081314086914
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 0.7526412010192871
        entropy_coeff: 0.0017600000137463212
        kl: 0.00966088380664587
        model: {}
        policy_loss: -0.024413106963038445
        total_loss: -0.022583477199077606
        vf_explained_var: 0.27395832538604736
        vf_loss: 12.220982551574707
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 1.0262640714645386
        entropy_coeff: 0.0017600000137463212
        kl: 0.012328270822763443
        model: {}
        policy_loss: -0.03492920100688934
        total_loss: -0.03280901536345482
        vf_explained_var: 0.1318974643945694
        vf_loss: 14.607532501220703
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 0.9857258200645447
        entropy_coeff: 0.0017600000137463212
        kl: 0.011587997898459435
        model: {}
        policy_loss: -0.031620513647794724
        total_loss: -0.02963513880968094
        vf_explained_var: 0.16639429330825806
        vf_loss: 14.026525497436523
    load_time_ms: 19892.424
    num_steps_sampled: 14112000
    num_steps_trained: 14112000
    sample_time_ms: 127860.231
    update_time_ms: 95.065
  iterations_since_restore: 67
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.23542600896861
    ram_util_percent: 14.79103139013453
  pid: 14340
  policy_reward_max:
    agent-0: 135.00000000000034
    agent-1: 135.00000000000034
    agent-2: 135.00000000000034
    agent-3: 135.00000000000034
    agent-4: 135.00000000000034
    agent-5: 135.00000000000034
  policy_reward_mean:
    agent-0: 98.74000000000025
    agent-1: 98.74000000000025
    agent-2: 98.74000000000025
    agent-3: 98.74000000000025
    agent-4: 98.74000000000025
    agent-5: 98.74000000000025
  policy_reward_min:
    agent-0: 12.166666666666657
    agent-1: 12.166666666666657
    agent-2: 12.166666666666657
    agent-3: 12.166666666666657
    agent-4: 12.166666666666657
    agent-5: 12.166666666666657
  sampler_perf:
    mean_env_wait_ms: 30.679815876166522
    mean_inference_ms: 14.52831901037178
    mean_processing_ms: 65.62825022976486
  time_since_restore: 11541.801622390747
  time_this_iter_s: 156.36570596694946
  time_total_s: 24092.618493795395
  timestamp: 1637046646
  timesteps_since_restore: 6432000
  timesteps_this_iter: 96000
  timesteps_total: 14112000
  training_iteration: 147
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    147 |          24092.6 | 14112000 |   592.44 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 4.48
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 23.95
    apples_agent-1_min: 0
    apples_agent-2_max: 455
    apples_agent-2_mean: 16.71
    apples_agent-2_min: 0
    apples_agent-3_max: 379
    apples_agent-3_mean: 83.39
    apples_agent-3_min: 17
    apples_agent-4_max: 72
    apples_agent-4_mean: 4.04
    apples_agent-4_min: 0
    apples_agent-5_max: 197
    apples_agent-5_mean: 88.64
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 529
    cleaning_beam_agent-0_mean: 319.62
    cleaning_beam_agent-0_min: 193
    cleaning_beam_agent-1_max: 385
    cleaning_beam_agent-1_mean: 225.05
    cleaning_beam_agent-1_min: 66
    cleaning_beam_agent-2_max: 550
    cleaning_beam_agent-2_mean: 294.24
    cleaning_beam_agent-2_min: 52
    cleaning_beam_agent-3_max: 188
    cleaning_beam_agent-3_mean: 67.94
    cleaning_beam_agent-3_min: 29
    cleaning_beam_agent-4_max: 480
    cleaning_beam_agent-4_mean: 325.39
    cleaning_beam_agent-4_min: 149
    cleaning_beam_agent-5_max: 292
    cleaning_beam_agent-5_mean: 90.28
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-13-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 925.9999999999894
  episode_reward_mean: 573.7399999999993
  episode_reward_min: 184.9999999999983
  episodes_this_iter: 96
  episodes_total: 14208
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13104.259
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 1.1581231355667114
        entropy_coeff: 0.0017600000137463212
        kl: 0.010056830942630768
        model: {}
        policy_loss: -0.029550492763519287
        total_loss: -0.0280003622174263
        vf_explained_var: 0.07222001254558563
        vf_loss: 15.77064037322998
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 1.0854321718215942
        entropy_coeff: 0.0017600000137463212
        kl: 0.012097503058612347
        model: {}
        policy_loss: -0.03215822950005531
        total_loss: -0.030010536313056946
        vf_explained_var: 0.036038339138031006
        vf_loss: 16.385513305664062
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 1.1249022483825684
        entropy_coeff: 0.0017600000137463212
        kl: 0.011970444582402706
        model: {}
        policy_loss: -0.02879537083208561
        total_loss: -0.02689698524773121
        vf_explained_var: 0.12649399042129517
        vf_loss: 14.841242790222168
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 0.7598088979721069
        entropy_coeff: 0.0017600000137463212
        kl: 0.009001409634947777
        model: {}
        policy_loss: -0.02466179057955742
        total_loss: -0.0228470116853714
        vf_explained_var: 0.20437853038311005
        vf_loss: 13.517578125
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 1.0290718078613281
        entropy_coeff: 0.0017600000137463212
        kl: 0.012079337611794472
        model: {}
        policy_loss: -0.03405378758907318
        total_loss: -0.031981706619262695
        vf_explained_var: 0.1361755132675171
        vf_loss: 14.673787117004395
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 0.9872444868087769
        entropy_coeff: 0.0017600000137463212
        kl: 0.011485142633318901
        model: {}
        policy_loss: -0.030663928017020226
        total_loss: -0.0286885853856802
        vf_explained_var: 0.16641639173030853
        vf_loss: 14.158636093139648
    load_time_ms: 18199.925
    num_steps_sampled: 14208000
    num_steps_trained: 14208000
    sample_time_ms: 127783.835
    update_time_ms: 99.808
  iterations_since_restore: 68
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.650450450450453
    ram_util_percent: 14.756756756756756
  pid: 14340
  policy_reward_max:
    agent-0: 154.33333333333337
    agent-1: 154.33333333333337
    agent-2: 154.33333333333337
    agent-3: 154.33333333333337
    agent-4: 154.33333333333337
    agent-5: 154.33333333333337
  policy_reward_mean:
    agent-0: 95.62333333333356
    agent-1: 95.62333333333356
    agent-2: 95.62333333333356
    agent-3: 95.62333333333356
    agent-4: 95.62333333333356
    agent-5: 95.62333333333356
  policy_reward_min:
    agent-0: 30.833333333333396
    agent-1: 30.833333333333396
    agent-2: 30.833333333333396
    agent-3: 30.833333333333396
    agent-4: 30.833333333333396
    agent-5: 30.833333333333396
  sampler_perf:
    mean_env_wait_ms: 30.672718091933003
    mean_inference_ms: 14.52873416541837
    mean_processing_ms: 65.63027079577074
  time_since_restore: 11697.324589967728
  time_this_iter_s: 155.5229675769806
  time_total_s: 24248.141461372375
  timestamp: 1637046802
  timesteps_since_restore: 6528000
  timesteps_this_iter: 96000
  timesteps_total: 14208000
  training_iteration: 148
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    148 |          24248.1 | 14208000 |   573.74 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 5.81
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 23.34
    apples_agent-1_min: 0
    apples_agent-2_max: 109
    apples_agent-2_mean: 9.45
    apples_agent-2_min: 0
    apples_agent-3_max: 149
    apples_agent-3_mean: 78.35
    apples_agent-3_min: 12
    apples_agent-4_max: 54
    apples_agent-4_mean: 3.34
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 88.39
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 478
    cleaning_beam_agent-0_mean: 328.9
    cleaning_beam_agent-0_min: 102
    cleaning_beam_agent-1_max: 373
    cleaning_beam_agent-1_mean: 220.47
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 473
    cleaning_beam_agent-2_mean: 307.54
    cleaning_beam_agent-2_min: 124
    cleaning_beam_agent-3_max: 120
    cleaning_beam_agent-3_mean: 65.18
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 477
    cleaning_beam_agent-4_mean: 328.7
    cleaning_beam_agent-4_min: 126
    cleaning_beam_agent-5_max: 457
    cleaning_beam_agent-5_mean: 88.23
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-16-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 863.9999999999695
  episode_reward_mean: 592.0999999999976
  episode_reward_min: 279.99999999999875
  episodes_this_iter: 96
  episodes_total: 14304
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13120.196
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 1.1488051414489746
        entropy_coeff: 0.0017600000137463212
        kl: 0.010545015335083008
        model: {}
        policy_loss: -0.029292874038219452
        total_loss: -0.027640050277113914
        vf_explained_var: 0.12867258489131927
        vf_loss: 15.657181739807129
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 1.095158576965332
        entropy_coeff: 0.0017600000137463212
        kl: 0.012730241753160954
        model: {}
        policy_loss: -0.033584438264369965
        total_loss: -0.031244957819581032
        vf_explained_var: 0.042617812752723694
        vf_loss: 17.209091186523438
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 1.1320279836654663
        entropy_coeff: 0.0017600000137463212
        kl: 0.011161740869283676
        model: {}
        policy_loss: -0.029467333108186722
        total_loss: -0.02765725739300251
        vf_explained_var: 0.12550795078277588
        vf_loss: 15.700948715209961
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 0.7486387491226196
        entropy_coeff: 0.0017600000137463212
        kl: 0.009340872056782246
        model: {}
        policy_loss: -0.02469777688384056
        total_loss: -0.022905804216861725
        vf_explained_var: 0.30956143140792847
        vf_loss: 12.41405963897705
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 1.0344552993774414
        entropy_coeff: 0.0017600000137463212
        kl: 0.012513333931565285
        model: {}
        policy_loss: -0.034897953271865845
        total_loss: -0.03271234408020973
        vf_explained_var: 0.16384492814540863
        vf_loss: 15.035884857177734
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 0.9830688834190369
        entropy_coeff: 0.0017600000137463212
        kl: 0.010881438851356506
        model: {}
        policy_loss: -0.031162768602371216
        total_loss: -0.02930668741464615
        vf_explained_var: 0.21549291908740997
        vf_loss: 14.099947929382324
    load_time_ms: 18205.925
    num_steps_sampled: 14304000
    num_steps_trained: 14304000
    sample_time_ms: 127477.637
    update_time_ms: 106.491
  iterations_since_restore: 69
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.486666666666668
    ram_util_percent: 14.867111111111111
  pid: 14340
  policy_reward_max:
    agent-0: 144.0000000000001
    agent-1: 144.0000000000001
    agent-2: 144.0000000000001
    agent-3: 144.0000000000001
    agent-4: 144.0000000000001
    agent-5: 144.0000000000001
  policy_reward_mean:
    agent-0: 98.68333333333356
    agent-1: 98.68333333333356
    agent-2: 98.68333333333356
    agent-3: 98.68333333333356
    agent-4: 98.68333333333356
    agent-5: 98.68333333333356
  policy_reward_min:
    agent-0: 46.666666666666615
    agent-1: 46.666666666666615
    agent-2: 46.666666666666615
    agent-3: 46.666666666666615
    agent-4: 46.666666666666615
    agent-5: 46.666666666666615
  sampler_perf:
    mean_env_wait_ms: 30.67117583736237
    mean_inference_ms: 14.528823025322717
    mean_processing_ms: 65.63414124134282
  time_since_restore: 11854.839146614075
  time_this_iter_s: 157.51455664634705
  time_total_s: 24405.656018018723
  timestamp: 1637046960
  timesteps_since_restore: 6624000
  timesteps_this_iter: 96000
  timesteps_total: 14304000
  training_iteration: 149
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    149 |          24405.7 | 14304000 |    592.1 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 4.49
    apples_agent-0_min: 0
    apples_agent-1_max: 159
    apples_agent-1_mean: 22.47
    apples_agent-1_min: 0
    apples_agent-2_max: 298
    apples_agent-2_mean: 14.55
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 81.19
    apples_agent-3_min: 26
    apples_agent-4_max: 48
    apples_agent-4_mean: 1.71
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 94.14
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 446
    cleaning_beam_agent-0_mean: 319.9
    cleaning_beam_agent-0_min: 69
    cleaning_beam_agent-1_max: 384
    cleaning_beam_agent-1_mean: 218.7
    cleaning_beam_agent-1_min: 51
    cleaning_beam_agent-2_max: 581
    cleaning_beam_agent-2_mean: 316.27
    cleaning_beam_agent-2_min: 77
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 61.61
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 526
    cleaning_beam_agent-4_mean: 333.71
    cleaning_beam_agent-4_min: 236
    cleaning_beam_agent-5_max: 298
    cleaning_beam_agent-5_mean: 90.64
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-18-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 857.9999999999897
  episode_reward_mean: 636.3699999999967
  episode_reward_min: 358.000000000006
  episodes_this_iter: 96
  episodes_total: 14400
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13155.36
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 1.1514216661453247
        entropy_coeff: 0.0017600000137463212
        kl: 0.009994796477258205
        model: {}
        policy_loss: -0.028322091326117516
        total_loss: -0.027018601074814796
        vf_explained_var: 0.09278889000415802
        vf_loss: 13.310266494750977
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 1.0851997137069702
        entropy_coeff: 0.0017600000137463212
        kl: 0.013141060248017311
        model: {}
        policy_loss: -0.03251169994473457
        total_loss: -0.030331093817949295
        vf_explained_var: 0.0031300485134124756
        vf_loss: 14.623452186584473
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 1.1220289468765259
        entropy_coeff: 0.0017600000137463212
        kl: 0.011573896743357182
        model: {}
        policy_loss: -0.02924431674182415
        total_loss: -0.02757706120610237
        vf_explained_var: 0.0956791490316391
        vf_loss: 13.272428512573242
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 0.7108045816421509
        entropy_coeff: 0.0017600000137463212
        kl: 0.008199669420719147
        model: {}
        policy_loss: -0.022310292348265648
        total_loss: -0.020762018859386444
        vf_explained_var: 0.21052542328834534
        vf_loss: 11.593527793884277
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 1.0313022136688232
        entropy_coeff: 0.0017600000137463212
        kl: 0.012040599249303341
        model: {}
        policy_loss: -0.033933546394109726
        total_loss: -0.031982164829969406
        vf_explained_var: 0.0731891542673111
        vf_loss: 13.583518028259277
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 0.9724013805389404
        entropy_coeff: 0.0017600000137463212
        kl: 0.01101953350007534
        model: {}
        policy_loss: -0.03071831539273262
        total_loss: -0.02893057093024254
        vf_explained_var: 0.117147296667099
        vf_loss: 12.952632904052734
    load_time_ms: 18208.413
    num_steps_sampled: 14400000
    num_steps_trained: 14400000
    sample_time_ms: 127434.594
    update_time_ms: 97.316
  iterations_since_restore: 70
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.654260089686094
    ram_util_percent: 14.79237668161435
  pid: 14340
  policy_reward_max:
    agent-0: 143.00000000000023
    agent-1: 143.00000000000023
    agent-2: 143.00000000000023
    agent-3: 143.00000000000023
    agent-4: 143.00000000000023
    agent-5: 143.00000000000023
  policy_reward_mean:
    agent-0: 106.06166666666697
    agent-1: 106.06166666666697
    agent-2: 106.06166666666697
    agent-3: 106.06166666666697
    agent-4: 106.06166666666697
    agent-5: 106.06166666666697
  policy_reward_min:
    agent-0: 59.66666666666653
    agent-1: 59.66666666666653
    agent-2: 59.66666666666653
    agent-3: 59.66666666666653
    agent-4: 59.66666666666653
    agent-5: 59.66666666666653
  sampler_perf:
    mean_env_wait_ms: 30.66611330569675
    mean_inference_ms: 14.528059834524955
    mean_processing_ms: 65.63883369948132
  time_since_restore: 12011.595161676407
  time_this_iter_s: 156.75601506233215
  time_total_s: 24562.412033081055
  timestamp: 1637047117
  timesteps_since_restore: 6720000
  timesteps_this_iter: 96000
  timesteps_total: 14400000
  training_iteration: 150
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    150 |          24562.4 | 14400000 |   636.37 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 3.57
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 23.59
    apples_agent-1_min: 0
    apples_agent-2_max: 207
    apples_agent-2_mean: 9.41
    apples_agent-2_min: 0
    apples_agent-3_max: 153
    apples_agent-3_mean: 86.78
    apples_agent-3_min: 28
    apples_agent-4_max: 39
    apples_agent-4_mean: 1.67
    apples_agent-4_min: 0
    apples_agent-5_max: 152
    apples_agent-5_mean: 87.84
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 468
    cleaning_beam_agent-0_mean: 332.32
    cleaning_beam_agent-0_min: 134
    cleaning_beam_agent-1_max: 375
    cleaning_beam_agent-1_mean: 211.66
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 526
    cleaning_beam_agent-2_mean: 308.08
    cleaning_beam_agent-2_min: 112
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 62.3
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 521
    cleaning_beam_agent-4_mean: 330.05
    cleaning_beam_agent-4_min: 199
    cleaning_beam_agent-5_max: 431
    cleaning_beam_agent-5_mean: 98.04
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-21-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 858.9999999999667
  episode_reward_mean: 639.9699999999951
  episode_reward_min: 256.9999999999951
  episodes_this_iter: 96
  episodes_total: 14496
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13224.934
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 1.1445422172546387
        entropy_coeff: 0.0017600000137463212
        kl: 0.01066388376057148
        model: {}
        policy_loss: -0.029156478121876717
        total_loss: -0.02748269774019718
        vf_explained_var: 0.09899026155471802
        vf_loss: 15.553947448730469
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 1.0907368659973145
        entropy_coeff: 0.0017600000137463212
        kl: 0.011972169391810894
        model: {}
        policy_loss: -0.031667985022068024
        total_loss: -0.02952190488576889
        vf_explained_var: 0.03254261612892151
        vf_loss: 16.713424682617188
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 1.1384721994400024
        entropy_coeff: 0.0017600000137463212
        kl: 0.011806530877947807
        model: {}
        policy_loss: -0.030395757406949997
        total_loss: -0.02852352149784565
        vf_explained_var: 0.12311893701553345
        vf_loss: 15.146404266357422
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 0.6991060972213745
        entropy_coeff: 0.0017600000137463212
        kl: 0.008732608519494534
        model: {}
        policy_loss: -0.022901982069015503
        total_loss: -0.02108345553278923
        vf_explained_var: 0.24623359739780426
        vf_loss: 13.024298667907715
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 1.0260144472122192
        entropy_coeff: 0.0017600000137463212
        kl: 0.012292442843317986
        model: {}
        policy_loss: -0.033092599362134933
        total_loss: -0.030889369547367096
        vf_explained_var: 0.10180719196796417
        vf_loss: 15.505311965942383
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 0.9884687662124634
        entropy_coeff: 0.0017600000137463212
        kl: 0.01174343004822731
        model: {}
        policy_loss: -0.031707651913166046
        total_loss: -0.029690396040678024
        vf_explained_var: 0.18431903421878815
        vf_loss: 14.082773208618164
    load_time_ms: 18162.368
    num_steps_sampled: 14496000
    num_steps_trained: 14496000
    sample_time_ms: 127310.414
    update_time_ms: 111.271
  iterations_since_restore: 71
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.613574660633486
    ram_util_percent: 14.785972850678732
  pid: 14340
  policy_reward_max:
    agent-0: 143.16666666666674
    agent-1: 143.16666666666674
    agent-2: 143.16666666666674
    agent-3: 143.16666666666674
    agent-4: 143.16666666666674
    agent-5: 143.16666666666674
  policy_reward_mean:
    agent-0: 106.66166666666699
    agent-1: 106.66166666666699
    agent-2: 106.66166666666699
    agent-3: 106.66166666666699
    agent-4: 106.66166666666699
    agent-5: 106.66166666666699
  policy_reward_min:
    agent-0: 42.83333333333327
    agent-1: 42.83333333333327
    agent-2: 42.83333333333327
    agent-3: 42.83333333333327
    agent-4: 42.83333333333327
    agent-5: 42.83333333333327
  sampler_perf:
    mean_env_wait_ms: 30.661134421375728
    mean_inference_ms: 14.527866321803765
    mean_processing_ms: 65.6382731644412
  time_since_restore: 12166.596945285797
  time_this_iter_s: 155.00178360939026
  time_total_s: 24717.413816690445
  timestamp: 1637047273
  timesteps_since_restore: 6816000
  timesteps_this_iter: 96000
  timesteps_total: 14496000
  training_iteration: 151
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    151 |          24717.4 | 14496000 |   639.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 150
    apples_agent-0_mean: 5.6
    apples_agent-0_min: 0
    apples_agent-1_max: 133
    apples_agent-1_mean: 27.07
    apples_agent-1_min: 0
    apples_agent-2_max: 114
    apples_agent-2_mean: 9.26
    apples_agent-2_min: 0
    apples_agent-3_max: 139
    apples_agent-3_mean: 81.18
    apples_agent-3_min: 14
    apples_agent-4_max: 62
    apples_agent-4_mean: 3.1
    apples_agent-4_min: 0
    apples_agent-5_max: 175
    apples_agent-5_mean: 90.71
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 474
    cleaning_beam_agent-0_mean: 318.24
    cleaning_beam_agent-0_min: 110
    cleaning_beam_agent-1_max: 409
    cleaning_beam_agent-1_mean: 202.19
    cleaning_beam_agent-1_min: 39
    cleaning_beam_agent-2_max: 505
    cleaning_beam_agent-2_mean: 319.02
    cleaning_beam_agent-2_min: 118
    cleaning_beam_agent-3_max: 178
    cleaning_beam_agent-3_mean: 61.96
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 537
    cleaning_beam_agent-4_mean: 335.22
    cleaning_beam_agent-4_min: 208
    cleaning_beam_agent-5_max: 359
    cleaning_beam_agent-5_mean: 89.55
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-23-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 850.9999999999728
  episode_reward_mean: 615.8499999999966
  episode_reward_min: 193.9999999999987
  episodes_this_iter: 96
  episodes_total: 14592
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13219.44
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 1.1335315704345703
        entropy_coeff: 0.0017600000137463212
        kl: 0.010528617538511753
        model: {}
        policy_loss: -0.02908090129494667
        total_loss: -0.027447212487459183
        vf_explained_var: 0.11043356359004974
        vf_loss: 15.22978401184082
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 1.0663702487945557
        entropy_coeff: 0.0017600000137463212
        kl: 0.01209403108805418
        model: {}
        policy_loss: -0.031930938363075256
        total_loss: -0.02972188964486122
        vf_explained_var: 0.025837764143943787
        vf_loss: 16.670507431030273
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 1.136263370513916
        entropy_coeff: 0.0017600000137463212
        kl: 0.010980784893035889
        model: {}
        policy_loss: -0.028578538447618484
        total_loss: -0.026882877573370934
        vf_explained_var: 0.12388063967227936
        vf_loss: 14.993280410766602
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 0.7294328808784485
        entropy_coeff: 0.0017600000137463212
        kl: 0.00913260504603386
        model: {}
        policy_loss: -0.023057369515299797
        total_loss: -0.021242793649435043
        vf_explained_var: 0.2564162015914917
        vf_loss: 12.71856689453125
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 1.0297067165374756
        entropy_coeff: 0.0017600000137463212
        kl: 0.012365145608782768
        model: {}
        policy_loss: -0.03278745710849762
        total_loss: -0.030554672703146935
        vf_explained_var: 0.0813877284526825
        vf_loss: 15.720409393310547
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 0.9814713597297668
        entropy_coeff: 0.0017600000137463212
        kl: 0.012060431763529778
        model: {}
        policy_loss: -0.03160439059138298
        total_loss: -0.029511481523513794
        vf_explained_var: 0.17679232358932495
        vf_loss: 14.08211898803711
    load_time_ms: 18144.769
    num_steps_sampled: 14592000
    num_steps_trained: 14592000
    sample_time_ms: 127368.584
    update_time_ms: 123.156
  iterations_since_restore: 72
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.561883408071747
    ram_util_percent: 14.863228699551565
  pid: 14340
  policy_reward_max:
    agent-0: 141.8333333333337
    agent-1: 141.8333333333337
    agent-2: 141.8333333333337
    agent-3: 141.8333333333337
    agent-4: 141.8333333333337
    agent-5: 141.8333333333337
  policy_reward_mean:
    agent-0: 102.64166666666694
    agent-1: 102.64166666666694
    agent-2: 102.64166666666694
    agent-3: 102.64166666666694
    agent-4: 102.64166666666694
    agent-5: 102.64166666666694
  policy_reward_min:
    agent-0: 32.333333333333385
    agent-1: 32.333333333333385
    agent-2: 32.333333333333385
    agent-3: 32.333333333333385
    agent-4: 32.333333333333385
    agent-5: 32.333333333333385
  sampler_perf:
    mean_env_wait_ms: 30.6566867897669
    mean_inference_ms: 14.528323488366246
    mean_processing_ms: 65.63613130625336
  time_since_restore: 12322.69528579712
  time_this_iter_s: 156.09834051132202
  time_total_s: 24873.512157201767
  timestamp: 1637047429
  timesteps_since_restore: 6912000
  timesteps_this_iter: 96000
  timesteps_total: 14592000
  training_iteration: 152
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    152 |          24873.5 | 14592000 |   615.85 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 3.83
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 27.18
    apples_agent-1_min: 0
    apples_agent-2_max: 359
    apples_agent-2_mean: 9.37
    apples_agent-2_min: 0
    apples_agent-3_max: 155
    apples_agent-3_mean: 74.93
    apples_agent-3_min: 17
    apples_agent-4_max: 45
    apples_agent-4_mean: 5.18
    apples_agent-4_min: 0
    apples_agent-5_max: 150
    apples_agent-5_mean: 91.43
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 496
    cleaning_beam_agent-0_mean: 323.45
    cleaning_beam_agent-0_min: 114
    cleaning_beam_agent-1_max: 354
    cleaning_beam_agent-1_mean: 208.85
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 718
    cleaning_beam_agent-2_mean: 333.3
    cleaning_beam_agent-2_min: 99
    cleaning_beam_agent-3_max: 151
    cleaning_beam_agent-3_mean: 61.24
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 496
    cleaning_beam_agent-4_mean: 320.08
    cleaning_beam_agent-4_min: 120
    cleaning_beam_agent-5_max: 382
    cleaning_beam_agent-5_mean: 99.36
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-26-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 951.9999999999817
  episode_reward_mean: 597.8199999999968
  episode_reward_min: 159.99999999999946
  episodes_this_iter: 96
  episodes_total: 14688
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13212.695
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 1.149991750717163
        entropy_coeff: 0.0017600000137463212
        kl: 0.010293803177773952
        model: {}
        policy_loss: -0.028580456972122192
        total_loss: -0.02701561525464058
        vf_explained_var: 0.12443146109580994
        vf_loss: 15.300630569458008
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 1.0689738988876343
        entropy_coeff: 0.0017600000137463212
        kl: 0.012192275375127792
        model: {}
        policy_loss: -0.0313241071999073
        total_loss: -0.029047129675745964
        vf_explained_var: 0.017233386635780334
        vf_loss: 17.19915008544922
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 1.1114745140075684
        entropy_coeff: 0.0017600000137463212
        kl: 0.010862546972930431
        model: {}
        policy_loss: -0.028343040496110916
        total_loss: -0.026496898382902145
        vf_explained_var: 0.06783406436443329
        vf_loss: 16.29828453063965
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 0.7207885384559631
        entropy_coeff: 0.0017600000137463212
        kl: 0.00810207799077034
        model: {}
        policy_loss: -0.023096369579434395
        total_loss: -0.021433569490909576
        vf_explained_var: 0.2502686083316803
        vf_loss: 13.109676361083984
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 1.045996904373169
        entropy_coeff: 0.0017600000137463212
        kl: 0.012682364322245121
        model: {}
        policy_loss: -0.03396322950720787
        total_loss: -0.031852804124355316
        vf_explained_var: 0.19046689569950104
        vf_loss: 14.149084091186523
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 0.9840290546417236
        entropy_coeff: 0.0017600000137463212
        kl: 0.011391514912247658
        model: {}
        policy_loss: -0.03038790076971054
        total_loss: -0.028489073738455772
        vf_explained_var: 0.22602444887161255
        vf_loss: 13.524157524108887
    load_time_ms: 18030.466
    num_steps_sampled: 14688000
    num_steps_trained: 14688000
    sample_time_ms: 127402.127
    update_time_ms: 124.684
  iterations_since_restore: 73
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.5632911392405
    ram_util_percent: 14.932067510548523
  pid: 14340
  policy_reward_max:
    agent-0: 158.6666666666663
    agent-1: 158.6666666666663
    agent-2: 158.6666666666663
    agent-3: 158.6666666666663
    agent-4: 158.6666666666663
    agent-5: 158.6666666666663
  policy_reward_mean:
    agent-0: 99.63666666666695
    agent-1: 99.63666666666695
    agent-2: 99.63666666666695
    agent-3: 99.63666666666695
    agent-4: 99.63666666666695
    agent-5: 99.63666666666695
  policy_reward_min:
    agent-0: 26.666666666666718
    agent-1: 26.666666666666718
    agent-2: 26.666666666666718
    agent-3: 26.666666666666718
    agent-4: 26.666666666666718
    agent-5: 26.666666666666718
  sampler_perf:
    mean_env_wait_ms: 30.65291480218416
    mean_inference_ms: 14.52854921038622
    mean_processing_ms: 65.64072041159507
  time_since_restore: 12488.18117427826
  time_this_iter_s: 165.48588848114014
  time_total_s: 25038.998045682907
  timestamp: 1637047596
  timesteps_since_restore: 7008000
  timesteps_this_iter: 96000
  timesteps_total: 14688000
  training_iteration: 153
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    153 |            25039 | 14688000 |   597.82 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 5.69
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 24.3
    apples_agent-1_min: 0
    apples_agent-2_max: 112
    apples_agent-2_mean: 8.58
    apples_agent-2_min: 0
    apples_agent-3_max: 130
    apples_agent-3_mean: 75.22
    apples_agent-3_min: 14
    apples_agent-4_max: 94
    apples_agent-4_mean: 5.11
    apples_agent-4_min: 0
    apples_agent-5_max: 199
    apples_agent-5_mean: 89.44
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 450
    cleaning_beam_agent-0_mean: 313.03
    cleaning_beam_agent-0_min: 101
    cleaning_beam_agent-1_max: 332
    cleaning_beam_agent-1_mean: 199.29
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 481
    cleaning_beam_agent-2_mean: 320.14
    cleaning_beam_agent-2_min: 124
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 60.69
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 554
    cleaning_beam_agent-4_mean: 330.38
    cleaning_beam_agent-4_min: 106
    cleaning_beam_agent-5_max: 204
    cleaning_beam_agent-5_mean: 81.41
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-29-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 896.9999999999872
  episode_reward_mean: 602.2899999999973
  episode_reward_min: 122.00000000000064
  episodes_this_iter: 96
  episodes_total: 14784
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13183.445
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 1.1332380771636963
        entropy_coeff: 0.0017600000137463212
        kl: 0.010262786410748959
        model: {}
        policy_loss: -0.027809172868728638
        total_loss: -0.02608567848801613
        vf_explained_var: 0.11698246002197266
        vf_loss: 16.65435791015625
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 1.079932689666748
        entropy_coeff: 0.0017600000137463212
        kl: 0.01175935473293066
        model: {}
        policy_loss: -0.03163063898682594
        total_loss: -0.029323264956474304
        vf_explained_var: 0.016092389822006226
        vf_loss: 18.56184196472168
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 1.1277344226837158
        entropy_coeff: 0.0017600000137463212
        kl: 0.011370623484253883
        model: {}
        policy_loss: -0.030436789616942406
        total_loss: -0.028424421325325966
        vf_explained_var: 0.08639034628868103
        vf_loss: 17.230581283569336
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 0.7312533855438232
        entropy_coeff: 0.0017600000137463212
        kl: 0.00853437278419733
        model: {}
        policy_loss: -0.023878619074821472
        total_loss: -0.022143132984638214
        vf_explained_var: 0.30129706859588623
        vf_loss: 13.156179428100586
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 1.0253925323486328
        entropy_coeff: 0.0017600000137463212
        kl: 0.011877581477165222
        model: {}
        policy_loss: -0.03210816532373428
        total_loss: -0.02992543764412403
        vf_explained_var: 0.14655159413814545
        vf_loss: 16.1190242767334
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 0.9764819145202637
        entropy_coeff: 0.0017600000137463212
        kl: 0.010851392522454262
        model: {}
        policy_loss: -0.029889291152358055
        total_loss: -0.027907537296414375
        vf_explained_var: 0.18940792977809906
        vf_loss: 15.300830841064453
    load_time_ms: 16718.941
    num_steps_sampled: 14784000
    num_steps_trained: 14784000
    sample_time_ms: 127339.268
    update_time_ms: 121.357
  iterations_since_restore: 74
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.338495575221238
    ram_util_percent: 14.82920353982301
  pid: 14340
  policy_reward_max:
    agent-0: 149.4999999999998
    agent-1: 149.4999999999998
    agent-2: 149.4999999999998
    agent-3: 149.4999999999998
    agent-4: 149.4999999999998
    agent-5: 149.4999999999998
  policy_reward_mean:
    agent-0: 100.38166666666692
    agent-1: 100.38166666666692
    agent-2: 100.38166666666692
    agent-3: 100.38166666666692
    agent-4: 100.38166666666692
    agent-5: 100.38166666666692
  policy_reward_min:
    agent-0: 20.33333333333333
    agent-1: 20.33333333333333
    agent-2: 20.33333333333333
    agent-3: 20.33333333333333
    agent-4: 20.33333333333333
    agent-5: 20.33333333333333
  sampler_perf:
    mean_env_wait_ms: 30.648810074764516
    mean_inference_ms: 14.52889992675719
    mean_processing_ms: 65.63724718780796
  time_since_restore: 12646.778403520584
  time_this_iter_s: 158.59722924232483
  time_total_s: 25197.595274925232
  timestamp: 1637047754
  timesteps_since_restore: 7104000
  timesteps_this_iter: 96000
  timesteps_total: 14784000
  training_iteration: 154
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    154 |          25197.6 | 14784000 |   602.29 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 3.73
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 23.23
    apples_agent-1_min: 0
    apples_agent-2_max: 115
    apples_agent-2_mean: 9.04
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 78.18
    apples_agent-3_min: 0
    apples_agent-4_max: 65
    apples_agent-4_mean: 3.18
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 92.74
    apples_agent-5_min: 11
    cleaning_beam_agent-0_max: 435
    cleaning_beam_agent-0_mean: 321.86
    cleaning_beam_agent-0_min: 143
    cleaning_beam_agent-1_max: 302
    cleaning_beam_agent-1_mean: 200.21
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 515
    cleaning_beam_agent-2_mean: 323.43
    cleaning_beam_agent-2_min: 128
    cleaning_beam_agent-3_max: 104
    cleaning_beam_agent-3_mean: 55.47
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 478
    cleaning_beam_agent-4_mean: 331.34
    cleaning_beam_agent-4_min: 143
    cleaning_beam_agent-5_max: 311
    cleaning_beam_agent-5_mean: 80.68
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-31-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 854.9999999999814
  episode_reward_mean: 634.3899999999952
  episode_reward_min: 149.00000000000017
  episodes_this_iter: 96
  episodes_total: 14880
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13108.735
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 1.1398066282272339
        entropy_coeff: 0.0017600000137463212
        kl: 0.009232735261321068
        model: {}
        policy_loss: -0.027797743678092957
        total_loss: -0.026237035170197487
        vf_explained_var: 0.09691877663135529
        vf_loss: 17.202211380004883
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 1.0915300846099854
        entropy_coeff: 0.0017600000137463212
        kl: 0.011557591147720814
        model: {}
        policy_loss: -0.031183548271656036
        total_loss: -0.028886474668979645
        vf_explained_var: -0.0014108717441558838
        vf_loss: 19.066486358642578
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 1.1262321472167969
        entropy_coeff: 0.0017600000137463212
        kl: 0.010650123469531536
        model: {}
        policy_loss: -0.028528790920972824
        total_loss: -0.026680011302232742
        vf_explained_var: 0.1067982167005539
        vf_loss: 17.009239196777344
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 0.7082375884056091
        entropy_coeff: 0.0017600000137463212
        kl: 0.008235782384872437
        model: {}
        policy_loss: -0.023817088454961777
        total_loss: -0.021974483504891396
        vf_explained_var: 0.24346643686294556
        vf_loss: 14.419473648071289
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 1.0278974771499634
        entropy_coeff: 0.0017600000137463212
        kl: 0.011676779948174953
        model: {}
        policy_loss: -0.03105020895600319
        total_loss: -0.028911646455526352
        vf_explained_var: 0.15315687656402588
        vf_loss: 16.1230525970459
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 0.958760142326355
        entropy_coeff: 0.0017600000137463212
        kl: 0.010128110647201538
        model: {}
        policy_loss: -0.028624000027775764
        total_loss: -0.02672511152923107
        vf_explained_var: 0.18043407797813416
        vf_loss: 15.606851577758789
    load_time_ms: 16665.631
    num_steps_sampled: 14880000
    num_steps_trained: 14880000
    sample_time_ms: 127472.719
    update_time_ms: 123.571
  iterations_since_restore: 75
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.76053811659193
    ram_util_percent: 14.720179372197311
  pid: 14340
  policy_reward_max:
    agent-0: 142.5000000000004
    agent-1: 142.5000000000004
    agent-2: 142.5000000000004
    agent-3: 142.5000000000004
    agent-4: 142.5000000000004
    agent-5: 142.5000000000004
  policy_reward_mean:
    agent-0: 105.73166666666697
    agent-1: 105.73166666666697
    agent-2: 105.73166666666697
    agent-3: 105.73166666666697
    agent-4: 105.73166666666697
    agent-5: 105.73166666666697
  policy_reward_min:
    agent-0: 24.83333333333336
    agent-1: 24.83333333333336
    agent-2: 24.83333333333336
    agent-3: 24.83333333333336
    agent-4: 24.83333333333336
    agent-5: 24.83333333333336
  sampler_perf:
    mean_env_wait_ms: 30.64604783111395
    mean_inference_ms: 14.53009071935586
    mean_processing_ms: 65.63941314976812
  time_since_restore: 12803.579752206802
  time_this_iter_s: 156.80134868621826
  time_total_s: 25354.39662361145
  timestamp: 1637047911
  timesteps_since_restore: 7200000
  timesteps_this_iter: 96000
  timesteps_total: 14880000
  training_iteration: 155
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    155 |          25354.4 | 14880000 |   634.39 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 3.54
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 23.1
    apples_agent-1_min: 0
    apples_agent-2_max: 218
    apples_agent-2_mean: 11.22
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 82.38
    apples_agent-3_min: 30
    apples_agent-4_max: 38
    apples_agent-4_mean: 2.13
    apples_agent-4_min: 0
    apples_agent-5_max: 190
    apples_agent-5_mean: 93.85
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 484
    cleaning_beam_agent-0_mean: 331.28
    cleaning_beam_agent-0_min: 130
    cleaning_beam_agent-1_max: 374
    cleaning_beam_agent-1_mean: 198.68
    cleaning_beam_agent-1_min: 48
    cleaning_beam_agent-2_max: 472
    cleaning_beam_agent-2_mean: 316.19
    cleaning_beam_agent-2_min: 95
    cleaning_beam_agent-3_max: 108
    cleaning_beam_agent-3_mean: 51.69
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 466
    cleaning_beam_agent-4_mean: 342.57
    cleaning_beam_agent-4_min: 142
    cleaning_beam_agent-5_max: 565
    cleaning_beam_agent-5_mean: 93.71
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-34-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 876.9999999999803
  episode_reward_mean: 641.0599999999957
  episode_reward_min: 221.99999999999827
  episodes_this_iter: 96
  episodes_total: 14976
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13110.891
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 1.1429409980773926
        entropy_coeff: 0.0017600000137463212
        kl: 0.009716055355966091
        model: {}
        policy_loss: -0.02733667753636837
        total_loss: -0.025804849341511726
        vf_explained_var: 0.09765663743019104
        vf_loss: 16.00192642211914
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 1.095919132232666
        entropy_coeff: 0.0017600000137463212
        kl: 0.011918049305677414
        model: {}
        policy_loss: -0.03212955966591835
        total_loss: -0.029966264963150024
        vf_explained_var: 0.0371825248003006
        vf_loss: 17.085052490234375
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 1.107654094696045
        entropy_coeff: 0.0017600000137463212
        kl: 0.010643894784152508
        model: {}
        policy_loss: -0.02824351377785206
        total_loss: -0.026528365910053253
        vf_explained_var: 0.1358034908771515
        vf_loss: 15.358379364013672
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 0.6951744556427002
        entropy_coeff: 0.0017600000137463212
        kl: 0.007787107490003109
        model: {}
        policy_loss: -0.02148432284593582
        total_loss: -0.019816264510154724
        vf_explained_var: 0.24907591938972473
        vf_loss: 13.341397285461426
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 1.0197854042053223
        entropy_coeff: 0.0017600000137463212
        kl: 0.0113113047555089
        model: {}
        policy_loss: -0.031790219247341156
        total_loss: -0.029696257784962654
        vf_explained_var: 0.08434617519378662
        vf_loss: 16.265226364135742
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 0.9558289051055908
        entropy_coeff: 0.0017600000137463212
        kl: 0.010511680506169796
        model: {}
        policy_loss: -0.028837088495492935
        total_loss: -0.026938877999782562
        vf_explained_var: 0.16622766852378845
        vf_loss: 14.781335830688477
    load_time_ms: 16520.397
    num_steps_sampled: 14976000
    num_steps_trained: 14976000
    sample_time_ms: 127390.329
    update_time_ms: 132.025
  iterations_since_restore: 76
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.59638009049774
    ram_util_percent: 14.670588235294119
  pid: 14340
  policy_reward_max:
    agent-0: 146.16666666666669
    agent-1: 146.16666666666669
    agent-2: 146.16666666666669
    agent-3: 146.16666666666669
    agent-4: 146.16666666666669
    agent-5: 146.16666666666669
  policy_reward_mean:
    agent-0: 106.84333333333362
    agent-1: 106.84333333333362
    agent-2: 106.84333333333362
    agent-3: 106.84333333333362
    agent-4: 106.84333333333362
    agent-5: 106.84333333333362
  policy_reward_min:
    agent-0: 36.99999999999998
    agent-1: 36.99999999999998
    agent-2: 36.99999999999998
    agent-3: 36.99999999999998
    agent-4: 36.99999999999998
    agent-5: 36.99999999999998
  sampler_perf:
    mean_env_wait_ms: 30.64261740270392
    mean_inference_ms: 14.529433698018902
    mean_processing_ms: 65.63811745656528
  time_since_restore: 12958.39852142334
  time_this_iter_s: 154.81876921653748
  time_total_s: 25509.215392827988
  timestamp: 1637048067
  timesteps_since_restore: 7296000
  timesteps_this_iter: 96000
  timesteps_total: 14976000
  training_iteration: 156
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    156 |          25509.2 | 14976000 |   641.06 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 6.34
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 25.45
    apples_agent-1_min: 0
    apples_agent-2_max: 85
    apples_agent-2_mean: 8.24
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 83.76
    apples_agent-3_min: 26
    apples_agent-4_max: 40
    apples_agent-4_mean: 1.98
    apples_agent-4_min: 0
    apples_agent-5_max: 160
    apples_agent-5_mean: 90.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 453
    cleaning_beam_agent-0_mean: 312.92
    cleaning_beam_agent-0_min: 86
    cleaning_beam_agent-1_max: 505
    cleaning_beam_agent-1_mean: 198.32
    cleaning_beam_agent-1_min: 50
    cleaning_beam_agent-2_max: 470
    cleaning_beam_agent-2_mean: 318.35
    cleaning_beam_agent-2_min: 90
    cleaning_beam_agent-3_max: 145
    cleaning_beam_agent-3_mean: 52.9
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 481
    cleaning_beam_agent-4_mean: 345.43
    cleaning_beam_agent-4_min: 171
    cleaning_beam_agent-5_max: 642
    cleaning_beam_agent-5_mean: 105.49
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-37-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 854.9999999999801
  episode_reward_mean: 655.7799999999955
  episode_reward_min: 351.000000000004
  episodes_this_iter: 96
  episodes_total: 15072
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13080.097
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 1.1330540180206299
        entropy_coeff: 0.0017600000137463212
        kl: 0.009251978248357773
        model: {}
        policy_loss: -0.02700696885585785
        total_loss: -0.025713395327329636
        vf_explained_var: 0.07544848322868347
        vf_loss: 14.373506546020508
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 1.0947368144989014
        entropy_coeff: 0.0017600000137463212
        kl: 0.011702954769134521
        model: {}
        policy_loss: -0.031166009604930878
        total_loss: -0.02920026332139969
        vf_explained_var: 0.0020730942487716675
        vf_loss: 15.51893424987793
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 1.1178843975067139
        entropy_coeff: 0.0017600000137463212
        kl: 0.009741413407027721
        model: {}
        policy_loss: -0.027423912659287453
        total_loss: -0.026026170700788498
        vf_explained_var: 0.08879996836185455
        vf_loss: 14.16935920715332
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 0.6756081581115723
        entropy_coeff: 0.0017600000137463212
        kl: 0.007234286982566118
        model: {}
        policy_loss: -0.020373810082674026
        total_loss: -0.01889755390584469
        vf_explained_var: 0.21675191819667816
        vf_loss: 12.184661865234375
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 1.0090312957763672
        entropy_coeff: 0.0017600000137463212
        kl: 0.010958980768918991
        model: {}
        policy_loss: -0.031033284962177277
        total_loss: -0.02918630838394165
        vf_explained_var: 0.07980971038341522
        vf_loss: 14.31080150604248
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 0.956668496131897
        entropy_coeff: 0.0017600000137463212
        kl: 0.010566947981715202
        model: {}
        policy_loss: -0.029699057340621948
        total_loss: -0.027942154556512833
        vf_explained_var: 0.14753487706184387
        vf_loss: 13.272497177124023
    load_time_ms: 16432.333
    num_steps_sampled: 15072000
    num_steps_trained: 15072000
    sample_time_ms: 127442.386
    update_time_ms: 115.372
  iterations_since_restore: 77
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.46081081081081
    ram_util_percent: 14.688288288288291
  pid: 14340
  policy_reward_max:
    agent-0: 142.5000000000002
    agent-1: 142.5000000000002
    agent-2: 142.5000000000002
    agent-3: 142.5000000000002
    agent-4: 142.5000000000002
    agent-5: 142.5000000000002
  policy_reward_mean:
    agent-0: 109.296666666667
    agent-1: 109.296666666667
    agent-2: 109.296666666667
    agent-3: 109.296666666667
    agent-4: 109.296666666667
    agent-5: 109.296666666667
  policy_reward_min:
    agent-0: 58.49999999999985
    agent-1: 58.49999999999985
    agent-2: 58.49999999999985
    agent-3: 58.49999999999985
    agent-4: 58.49999999999985
    agent-5: 58.49999999999985
  sampler_perf:
    mean_env_wait_ms: 30.641855219816247
    mean_inference_ms: 14.529884324606293
    mean_processing_ms: 65.63658513735749
  time_since_restore: 13113.938304185867
  time_this_iter_s: 155.53978276252747
  time_total_s: 25664.755175590515
  timestamp: 1637048223
  timesteps_since_restore: 7392000
  timesteps_this_iter: 96000
  timesteps_total: 15072000
  training_iteration: 157
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    157 |          25664.8 | 15072000 |   655.78 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 5.56
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 24.13
    apples_agent-1_min: 0
    apples_agent-2_max: 306
    apples_agent-2_mean: 10.61
    apples_agent-2_min: 0
    apples_agent-3_max: 134
    apples_agent-3_mean: 80.34
    apples_agent-3_min: 18
    apples_agent-4_max: 53
    apples_agent-4_mean: 1.92
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 93.9
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 478
    cleaning_beam_agent-0_mean: 307.47
    cleaning_beam_agent-0_min: 90
    cleaning_beam_agent-1_max: 380
    cleaning_beam_agent-1_mean: 209.62
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 599
    cleaning_beam_agent-2_mean: 311.37
    cleaning_beam_agent-2_min: 62
    cleaning_beam_agent-3_max: 112
    cleaning_beam_agent-3_mean: 49.6
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 528
    cleaning_beam_agent-4_mean: 345.04
    cleaning_beam_agent-4_min: 183
    cleaning_beam_agent-5_max: 248
    cleaning_beam_agent-5_mean: 84.75
    cleaning_beam_agent-5_min: 24
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-39-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 904.999999999982
  episode_reward_mean: 646.6099999999954
  episode_reward_min: 301.9999999999995
  episodes_this_iter: 96
  episodes_total: 15168
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13058.653
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 1.1442337036132812
        entropy_coeff: 0.0017600000137463212
        kl: 0.009127313271164894
        model: {}
        policy_loss: -0.026613079011440277
        total_loss: -0.025322170928120613
        vf_explained_var: 0.12067724764347076
        vf_loss: 14.792987823486328
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 1.1069657802581787
        entropy_coeff: 0.0017600000137463212
        kl: 0.011345711536705494
        model: {}
        policy_loss: -0.03062320128083229
        total_loss: -0.02863306924700737
        vf_explained_var: 0.008421719074249268
        vf_loss: 16.692502975463867
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 1.115541934967041
        entropy_coeff: 0.0017600000137463212
        kl: 0.010757973417639732
        model: {}
        policy_loss: -0.028597373515367508
        total_loss: -0.026943644508719444
        vf_explained_var: 0.12802982330322266
        vf_loss: 14.65489673614502
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 0.6874747276306152
        entropy_coeff: 0.0017600000137463212
        kl: 0.007364499848335981
        model: {}
        policy_loss: -0.020979460328817368
        total_loss: -0.019396215677261353
        vf_explained_var: 0.21642743051052094
        vf_loss: 13.20296859741211
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 1.0104396343231201
        entropy_coeff: 0.0017600000137463212
        kl: 0.011000852100551128
        model: {}
        policy_loss: -0.030124066397547722
        total_loss: -0.028067294508218765
        vf_explained_var: 0.029658228158950806
        vf_loss: 16.349796295166016
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 0.9637951254844666
        entropy_coeff: 0.0017600000137463212
        kl: 0.01035268884152174
        model: {}
        policy_loss: -0.027786660939455032
        total_loss: -0.02600981295108795
        vf_explained_var: 0.1665881872177124
        vf_loss: 14.025900840759277
    load_time_ms: 16194.895
    num_steps_sampled: 15168000
    num_steps_trained: 15168000
    sample_time_ms: 127634.967
    update_time_ms: 110.776
  iterations_since_restore: 78
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.443891402714932
    ram_util_percent: 14.66289592760181
  pid: 14340
  policy_reward_max:
    agent-0: 150.83333333333326
    agent-1: 150.83333333333326
    agent-2: 150.83333333333326
    agent-3: 150.83333333333326
    agent-4: 150.83333333333326
    agent-5: 150.83333333333326
  policy_reward_mean:
    agent-0: 107.76833333333364
    agent-1: 107.76833333333364
    agent-2: 107.76833333333364
    agent-3: 107.76833333333364
    agent-4: 107.76833333333364
    agent-5: 107.76833333333364
  policy_reward_min:
    agent-0: 50.3333333333332
    agent-1: 50.3333333333332
    agent-2: 50.3333333333332
    agent-3: 50.3333333333332
    agent-4: 50.3333333333332
    agent-5: 50.3333333333332
  sampler_perf:
    mean_env_wait_ms: 30.639041409161276
    mean_inference_ms: 14.531271245174828
    mean_processing_ms: 65.64105070917542
  time_since_restore: 13268.77190876007
  time_this_iter_s: 154.8336045742035
  time_total_s: 25819.58878016472
  timestamp: 1637048378
  timesteps_since_restore: 7488000
  timesteps_this_iter: 96000
  timesteps_total: 15168000
  training_iteration: 158
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    158 |          25819.6 | 15168000 |   646.61 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 4.47
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 21.09
    apples_agent-1_min: 0
    apples_agent-2_max: 127
    apples_agent-2_mean: 11.17
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 78.26
    apples_agent-3_min: 22
    apples_agent-4_max: 45
    apples_agent-4_mean: 2.55
    apples_agent-4_min: 0
    apples_agent-5_max: 191
    apples_agent-5_mean: 92.36
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 562
    cleaning_beam_agent-0_mean: 325.85
    cleaning_beam_agent-0_min: 153
    cleaning_beam_agent-1_max: 387
    cleaning_beam_agent-1_mean: 222.78
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 454
    cleaning_beam_agent-2_mean: 290.75
    cleaning_beam_agent-2_min: 98
    cleaning_beam_agent-3_max: 132
    cleaning_beam_agent-3_mean: 50.87
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 468
    cleaning_beam_agent-4_mean: 330.24
    cleaning_beam_agent-4_min: 220
    cleaning_beam_agent-5_max: 275
    cleaning_beam_agent-5_mean: 86.78
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-42-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 903.9999999999793
  episode_reward_mean: 630.6099999999947
  episode_reward_min: 116.0000000000007
  episodes_this_iter: 96
  episodes_total: 15264
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13003.485
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 1.1426151990890503
        entropy_coeff: 0.0017600000137463212
        kl: 0.009248646907508373
        model: {}
        policy_loss: -0.027763407677412033
        total_loss: -0.02632349543273449
        vf_explained_var: 0.1093260794878006
        vf_loss: 16.011810302734375
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 1.1132621765136719
        entropy_coeff: 0.0017600000137463212
        kl: 0.012564478442072868
        model: {}
        policy_loss: -0.031281352043151855
        total_loss: -0.028957625851035118
        vf_explained_var: 0.014667674899101257
        vf_loss: 17.70172119140625
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 1.1286978721618652
        entropy_coeff: 0.0017600000137463212
        kl: 0.011007842607796192
        model: {}
        policy_loss: -0.028548263013362885
        total_loss: -0.026749495416879654
        vf_explained_var: 0.11949987709522247
        vf_loss: 15.837091445922852
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 0.6873366236686707
        entropy_coeff: 0.0017600000137463212
        kl: 0.00727471336722374
        model: {}
        policy_loss: -0.021170910447835922
        total_loss: -0.019582325592637062
        vf_explained_var: 0.2531431019306183
        vf_loss: 13.433511734008789
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 1.019918441772461
        entropy_coeff: 0.0017600000137463212
        kl: 0.01113674882799387
        model: {}
        policy_loss: -0.03211890906095505
        total_loss: -0.030124403536319733
        vf_explained_var: 0.13061745464801788
        vf_loss: 15.622074127197266
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 0.9565541744232178
        entropy_coeff: 0.0017600000137463212
        kl: 0.010362297296524048
        model: {}
        policy_loss: -0.029019232839345932
        total_loss: -0.027212319895625114
        vf_explained_var: 0.2108154445886612
        vf_loss: 14.179915428161621
    load_time_ms: 16094.962
    num_steps_sampled: 15264000
    num_steps_trained: 15264000
    sample_time_ms: 127554.632
    update_time_ms: 103.601
  iterations_since_restore: 79
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.547272727272734
    ram_util_percent: 14.60590909090909
  pid: 14340
  policy_reward_max:
    agent-0: 150.6666666666662
    agent-1: 150.6666666666662
    agent-2: 150.6666666666662
    agent-3: 150.6666666666662
    agent-4: 150.6666666666662
    agent-5: 150.6666666666662
  policy_reward_mean:
    agent-0: 105.10166666666699
    agent-1: 105.10166666666699
    agent-2: 105.10166666666699
    agent-3: 105.10166666666699
    agent-4: 105.10166666666699
    agent-5: 105.10166666666699
  policy_reward_min:
    agent-0: 19.333333333333336
    agent-1: 19.333333333333336
    agent-2: 19.333333333333336
    agent-3: 19.333333333333336
    agent-4: 19.333333333333336
    agent-5: 19.333333333333336
  sampler_perf:
    mean_env_wait_ms: 30.634094664055624
    mean_inference_ms: 14.53119640978948
    mean_processing_ms: 65.64089712444786
  time_since_restore: 13423.580319166183
  time_this_iter_s: 154.80841040611267
  time_total_s: 25974.39719057083
  timestamp: 1637048533
  timesteps_since_restore: 7584000
  timesteps_this_iter: 96000
  timesteps_total: 15264000
  training_iteration: 159
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    159 |          25974.4 | 15264000 |   630.61 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 3.22
    apples_agent-0_min: 0
    apples_agent-1_max: 132
    apples_agent-1_mean: 21.64
    apples_agent-1_min: 0
    apples_agent-2_max: 184
    apples_agent-2_mean: 15.39
    apples_agent-2_min: 0
    apples_agent-3_max: 131
    apples_agent-3_mean: 82.48
    apples_agent-3_min: 2
    apples_agent-4_max: 59
    apples_agent-4_mean: 3.25
    apples_agent-4_min: 0
    apples_agent-5_max: 200
    apples_agent-5_mean: 95.71
    apples_agent-5_min: 27
    cleaning_beam_agent-0_max: 468
    cleaning_beam_agent-0_mean: 337.33
    cleaning_beam_agent-0_min: 104
    cleaning_beam_agent-1_max: 375
    cleaning_beam_agent-1_mean: 209.4
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 457
    cleaning_beam_agent-2_mean: 295.27
    cleaning_beam_agent-2_min: 99
    cleaning_beam_agent-3_max: 133
    cleaning_beam_agent-3_mean: 49.99
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 468
    cleaning_beam_agent-4_mean: 341.55
    cleaning_beam_agent-4_min: 209
    cleaning_beam_agent-5_max: 342
    cleaning_beam_agent-5_mean: 80.04
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-44-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 864.999999999993
  episode_reward_mean: 665.8799999999942
  episode_reward_min: 249.99999999999736
  episodes_this_iter: 96
  episodes_total: 15360
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12956.735
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 1.1222681999206543
        entropy_coeff: 0.0017600000137463212
        kl: 0.00958354864269495
        model: {}
        policy_loss: -0.027629278600215912
        total_loss: -0.02610669657588005
        vf_explained_var: 0.05398161709308624
        vf_loss: 15.810661315917969
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 1.126828908920288
        entropy_coeff: 0.0017600000137463212
        kl: 0.011662781238555908
        model: {}
        policy_loss: -0.031835079193115234
        total_loss: -0.029867567121982574
        vf_explained_var: 0.03171446919441223
        vf_loss: 16.18171501159668
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 1.1312460899353027
        entropy_coeff: 0.0017600000137463212
        kl: 0.01068115234375
        model: {}
        policy_loss: -0.02782238833606243
        total_loss: -0.026189783588051796
        vf_explained_var: 0.11053256690502167
        vf_loss: 14.873638153076172
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 0.6602401733398438
        entropy_coeff: 0.0017600000137463212
        kl: 0.007033316884189844
        model: {}
        policy_loss: -0.020433055236935616
        total_loss: -0.018871909007430077
        vf_explained_var: 0.21245795488357544
        vf_loss: 13.165082931518555
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 1.0154372453689575
        entropy_coeff: 0.0017600000137463212
        kl: 0.010719723999500275
        model: {}
        policy_loss: -0.03125516697764397
        total_loss: -0.029410356655716896
        vf_explained_var: 0.11016461253166199
        vf_loss: 14.880352020263672
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 0.9472563862800598
        entropy_coeff: 0.0017600000137463212
        kl: 0.010367616079747677
        model: {}
        policy_loss: -0.029842909425497055
        total_loss: -0.02807142399251461
        vf_explained_var: 0.18397237360477448
        vf_loss: 13.651308059692383
    load_time_ms: 15883.374
    num_steps_sampled: 15360000
    num_steps_trained: 15360000
    sample_time_ms: 127424.435
    update_time_ms: 103.666
  iterations_since_restore: 80
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.8908256880734
    ram_util_percent: 14.70779816513762
  pid: 14340
  policy_reward_max:
    agent-0: 144.1666666666671
    agent-1: 144.1666666666671
    agent-2: 144.1666666666671
    agent-3: 144.1666666666671
    agent-4: 144.1666666666671
    agent-5: 144.1666666666671
  policy_reward_mean:
    agent-0: 110.98000000000036
    agent-1: 110.98000000000036
    agent-2: 110.98000000000036
    agent-3: 110.98000000000036
    agent-4: 110.98000000000036
    agent-5: 110.98000000000036
  policy_reward_min:
    agent-0: 41.66666666666667
    agent-1: 41.66666666666667
    agent-2: 41.66666666666667
    agent-3: 41.66666666666667
    agent-4: 41.66666666666667
    agent-5: 41.66666666666667
  sampler_perf:
    mean_env_wait_ms: 30.63053533582971
    mean_inference_ms: 14.531386659271334
    mean_processing_ms: 65.64140234901198
  time_since_restore: 13576.143894195557
  time_this_iter_s: 152.56357502937317
  time_total_s: 26126.960765600204
  timestamp: 1637048686
  timesteps_since_restore: 7680000
  timesteps_this_iter: 96000
  timesteps_total: 15360000
  training_iteration: 160
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    160 |            26127 | 15360000 |   665.88 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 76
    apples_agent-0_mean: 4.97
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 20.43
    apples_agent-1_min: 0
    apples_agent-2_max: 132
    apples_agent-2_mean: 11.11
    apples_agent-2_min: 0
    apples_agent-3_max: 128
    apples_agent-3_mean: 74.57
    apples_agent-3_min: 21
    apples_agent-4_max: 61
    apples_agent-4_mean: 2.12
    apples_agent-4_min: 0
    apples_agent-5_max: 181
    apples_agent-5_mean: 92.02
    apples_agent-5_min: 27
    cleaning_beam_agent-0_max: 468
    cleaning_beam_agent-0_mean: 320.1
    cleaning_beam_agent-0_min: 126
    cleaning_beam_agent-1_max: 358
    cleaning_beam_agent-1_mean: 208.29
    cleaning_beam_agent-1_min: 69
    cleaning_beam_agent-2_max: 445
    cleaning_beam_agent-2_mean: 290.6
    cleaning_beam_agent-2_min: 110
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 55.25
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 436
    cleaning_beam_agent-4_mean: 331.01
    cleaning_beam_agent-4_min: 162
    cleaning_beam_agent-5_max: 298
    cleaning_beam_agent-5_mean: 82.31
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-47-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 857.999999999976
  episode_reward_mean: 614.329999999998
  episode_reward_min: 165.99999999999955
  episodes_this_iter: 96
  episodes_total: 15456
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12894.643
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 1.1371393203735352
        entropy_coeff: 0.0017600000137463212
        kl: 0.009363295510411263
        model: {}
        policy_loss: -0.027760080993175507
        total_loss: -0.026366446167230606
        vf_explained_var: 0.1352124661207199
        vf_loss: 15.223395347595215
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 1.1067581176757812
        entropy_coeff: 0.0017600000137463212
        kl: 0.01110013946890831
        model: {}
        policy_loss: -0.03143509849905968
        total_loss: -0.02939460054039955
        vf_explained_var: -0.0060867369174957275
        vf_loss: 17.683645248413086
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 1.1209490299224854
        entropy_coeff: 0.0017600000137463212
        kl: 0.010292519815266132
        model: {}
        policy_loss: -0.027684276923537254
        total_loss: -0.0260136928409338
        vf_explained_var: 0.09917500615119934
        vf_loss: 15.849505424499512
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.7106307744979858
        entropy_coeff: 0.0017600000137463212
        kl: 0.007660537958145142
        model: {}
        policy_loss: -0.021852264180779457
        total_loss: -0.020242061465978622
        vf_explained_var: 0.24515211582183838
        vf_loss: 13.28807258605957
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 1.0156298875808716
        entropy_coeff: 0.0017600000137463212
        kl: 0.010744640603661537
        model: {}
        policy_loss: -0.030555525794625282
        total_loss: -0.028681442141532898
        vf_explained_var: 0.14009717106819153
        vf_loss: 15.126630783081055
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.9659470319747925
        entropy_coeff: 0.0017600000137463212
        kl: 0.010280361399054527
        model: {}
        policy_loss: -0.029685737565159798
        total_loss: -0.027955111116170883
        vf_explained_var: 0.2184789776802063
        vf_loss: 13.746207237243652
    load_time_ms: 17396.952
    num_steps_sampled: 15456000
    num_steps_trained: 15456000
    sample_time_ms: 127531.641
    update_time_ms: 80.356
  iterations_since_restore: 81
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.161475409836065
    ram_util_percent: 14.944672131147541
  pid: 14340
  policy_reward_max:
    agent-0: 143.00000000000014
    agent-1: 143.00000000000014
    agent-2: 143.00000000000014
    agent-3: 143.00000000000014
    agent-4: 143.00000000000014
    agent-5: 143.00000000000014
  policy_reward_mean:
    agent-0: 102.38833333333363
    agent-1: 102.38833333333363
    agent-2: 102.38833333333363
    agent-3: 102.38833333333363
    agent-4: 102.38833333333363
    agent-5: 102.38833333333363
  policy_reward_min:
    agent-0: 27.666666666666707
    agent-1: 27.666666666666707
    agent-2: 27.666666666666707
    agent-3: 27.666666666666707
    agent-4: 27.666666666666707
    agent-5: 27.666666666666707
  sampler_perf:
    mean_env_wait_ms: 30.625141563275996
    mean_inference_ms: 14.532719072419905
    mean_processing_ms: 65.68664020792862
  time_since_restore: 13746.556488990784
  time_this_iter_s: 170.41259479522705
  time_total_s: 26297.37336039543
  timestamp: 1637048857
  timesteps_since_restore: 7776000
  timesteps_this_iter: 96000
  timesteps_total: 15456000
  training_iteration: 161
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    161 |          26297.4 | 15456000 |   614.33 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 5.11
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 20.26
    apples_agent-1_min: 0
    apples_agent-2_max: 145
    apples_agent-2_mean: 13.76
    apples_agent-2_min: 0
    apples_agent-3_max: 133
    apples_agent-3_mean: 78.19
    apples_agent-3_min: 36
    apples_agent-4_max: 34
    apples_agent-4_mean: 1.57
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 95.19
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 449
    cleaning_beam_agent-0_mean: 320.53
    cleaning_beam_agent-0_min: 48
    cleaning_beam_agent-1_max: 351
    cleaning_beam_agent-1_mean: 203.6
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 499
    cleaning_beam_agent-2_mean: 300.22
    cleaning_beam_agent-2_min: 86
    cleaning_beam_agent-3_max: 124
    cleaning_beam_agent-3_mean: 53.11
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 455
    cleaning_beam_agent-4_mean: 341.35
    cleaning_beam_agent-4_min: 223
    cleaning_beam_agent-5_max: 373
    cleaning_beam_agent-5_mean: 76.68
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-50-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 935.9999999999857
  episode_reward_mean: 659.959999999997
  episode_reward_min: 390.0000000000061
  episodes_this_iter: 96
  episodes_total: 15552
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12911.388
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 1.1395225524902344
        entropy_coeff: 0.0017600000137463212
        kl: 0.009026145562529564
        model: {}
        policy_loss: -0.027707509696483612
        total_loss: -0.026465997099876404
        vf_explained_var: 0.06176559627056122
        vf_loss: 14.418468475341797
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 1.1204966306686401
        entropy_coeff: 0.0017600000137463212
        kl: 0.010999565944075584
        model: {}
        policy_loss: -0.030406462028622627
        total_loss: -0.028656084090471268
        vf_explained_var: 0.009845167398452759
        vf_loss: 15.225387573242188
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 1.1189523935317993
        entropy_coeff: 0.0017600000137463212
        kl: 0.009872669354081154
        model: {}
        policy_loss: -0.026788124814629555
        total_loss: -0.025357313454151154
        vf_explained_var: 0.0725555270910263
        vf_loss: 14.256355285644531
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.6411423683166504
        entropy_coeff: 0.0017600000137463212
        kl: 0.006643189117312431
        model: {}
        policy_loss: -0.019824445247650146
        total_loss: -0.018396029248833656
        vf_explained_var: 0.2013130486011505
        vf_loss: 12.281867980957031
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 1.006901502609253
        entropy_coeff: 0.0017600000137463212
        kl: 0.010804455727338791
        model: {}
        policy_loss: -0.031055230647325516
        total_loss: -0.0292685404419899
        vf_explained_var: 0.08976094424724579
        vf_loss: 13.97943115234375
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.9462460279464722
        entropy_coeff: 0.0017600000137463212
        kl: 0.010065100155770779
        model: {}
        policy_loss: -0.02800954133272171
        total_loss: -0.026388945057988167
        vf_explained_var: 0.1714397370815277
        vf_loss: 12.729701042175293
    load_time_ms: 17408.96
    num_steps_sampled: 15552000
    num_steps_trained: 15552000
    sample_time_ms: 127911.156
    update_time_ms: 67.006
  iterations_since_restore: 82
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.26447368421053
    ram_util_percent: 14.712280701754386
  pid: 14340
  policy_reward_max:
    agent-0: 155.99999999999991
    agent-1: 155.99999999999991
    agent-2: 155.99999999999991
    agent-3: 155.99999999999991
    agent-4: 155.99999999999991
    agent-5: 155.99999999999991
  policy_reward_mean:
    agent-0: 109.99333333333365
    agent-1: 109.99333333333365
    agent-2: 109.99333333333365
    agent-3: 109.99333333333365
    agent-4: 109.99333333333365
    agent-5: 109.99333333333365
  policy_reward_min:
    agent-0: 64.99999999999974
    agent-1: 64.99999999999974
    agent-2: 64.99999999999974
    agent-3: 64.99999999999974
    agent-4: 64.99999999999974
    agent-5: 64.99999999999974
  sampler_perf:
    mean_env_wait_ms: 30.62290257278508
    mean_inference_ms: 14.534176046292119
    mean_processing_ms: 65.69392023394896
  time_since_restore: 13906.568857192993
  time_this_iter_s: 160.01236820220947
  time_total_s: 26457.38572859764
  timestamp: 1637049017
  timesteps_since_restore: 7872000
  timesteps_this_iter: 96000
  timesteps_total: 15552000
  training_iteration: 162
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    162 |          26457.4 | 15552000 |   659.96 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 3.3
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 18.12
    apples_agent-1_min: 0
    apples_agent-2_max: 86
    apples_agent-2_mean: 10.39
    apples_agent-2_min: 0
    apples_agent-3_max: 132
    apples_agent-3_mean: 71.04
    apples_agent-3_min: 18
    apples_agent-4_max: 68
    apples_agent-4_mean: 3.08
    apples_agent-4_min: 0
    apples_agent-5_max: 157
    apples_agent-5_mean: 80.94
    apples_agent-5_min: 7
    cleaning_beam_agent-0_max: 484
    cleaning_beam_agent-0_mean: 321.85
    cleaning_beam_agent-0_min: 131
    cleaning_beam_agent-1_max: 337
    cleaning_beam_agent-1_mean: 205.64
    cleaning_beam_agent-1_min: 87
    cleaning_beam_agent-2_max: 451
    cleaning_beam_agent-2_mean: 270.73
    cleaning_beam_agent-2_min: 64
    cleaning_beam_agent-3_max: 118
    cleaning_beam_agent-3_mean: 58.07
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 456
    cleaning_beam_agent-4_mean: 325.06
    cleaning_beam_agent-4_min: 107
    cleaning_beam_agent-5_max: 456
    cleaning_beam_agent-5_mean: 91.2
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-52-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 890.9999999999778
  episode_reward_mean: 589.8799999999992
  episode_reward_min: 181.99999999999835
  episodes_this_iter: 96
  episodes_total: 15648
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12908.897
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 1.1394048929214478
        entropy_coeff: 0.0017600000137463212
        kl: 0.009266744367778301
        model: {}
        policy_loss: -0.026932157576084137
        total_loss: -0.02551542967557907
        vf_explained_var: 0.10087713599205017
        vf_loss: 15.687331199645996
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 1.113187313079834
        entropy_coeff: 0.0017600000137463212
        kl: 0.011759024113416672
        model: {}
        policy_loss: -0.030590182170271873
        total_loss: -0.028515484184026718
        vf_explained_var: 0.035319000482559204
        vf_loss: 16.821041107177734
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 1.118533968925476
        entropy_coeff: 0.0017600000137463212
        kl: 0.01002632174640894
        model: {}
        policy_loss: -0.027852525934576988
        total_loss: -0.02633701078593731
        vf_explained_var: 0.15144769847393036
        vf_loss: 14.788728713989258
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.7002885341644287
        entropy_coeff: 0.0017600000137463212
        kl: 0.008107046596705914
        model: {}
        policy_loss: -0.022386744618415833
        total_loss: -0.020656272768974304
        vf_explained_var: 0.23068726062774658
        vf_loss: 13.415693283081055
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 1.0145695209503174
        entropy_coeff: 0.0017600000137463212
        kl: 0.010288196615874767
        model: {}
        policy_loss: -0.02967112697660923
        total_loss: -0.02780737727880478
        vf_explained_var: 0.08704519271850586
        vf_loss: 15.9175443649292
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.9765107035636902
        entropy_coeff: 0.0017600000137463212
        kl: 0.010148270055651665
        model: {}
        policy_loss: -0.02898925170302391
        total_loss: -0.027259167283773422
        vf_explained_var: 0.1863953173160553
        vf_loss: 14.190921783447266
    load_time_ms: 16441.201
    num_steps_sampled: 15648000
    num_steps_trained: 15648000
    sample_time_ms: 127911.188
    update_time_ms: 71.349
  iterations_since_restore: 83
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.500896860986543
    ram_util_percent: 14.79641255605381
  pid: 14340
  policy_reward_max:
    agent-0: 148.50000000000045
    agent-1: 148.50000000000045
    agent-2: 148.50000000000045
    agent-3: 148.50000000000045
    agent-4: 148.50000000000045
    agent-5: 148.50000000000045
  policy_reward_mean:
    agent-0: 98.31333333333359
    agent-1: 98.31333333333359
    agent-2: 98.31333333333359
    agent-3: 98.31333333333359
    agent-4: 98.31333333333359
    agent-5: 98.31333333333359
  policy_reward_min:
    agent-0: 30.3333333333334
    agent-1: 30.3333333333334
    agent-2: 30.3333333333334
    agent-3: 30.3333333333334
    agent-4: 30.3333333333334
    agent-5: 30.3333333333334
  sampler_perf:
    mean_env_wait_ms: 30.61678175056635
    mean_inference_ms: 14.53450854517422
    mean_processing_ms: 65.69504582534286
  time_since_restore: 14062.387818336487
  time_this_iter_s: 155.81896114349365
  time_total_s: 26613.204689741135
  timestamp: 1637049177
  timesteps_since_restore: 7968000
  timesteps_this_iter: 96000
  timesteps_total: 15648000
  training_iteration: 163
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    163 |          26613.2 | 15648000 |   589.88 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 4.17
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 21.45
    apples_agent-1_min: 0
    apples_agent-2_max: 222
    apples_agent-2_mean: 13.2
    apples_agent-2_min: 0
    apples_agent-3_max: 127
    apples_agent-3_mean: 81.07
    apples_agent-3_min: 26
    apples_agent-4_max: 46
    apples_agent-4_mean: 2.31
    apples_agent-4_min: 0
    apples_agent-5_max: 174
    apples_agent-5_mean: 96.19
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 510
    cleaning_beam_agent-0_mean: 325.88
    cleaning_beam_agent-0_min: 93
    cleaning_beam_agent-1_max: 344
    cleaning_beam_agent-1_mean: 206.28
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 507
    cleaning_beam_agent-2_mean: 288.15
    cleaning_beam_agent-2_min: 99
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 55.57
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 429
    cleaning_beam_agent-4_mean: 331.14
    cleaning_beam_agent-4_min: 133
    cleaning_beam_agent-5_max: 377
    cleaning_beam_agent-5_mean: 86.05
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-55-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 893.9999999999961
  episode_reward_mean: 619.2099999999987
  episode_reward_min: 147.00000000000065
  episodes_this_iter: 96
  episodes_total: 15744
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12901.192
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 1.1269757747650146
        entropy_coeff: 0.0017600000137463212
        kl: 0.009023771621286869
        model: {}
        policy_loss: -0.02695840410888195
        total_loss: -0.025624245405197144
        vf_explained_var: 0.09369343519210815
        vf_loss: 15.1287841796875
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 1.1159173250198364
        entropy_coeff: 0.0017600000137463212
        kl: 0.011075113900005817
        model: {}
        policy_loss: -0.03084782510995865
        total_loss: -0.028986286371946335
        vf_explained_var: 0.03655853867530823
        vf_loss: 16.10528564453125
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 1.119626522064209
        entropy_coeff: 0.0017600000137463212
        kl: 0.010702920146286488
        model: {}
        policy_loss: -0.02838476374745369
        total_loss: -0.02676975354552269
        vf_explained_var: 0.13425683975219727
        vf_loss: 14.449670791625977
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.69129478931427
        entropy_coeff: 0.0017600000137463212
        kl: 0.007736631669104099
        model: {}
        policy_loss: -0.021571142598986626
        total_loss: -0.01996787078678608
        vf_explained_var: 0.2376956343650818
        vf_loss: 12.726255416870117
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 1.0213875770568848
        entropy_coeff: 0.0017600000137463212
        kl: 0.010123079642653465
        model: {}
        policy_loss: -0.029695719480514526
        total_loss: -0.027949970215559006
        vf_explained_var: 0.0902339369058609
        vf_loss: 15.187721252441406
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.9484418034553528
        entropy_coeff: 0.0017600000137463212
        kl: 0.009316832758486271
        model: {}
        policy_loss: -0.027888134121894836
        total_loss: -0.026350930333137512
        vf_explained_var: 0.19546492397785187
        vf_loss: 13.430924415588379
    load_time_ms: 16229.651
    num_steps_sampled: 15744000
    num_steps_trained: 15744000
    sample_time_ms: 127939.797
    update_time_ms: 89.636
  iterations_since_restore: 84
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.467999999999996
    ram_util_percent: 14.796000000000001
  pid: 14340
  policy_reward_max:
    agent-0: 149.00000000000014
    agent-1: 149.00000000000014
    agent-2: 149.00000000000014
    agent-3: 149.00000000000014
    agent-4: 149.00000000000014
    agent-5: 149.00000000000014
  policy_reward_mean:
    agent-0: 103.20166666666695
    agent-1: 103.20166666666695
    agent-2: 103.20166666666695
    agent-3: 103.20166666666695
    agent-4: 103.20166666666695
    agent-5: 103.20166666666695
  policy_reward_min:
    agent-0: 24.50000000000002
    agent-1: 24.50000000000002
    agent-2: 24.50000000000002
    agent-3: 24.50000000000002
    agent-4: 24.50000000000002
    agent-5: 24.50000000000002
  sampler_perf:
    mean_env_wait_ms: 30.61329766432085
    mean_inference_ms: 14.534463399292468
    mean_processing_ms: 65.69568071559247
  time_since_restore: 14219.286108732224
  time_this_iter_s: 156.8982903957367
  time_total_s: 26770.10298013687
  timestamp: 1637049335
  timesteps_since_restore: 8064000
  timesteps_this_iter: 96000
  timesteps_total: 15744000
  training_iteration: 164
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    164 |          26770.1 | 15744000 |   619.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 5.83
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 21.17
    apples_agent-1_min: 0
    apples_agent-2_max: 87
    apples_agent-2_mean: 9.55
    apples_agent-2_min: 0
    apples_agent-3_max: 138
    apples_agent-3_mean: 82.49
    apples_agent-3_min: 28
    apples_agent-4_max: 44
    apples_agent-4_mean: 1.52
    apples_agent-4_min: 0
    apples_agent-5_max: 162
    apples_agent-5_mean: 94.55
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 465
    cleaning_beam_agent-0_mean: 312.72
    cleaning_beam_agent-0_min: 97
    cleaning_beam_agent-1_max: 332
    cleaning_beam_agent-1_mean: 205.55
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 512
    cleaning_beam_agent-2_mean: 294.26
    cleaning_beam_agent-2_min: 120
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 50.24
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 439
    cleaning_beam_agent-4_mean: 333.94
    cleaning_beam_agent-4_min: 234
    cleaning_beam_agent-5_max: 477
    cleaning_beam_agent-5_mean: 83.11
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-58-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 859.9999999999955
  episode_reward_mean: 626.0099999999981
  episode_reward_min: 303.99999999999864
  episodes_this_iter: 96
  episodes_total: 15840
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12911.665
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 1.1343028545379639
        entropy_coeff: 0.0017600000137463212
        kl: 0.009267875924706459
        model: {}
        policy_loss: -0.027443336322903633
        total_loss: -0.026048459112644196
        vf_explained_var: 0.04752042889595032
        vf_loss: 15.376715660095215
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 1.124100685119629
        entropy_coeff: 0.0017600000137463212
        kl: 0.01096055842936039
        model: {}
        policy_loss: -0.030854247510433197
        total_loss: -0.029090063646435738
        vf_explained_var: 0.03975948691368103
        vf_loss: 15.504881858825684
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 1.112379550933838
        entropy_coeff: 0.0017600000137463212
        kl: 0.010159611701965332
        model: {}
        policy_loss: -0.028349366039037704
        total_loss: -0.02685570903122425
        vf_explained_var: 0.12100890278816223
        vf_loss: 14.195231437683105
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.6711201071739197
        entropy_coeff: 0.0017600000137463212
        kl: 0.007067074067890644
        model: {}
        policy_loss: -0.019922247156500816
        total_loss: -0.018484018743038177
        vf_explained_var: 0.25345924496650696
        vf_loss: 12.059843063354492
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 1.02727472782135
        entropy_coeff: 0.0017600000137463212
        kl: 0.01055189874023199
        model: {}
        policy_loss: -0.030077014118433
        total_loss: -0.028278162702918053
        vf_explained_var: 0.07270719110965729
        vf_loss: 14.964750289916992
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.9621523022651672
        entropy_coeff: 0.0017600000137463212
        kl: 0.009775420650839806
        model: {}
        policy_loss: -0.028124120086431503
        total_loss: -0.02655116841197014
        vf_explained_var: 0.1880704164505005
        vf_loss: 13.112528800964355
    load_time_ms: 17816.156
    num_steps_sampled: 15840000
    num_steps_trained: 15840000
    sample_time_ms: 127759.295
    update_time_ms: 98.428
  iterations_since_restore: 85
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.036475409836065
    ram_util_percent: 15.040983606557377
  pid: 14340
  policy_reward_max:
    agent-0: 143.33333333333357
    agent-1: 143.33333333333357
    agent-2: 143.33333333333357
    agent-3: 143.33333333333357
    agent-4: 143.33333333333357
    agent-5: 143.33333333333357
  policy_reward_mean:
    agent-0: 104.33500000000029
    agent-1: 104.33500000000029
    agent-2: 104.33500000000029
    agent-3: 104.33500000000029
    agent-4: 104.33500000000029
    agent-5: 104.33500000000029
  policy_reward_min:
    agent-0: 50.66666666666657
    agent-1: 50.66666666666657
    agent-2: 50.66666666666657
    agent-3: 50.66666666666657
    agent-4: 50.66666666666657
    agent-5: 50.66666666666657
  sampler_perf:
    mean_env_wait_ms: 30.60916833916732
    mean_inference_ms: 14.535078760270212
    mean_processing_ms: 65.69842363319182
  time_since_restore: 14390.307388544083
  time_this_iter_s: 171.02127981185913
  time_total_s: 26941.12425994873
  timestamp: 1637049506
  timesteps_since_restore: 8160000
  timesteps_this_iter: 96000
  timesteps_total: 15840000
  training_iteration: 165
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    165 |          26941.1 | 15840000 |   626.01 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 77
    apples_agent-0_mean: 6.51
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 28.1
    apples_agent-1_min: 0
    apples_agent-2_max: 193
    apples_agent-2_mean: 10.51
    apples_agent-2_min: 0
    apples_agent-3_max: 127
    apples_agent-3_mean: 83.72
    apples_agent-3_min: 35
    apples_agent-4_max: 14
    apples_agent-4_mean: 0.5
    apples_agent-4_min: 0
    apples_agent-5_max: 201
    apples_agent-5_mean: 97.99
    apples_agent-5_min: 28
    cleaning_beam_agent-0_max: 496
    cleaning_beam_agent-0_mean: 311.77
    cleaning_beam_agent-0_min: 59
    cleaning_beam_agent-1_max: 308
    cleaning_beam_agent-1_mean: 197.62
    cleaning_beam_agent-1_min: 74
    cleaning_beam_agent-2_max: 465
    cleaning_beam_agent-2_mean: 290.56
    cleaning_beam_agent-2_min: 120
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 52.45
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 448
    cleaning_beam_agent-4_mean: 339.25
    cleaning_beam_agent-4_min: 222
    cleaning_beam_agent-5_max: 325
    cleaning_beam_agent-5_mean: 70.14
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 7
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-01-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 829.9999999999973
  episode_reward_mean: 623.0499999999984
  episode_reward_min: 267.9999999999988
  episodes_this_iter: 96
  episodes_total: 15936
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13036.981
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 1.131004810333252
        entropy_coeff: 0.0017600000137463212
        kl: 0.009270933456718922
        model: {}
        policy_loss: -0.025634007528424263
        total_loss: -0.024352995678782463
        vf_explained_var: 0.08169004321098328
        vf_loss: 14.173932075500488
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 1.0981894731521606
        entropy_coeff: 0.0017600000137463212
        kl: 0.010678919032216072
        model: {}
        policy_loss: -0.029219739139080048
        total_loss: -0.0274977907538414
        vf_explained_var: 0.01553790271282196
        vf_loss: 15.189811706542969
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 1.1104265451431274
        entropy_coeff: 0.0017600000137463212
        kl: 0.01003536768257618
        model: {}
        policy_loss: -0.02626046910881996
        total_loss: -0.02478579431772232
        vf_explained_var: 0.07926216721534729
        vf_loss: 14.219537734985352
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.6845486164093018
        entropy_coeff: 0.0017600000137463212
        kl: 0.007338426075875759
        model: {}
        policy_loss: -0.01984781213104725
        total_loss: -0.01830030232667923
        vf_explained_var: 0.16837847232818604
        vf_loss: 12.846321105957031
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 1.019366979598999
        entropy_coeff: 0.0017600000137463212
        kl: 0.010506145656108856
        model: {}
        policy_loss: -0.029458431527018547
        total_loss: -0.02771763503551483
        vf_explained_var: 0.07029664516448975
        vf_loss: 14.33655834197998
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.967052698135376
        entropy_coeff: 0.0017600000137463212
        kl: 0.009714392013847828
        model: {}
        policy_loss: -0.027727708220481873
        total_loss: -0.026224873960018158
        vf_explained_var: 0.18165604770183563
        vf_loss: 12.619693756103516
    load_time_ms: 17939.296
    num_steps_sampled: 15936000
    num_steps_trained: 15936000
    sample_time_ms: 128059.246
    update_time_ms: 88.345
  iterations_since_restore: 86
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.294298245614037
    ram_util_percent: 14.727192982456137
  pid: 14340
  policy_reward_max:
    agent-0: 138.3333333333334
    agent-1: 138.3333333333334
    agent-2: 138.3333333333334
    agent-3: 138.3333333333334
    agent-4: 138.3333333333334
    agent-5: 138.3333333333334
  policy_reward_mean:
    agent-0: 103.84166666666695
    agent-1: 103.84166666666695
    agent-2: 103.84166666666695
    agent-3: 103.84166666666695
    agent-4: 103.84166666666695
    agent-5: 103.84166666666695
  policy_reward_min:
    agent-0: 44.66666666666659
    agent-1: 44.66666666666659
    agent-2: 44.66666666666659
    agent-3: 44.66666666666659
    agent-4: 44.66666666666659
    agent-5: 44.66666666666659
  sampler_perf:
    mean_env_wait_ms: 30.602587836280094
    mean_inference_ms: 14.53527580050103
    mean_processing_ms: 65.6932038571597
  time_since_restore: 14550.52507686615
  time_this_iter_s: 160.21768832206726
  time_total_s: 27101.341948270798
  timestamp: 1637049667
  timesteps_since_restore: 8256000
  timesteps_this_iter: 96000
  timesteps_total: 15936000
  training_iteration: 166
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    166 |          27101.3 | 15936000 |   623.05 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 145
    apples_agent-0_mean: 5.08
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 25.55
    apples_agent-1_min: 0
    apples_agent-2_max: 88
    apples_agent-2_mean: 10.77
    apples_agent-2_min: 0
    apples_agent-3_max: 179
    apples_agent-3_mean: 78.78
    apples_agent-3_min: 20
    apples_agent-4_max: 31
    apples_agent-4_mean: 0.7
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 91.17
    apples_agent-5_min: 7
    cleaning_beam_agent-0_max: 468
    cleaning_beam_agent-0_mean: 316.37
    cleaning_beam_agent-0_min: 143
    cleaning_beam_agent-1_max: 313
    cleaning_beam_agent-1_mean: 199.73
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 416
    cleaning_beam_agent-2_mean: 265.08
    cleaning_beam_agent-2_min: 65
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 47.54
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 464
    cleaning_beam_agent-4_mean: 341.11
    cleaning_beam_agent-4_min: 238
    cleaning_beam_agent-5_max: 506
    cleaning_beam_agent-5_mean: 79.98
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-03-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 864.9999999999802
  episode_reward_mean: 620.2899999999971
  episode_reward_min: 263.99999999999807
  episodes_this_iter: 96
  episodes_total: 16032
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13133.835
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 1.140615701675415
        entropy_coeff: 0.0017600000137463212
        kl: 0.008639661595225334
        model: {}
        policy_loss: -0.025399120524525642
        total_loss: -0.024097293615341187
        vf_explained_var: 0.08926184475421906
        vf_loss: 15.813820838928223
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 1.117148756980896
        entropy_coeff: 0.0017600000137463212
        kl: 0.010480194352567196
        model: {}
        policy_loss: -0.0292246975004673
        total_loss: -0.027363521978259087
        vf_explained_var: 0.0028014183044433594
        vf_loss: 17.313215255737305
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 1.1126413345336914
        entropy_coeff: 0.0017600000137463212
        kl: 0.009749077260494232
        model: {}
        policy_loss: -0.02693391777575016
        total_loss: -0.025396540760993958
        vf_explained_var: 0.10943359136581421
        vf_loss: 15.45809555053711
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.6732702255249023
        entropy_coeff: 0.0017600000137463212
        kl: 0.007178137078881264
        model: {}
        policy_loss: -0.01924346573650837
        total_loss: -0.017646219581365585
        vf_explained_var: 0.2231699824333191
        vf_loss: 13.465822219848633
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 1.0125365257263184
        entropy_coeff: 0.0017600000137463212
        kl: 0.0102113988250494
        model: {}
        policy_loss: -0.03004058077931404
        total_loss: -0.02819785103201866
        vf_explained_var: 0.08770202100276947
        vf_loss: 15.825148582458496
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.9697542190551758
        entropy_coeff: 0.0017600000137463212
        kl: 0.009110966697335243
        model: {}
        policy_loss: -0.027352498844265938
        total_loss: -0.02582179382443428
        vf_explained_var: 0.18402181565761566
        vf_loss: 14.152806282043457
    load_time_ms: 18014.873
    num_steps_sampled: 16032000
    num_steps_trained: 16032000
    sample_time_ms: 127978.348
    update_time_ms: 93.376
  iterations_since_restore: 87
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.347767857142856
    ram_util_percent: 14.852678571428571
  pid: 14340
  policy_reward_max:
    agent-0: 144.16666666666686
    agent-1: 144.16666666666686
    agent-2: 144.16666666666686
    agent-3: 144.16666666666686
    agent-4: 144.16666666666686
    agent-5: 144.16666666666686
  policy_reward_mean:
    agent-0: 103.38166666666693
    agent-1: 103.38166666666693
    agent-2: 103.38166666666693
    agent-3: 103.38166666666693
    agent-4: 103.38166666666693
    agent-5: 103.38166666666693
  policy_reward_min:
    agent-0: 43.99999999999992
    agent-1: 43.99999999999992
    agent-2: 43.99999999999992
    agent-3: 43.99999999999992
    agent-4: 43.99999999999992
    agent-5: 43.99999999999992
  sampler_perf:
    mean_env_wait_ms: 30.5985663113381
    mean_inference_ms: 14.536089647664612
    mean_processing_ms: 65.69367781203151
  time_since_restore: 14707.436530590057
  time_this_iter_s: 156.91145372390747
  time_total_s: 27258.253401994705
  timestamp: 1637049824
  timesteps_since_restore: 8352000
  timesteps_this_iter: 96000
  timesteps_total: 16032000
  training_iteration: 167
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    167 |          27258.3 | 16032000 |   620.29 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 2.87
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 22.38
    apples_agent-1_min: 0
    apples_agent-2_max: 96
    apples_agent-2_mean: 9.32
    apples_agent-2_min: 0
    apples_agent-3_max: 132
    apples_agent-3_mean: 75.7
    apples_agent-3_min: 16
    apples_agent-4_max: 28
    apples_agent-4_mean: 1.72
    apples_agent-4_min: 0
    apples_agent-5_max: 178
    apples_agent-5_mean: 95.11
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 466
    cleaning_beam_agent-0_mean: 309.99
    cleaning_beam_agent-0_min: 121
    cleaning_beam_agent-1_max: 319
    cleaning_beam_agent-1_mean: 196.9
    cleaning_beam_agent-1_min: 74
    cleaning_beam_agent-2_max: 449
    cleaning_beam_agent-2_mean: 279.07
    cleaning_beam_agent-2_min: 65
    cleaning_beam_agent-3_max: 144
    cleaning_beam_agent-3_mean: 46.36
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 452
    cleaning_beam_agent-4_mean: 340.76
    cleaning_beam_agent-4_min: 167
    cleaning_beam_agent-5_max: 350
    cleaning_beam_agent-5_mean: 76.86
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-06-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 896.9999999999792
  episode_reward_mean: 615.6299999999968
  episode_reward_min: 109.00000000000082
  episodes_this_iter: 96
  episodes_total: 16128
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13158.336
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 1.1491410732269287
        entropy_coeff: 0.0017600000137463212
        kl: 0.008588386699557304
        model: {}
        policy_loss: -0.02561940625309944
        total_loss: -0.02431386336684227
        vf_explained_var: 0.11222396790981293
        vf_loss: 16.103557586669922
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 1.1168620586395264
        entropy_coeff: 0.0017600000137463212
        kl: 0.010338550433516502
        model: {}
        policy_loss: -0.029263094067573547
        total_loss: -0.027422193437814713
        vf_explained_var: 0.04166291654109955
        vf_loss: 17.3886775970459
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 1.1042777299880981
        entropy_coeff: 0.0017600000137463212
        kl: 0.009474196471273899
        model: {}
        policy_loss: -0.02672802284359932
        total_loss: -0.02514137700200081
        vf_explained_var: 0.09884990751743317
        vf_loss: 16.35333251953125
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.6814050674438477
        entropy_coeff: 0.0017600000137463212
        kl: 0.007038862910121679
        model: {}
        policy_loss: -0.021562110632658005
        total_loss: -0.019950680434703827
        vf_explained_var: 0.22596099972724915
        vf_loss: 14.029309272766113
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 1.0224579572677612
        entropy_coeff: 0.0017600000137463212
        kl: 0.009536855854094028
        model: {}
        policy_loss: -0.029117390513420105
        total_loss: -0.027416663244366646
        vf_explained_var: 0.12182222306728363
        vf_loss: 15.928778648376465
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.9371334314346313
        entropy_coeff: 0.0017600000137463212
        kl: 0.009623460471630096
        model: {}
        policy_loss: -0.02733270451426506
        total_loss: -0.025591246783733368
        vf_explained_var: 0.19218458235263824
        vf_loss: 14.661147117614746
    load_time_ms: 18212.435
    num_steps_sampled: 16128000
    num_steps_trained: 16128000
    sample_time_ms: 127958.841
    update_time_ms: 111.114
  iterations_since_restore: 88
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.333482142857143
    ram_util_percent: 14.800892857142859
  pid: 14340
  policy_reward_max:
    agent-0: 149.50000000000003
    agent-1: 149.50000000000003
    agent-2: 149.50000000000003
    agent-3: 149.50000000000003
    agent-4: 149.50000000000003
    agent-5: 149.50000000000003
  policy_reward_mean:
    agent-0: 102.60500000000026
    agent-1: 102.60500000000026
    agent-2: 102.60500000000026
    agent-3: 102.60500000000026
    agent-4: 102.60500000000026
    agent-5: 102.60500000000026
  policy_reward_min:
    agent-0: 18.166666666666657
    agent-1: 18.166666666666657
    agent-2: 18.166666666666657
    agent-3: 18.166666666666657
    agent-4: 18.166666666666657
    agent-5: 18.166666666666657
  sampler_perf:
    mean_env_wait_ms: 30.59233990211015
    mean_inference_ms: 14.536150692153697
    mean_processing_ms: 65.6937949212049
  time_since_restore: 14864.438827991486
  time_this_iter_s: 157.00229740142822
  time_total_s: 27415.255699396133
  timestamp: 1637049982
  timesteps_since_restore: 8448000
  timesteps_this_iter: 96000
  timesteps_total: 16128000
  training_iteration: 168
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    168 |          27415.3 | 16128000 |   615.63 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 6.3
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 22.95
    apples_agent-1_min: 0
    apples_agent-2_max: 139
    apples_agent-2_mean: 6.9
    apples_agent-2_min: 0
    apples_agent-3_max: 129
    apples_agent-3_mean: 76.16
    apples_agent-3_min: 16
    apples_agent-4_max: 40
    apples_agent-4_mean: 2.53
    apples_agent-4_min: 0
    apples_agent-5_max: 208
    apples_agent-5_mean: 96.74
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 444
    cleaning_beam_agent-0_mean: 305.64
    cleaning_beam_agent-0_min: 85
    cleaning_beam_agent-1_max: 318
    cleaning_beam_agent-1_mean: 200.3
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 464
    cleaning_beam_agent-2_mean: 286.81
    cleaning_beam_agent-2_min: 111
    cleaning_beam_agent-3_max: 102
    cleaning_beam_agent-3_mean: 47.94
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 517
    cleaning_beam_agent-4_mean: 346.11
    cleaning_beam_agent-4_min: 180
    cleaning_beam_agent-5_max: 442
    cleaning_beam_agent-5_mean: 74.72
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-09-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 858.9999999999751
  episode_reward_mean: 618.0199999999967
  episode_reward_min: 109.00000000000082
  episodes_this_iter: 96
  episodes_total: 16224
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13163.348
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 1.1391268968582153
        entropy_coeff: 0.0017600000137463212
        kl: 0.008550304919481277
        model: {}
        policy_loss: -0.025385254994034767
        total_loss: -0.024055037647485733
        vf_explained_var: 0.11412878334522247
        vf_loss: 16.25017738342285
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 1.1115096807479858
        entropy_coeff: 0.0017600000137463212
        kl: 0.0103158513084054
        model: {}
        policy_loss: -0.029332701116800308
        total_loss: -0.02746633067727089
        vf_explained_var: 0.04034025967121124
        vf_loss: 17.594581604003906
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 1.1184214353561401
        entropy_coeff: 0.0017600000137463212
        kl: 0.009377727285027504
        model: {}
        policy_loss: -0.026475906372070312
        total_loss: -0.024959340691566467
        vf_explained_var: 0.12207752466201782
        vf_loss: 16.09438705444336
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.6856153607368469
        entropy_coeff: 0.0017600000137463212
        kl: 0.008209595456719398
        model: {}
        policy_loss: -0.0202014297246933
        total_loss: -0.018387306481599808
        vf_explained_var: 0.24699699878692627
        vf_loss: 13.788846015930176
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 1.012812614440918
        entropy_coeff: 0.0017600000137463212
        kl: 0.009641961194574833
        model: {}
        policy_loss: -0.02851773612201214
        total_loss: -0.02670222520828247
        vf_explained_var: 0.08874545991420746
        vf_loss: 16.696697235107422
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.9417549967765808
        entropy_coeff: 0.0017600000137463212
        kl: 0.009314177557826042
        model: {}
        policy_loss: -0.02695966139435768
        total_loss: -0.02530912309885025
        vf_explained_var: 0.21061058342456818
        vf_loss: 14.451899528503418
    load_time_ms: 18312.998
    num_steps_sampled: 16224000
    num_steps_trained: 16224000
    sample_time_ms: 127878.135
    update_time_ms: 112.022
  iterations_since_restore: 89
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.876734693877555
    ram_util_percent: 14.83755102040816
  pid: 14340
  policy_reward_max:
    agent-0: 143.16666666666674
    agent-1: 143.16666666666674
    agent-2: 143.16666666666674
    agent-3: 143.16666666666674
    agent-4: 143.16666666666674
    agent-5: 143.16666666666674
  policy_reward_mean:
    agent-0: 103.0033333333336
    agent-1: 103.0033333333336
    agent-2: 103.0033333333336
    agent-3: 103.0033333333336
    agent-4: 103.0033333333336
    agent-5: 103.0033333333336
  policy_reward_min:
    agent-0: 18.166666666666657
    agent-1: 18.166666666666657
    agent-2: 18.166666666666657
    agent-3: 18.166666666666657
    agent-4: 18.166666666666657
    agent-5: 18.166666666666657
  sampler_perf:
    mean_env_wait_ms: 30.587107583899204
    mean_inference_ms: 14.53605568214836
    mean_processing_ms: 65.696870799379
  time_since_restore: 15019.52360534668
  time_this_iter_s: 155.0847773551941
  time_total_s: 27570.340476751328
  timestamp: 1637050154
  timesteps_since_restore: 8544000
  timesteps_this_iter: 96000
  timesteps_total: 16224000
  training_iteration: 169
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    169 |          27570.3 | 16224000 |   618.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 4.65
    apples_agent-0_min: 0
    apples_agent-1_max: 111
    apples_agent-1_mean: 28.15
    apples_agent-1_min: 0
    apples_agent-2_max: 190
    apples_agent-2_mean: 7.38
    apples_agent-2_min: 0
    apples_agent-3_max: 149
    apples_agent-3_mean: 81.79
    apples_agent-3_min: 27
    apples_agent-4_max: 50
    apples_agent-4_mean: 1.01
    apples_agent-4_min: 0
    apples_agent-5_max: 178
    apples_agent-5_mean: 98.08
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 449
    cleaning_beam_agent-0_mean: 301.4
    cleaning_beam_agent-0_min: 60
    cleaning_beam_agent-1_max: 347
    cleaning_beam_agent-1_mean: 196.63
    cleaning_beam_agent-1_min: 77
    cleaning_beam_agent-2_max: 507
    cleaning_beam_agent-2_mean: 296.87
    cleaning_beam_agent-2_min: 130
    cleaning_beam_agent-3_max: 182
    cleaning_beam_agent-3_mean: 42.78
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 437
    cleaning_beam_agent-4_mean: 337.76
    cleaning_beam_agent-4_min: 229
    cleaning_beam_agent-5_max: 442
    cleaning_beam_agent-5_mean: 76.1
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-11-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 853.9999999999865
  episode_reward_mean: 639.5999999999958
  episode_reward_min: 303.9999999999996
  episodes_this_iter: 96
  episodes_total: 16320
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13214.485
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 1.1493288278579712
        entropy_coeff: 0.0017600000137463212
        kl: 0.008561444468796253
        model: {}
        policy_loss: -0.024733424186706543
        total_loss: -0.023508723825216293
        vf_explained_var: 0.0494072288274765
        vf_loss: 15.352344512939453
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 1.1127406358718872
        entropy_coeff: 0.0017600000137463212
        kl: 0.010303734801709652
        model: {}
        policy_loss: -0.02926371991634369
        total_loss: -0.02757958695292473
        vf_explained_var: 0.020156845450401306
        vf_loss: 15.81809139251709
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 1.1136887073516846
        entropy_coeff: 0.0017600000137463212
        kl: 0.00939517468214035
        model: {}
        policy_loss: -0.026124203577637672
        total_loss: -0.024736111983656883
        vf_explained_var: 0.08874447643756866
        vf_loss: 14.691484451293945
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 0.6569154262542725
        entropy_coeff: 0.0017600000137463212
        kl: 0.0068815420381724834
        model: {}
        policy_loss: -0.019290462136268616
        total_loss: -0.017821192741394043
        vf_explained_var: 0.2265508770942688
        vf_loss: 12.491317749023438
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 1.018878698348999
        entropy_coeff: 0.0017600000137463212
        kl: 0.010157302021980286
        model: {}
        policy_loss: -0.027723737061023712
        total_loss: -0.025936905294656754
        vf_explained_var: 0.04078996181488037
        vf_loss: 15.485926628112793
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 0.9486621022224426
        entropy_coeff: 0.0017600000137463212
        kl: 0.009590137749910355
        model: {}
        policy_loss: -0.026482578366994858
        total_loss: -0.024877509102225304
        vf_explained_var: 0.15916889905929565
        vf_loss: 13.566848754882812
    load_time_ms: 18570.966
    num_steps_sampled: 16320000
    num_steps_trained: 16320000
    sample_time_ms: 128030.47
    update_time_ms: 139.651
  iterations_since_restore: 90
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.39513274336283
    ram_util_percent: 14.874778761061947
  pid: 14340
  policy_reward_max:
    agent-0: 142.33333333333368
    agent-1: 142.33333333333368
    agent-2: 142.33333333333368
    agent-3: 142.33333333333368
    agent-4: 142.33333333333368
    agent-5: 142.33333333333368
  policy_reward_mean:
    agent-0: 106.60000000000032
    agent-1: 106.60000000000032
    agent-2: 106.60000000000032
    agent-3: 106.60000000000032
    agent-4: 106.60000000000032
    agent-5: 106.60000000000032
  policy_reward_min:
    agent-0: 50.66666666666657
    agent-1: 50.66666666666657
    agent-2: 50.66666666666657
    agent-3: 50.66666666666657
    agent-4: 50.66666666666657
    agent-5: 50.66666666666657
  sampler_perf:
    mean_env_wait_ms: 30.583186451292608
    mean_inference_ms: 14.535720160267745
    mean_processing_ms: 65.69841999265323
  time_since_restore: 15177.408557415009
  time_this_iter_s: 157.88495206832886
  time_total_s: 27728.225428819656
  timestamp: 1637050312
  timesteps_since_restore: 8640000
  timesteps_this_iter: 96000
  timesteps_total: 16320000
  training_iteration: 170
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    170 |          27728.2 | 16320000 |    639.6 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 4.67
    apples_agent-0_min: 0
    apples_agent-1_max: 133
    apples_agent-1_mean: 26.0
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 4.58
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 78.47
    apples_agent-3_min: 16
    apples_agent-4_max: 93
    apples_agent-4_mean: 2.43
    apples_agent-4_min: 0
    apples_agent-5_max: 178
    apples_agent-5_mean: 101.9
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 300.46
    cleaning_beam_agent-0_min: 160
    cleaning_beam_agent-1_max: 310
    cleaning_beam_agent-1_mean: 196.12
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 484
    cleaning_beam_agent-2_mean: 294.56
    cleaning_beam_agent-2_min: 153
    cleaning_beam_agent-3_max: 113
    cleaning_beam_agent-3_mean: 46.59
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 507
    cleaning_beam_agent-4_mean: 342.84
    cleaning_beam_agent-4_min: 162
    cleaning_beam_agent-5_max: 297
    cleaning_beam_agent-5_mean: 66.86
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-14-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 852.9999999999677
  episode_reward_mean: 640.859999999995
  episode_reward_min: 226.99999999999756
  episodes_this_iter: 96
  episodes_total: 16416
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13216.303
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 1.1516321897506714
        entropy_coeff: 0.0017600000137463212
        kl: 0.008533528074622154
        model: {}
        policy_loss: -0.025256164371967316
        total_loss: -0.02406570315361023
        vf_explained_var: 0.11852870881557465
        vf_loss: 15.10627269744873
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 1.1068754196166992
        entropy_coeff: 0.0017600000137463212
        kl: 0.009932955726981163
        model: {}
        policy_loss: -0.028977317735552788
        total_loss: -0.0272190161049366
        vf_explained_var: -0.0022310763597488403
        vf_loss: 17.19813346862793
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 1.1068997383117676
        entropy_coeff: 0.0017600000137463212
        kl: 0.009213507175445557
        model: {}
        policy_loss: -0.02572818472981453
        total_loss: -0.02428976260125637
        vf_explained_var: 0.09951677918434143
        vf_loss: 15.4386568069458
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 0.6583523750305176
        entropy_coeff: 0.0017600000137463212
        kl: 0.0068370443768799305
        model: {}
        policy_loss: -0.01933097466826439
        total_loss: -0.01779896952211857
        vf_explained_var: 0.22860196232795715
        vf_loss: 13.232931137084961
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 1.0268046855926514
        entropy_coeff: 0.0017600000137463212
        kl: 0.009659318253397942
        model: {}
        policy_loss: -0.028581837192177773
        total_loss: -0.02692175656557083
        vf_explained_var: 0.10456196963787079
        vf_loss: 15.353900909423828
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 0.9485104084014893
        entropy_coeff: 0.0017600000137463212
        kl: 0.009468503296375275
        model: {}
        policy_loss: -0.025973806157708168
        total_loss: -0.024337608367204666
        vf_explained_var: 0.17686480283737183
        vf_loss: 14.11874008178711
    load_time_ms: 18876.02
    num_steps_sampled: 16416000
    num_steps_trained: 16416000
    sample_time_ms: 127992.782
    update_time_ms: 161.933
  iterations_since_restore: 91
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.938461538461535
    ram_util_percent: 15.111740890688255
  pid: 14340
  policy_reward_max:
    agent-0: 142.16666666666654
    agent-1: 142.16666666666654
    agent-2: 142.16666666666654
    agent-3: 142.16666666666654
    agent-4: 142.16666666666654
    agent-5: 142.16666666666654
  policy_reward_mean:
    agent-0: 106.81000000000034
    agent-1: 106.81000000000034
    agent-2: 106.81000000000034
    agent-3: 106.81000000000034
    agent-4: 106.81000000000034
    agent-5: 106.81000000000034
  policy_reward_min:
    agent-0: 37.83333333333333
    agent-1: 37.83333333333333
    agent-2: 37.83333333333333
    agent-3: 37.83333333333333
    agent-4: 37.83333333333333
    agent-5: 37.83333333333333
  sampler_perf:
    mean_env_wait_ms: 30.577578732854672
    mean_inference_ms: 14.53597241987082
    mean_processing_ms: 65.69891064949185
  time_since_restore: 15350.682690382004
  time_this_iter_s: 173.27413296699524
  time_total_s: 27901.49956178665
  timestamp: 1637050486
  timesteps_since_restore: 8736000
  timesteps_this_iter: 96000
  timesteps_total: 16416000
  training_iteration: 171
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    171 |          27901.5 | 16416000 |   640.86 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 5.48
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 26.27
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 7.56
    apples_agent-2_min: 0
    apples_agent-3_max: 120
    apples_agent-3_mean: 78.02
    apples_agent-3_min: 22
    apples_agent-4_max: 34
    apples_agent-4_mean: 1.33
    apples_agent-4_min: 0
    apples_agent-5_max: 170
    apples_agent-5_mean: 104.07
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 544
    cleaning_beam_agent-0_mean: 301.74
    cleaning_beam_agent-0_min: 175
    cleaning_beam_agent-1_max: 412
    cleaning_beam_agent-1_mean: 207.56
    cleaning_beam_agent-1_min: 89
    cleaning_beam_agent-2_max: 457
    cleaning_beam_agent-2_mean: 289.94
    cleaning_beam_agent-2_min: 95
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 47.36
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 457
    cleaning_beam_agent-4_mean: 359.79
    cleaning_beam_agent-4_min: 231
    cleaning_beam_agent-5_max: 475
    cleaning_beam_agent-5_mean: 68.24
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-17-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 827.9999999999832
  episode_reward_mean: 643.2699999999955
  episode_reward_min: 247.9999999999961
  episodes_this_iter: 96
  episodes_total: 16512
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13186.631
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 1.1438395977020264
        entropy_coeff: 0.0017600000137463212
        kl: 0.008767210878431797
        model: {}
        policy_loss: -0.024864746257662773
        total_loss: -0.023599807173013687
        vf_explained_var: 0.13618998229503632
        vf_loss: 15.24652099609375
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 1.0968786478042603
        entropy_coeff: 0.0017600000137463212
        kl: 0.009759117849171162
        model: {}
        policy_loss: -0.027944868430495262
        total_loss: -0.02616667002439499
        vf_explained_var: 0.0055337101221084595
        vf_loss: 17.568737030029297
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 1.0995467901229858
        entropy_coeff: 0.0017600000137463212
        kl: 0.009450734592974186
        model: {}
        policy_loss: -0.02719046175479889
        total_loss: -0.025648076087236404
        vf_explained_var: 0.10118377208709717
        vf_loss: 15.874384880065918
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 0.667736828327179
        entropy_coeff: 0.0017600000137463212
        kl: 0.006418818607926369
        model: {}
        policy_loss: -0.018353989347815514
        total_loss: -0.016899554058909416
        vf_explained_var: 0.2379145324230194
        vf_loss: 13.45888900756836
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 1.0011212825775146
        entropy_coeff: 0.0017600000137463212
        kl: 0.009207029826939106
        model: {}
        policy_loss: -0.02719978801906109
        total_loss: -0.02551100216805935
        vf_explained_var: 0.08884270489215851
        vf_loss: 16.093502044677734
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 0.9413257241249084
        entropy_coeff: 0.0017600000137463212
        kl: 0.009246405214071274
        model: {}
        policy_loss: -0.02564118057489395
        total_loss: -0.024000445380806923
        vf_explained_var: 0.1798943430185318
        vf_loss: 14.481913566589355
    load_time_ms: 19385.707
    num_steps_sampled: 16512000
    num_steps_trained: 16512000
    sample_time_ms: 127744.712
    update_time_ms: 160.982
  iterations_since_restore: 92
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.00822510822511
    ram_util_percent: 14.88268398268398
  pid: 14340
  policy_reward_max:
    agent-0: 138.00000000000037
    agent-1: 138.00000000000037
    agent-2: 138.00000000000037
    agent-3: 138.00000000000037
    agent-4: 138.00000000000037
    agent-5: 138.00000000000037
  policy_reward_mean:
    agent-0: 107.211666666667
    agent-1: 107.211666666667
    agent-2: 107.211666666667
    agent-3: 107.211666666667
    agent-4: 107.211666666667
    agent-5: 107.211666666667
  policy_reward_min:
    agent-0: 41.3333333333333
    agent-1: 41.3333333333333
    agent-2: 41.3333333333333
    agent-3: 41.3333333333333
    agent-4: 41.3333333333333
    agent-5: 41.3333333333333
  sampler_perf:
    mean_env_wait_ms: 30.57468996889606
    mean_inference_ms: 14.536219281751999
    mean_processing_ms: 65.70190680064204
  time_since_restore: 15513.04007601738
  time_this_iter_s: 162.35738563537598
  time_total_s: 28063.856947422028
  timestamp: 1637050649
  timesteps_since_restore: 8832000
  timesteps_this_iter: 96000
  timesteps_total: 16512000
  training_iteration: 172
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    172 |          28063.9 | 16512000 |   643.27 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 4.14
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 24.5
    apples_agent-1_min: 0
    apples_agent-2_max: 78
    apples_agent-2_mean: 5.74
    apples_agent-2_min: 0
    apples_agent-3_max: 116
    apples_agent-3_mean: 78.48
    apples_agent-3_min: 26
    apples_agent-4_max: 78
    apples_agent-4_mean: 3.49
    apples_agent-4_min: 0
    apples_agent-5_max: 188
    apples_agent-5_mean: 95.43
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 429
    cleaning_beam_agent-0_mean: 293.87
    cleaning_beam_agent-0_min: 81
    cleaning_beam_agent-1_max: 357
    cleaning_beam_agent-1_mean: 205.44
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 487
    cleaning_beam_agent-2_mean: 303.7
    cleaning_beam_agent-2_min: 113
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 48.17
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 457
    cleaning_beam_agent-4_mean: 345.43
    cleaning_beam_agent-4_min: 184
    cleaning_beam_agent-5_max: 475
    cleaning_beam_agent-5_mean: 78.22
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-20-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 876.9999999999836
  episode_reward_mean: 639.4499999999974
  episode_reward_min: 155.99999999999918
  episodes_this_iter: 96
  episodes_total: 16608
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13192.027
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 1.1516201496124268
        entropy_coeff: 0.0017600000137463212
        kl: 0.008881228044629097
        model: {}
        policy_loss: -0.0253581739962101
        total_loss: -0.024051859974861145
        vf_explained_var: 0.08420157432556152
        vf_loss: 15.569173812866211
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 1.1053900718688965
        entropy_coeff: 0.0017600000137463212
        kl: 0.009693167172372341
        model: {}
        policy_loss: -0.028047053143382072
        total_loss: -0.02637512981891632
        vf_explained_var: 0.012776374816894531
        vf_loss: 16.787702560424805
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 1.0907893180847168
        entropy_coeff: 0.0017600000137463212
        kl: 0.009036713279783726
        model: {}
        policy_loss: -0.025858409702777863
        total_loss: -0.024349752813577652
        vf_explained_var: 0.0475742369890213
        vf_loss: 16.21102523803711
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 0.6689938306808472
        entropy_coeff: 0.0017600000137463212
        kl: 0.006460210308432579
        model: {}
        policy_loss: -0.018832283094525337
        total_loss: -0.017329659312963486
        vf_explained_var: 0.18499353528022766
        vf_loss: 13.880086898803711
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 1.01602303981781
        entropy_coeff: 0.0017600000137463212
        kl: 0.009692462161183357
        model: {}
        policy_loss: -0.028343668207526207
        total_loss: -0.026727523654699326
        vf_explained_var: 0.13778144121170044
        vf_loss: 14.65857219696045
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 0.9413513541221619
        entropy_coeff: 0.0017600000137463212
        kl: 0.008723217062652111
        model: {}
        policy_loss: -0.025754289701581
        total_loss: -0.024232914671301842
        vf_explained_var: 0.15650945901870728
        vf_loss: 14.33513355255127
    load_time_ms: 19366.095
    num_steps_sampled: 16608000
    num_steps_trained: 16608000
    sample_time_ms: 127904.499
    update_time_ms: 149.495
  iterations_since_restore: 93
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.532444444444444
    ram_util_percent: 14.873777777777775
  pid: 14340
  policy_reward_max:
    agent-0: 146.16666666666683
    agent-1: 146.16666666666683
    agent-2: 146.16666666666683
    agent-3: 146.16666666666683
    agent-4: 146.16666666666683
    agent-5: 146.16666666666683
  policy_reward_mean:
    agent-0: 106.57500000000027
    agent-1: 106.57500000000027
    agent-2: 106.57500000000027
    agent-3: 106.57500000000027
    agent-4: 106.57500000000027
    agent-5: 106.57500000000027
  policy_reward_min:
    agent-0: 26.000000000000043
    agent-1: 26.000000000000043
    agent-2: 26.000000000000043
    agent-3: 26.000000000000043
    agent-4: 26.000000000000043
    agent-5: 26.000000000000043
  sampler_perf:
    mean_env_wait_ms: 30.57102907388251
    mean_inference_ms: 14.537837644149613
    mean_processing_ms: 65.70513315894533
  time_since_restore: 15670.649147987366
  time_this_iter_s: 157.60907196998596
  time_total_s: 28221.466019392014
  timestamp: 1637050806
  timesteps_since_restore: 8928000
  timesteps_this_iter: 96000
  timesteps_total: 16608000
  training_iteration: 173
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    173 |          28221.5 | 16608000 |   639.45 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 4.76
    apples_agent-0_min: 0
    apples_agent-1_max: 142
    apples_agent-1_mean: 25.18
    apples_agent-1_min: 0
    apples_agent-2_max: 177
    apples_agent-2_mean: 8.82
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 79.28
    apples_agent-3_min: 35
    apples_agent-4_max: 66
    apples_agent-4_mean: 4.33
    apples_agent-4_min: 0
    apples_agent-5_max: 176
    apples_agent-5_mean: 93.0
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 435
    cleaning_beam_agent-0_mean: 299.17
    cleaning_beam_agent-0_min: 91
    cleaning_beam_agent-1_max: 372
    cleaning_beam_agent-1_mean: 209.58
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 481
    cleaning_beam_agent-2_mean: 285.13
    cleaning_beam_agent-2_min: 107
    cleaning_beam_agent-3_max: 129
    cleaning_beam_agent-3_mean: 48.14
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 450
    cleaning_beam_agent-4_mean: 347.68
    cleaning_beam_agent-4_min: 141
    cleaning_beam_agent-5_max: 448
    cleaning_beam_agent-5_mean: 84.04
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 5
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.04
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-23-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 921.9999999999764
  episode_reward_mean: 626.5399999999966
  episode_reward_min: 195.999999999999
  episodes_this_iter: 96
  episodes_total: 16704
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13191.613
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 1.1533066034317017
        entropy_coeff: 0.0017600000137463212
        kl: 0.008325384929776192
        model: {}
        policy_loss: -0.02472614124417305
        total_loss: -0.02335532009601593
        vf_explained_var: 0.06961369514465332
        vf_loss: 17.355640411376953
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 1.1099141836166382
        entropy_coeff: 0.0017600000137463212
        kl: 0.009353140369057655
        model: {}
        policy_loss: -0.027461104094982147
        total_loss: -0.02570318430662155
        vf_explained_var: 0.012997344136238098
        vf_loss: 18.40741539001465
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 1.0965445041656494
        entropy_coeff: 0.0017600000137463212
        kl: 0.008796274662017822
        model: {}
        policy_loss: -0.02462201565504074
        total_loss: -0.023002443835139275
        vf_explained_var: 0.04069504141807556
        vf_loss: 17.9023380279541
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 0.6873318552970886
        entropy_coeff: 0.0017600000137463212
        kl: 0.00690879113972187
        model: {}
        policy_loss: -0.02000676468014717
        total_loss: -0.0184475164860487
        vf_explained_var: 0.2567620873451233
        vf_loss: 13.87193489074707
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 1.0078561305999756
        entropy_coeff: 0.0017600000137463212
        kl: 0.008884943090379238
        model: {}
        policy_loss: -0.026838481426239014
        total_loss: -0.02521245740354061
        vf_explained_var: 0.12931539118289948
        vf_loss: 16.228626251220703
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 0.9515366554260254
        entropy_coeff: 0.0017600000137463212
        kl: 0.008893197402358055
        model: {}
        policy_loss: -0.025847695767879486
        total_loss: -0.024207476526498795
        vf_explained_var: 0.17707128822803497
        vf_loss: 15.362826347351074
    load_time_ms: 21112.152
    num_steps_sampled: 16704000
    num_steps_trained: 16704000
    sample_time_ms: 127755.237
    update_time_ms: 130.302
  iterations_since_restore: 94
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.93739837398374
    ram_util_percent: 15.09634146341463
  pid: 14340
  policy_reward_max:
    agent-0: 153.6666666666666
    agent-1: 153.6666666666666
    agent-2: 153.6666666666666
    agent-3: 153.6666666666666
    agent-4: 153.6666666666666
    agent-5: 153.6666666666666
  policy_reward_mean:
    agent-0: 104.42333333333363
    agent-1: 104.42333333333363
    agent-2: 104.42333333333363
    agent-3: 104.42333333333363
    agent-4: 104.42333333333363
    agent-5: 104.42333333333363
  policy_reward_min:
    agent-0: 32.6666666666667
    agent-1: 32.6666666666667
    agent-2: 32.6666666666667
    agent-3: 32.6666666666667
    agent-4: 32.6666666666667
    agent-5: 32.6666666666667
  sampler_perf:
    mean_env_wait_ms: 30.566157501689503
    mean_inference_ms: 14.537561279467193
    mean_processing_ms: 65.70459694120206
  time_since_restore: 15843.276145458221
  time_this_iter_s: 172.6269974708557
  time_total_s: 28394.09301686287
  timestamp: 1637050980
  timesteps_since_restore: 9024000
  timesteps_this_iter: 96000
  timesteps_total: 16704000
  training_iteration: 174
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    174 |          28394.1 | 16704000 |   626.54 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 6.31
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 22.13
    apples_agent-1_min: 0
    apples_agent-2_max: 62
    apples_agent-2_mean: 5.31
    apples_agent-2_min: 0
    apples_agent-3_max: 133
    apples_agent-3_mean: 80.72
    apples_agent-3_min: 42
    apples_agent-4_max: 45
    apples_agent-4_mean: 2.1
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 94.24
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 441
    cleaning_beam_agent-0_mean: 290.41
    cleaning_beam_agent-0_min: 143
    cleaning_beam_agent-1_max: 356
    cleaning_beam_agent-1_mean: 203.13
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 462
    cleaning_beam_agent-2_mean: 308.23
    cleaning_beam_agent-2_min: 159
    cleaning_beam_agent-3_max: 129
    cleaning_beam_agent-3_mean: 47.02
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 472
    cleaning_beam_agent-4_mean: 347.46
    cleaning_beam_agent-4_min: 213
    cleaning_beam_agent-5_max: 292
    cleaning_beam_agent-5_mean: 59.95
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-25-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 899.9999999999723
  episode_reward_mean: 644.1999999999957
  episode_reward_min: 295.9999999999959
  episodes_this_iter: 96
  episodes_total: 16800
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13191.419
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 1.156753659248352
        entropy_coeff: 0.0017600000137463212
        kl: 0.007748872973024845
        model: {}
        policy_loss: -0.024109479039907455
        total_loss: -0.022955510765314102
        vf_explained_var: 0.05109383165836334
        vf_loss: 16.40081024169922
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 1.1221177577972412
        entropy_coeff: 0.0017600000137463212
        kl: 0.009875837713479996
        model: {}
        policy_loss: -0.02780032716691494
        total_loss: -0.026087239384651184
        vf_explained_var: 0.007460683584213257
        vf_loss: 17.12844467163086
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 1.0907107591629028
        entropy_coeff: 0.0017600000137463212
        kl: 0.008866723626852036
        model: {}
        policy_loss: -0.02436535246670246
        total_loss: -0.022916946560144424
        vf_explained_var: 0.0762840062379837
        vf_loss: 15.947151184082031
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 0.6656714081764221
        entropy_coeff: 0.0017600000137463212
        kl: 0.006524225696921349
        model: {}
        policy_loss: -0.01888168603181839
        total_loss: -0.01734679937362671
        vf_explained_var: 0.18921643495559692
        vf_loss: 14.016243934631348
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 1.0095560550689697
        entropy_coeff: 0.0017600000137463212
        kl: 0.00923573225736618
        model: {}
        policy_loss: -0.02639429271221161
        total_loss: -0.02477027103304863
        vf_explained_var: 0.10028308629989624
        vf_loss: 15.536911010742188
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 0.9441325664520264
        entropy_coeff: 0.0017600000137463212
        kl: 0.008644942194223404
        model: {}
        policy_loss: -0.024755846709012985
        total_loss: -0.02327352948486805
        vf_explained_var: 0.18009912967681885
        vf_loss: 14.15003776550293
    load_time_ms: 21366.248
    num_steps_sampled: 16800000
    num_steps_trained: 16800000
    sample_time_ms: 128336.916
    update_time_ms: 117.021
  iterations_since_restore: 95
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.10078431372549
    ram_util_percent: 15.105098039215683
  pid: 14340
  policy_reward_max:
    agent-0: 150.0
    agent-1: 150.0
    agent-2: 150.0
    agent-3: 150.0
    agent-4: 150.0
    agent-5: 150.0
  policy_reward_mean:
    agent-0: 107.36666666666699
    agent-1: 107.36666666666699
    agent-2: 107.36666666666699
    agent-3: 107.36666666666699
    agent-4: 107.36666666666699
    agent-5: 107.36666666666699
  policy_reward_min:
    agent-0: 49.33333333333327
    agent-1: 49.33333333333327
    agent-2: 49.33333333333327
    agent-3: 49.33333333333327
    agent-4: 49.33333333333327
    agent-5: 49.33333333333327
  sampler_perf:
    mean_env_wait_ms: 30.560448972593505
    mean_inference_ms: 14.537764042048732
    mean_processing_ms: 65.7043783211621
  time_since_restore: 16022.52736401558
  time_this_iter_s: 179.2512185573578
  time_total_s: 28573.344235420227
  timestamp: 1637051159
  timesteps_since_restore: 9120000
  timesteps_this_iter: 96000
  timesteps_total: 16800000
  training_iteration: 175
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    175 |          28573.3 | 16800000 |    644.2 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 5.19
    apples_agent-0_min: 0
    apples_agent-1_max: 64
    apples_agent-1_mean: 21.06
    apples_agent-1_min: 0
    apples_agent-2_max: 347
    apples_agent-2_mean: 10.66
    apples_agent-2_min: 0
    apples_agent-3_max: 179
    apples_agent-3_mean: 85.32
    apples_agent-3_min: 21
    apples_agent-4_max: 56
    apples_agent-4_mean: 2.61
    apples_agent-4_min: 0
    apples_agent-5_max: 196
    apples_agent-5_mean: 99.84
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 466
    cleaning_beam_agent-0_mean: 307.77
    cleaning_beam_agent-0_min: 91
    cleaning_beam_agent-1_max: 358
    cleaning_beam_agent-1_mean: 205.75
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 559
    cleaning_beam_agent-2_mean: 304.53
    cleaning_beam_agent-2_min: 69
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 45.13
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 452
    cleaning_beam_agent-4_mean: 355.43
    cleaning_beam_agent-4_min: 180
    cleaning_beam_agent-5_max: 573
    cleaning_beam_agent-5_mean: 74.26
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-28-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 900.9999999999818
  episode_reward_mean: 643.109999999996
  episode_reward_min: 150.99999999999986
  episodes_this_iter: 96
  episodes_total: 16896
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13073.943
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 1.1458830833435059
        entropy_coeff: 0.0017600000137463212
        kl: 0.007311265915632248
        model: {}
        policy_loss: -0.022238461300730705
        total_loss: -0.021189136430621147
        vf_explained_var: 0.09344978630542755
        vf_loss: 16.03826332092285
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 1.1105008125305176
        entropy_coeff: 0.0017600000137463212
        kl: 0.00936107523739338
        model: {}
        policy_loss: -0.02679874561727047
        total_loss: -0.02514227107167244
        vf_explained_var: 0.017093539237976074
        vf_loss: 17.387420654296875
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 1.0806888341903687
        entropy_coeff: 0.0017600000137463212
        kl: 0.00914754532277584
        model: {}
        policy_loss: -0.024079814553260803
        total_loss: -0.022443577647209167
        vf_explained_var: 0.034094780683517456
        vf_loss: 17.0873966217041
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 0.6831904649734497
        entropy_coeff: 0.0017600000137463212
        kl: 0.006287319585680962
        model: {}
        policy_loss: -0.018515843898057938
        total_loss: -0.01711999624967575
        vf_explained_var: 0.2416120022535324
        vf_loss: 13.408004760742188
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 1.0034334659576416
        entropy_coeff: 0.0017600000137463212
        kl: 0.008821859955787659
        model: {}
        policy_loss: -0.02652101404964924
        total_loss: -0.025008799508213997
        vf_explained_var: 0.1436854898929596
        vf_loss: 15.13886833190918
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 0.9401117563247681
        entropy_coeff: 0.0017600000137463212
        kl: 0.008049380965530872
        model: {}
        policy_loss: -0.02436523139476776
        total_loss: -0.0229923315346241
        vf_explained_var: 0.19858171045780182
        vf_loss: 14.176193237304688
    load_time_ms: 23202.67
    num_steps_sampled: 16896000
    num_steps_trained: 16896000
    sample_time_ms: 128066.273
    update_time_ms: 120.657
  iterations_since_restore: 96
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.555020080321285
    ram_util_percent: 15.04096385542169
  pid: 14340
  policy_reward_max:
    agent-0: 150.16666666666663
    agent-1: 150.16666666666663
    agent-2: 150.16666666666663
    agent-3: 150.16666666666663
    agent-4: 150.16666666666663
    agent-5: 150.16666666666663
  policy_reward_mean:
    agent-0: 107.18500000000029
    agent-1: 107.18500000000029
    agent-2: 107.18500000000029
    agent-3: 107.18500000000029
    agent-4: 107.18500000000029
    agent-5: 107.18500000000029
  policy_reward_min:
    agent-0: 25.166666666666703
    agent-1: 25.166666666666703
    agent-2: 25.166666666666703
    agent-3: 25.166666666666703
    agent-4: 25.166666666666703
    agent-5: 25.166666666666703
  sampler_perf:
    mean_env_wait_ms: 30.555666812925555
    mean_inference_ms: 14.537837007153655
    mean_processing_ms: 65.70090800725225
  time_since_restore: 16197.2843542099
  time_this_iter_s: 174.75699019432068
  time_total_s: 28748.101225614548
  timestamp: 1637051334
  timesteps_since_restore: 9216000
  timesteps_this_iter: 96000
  timesteps_total: 16896000
  training_iteration: 176
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    176 |          28748.1 | 16896000 |   643.11 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 5.79
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 24.65
    apples_agent-1_min: 0
    apples_agent-2_max: 106
    apples_agent-2_mean: 11.43
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 84.51
    apples_agent-3_min: 15
    apples_agent-4_max: 50
    apples_agent-4_mean: 2.44
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 91.85
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 460
    cleaning_beam_agent-0_mean: 304.62
    cleaning_beam_agent-0_min: 118
    cleaning_beam_agent-1_max: 331
    cleaning_beam_agent-1_mean: 205.08
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 604
    cleaning_beam_agent-2_mean: 307.1
    cleaning_beam_agent-2_min: 133
    cleaning_beam_agent-3_max: 96
    cleaning_beam_agent-3_mean: 42.51
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 455
    cleaning_beam_agent-4_mean: 351.0
    cleaning_beam_agent-4_min: 190
    cleaning_beam_agent-5_max: 539
    cleaning_beam_agent-5_mean: 78.76
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-31-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 898.999999999974
  episode_reward_mean: 658.3599999999944
  episode_reward_min: 280.9999999999988
  episodes_this_iter: 96
  episodes_total: 16992
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12986.188
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 1.1373611688613892
        entropy_coeff: 0.0017600000137463212
        kl: 0.007990123704075813
        model: {}
        policy_loss: -0.02350580506026745
        total_loss: -0.022251902148127556
        vf_explained_var: 0.08426500856876373
        vf_loss: 16.57632064819336
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 1.1111640930175781
        entropy_coeff: 0.0017600000137463212
        kl: 0.008895614184439182
        model: {}
        policy_loss: -0.02659756876528263
        total_loss: -0.02498425915837288
        vf_explained_var: 0.010469838976860046
        vf_loss: 17.89834976196289
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 1.0842859745025635
        entropy_coeff: 0.0017600000137463212
        kl: 0.008570900186896324
        model: {}
        policy_loss: -0.023997150361537933
        total_loss: -0.022488001734018326
        vf_explained_var: 0.058802530169487
        vf_loss: 17.033161163330078
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 0.6670039892196655
        entropy_coeff: 0.0017600000137463212
        kl: 0.006212323438376188
        model: {}
        policy_loss: -0.01851293072104454
        total_loss: -0.017037466168403625
        vf_explained_var: 0.2220182716846466
        vf_loss: 14.069266319274902
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 1.0079067945480347
        entropy_coeff: 0.0017600000137463212
        kl: 0.008790450170636177
        model: {}
        policy_loss: -0.025821886956691742
        total_loss: -0.024239692836999893
        vf_explained_var: 0.11618492007255554
        vf_loss: 15.980189323425293
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 0.9387362599372864
        entropy_coeff: 0.0017600000137463212
        kl: 0.008543743751943111
        model: {}
        policy_loss: -0.024670515209436417
        total_loss: -0.0231429822742939
        vf_explained_var: 0.18731433153152466
        vf_loss: 14.709600448608398
    load_time_ms: 23223.327
    num_steps_sampled: 16992000
    num_steps_trained: 16992000
    sample_time_ms: 128692.909
    update_time_ms: 121.24
  iterations_since_restore: 97
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.10991379310345
    ram_util_percent: 14.880172413793103
  pid: 14340
  policy_reward_max:
    agent-0: 149.83333333333331
    agent-1: 149.83333333333331
    agent-2: 149.83333333333331
    agent-3: 149.83333333333331
    agent-4: 149.83333333333331
    agent-5: 149.83333333333331
  policy_reward_mean:
    agent-0: 109.72666666666699
    agent-1: 109.72666666666699
    agent-2: 109.72666666666699
    agent-3: 109.72666666666699
    agent-4: 109.72666666666699
    agent-5: 109.72666666666699
  policy_reward_min:
    agent-0: 46.83333333333336
    agent-1: 46.83333333333336
    agent-2: 46.83333333333336
    agent-3: 46.83333333333336
    agent-4: 46.83333333333336
    agent-5: 46.83333333333336
  sampler_perf:
    mean_env_wait_ms: 30.551734334480017
    mean_inference_ms: 14.538221737575975
    mean_processing_ms: 65.69934403464103
  time_since_restore: 16359.86956000328
  time_this_iter_s: 162.58520579338074
  time_total_s: 28910.68643140793
  timestamp: 1637051497
  timesteps_since_restore: 9312000
  timesteps_this_iter: 96000
  timesteps_total: 16992000
  training_iteration: 177
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    177 |          28910.7 | 16992000 |   658.36 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 64
    apples_agent-0_mean: 5.27
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 26.73
    apples_agent-1_min: 0
    apples_agent-2_max: 155
    apples_agent-2_mean: 10.54
    apples_agent-2_min: 0
    apples_agent-3_max: 134
    apples_agent-3_mean: 83.66
    apples_agent-3_min: 27
    apples_agent-4_max: 51
    apples_agent-4_mean: 2.43
    apples_agent-4_min: 0
    apples_agent-5_max: 221
    apples_agent-5_mean: 95.51
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 460
    cleaning_beam_agent-0_mean: 301.78
    cleaning_beam_agent-0_min: 115
    cleaning_beam_agent-1_max: 345
    cleaning_beam_agent-1_mean: 197.56
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 597
    cleaning_beam_agent-2_mean: 300.76
    cleaning_beam_agent-2_min: 96
    cleaning_beam_agent-3_max: 99
    cleaning_beam_agent-3_mean: 44.71
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 500
    cleaning_beam_agent-4_mean: 351.68
    cleaning_beam_agent-4_min: 200
    cleaning_beam_agent-5_max: 701
    cleaning_beam_agent-5_mean: 81.44
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-34-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1010.999999999979
  episode_reward_mean: 656.839999999994
  episode_reward_min: 284.99999999999847
  episodes_this_iter: 96
  episodes_total: 17088
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12943.953
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 1.1430516242980957
        entropy_coeff: 0.0017600000137463212
        kl: 0.00790427066385746
        model: {}
        policy_loss: -0.023197481408715248
        total_loss: -0.022052772343158722
        vf_explained_var: 0.07676577568054199
        vf_loss: 15.756242752075195
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 1.1198859214782715
        entropy_coeff: 0.0017600000137463212
        kl: 0.009599441662430763
        model: {}
        policy_loss: -0.026711776852607727
        total_loss: -0.025074491277337074
        vf_explained_var: 0.010787680745124817
        vf_loss: 16.883995056152344
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 1.0905678272247314
        entropy_coeff: 0.0017600000137463212
        kl: 0.008199205622076988
        model: {}
        policy_loss: -0.023403890430927277
        total_loss: -0.022108931094408035
        vf_explained_var: 0.07709123194217682
        vf_loss: 15.745211601257324
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 0.6751202940940857
        entropy_coeff: 0.0017600000137463212
        kl: 0.005902618635445833
        model: {}
        policy_loss: -0.017258524894714355
        total_loss: -0.0159444697201252
        vf_explained_var: 0.22585532069206238
        vf_loss: 13.217421531677246
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 1.0155465602874756
        entropy_coeff: 0.0017600000137463212
        kl: 0.008155453950166702
        model: {}
        policy_loss: -0.024612613022327423
        total_loss: -0.023266471922397614
        vf_explained_var: 0.119624063372612
        vf_loss: 15.024103164672852
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 0.94719398021698
        entropy_coeff: 0.0017600000137463212
        kl: 0.008076833561062813
        model: {}
        policy_loss: -0.024079162627458572
        total_loss: -0.022723769769072533
        vf_explained_var: 0.17510077357292175
        vf_loss: 14.070941925048828
    load_time_ms: 23206.434
    num_steps_sampled: 17088000
    num_steps_trained: 17088000
    sample_time_ms: 128578.261
    update_time_ms: 121.629
  iterations_since_restore: 98
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.36396396396396
    ram_util_percent: 14.802702702702703
  pid: 14340
  policy_reward_max:
    agent-0: 168.4999999999996
    agent-1: 168.4999999999996
    agent-2: 168.4999999999996
    agent-3: 168.4999999999996
    agent-4: 168.4999999999996
    agent-5: 168.4999999999996
  policy_reward_mean:
    agent-0: 109.4733333333336
    agent-1: 109.4733333333336
    agent-2: 109.4733333333336
    agent-3: 109.4733333333336
    agent-4: 109.4733333333336
    agent-5: 109.4733333333336
  policy_reward_min:
    agent-0: 47.499999999999844
    agent-1: 47.499999999999844
    agent-2: 47.499999999999844
    agent-3: 47.499999999999844
    agent-4: 47.499999999999844
    agent-5: 47.499999999999844
  sampler_perf:
    mean_env_wait_ms: 30.548033588275995
    mean_inference_ms: 14.538289106382063
    mean_processing_ms: 65.69887573047323
  time_since_restore: 16515.14816594124
  time_this_iter_s: 155.27860593795776
  time_total_s: 29065.965037345886
  timestamp: 1637051652
  timesteps_since_restore: 9408000
  timesteps_this_iter: 96000
  timesteps_total: 17088000
  training_iteration: 178
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    178 |            29066 | 17088000 |   656.84 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 64
    apples_agent-0_mean: 5.71
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 25.54
    apples_agent-1_min: 0
    apples_agent-2_max: 64
    apples_agent-2_mean: 6.81
    apples_agent-2_min: 0
    apples_agent-3_max: 132
    apples_agent-3_mean: 83.17
    apples_agent-3_min: 27
    apples_agent-4_max: 62
    apples_agent-4_mean: 2.93
    apples_agent-4_min: 0
    apples_agent-5_max: 179
    apples_agent-5_mean: 96.96
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 548
    cleaning_beam_agent-0_mean: 314.54
    cleaning_beam_agent-0_min: 93
    cleaning_beam_agent-1_max: 348
    cleaning_beam_agent-1_mean: 190.99
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 488
    cleaning_beam_agent-2_mean: 305.29
    cleaning_beam_agent-2_min: 110
    cleaning_beam_agent-3_max: 113
    cleaning_beam_agent-3_mean: 44.25
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 461
    cleaning_beam_agent-4_mean: 339.74
    cleaning_beam_agent-4_min: 153
    cleaning_beam_agent-5_max: 701
    cleaning_beam_agent-5_mean: 75.21
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-36-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 891.9999999999881
  episode_reward_mean: 656.7799999999946
  episode_reward_min: 238.99999999999625
  episodes_this_iter: 96
  episodes_total: 17184
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12931.933
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 1.1281076669692993
        entropy_coeff: 0.0017600000137463212
        kl: 0.007778266444802284
        model: {}
        policy_loss: -0.02187281847000122
        total_loss: -0.020569846034049988
        vf_explained_var: 0.08801007270812988
        vf_loss: 17.327877044677734
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 1.1211479902267456
        entropy_coeff: 0.0017600000137463212
        kl: 0.008781570941209793
        model: {}
        policy_loss: -0.026276426389813423
        total_loss: -0.024602079764008522
        vf_explained_var: 0.0071992576122283936
        vf_loss: 18.912494659423828
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 1.0936241149902344
        entropy_coeff: 0.0017600000137463212
        kl: 0.008015656843781471
        model: {}
        policy_loss: -0.02295067347586155
        total_loss: -0.02155877836048603
        vf_explained_var: 0.09768234193325043
        vf_loss: 17.13543128967285
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 0.6527522206306458
        entropy_coeff: 0.0017600000137463212
        kl: 0.006406435277312994
        model: {}
        policy_loss: -0.018317464739084244
        total_loss: -0.01673904061317444
        vf_explained_var: 0.2388104498386383
        vf_loss: 14.459814071655273
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 1.0176297426223755
        entropy_coeff: 0.0017600000137463212
        kl: 0.007989953272044659
        model: {}
        policy_loss: -0.02514166571199894
        total_loss: -0.023601852357387543
        vf_explained_var: 0.08876101672649384
        vf_loss: 17.328550338745117
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 0.9331158399581909
        entropy_coeff: 0.0017600000137463212
        kl: 0.008550272323191166
        model: {}
        policy_loss: -0.023903274908661842
        total_loss: -0.022327814251184464
        vf_explained_var: 0.20699642598628998
        vf_loss: 15.076915740966797
    load_time_ms: 23169.33
    num_steps_sampled: 17184000
    num_steps_trained: 17184000
    sample_time_ms: 128656.823
    update_time_ms: 121.608
  iterations_since_restore: 99
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.111842105263158
    ram_util_percent: 14.682894736842107
  pid: 14340
  policy_reward_max:
    agent-0: 148.66666666666694
    agent-1: 148.66666666666694
    agent-2: 148.66666666666694
    agent-3: 148.66666666666694
    agent-4: 148.66666666666694
    agent-5: 148.66666666666694
  policy_reward_mean:
    agent-0: 109.46333333333361
    agent-1: 109.46333333333361
    agent-2: 109.46333333333361
    agent-3: 109.46333333333361
    agent-4: 109.46333333333361
    agent-5: 109.46333333333361
  policy_reward_min:
    agent-0: 39.833333333333336
    agent-1: 39.833333333333336
    agent-2: 39.833333333333336
    agent-3: 39.833333333333336
    agent-4: 39.833333333333336
    agent-5: 39.833333333333336
  sampler_perf:
    mean_env_wait_ms: 30.543943942286724
    mean_inference_ms: 14.538708665436422
    mean_processing_ms: 65.6993519360828
  time_since_restore: 16671.052543878555
  time_this_iter_s: 155.9043779373169
  time_total_s: 29221.869415283203
  timestamp: 1637051813
  timesteps_since_restore: 9504000
  timesteps_this_iter: 96000
  timesteps_total: 17184000
  training_iteration: 179
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    179 |          29221.9 | 17184000 |   656.78 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 3.31
    apples_agent-0_min: 0
    apples_agent-1_max: 73
    apples_agent-1_mean: 18.78
    apples_agent-1_min: 0
    apples_agent-2_max: 185
    apples_agent-2_mean: 9.7
    apples_agent-2_min: 0
    apples_agent-3_max: 141
    apples_agent-3_mean: 85.61
    apples_agent-3_min: 37
    apples_agent-4_max: 35
    apples_agent-4_mean: 2.24
    apples_agent-4_min: 0
    apples_agent-5_max: 186
    apples_agent-5_mean: 100.72
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 548
    cleaning_beam_agent-0_mean: 320.26
    cleaning_beam_agent-0_min: 178
    cleaning_beam_agent-1_max: 363
    cleaning_beam_agent-1_mean: 198.82
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 475
    cleaning_beam_agent-2_mean: 292.4
    cleaning_beam_agent-2_min: 134
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 42.26
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 446
    cleaning_beam_agent-4_mean: 351.38
    cleaning_beam_agent-4_min: 195
    cleaning_beam_agent-5_max: 405
    cleaning_beam_agent-5_mean: 69.62
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-39-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 919.99999999998
  episode_reward_mean: 661.9899999999948
  episode_reward_min: 278.9999999999973
  episodes_this_iter: 96
  episodes_total: 17280
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12883.77
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 1.1638062000274658
        entropy_coeff: 0.0017600000137463212
        kl: 0.007569083943963051
        model: {}
        policy_loss: -0.021285908296704292
        total_loss: -0.020176908001303673
        vf_explained_var: 0.04813539981842041
        vf_loss: 16.434799194335938
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 1.1293087005615234
        entropy_coeff: 0.0017600000137463212
        kl: 0.008842648938298225
        model: {}
        policy_loss: -0.025243720039725304
        total_loss: -0.023793699219822884
        vf_explained_var: 0.03316757082939148
        vf_loss: 16.690738677978516
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 1.1099995374679565
        entropy_coeff: 0.0017600000137463212
        kl: 0.007770594209432602
        model: {}
        policy_loss: -0.02214084006845951
        total_loss: -0.020910335704684258
        vf_explained_var: 0.05690932273864746
        vf_loss: 16.299854278564453
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 0.6367919445037842
        entropy_coeff: 0.0017600000137463212
        kl: 0.005831799004226923
        model: {}
        policy_loss: -0.01672036200761795
        total_loss: -0.015317019075155258
        vf_explained_var: 0.21389168500900269
        vf_loss: 13.577381134033203
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 1.0047039985656738
        entropy_coeff: 0.0017600000137463212
        kl: 0.008269239217042923
        model: {}
        policy_loss: -0.024383200332522392
        total_loss: -0.0229606032371521
        vf_explained_var: 0.10841307044029236
        vf_loss: 15.370277404785156
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 0.9350150227546692
        entropy_coeff: 0.0017600000137463212
        kl: 0.007799777202308178
        model: {}
        policy_loss: -0.022856704890727997
        total_loss: -0.021558210253715515
        vf_explained_var: 0.19728173315525055
        vf_loss: 13.8416748046875
    load_time_ms: 22997.847
    num_steps_sampled: 17280000
    num_steps_trained: 17280000
    sample_time_ms: 128550.186
    update_time_ms: 96.522
  iterations_since_restore: 100
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.413122171945705
    ram_util_percent: 14.747963800904978
  pid: 14340
  policy_reward_max:
    agent-0: 153.33333333333306
    agent-1: 153.33333333333306
    agent-2: 153.33333333333306
    agent-3: 153.33333333333306
    agent-4: 153.33333333333306
    agent-5: 153.33333333333306
  policy_reward_mean:
    agent-0: 110.33166666666698
    agent-1: 110.33166666666698
    agent-2: 110.33166666666698
    agent-3: 110.33166666666698
    agent-4: 110.33166666666698
    agent-5: 110.33166666666698
  policy_reward_min:
    agent-0: 46.49999999999996
    agent-1: 46.49999999999996
    agent-2: 46.49999999999996
    agent-3: 46.49999999999996
    agent-4: 46.49999999999996
    agent-5: 46.49999999999996
  sampler_perf:
    mean_env_wait_ms: 30.539985983701172
    mean_inference_ms: 14.538538383633458
    mean_processing_ms: 65.6971808664037
  time_since_restore: 16824.959885835648
  time_this_iter_s: 153.90734195709229
  time_total_s: 29375.776757240295
  timestamp: 1637051968
  timesteps_since_restore: 9600000
  timesteps_this_iter: 96000
  timesteps_total: 17280000
  training_iteration: 180
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    180 |          29375.8 | 17280000 |   661.99 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 74
    apples_agent-0_mean: 3.95
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 27.97
    apples_agent-1_min: 0
    apples_agent-2_max: 207
    apples_agent-2_mean: 9.53
    apples_agent-2_min: 0
    apples_agent-3_max: 138
    apples_agent-3_mean: 85.27
    apples_agent-3_min: 17
    apples_agent-4_max: 73
    apples_agent-4_mean: 3.91
    apples_agent-4_min: 0
    apples_agent-5_max: 192
    apples_agent-5_mean: 103.75
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 461
    cleaning_beam_agent-0_mean: 313.06
    cleaning_beam_agent-0_min: 121
    cleaning_beam_agent-1_max: 393
    cleaning_beam_agent-1_mean: 201.13
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 511
    cleaning_beam_agent-2_mean: 300.4
    cleaning_beam_agent-2_min: 151
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 42.09
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 495
    cleaning_beam_agent-4_mean: 352.73
    cleaning_beam_agent-4_min: 209
    cleaning_beam_agent-5_max: 300
    cleaning_beam_agent-5_mean: 74.15
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-42-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 881.9999999999845
  episode_reward_mean: 663.3999999999946
  episode_reward_min: 244.99999999999605
  episodes_this_iter: 96
  episodes_total: 17376
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12880.644
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 1.1383519172668457
        entropy_coeff: 0.0017600000137463212
        kl: 0.00705362344160676
        model: {}
        policy_loss: -0.021043095737695694
        total_loss: -0.020090529695153236
        vf_explained_var: 0.10626411437988281
        vf_loss: 15.45340347290039
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 1.1125974655151367
        entropy_coeff: 0.0017600000137463212
        kl: 0.00855241622775793
        model: {}
        policy_loss: -0.024762049317359924
        total_loss: -0.023299194872379303
        vf_explained_var: 0.011196985840797424
        vf_loss: 17.105409622192383
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 1.0917723178863525
        entropy_coeff: 0.0017600000137463212
        kl: 0.00782646145671606
        model: {}
        policy_loss: -0.022623257711529732
        total_loss: -0.02136019989848137
        vf_explained_var: 0.06399239599704742
        vf_loss: 16.19287872314453
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 0.6419315934181213
        entropy_coeff: 0.0017600000137463212
        kl: 0.006039228290319443
        model: {}
        policy_loss: -0.017636127769947052
        total_loss: -0.01620854251086712
        vf_explained_var: 0.21908852458000183
        vf_loss: 13.49540901184082
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 0.9937419891357422
        entropy_coeff: 0.0017600000137463212
        kl: 0.00822392851114273
        model: {}
        policy_loss: -0.024270756170153618
        total_loss: -0.022843580693006516
        vf_explained_var: 0.11418753862380981
        vf_loss: 15.31375789642334
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 0.9432485103607178
        entropy_coeff: 0.0017600000137463212
        kl: 0.007964697666466236
        model: {}
        policy_loss: -0.023034587502479553
        total_loss: -0.021669715642929077
        vf_explained_var: 0.1717928797006607
        vf_loss: 14.32050895690918
    load_time_ms: 21138.068
    num_steps_sampled: 17376000
    num_steps_trained: 17376000
    sample_time_ms: 128590.716
    update_time_ms: 89.875
  iterations_since_restore: 101
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.491071428571427
    ram_util_percent: 14.696875000000002
  pid: 14340
  policy_reward_max:
    agent-0: 146.99999999999991
    agent-1: 146.99999999999991
    agent-2: 146.99999999999991
    agent-3: 146.99999999999991
    agent-4: 146.99999999999991
    agent-5: 146.99999999999991
  policy_reward_mean:
    agent-0: 110.566666666667
    agent-1: 110.566666666667
    agent-2: 110.566666666667
    agent-3: 110.566666666667
    agent-4: 110.566666666667
    agent-5: 110.566666666667
  policy_reward_min:
    agent-0: 40.8333333333333
    agent-1: 40.8333333333333
    agent-2: 40.8333333333333
    agent-3: 40.8333333333333
    agent-4: 40.8333333333333
    agent-5: 40.8333333333333
  sampler_perf:
    mean_env_wait_ms: 30.535980931846048
    mean_inference_ms: 14.538345983052038
    mean_processing_ms: 65.69720884071609
  time_since_restore: 16979.94765329361
  time_this_iter_s: 154.98776745796204
  time_total_s: 29530.764524698257
  timestamp: 1637052125
  timesteps_since_restore: 9696000
  timesteps_this_iter: 96000
  timesteps_total: 17376000
  training_iteration: 181
  trial_id: '00000'
  
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    181 |          29530.8 | 17376000 |    663.4 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 74
    apples_agent-0_mean: 4.57
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 28.15
    apples_agent-1_min: 0
    apples_agent-2_max: 82
    apples_agent-2_mean: 7.27
    apples_agent-2_min: 0
    apples_agent-3_max: 151
    apples_agent-3_mean: 87.98
    apples_agent-3_min: 46
    apples_agent-4_max: 69
    apples_agent-4_mean: 2.98
    apples_agent-4_min: 0
    apples_agent-5_max: 202
    apples_agent-5_mean: 97.61
    apples_agent-5_min: 11
    cleaning_beam_agent-0_max: 424
    cleaning_beam_agent-0_mean: 308.91
    cleaning_beam_agent-0_min: 136
    cleaning_beam_agent-1_max: 393
    cleaning_beam_agent-1_mean: 211.4
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 520
    cleaning_beam_agent-2_mean: 300.65
    cleaning_beam_agent-2_min: 106
    cleaning_beam_agent-3_max: 142
    cleaning_beam_agent-3_mean: 40.11
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 455
    cleaning_beam_agent-4_mean: 358.68
    cleaning_beam_agent-4_min: 164
    cleaning_beam_agent-5_max: 426
    cleaning_beam_agent-5_mean: 75.02
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-44-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 889.9999999999825
  episode_reward_mean: 670.3299999999936
  episode_reward_min: 275.9999999999967
  episodes_this_iter: 96
  episodes_total: 17472
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12918.478
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 1.1358041763305664
        entropy_coeff: 0.0017600000137463212
        kl: 0.0068528372794389725
        model: {}
        policy_loss: -0.020921219140291214
        total_loss: -0.019877787679433823
        vf_explained_var: 0.11293892562389374
        vf_loss: 16.71880531311035
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 1.107287883758545
        entropy_coeff: 0.0017600000137463212
        kl: 0.008690576069056988
        model: {}
        policy_loss: -0.0249144546687603
        total_loss: -0.023265650495886803
        vf_explained_var: 0.015366435050964355
        vf_loss: 18.595088958740234
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 1.1083606481552124
        entropy_coeff: 0.0017600000137463212
        kl: 0.007289104163646698
        model: {}
        policy_loss: -0.021792184561491013
        total_loss: -0.02058318257331848
        vf_explained_var: 0.09839534759521484
        vf_loss: 17.01899528503418
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 0.6384970545768738
        entropy_coeff: 0.0017600000137463212
        kl: 0.005708122625946999
        model: {}
        policy_loss: -0.017138058319687843
        total_loss: -0.0156828835606575
        vf_explained_var: 0.2377341091632843
        vf_loss: 14.373028755187988
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 1.0013912916183472
        entropy_coeff: 0.0017600000137463212
        kl: 0.007786909583956003
        model: {}
        policy_loss: -0.02367103472352028
        total_loss: -0.022177446633577347
        vf_explained_var: 0.09897240996360779
        vf_loss: 16.986543655395508
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 0.9450868964195251
        entropy_coeff: 0.0017600000137463212
        kl: 0.007759574800729752
        model: {}
        policy_loss: -0.02320805937051773
        total_loss: -0.021801631897687912
        vf_explained_var: 0.19467996060848236
        vf_loss: 15.178664207458496
    load_time_ms: 20578.116
    num_steps_sampled: 17472000
    num_steps_trained: 17472000
    sample_time_ms: 128431.388
    update_time_ms: 89.42
  iterations_since_restore: 102
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.744343891402714
    ram_util_percent: 14.769230769230774
  pid: 14340
  policy_reward_max:
    agent-0: 148.33333333333326
    agent-1: 148.33333333333326
    agent-2: 148.33333333333326
    agent-3: 148.33333333333326
    agent-4: 148.33333333333326
    agent-5: 148.33333333333326
  policy_reward_mean:
    agent-0: 111.72166666666698
    agent-1: 111.72166666666698
    agent-2: 111.72166666666698
    agent-3: 111.72166666666698
    agent-4: 111.72166666666698
    agent-5: 111.72166666666698
  policy_reward_min:
    agent-0: 45.99999999999992
    agent-1: 45.99999999999992
    agent-2: 45.99999999999992
    agent-3: 45.99999999999992
    agent-4: 45.99999999999992
    agent-5: 45.99999999999992
  sampler_perf:
    mean_env_wait_ms: 30.534825327278128
    mean_inference_ms: 14.538266547014354
    mean_processing_ms: 65.69655952284226
  time_since_restore: 17135.542329072952
  time_this_iter_s: 155.59467577934265
  time_total_s: 29686.3592004776
  timestamp: 1637052281
  timesteps_since_restore: 9792000
  timesteps_this_iter: 96000
  timesteps_total: 17472000
  training_iteration: 182
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    182 |          29686.4 | 17472000 |   670.33 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 75
    apples_agent-0_mean: 6.89
    apples_agent-0_min: 0
    apples_agent-1_max: 80
    apples_agent-1_mean: 23.01
    apples_agent-1_min: 0
    apples_agent-2_max: 221
    apples_agent-2_mean: 8.85
    apples_agent-2_min: 0
    apples_agent-3_max: 141
    apples_agent-3_mean: 88.48
    apples_agent-3_min: 36
    apples_agent-4_max: 54
    apples_agent-4_mean: 2.99
    apples_agent-4_min: 0
    apples_agent-5_max: 185
    apples_agent-5_mean: 102.16
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 537
    cleaning_beam_agent-0_mean: 311.52
    cleaning_beam_agent-0_min: 76
    cleaning_beam_agent-1_max: 344
    cleaning_beam_agent-1_mean: 213.17
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 561
    cleaning_beam_agent-2_mean: 296.32
    cleaning_beam_agent-2_min: 71
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 42.91
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 471
    cleaning_beam_agent-4_mean: 349.7
    cleaning_beam_agent-4_min: 104
    cleaning_beam_agent-5_max: 444
    cleaning_beam_agent-5_mean: 71.71
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 4
    fire_beam_agent-3_mean: 0.05
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-47-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 838.9999999999865
  episode_reward_mean: 650.4899999999947
  episode_reward_min: 129.99999999999807
  episodes_this_iter: 96
  episodes_total: 17568
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13012.02
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 1.129034399986267
        entropy_coeff: 0.0017600000137463212
        kl: 0.006837418768554926
        model: {}
        policy_loss: -0.020581867545843124
        total_loss: -0.019530365243554115
        vf_explained_var: 0.10349148511886597
        vf_loss: 16.71118927001953
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 1.108852744102478
        entropy_coeff: 0.0017600000137463212
        kl: 0.008395858108997345
        model: {}
        policy_loss: -0.023421473801136017
        total_loss: -0.02186182513833046
        vf_explained_var: 0.016727998852729797
        vf_loss: 18.320594787597656
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 1.099379062652588
        entropy_coeff: 0.0017600000137463212
        kl: 0.0069325813092291355
        model: {}
        policy_loss: -0.020316269248723984
        total_loss: -0.019152183085680008
        vf_explained_var: 0.08085821568965912
        vf_loss: 17.12478256225586
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 0.652597188949585
        entropy_coeff: 0.0017600000137463212
        kl: 0.005788770504295826
        model: {}
        policy_loss: -0.017178241163492203
        total_loss: -0.015671949833631516
        vf_explained_var: 0.19688551127910614
        vf_loss: 14.97107982635498
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 1.0113897323608398
        entropy_coeff: 0.0017600000137463212
        kl: 0.007905151695013046
        model: {}
        policy_loss: -0.023438503965735435
        total_loss: -0.021959340199828148
        vf_explained_var: 0.09947171807289124
        vf_loss: 16.78182601928711
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 0.9556824564933777
        entropy_coeff: 0.0017600000137463212
        kl: 0.007374512497335672
        model: {}
        policy_loss: -0.021431274712085724
        total_loss: -0.0201015155762434
        vf_explained_var: 0.17512625455856323
        vf_loss: 15.368602752685547
    load_time_ms: 20512.051
    num_steps_sampled: 17568000
    num_steps_trained: 17568000
    sample_time_ms: 128382.546
    update_time_ms: 82.367
  iterations_since_restore: 103
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.645739910313896
    ram_util_percent: 14.673094170403589
  pid: 14340
  policy_reward_max:
    agent-0: 139.8333333333335
    agent-1: 139.8333333333335
    agent-2: 139.8333333333335
    agent-3: 139.8333333333335
    agent-4: 139.8333333333335
    agent-5: 139.8333333333335
  policy_reward_mean:
    agent-0: 108.41500000000033
    agent-1: 108.41500000000033
    agent-2: 108.41500000000033
    agent-3: 108.41500000000033
    agent-4: 108.41500000000033
    agent-5: 108.41500000000033
  policy_reward_min:
    agent-0: 21.66666666666661
    agent-1: 21.66666666666661
    agent-2: 21.66666666666661
    agent-3: 21.66666666666661
    agent-4: 21.66666666666661
    agent-5: 21.66666666666661
  sampler_perf:
    mean_env_wait_ms: 30.533855043346502
    mean_inference_ms: 14.538567307312132
    mean_processing_ms: 65.69999028226697
  time_since_restore: 17292.407118320465
  time_this_iter_s: 156.86478924751282
  time_total_s: 29843.223989725113
  timestamp: 1637052438
  timesteps_since_restore: 9888000
  timesteps_this_iter: 96000
  timesteps_total: 17568000
  training_iteration: 183
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    183 |          29843.2 | 17568000 |   650.49 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 2.93
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 21.79
    apples_agent-1_min: 0
    apples_agent-2_max: 144
    apples_agent-2_mean: 9.98
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 91.94
    apples_agent-3_min: 36
    apples_agent-4_max: 39
    apples_agent-4_mean: 2.09
    apples_agent-4_min: 0
    apples_agent-5_max: 182
    apples_agent-5_mean: 98.98
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 461
    cleaning_beam_agent-0_mean: 331.31
    cleaning_beam_agent-0_min: 137
    cleaning_beam_agent-1_max: 349
    cleaning_beam_agent-1_mean: 211.34
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 430
    cleaning_beam_agent-2_mean: 280.81
    cleaning_beam_agent-2_min: 76
    cleaning_beam_agent-3_max: 90
    cleaning_beam_agent-3_mean: 39.16
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 474
    cleaning_beam_agent-4_mean: 345.18
    cleaning_beam_agent-4_min: 184
    cleaning_beam_agent-5_max: 251
    cleaning_beam_agent-5_mean: 76.73
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-50-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 909.9999999999782
  episode_reward_mean: 675.099999999994
  episode_reward_min: 322.00000000000233
  episodes_this_iter: 96
  episodes_total: 17664
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13024.257
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 1.1420135498046875
        entropy_coeff: 0.0017600000137463212
        kl: 0.006815034430474043
        model: {}
        policy_loss: -0.019684627652168274
        total_loss: -0.018705375492572784
        vf_explained_var: 0.08189663290977478
        vf_loss: 16.261903762817383
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 1.1127656698226929
        entropy_coeff: 0.0017600000137463212
        kl: 0.008214025758206844
        model: {}
        policy_loss: -0.02398473024368286
        total_loss: -0.02256268635392189
        vf_explained_var: 0.021521881222724915
        vf_loss: 17.3770809173584
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 1.115810751914978
        entropy_coeff: 0.0017600000137463212
        kl: 0.0071641081012785435
        model: {}
        policy_loss: -0.020695390179753304
        total_loss: -0.019647741690278053
        vf_explained_var: 0.11085242033004761
        vf_loss: 15.786524772644043
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 0.6096529364585876
        entropy_coeff: 0.0017600000137463212
        kl: 0.005359652452170849
        model: {}
        policy_loss: -0.01633537746965885
        total_loss: -0.014942699111998081
        vf_explained_var: 0.21423311531543732
        vf_loss: 13.937387466430664
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 1.0082826614379883
        entropy_coeff: 0.0017600000137463212
        kl: 0.007680186070501804
        model: {}
        policy_loss: -0.023735765367746353
        total_loss: -0.022424889728426933
        vf_explained_var: 0.12619011104106903
        vf_loss: 15.494169235229492
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 0.9502472877502441
        entropy_coeff: 0.0017600000137463212
        kl: 0.007348780985921621
        model: {}
        policy_loss: -0.02166154235601425
        total_loss: -0.020419849082827568
        vf_explained_var: 0.18667271733283997
        vf_loss: 14.443695068359375
    load_time_ms: 19632.197
    num_steps_sampled: 17664000
    num_steps_trained: 17664000
    sample_time_ms: 128316.765
    update_time_ms: 87.135
  iterations_since_restore: 104
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.78846153846154
    ram_util_percent: 14.894017094017094
  pid: 14340
  policy_reward_max:
    agent-0: 151.6666666666668
    agent-1: 151.6666666666668
    agent-2: 151.6666666666668
    agent-3: 151.6666666666668
    agent-4: 151.6666666666668
    agent-5: 151.6666666666668
  policy_reward_mean:
    agent-0: 112.51666666666694
    agent-1: 112.51666666666694
    agent-2: 112.51666666666694
    agent-3: 112.51666666666694
    agent-4: 112.51666666666694
    agent-5: 112.51666666666694
  policy_reward_min:
    agent-0: 53.66666666666655
    agent-1: 53.66666666666655
    agent-2: 53.66666666666655
    agent-3: 53.66666666666655
    agent-4: 53.66666666666655
    agent-5: 53.66666666666655
  sampler_perf:
    mean_env_wait_ms: 30.53095767380189
    mean_inference_ms: 14.582699778665027
    mean_processing_ms: 65.69916075299469
  time_since_restore: 17455.781660318375
  time_this_iter_s: 163.37454199790955
  time_total_s: 30006.598531723022
  timestamp: 1637052602
  timesteps_since_restore: 9984000
  timesteps_this_iter: 96000
  timesteps_total: 17664000
  training_iteration: 184
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    184 |          30006.6 | 17664000 |    675.1 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 4.79
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 21.68
    apples_agent-1_min: 0
    apples_agent-2_max: 75
    apples_agent-2_mean: 6.67
    apples_agent-2_min: 0
    apples_agent-3_max: 140
    apples_agent-3_mean: 91.52
    apples_agent-3_min: 26
    apples_agent-4_max: 37
    apples_agent-4_mean: 2.73
    apples_agent-4_min: 0
    apples_agent-5_max: 200
    apples_agent-5_mean: 98.78
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 474
    cleaning_beam_agent-0_mean: 311.65
    cleaning_beam_agent-0_min: 123
    cleaning_beam_agent-1_max: 321
    cleaning_beam_agent-1_mean: 198.52
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 507
    cleaning_beam_agent-2_mean: 283.32
    cleaning_beam_agent-2_min: 130
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 45.28
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 464
    cleaning_beam_agent-4_mean: 348.77
    cleaning_beam_agent-4_min: 220
    cleaning_beam_agent-5_max: 293
    cleaning_beam_agent-5_mean: 70.02
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-52-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 898.9999999999781
  episode_reward_mean: 650.1799999999952
  episode_reward_min: 246.999999999996
  episodes_this_iter: 96
  episodes_total: 17760
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13022.633
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 1.1490044593811035
        entropy_coeff: 0.0017600000137463212
        kl: 0.0070714084431529045
        model: {}
        policy_loss: -0.020900486037135124
        total_loss: -0.019842280074954033
        vf_explained_var: 0.11309517920017242
        vf_loss: 16.6617374420166
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 1.1058576107025146
        entropy_coeff: 0.0017600000137463212
        kl: 0.00822652131319046
        model: {}
        policy_loss: -0.02356579899787903
        total_loss: -0.021998204290866852
        vf_explained_var: 0.005916818976402283
        vf_loss: 18.685989379882812
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 1.1117818355560303
        entropy_coeff: 0.0017600000137463212
        kl: 0.007937141694128513
        model: {}
        policy_loss: -0.021607497707009315
        total_loss: -0.02032221481204033
        vf_explained_var: 0.12077394127845764
        vf_loss: 16.5459041595459
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 0.6490821838378906
        entropy_coeff: 0.0017600000137463212
        kl: 0.00551673024892807
        model: {}
        policy_loss: -0.016562391072511673
        total_loss: -0.015128858387470245
        vf_explained_var: 0.21650949120521545
        vf_loss: 14.725712776184082
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 1.0073087215423584
        entropy_coeff: 0.0017600000137463212
        kl: 0.0074561480432748795
        model: {}
        policy_loss: -0.022909771651029587
        total_loss: -0.021562976762652397
        vf_explained_var: 0.1337892860174179
        vf_loss: 16.28422737121582
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 0.961359441280365
        entropy_coeff: 0.0017600000137463212
        kl: 0.00727476179599762
        model: {}
        policy_loss: -0.021945016458630562
        total_loss: -0.0206761434674263
        vf_explained_var: 0.19864821434020996
        vf_loss: 15.059110641479492
    load_time_ms: 17848.621
    num_steps_sampled: 17760000
    num_steps_trained: 17760000
    sample_time_ms: 127767.957
    update_time_ms: 88.697
  iterations_since_restore: 105
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.58783783783784
    ram_util_percent: 14.842342342342342
  pid: 14340
  policy_reward_max:
    agent-0: 149.83333333333348
    agent-1: 149.83333333333348
    agent-2: 149.83333333333348
    agent-3: 149.83333333333348
    agent-4: 149.83333333333348
    agent-5: 149.83333333333348
  policy_reward_mean:
    agent-0: 108.36333333333364
    agent-1: 108.36333333333364
    agent-2: 108.36333333333364
    agent-3: 108.36333333333364
    agent-4: 108.36333333333364
    agent-5: 108.36333333333364
  policy_reward_min:
    agent-0: 41.16666666666659
    agent-1: 41.16666666666659
    agent-2: 41.16666666666659
    agent-3: 41.16666666666659
    agent-4: 41.16666666666659
    agent-5: 41.16666666666659
  sampler_perf:
    mean_env_wait_ms: 30.52639938250364
    mean_inference_ms: 14.588389454863705
    mean_processing_ms: 65.6971192708408
  time_since_restore: 17611.76002883911
  time_this_iter_s: 155.9783685207367
  time_total_s: 30162.57690024376
  timestamp: 1637052758
  timesteps_since_restore: 10080000
  timesteps_this_iter: 96000
  timesteps_total: 17760000
  training_iteration: 185
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    185 |          30162.6 | 17760000 |   650.18 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 113
    apples_agent-0_mean: 5.19
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 22.33
    apples_agent-1_min: 0
    apples_agent-2_max: 231
    apples_agent-2_mean: 10.53
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 87.17
    apples_agent-3_min: 11
    apples_agent-4_max: 64
    apples_agent-4_mean: 2.43
    apples_agent-4_min: 0
    apples_agent-5_max: 175
    apples_agent-5_mean: 97.9
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 421
    cleaning_beam_agent-0_mean: 295.75
    cleaning_beam_agent-0_min: 115
    cleaning_beam_agent-1_max: 348
    cleaning_beam_agent-1_mean: 214.74
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 472
    cleaning_beam_agent-2_mean: 289.58
    cleaning_beam_agent-2_min: 113
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 47.53
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 584
    cleaning_beam_agent-4_mean: 362.14
    cleaning_beam_agent-4_min: 99
    cleaning_beam_agent-5_max: 480
    cleaning_beam_agent-5_mean: 73.55
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-55-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 894.9999999999899
  episode_reward_mean: 667.8399999999936
  episode_reward_min: 223.99999999999764
  episodes_this_iter: 96
  episodes_total: 17856
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13019.496
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 1.1542302370071411
        entropy_coeff: 0.0017600000137463212
        kl: 0.0069678304716944695
        model: {}
        policy_loss: -0.019987236708402634
        total_loss: -0.018930455669760704
        vf_explained_var: 0.09510800242424011
        vf_loss: 16.946544647216797
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 1.1044496297836304
        entropy_coeff: 0.0017600000137463212
        kl: 0.00763595150783658
        model: {}
        policy_loss: -0.022802677005529404
        total_loss: -0.021359505131840706
        vf_explained_var: 0.005922213196754456
        vf_loss: 18.598154067993164
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 1.1044217348098755
        entropy_coeff: 0.0017600000137463212
        kl: 0.007562297396361828
        model: {}
        policy_loss: -0.020919272676110268
        total_loss: -0.019737200811505318
        vf_explained_var: 0.1354139745235443
        vf_loss: 16.13393783569336
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 0.6211545467376709
        entropy_coeff: 0.0017600000137463212
        kl: 0.005493592470884323
        model: {}
        policy_loss: -0.016100682318210602
        total_loss: -0.01464226096868515
        vf_explained_var: 0.22244012355804443
        vf_loss: 14.529389381408691
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 0.9912098050117493
        entropy_coeff: 0.0017600000137463212
        kl: 0.00682807108387351
        model: {}
        policy_loss: -0.021030105650424957
        total_loss: -0.019762573763728142
        vf_explained_var: 0.1173744797706604
        vf_loss: 16.4644832611084
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 0.9464561939239502
        entropy_coeff: 0.0017600000137463212
        kl: 0.007066406309604645
        model: {}
        policy_loss: -0.02156677097082138
        total_loss: -0.02027975395321846
        vf_explained_var: 0.17662538588047028
        vf_loss: 15.394968032836914
    load_time_ms: 17056.616
    num_steps_sampled: 17856000
    num_steps_trained: 17856000
    sample_time_ms: 127650.039
    update_time_ms: 94.159
  iterations_since_restore: 106
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.65338983050847
    ram_util_percent: 15.019915254237285
  pid: 14340
  policy_reward_max:
    agent-0: 149.1666666666667
    agent-1: 149.1666666666667
    agent-2: 149.1666666666667
    agent-3: 149.1666666666667
    agent-4: 149.1666666666667
    agent-5: 149.1666666666667
  policy_reward_mean:
    agent-0: 111.30666666666693
    agent-1: 111.30666666666693
    agent-2: 111.30666666666693
    agent-3: 111.30666666666693
    agent-4: 111.30666666666693
    agent-5: 111.30666666666693
  policy_reward_min:
    agent-0: 37.33333333333333
    agent-1: 37.33333333333333
    agent-2: 37.33333333333333
    agent-3: 37.33333333333333
    agent-4: 37.33333333333333
    agent-5: 37.33333333333333
  sampler_perf:
    mean_env_wait_ms: 30.524979463634764
    mean_inference_ms: 14.58815055792643
    mean_processing_ms: 65.69549557655587
  time_since_restore: 17777.409462690353
  time_this_iter_s: 165.64943385124207
  time_total_s: 30328.226334095
  timestamp: 1637052924
  timesteps_since_restore: 10176000
  timesteps_this_iter: 96000
  timesteps_total: 17856000
  training_iteration: 186
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    186 |          30328.2 | 17856000 |   667.84 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 3.99
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 22.64
    apples_agent-1_min: 0
    apples_agent-2_max: 102
    apples_agent-2_mean: 7.55
    apples_agent-2_min: 0
    apples_agent-3_max: 151
    apples_agent-3_mean: 90.42
    apples_agent-3_min: 20
    apples_agent-4_max: 32
    apples_agent-4_mean: 1.02
    apples_agent-4_min: 0
    apples_agent-5_max: 177
    apples_agent-5_mean: 101.18
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 488
    cleaning_beam_agent-0_mean: 296.2
    cleaning_beam_agent-0_min: 66
    cleaning_beam_agent-1_max: 322
    cleaning_beam_agent-1_mean: 196.8
    cleaning_beam_agent-1_min: 52
    cleaning_beam_agent-2_max: 448
    cleaning_beam_agent-2_mean: 294.92
    cleaning_beam_agent-2_min: 129
    cleaning_beam_agent-3_max: 127
    cleaning_beam_agent-3_mean: 41.25
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 477
    cleaning_beam_agent-4_mean: 356.97
    cleaning_beam_agent-4_min: 201
    cleaning_beam_agent-5_max: 350
    cleaning_beam_agent-5_mean: 68.47
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-58-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 928.9999999999887
  episode_reward_mean: 673.7999999999947
  episode_reward_min: 272.99999999999767
  episodes_this_iter: 96
  episodes_total: 17952
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13020.534
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 1.1589126586914062
        entropy_coeff: 0.0017600000137463212
        kl: 0.006413701456040144
        model: {}
        policy_loss: -0.018735801801085472
        total_loss: -0.017895687371492386
        vf_explained_var: 0.06311078369617462
        vf_loss: 15.970626831054688
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 1.1169135570526123
        entropy_coeff: 0.0017600000137463212
        kl: 0.007424224633723497
        model: {}
        policy_loss: -0.022900870069861412
        total_loss: -0.021674957126379013
        vf_explained_var: -0.0012718290090560913
        vf_loss: 17.068344116210938
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 1.1021190881729126
        entropy_coeff: 0.0017600000137463212
        kl: 0.007131725549697876
        model: {}
        policy_loss: -0.02023887448012829
        total_loss: -0.01915563829243183
        vf_explained_var: 0.06310407817363739
        vf_loss: 15.966176986694336
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 0.6384049654006958
        entropy_coeff: 0.0017600000137463212
        kl: 0.004911961033940315
        model: {}
        policy_loss: -0.014998691156506538
        total_loss: -0.01377999596297741
        vf_explained_var: 0.20291464030742645
        vf_loss: 13.598953247070312
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 1.004195213317871
        entropy_coeff: 0.0017600000137463212
        kl: 0.006739887408912182
        model: {}
        policy_loss: -0.02076834626495838
        total_loss: -0.019648194313049316
        vf_explained_var: 0.09625320136547089
        vf_loss: 15.395584106445312
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 0.9452714323997498
        entropy_coeff: 0.0017600000137463212
        kl: 0.006492984015494585
        model: {}
        policy_loss: -0.020398985594511032
        total_loss: -0.019380133599042892
        vf_explained_var: 0.18722514808177948
        vf_loss: 13.8392915725708
    load_time_ms: 18839.922
    num_steps_sampled: 17952000
    num_steps_trained: 17952000
    sample_time_ms: 127293.942
    update_time_ms: 88.735
  iterations_since_restore: 107
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.231075697211157
    ram_util_percent: 15.079681274900398
  pid: 14340
  policy_reward_max:
    agent-0: 154.83333333333323
    agent-1: 154.83333333333323
    agent-2: 154.83333333333323
    agent-3: 154.83333333333323
    agent-4: 154.83333333333323
    agent-5: 154.83333333333323
  policy_reward_mean:
    agent-0: 112.30000000000027
    agent-1: 112.30000000000027
    agent-2: 112.30000000000027
    agent-3: 112.30000000000027
    agent-4: 112.30000000000027
    agent-5: 112.30000000000027
  policy_reward_min:
    agent-0: 45.49999999999987
    agent-1: 45.49999999999987
    agent-2: 45.49999999999987
    agent-3: 45.49999999999987
    agent-4: 45.49999999999987
    agent-5: 45.49999999999987
  sampler_perf:
    mean_env_wait_ms: 30.521724874444704
    mean_inference_ms: 14.587933122917896
    mean_processing_ms: 65.69546533872483
  time_since_restore: 17953.74205350876
  time_this_iter_s: 176.33259081840515
  time_total_s: 30504.558924913406
  timestamp: 1637053101
  timesteps_since_restore: 10272000
  timesteps_this_iter: 96000
  timesteps_total: 17952000
  training_iteration: 187
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    187 |          30504.6 | 17952000 |    673.8 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 3.22
    apples_agent-0_min: 0
    apples_agent-1_max: 70
    apples_agent-1_mean: 23.18
    apples_agent-1_min: 0
    apples_agent-2_max: 138
    apples_agent-2_mean: 7.23
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 94.4
    apples_agent-3_min: 1
    apples_agent-4_max: 59
    apples_agent-4_mean: 2.11
    apples_agent-4_min: 0
    apples_agent-5_max: 195
    apples_agent-5_mean: 105.85
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 421
    cleaning_beam_agent-0_mean: 306.02
    cleaning_beam_agent-0_min: 138
    cleaning_beam_agent-1_max: 348
    cleaning_beam_agent-1_mean: 195.57
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 462
    cleaning_beam_agent-2_mean: 305.63
    cleaning_beam_agent-2_min: 113
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 38.64
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 471
    cleaning_beam_agent-4_mean: 360.05
    cleaning_beam_agent-4_min: 211
    cleaning_beam_agent-5_max: 512
    cleaning_beam_agent-5_mean: 69.09
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-00-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 929.9999999999956
  episode_reward_mean: 685.5999999999942
  episode_reward_min: 321.0000000000024
  episodes_this_iter: 96
  episodes_total: 18048
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13058.166
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001397952000843361
        entropy: 1.1490719318389893
        entropy_coeff: 0.0017600000137463212
        kl: 0.006118273828178644
        model: {}
        policy_loss: -0.018616974353790283
        total_loss: -0.017789090052247047
        vf_explained_var: 0.09270665049552917
        vf_loss: 16.265979766845703
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001397952000843361
        entropy: 1.122607707977295
        entropy_coeff: 0.0017600000137463212
        kl: 0.0072616771794855595
        model: {}
        policy_loss: -0.021292731165885925
        total_loss: -0.020039374008774757
        vf_explained_var: 0.009610548615455627
        vf_loss: 17.768104553222656
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001397952000843361
        entropy: 1.1014565229415894
        entropy_coeff: 0.0017600000137463212
        kl: 0.006692893803119659
        model: {}
        policy_loss: -0.018746545538306236
        total_loss: -0.017775705084204674
        vf_explained_var: 0.12372273206710815
        vf_loss: 15.708230972290039
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001397952000843361
        entropy: 0.6293823719024658
        entropy_coeff: 0.0017600000137463212
        kl: 0.0058294981718063354
        model: {}
        policy_loss: -0.014992299489676952
        total_loss: -0.01408686675131321
        vf_explained_var: 0.20188133418560028
        vf_loss: 14.301959991455078
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001397952000843361
        entropy: 0.9863697290420532
        entropy_coeff: 0.0017600000137463212
        kl: 0.007221112959086895
        model: {}
        policy_loss: -0.021016810089349747
        total_loss: -0.019678713753819466
        vf_explained_var: 0.09100496768951416
        vf_loss: 16.29882049560547
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001397952000843361
        entropy: 0.9297935962677002
        entropy_coeff: 0.0017600000137463212
        kl: 0.0066941874101758
        model: {}
        policy_loss: -0.020461656153202057
        total_loss: -0.019231047481298447
        vf_explained_var: 0.14661794900894165
        vf_loss: 15.282055854797363
    load_time_ms: 18888.609
    num_steps_sampled: 18048000
    num_steps_trained: 18048000
    sample_time_ms: 127478.195
    update_time_ms: 77.219
  iterations_since_restore: 108
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.33377777777778
    ram_util_percent: 14.89911111111111
  pid: 14340
  policy_reward_max:
    agent-0: 155.0
    agent-1: 155.0
    agent-2: 155.0
    agent-3: 155.0
    agent-4: 155.0
    agent-5: 155.0
  policy_reward_mean:
    agent-0: 114.26666666666695
    agent-1: 114.26666666666695
    agent-2: 114.26666666666695
    agent-3: 114.26666666666695
    agent-4: 114.26666666666695
    agent-5: 114.26666666666695
  policy_reward_min:
    agent-0: 53.499999999999936
    agent-1: 53.499999999999936
    agent-2: 53.499999999999936
    agent-3: 53.499999999999936
    agent-4: 53.499999999999936
    agent-5: 53.499999999999936
  sampler_perf:
    mean_env_wait_ms: 30.519707816696997
    mean_inference_ms: 14.587643763351412
    mean_processing_ms: 65.69520315834191
  time_since_restore: 18111.647796869278
  time_this_iter_s: 157.9057433605194
  time_total_s: 30662.464668273926
  timestamp: 1637053259
  timesteps_since_restore: 10368000
  timesteps_this_iter: 96000
  timesteps_total: 18048000
  training_iteration: 188
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    188 |          30662.5 | 18048000 |    685.6 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 3.44
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 24.22
    apples_agent-1_min: 0
    apples_agent-2_max: 135
    apples_agent-2_mean: 7.69
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 92.02
    apples_agent-3_min: 17
    apples_agent-4_max: 47
    apples_agent-4_mean: 2.41
    apples_agent-4_min: 0
    apples_agent-5_max: 169
    apples_agent-5_mean: 102.52
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 533
    cleaning_beam_agent-0_mean: 313.24
    cleaning_beam_agent-0_min: 61
    cleaning_beam_agent-1_max: 323
    cleaning_beam_agent-1_mean: 195.61
    cleaning_beam_agent-1_min: 87
    cleaning_beam_agent-2_max: 454
    cleaning_beam_agent-2_mean: 299.32
    cleaning_beam_agent-2_min: 112
    cleaning_beam_agent-3_max: 183
    cleaning_beam_agent-3_mean: 41.31
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 589
    cleaning_beam_agent-4_mean: 353.21
    cleaning_beam_agent-4_min: 178
    cleaning_beam_agent-5_max: 512
    cleaning_beam_agent-5_mean: 76.54
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 5
    fire_beam_agent-0_mean: 0.07
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-03-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 979.9999999999819
  episode_reward_mean: 689.279999999992
  episode_reward_min: 285.99999999999926
  episodes_this_iter: 96
  episodes_total: 18144
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13062.654
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001338048023171723
        entropy: 1.1350703239440918
        entropy_coeff: 0.0017600000137463212
        kl: 0.00610858341678977
        model: {}
        policy_loss: -0.018189243972301483
        total_loss: -0.017322978004813194
        vf_explained_var: 0.06866055727005005
        vf_loss: 16.422731399536133
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001338048023171723
        entropy: 1.117617130279541
        entropy_coeff: 0.0017600000137463212
        kl: 0.0073338779620826244
        model: {}
        policy_loss: -0.022502819076180458
        total_loss: -0.02122998610138893
        vf_explained_var: -0.0028841346502304077
        vf_loss: 17.730648040771484
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001338048023171723
        entropy: 1.104714274406433
        entropy_coeff: 0.0017600000137463212
        kl: 0.006380319129675627
        model: {}
        policy_loss: -0.018337413668632507
        total_loss: -0.017416171729564667
        vf_explained_var: 0.09882058203220367
        vf_loss: 15.894758224487305
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001338048023171723
        entropy: 0.6157100200653076
        entropy_coeff: 0.0017600000137463212
        kl: 0.005640356335788965
        model: {}
        policy_loss: -0.014512557536363602
        total_loss: -0.01362333633005619
        vf_explained_var: 0.2017771452665329
        vf_loss: 14.088359832763672
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001338048023171723
        entropy: 1.0016478300094604
        entropy_coeff: 0.0017600000137463212
        kl: 0.006831315811723471
        model: {}
        policy_loss: -0.021359480917453766
        total_loss: -0.02021576464176178
        vf_explained_var: 0.1276472955942154
        vf_loss: 15.403505325317383
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001338048023171723
        entropy: 0.935772180557251
        entropy_coeff: 0.0017600000137463212
        kl: 0.006739810574799776
        model: {}
        policy_loss: -0.019451066851615906
        total_loss: -0.018268469721078873
        vf_explained_var: 0.15975940227508545
        vf_loss: 14.81591796875
    load_time_ms: 18944.893
    num_steps_sampled: 18144000
    num_steps_trained: 18144000
    sample_time_ms: 127450.001
    update_time_ms: 91.167
  iterations_since_restore: 109
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.379729729729725
    ram_util_percent: 14.869369369369366
  pid: 14340
  policy_reward_max:
    agent-0: 163.33333333333314
    agent-1: 163.33333333333314
    agent-2: 163.33333333333314
    agent-3: 163.33333333333314
    agent-4: 163.33333333333314
    agent-5: 163.33333333333314
  policy_reward_mean:
    agent-0: 114.88000000000032
    agent-1: 114.88000000000032
    agent-2: 114.88000000000032
    agent-3: 114.88000000000032
    agent-4: 114.88000000000032
    agent-5: 114.88000000000032
  policy_reward_min:
    agent-0: 47.66666666666655
    agent-1: 47.66666666666655
    agent-2: 47.66666666666655
    agent-3: 47.66666666666655
    agent-4: 47.66666666666655
    agent-5: 47.66666666666655
  sampler_perf:
    mean_env_wait_ms: 30.517308132346457
    mean_inference_ms: 14.587485688214931
    mean_processing_ms: 65.69589228348596
  time_since_restore: 18267.478378772736
  time_this_iter_s: 155.83058190345764
  time_total_s: 30818.295250177383
  timestamp: 1637053415
  timesteps_since_restore: 10464000
  timesteps_this_iter: 96000
  timesteps_total: 18144000
  training_iteration: 189
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    189 |          30818.3 | 18144000 |   689.28 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 68
    apples_agent-0_mean: 4.77
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 23.32
    apples_agent-1_min: 0
    apples_agent-2_max: 84
    apples_agent-2_mean: 6.03
    apples_agent-2_min: 0
    apples_agent-3_max: 169
    apples_agent-3_mean: 89.96
    apples_agent-3_min: 30
    apples_agent-4_max: 57
    apples_agent-4_mean: 3.29
    apples_agent-4_min: 0
    apples_agent-5_max: 176
    apples_agent-5_mean: 100.51
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 500
    cleaning_beam_agent-0_mean: 296.76
    cleaning_beam_agent-0_min: 141
    cleaning_beam_agent-1_max: 316
    cleaning_beam_agent-1_mean: 199.32
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 562
    cleaning_beam_agent-2_mean: 304.57
    cleaning_beam_agent-2_min: 104
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 42.91
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 454
    cleaning_beam_agent-4_mean: 353.97
    cleaning_beam_agent-4_min: 147
    cleaning_beam_agent-5_max: 253
    cleaning_beam_agent-5_mean: 64.05
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-06-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 905.9999999999818
  episode_reward_mean: 669.989999999993
  episode_reward_min: 225.99999999999628
  episodes_this_iter: 96
  episodes_total: 18240
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13094.55
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012781440455000848
        entropy: 1.141462802886963
        entropy_coeff: 0.0017600000137463212
        kl: 0.005681474227458239
        model: {}
        policy_loss: -0.017253080382943153
        total_loss: -0.01652434468269348
        vf_explained_var: 0.08714219927787781
        vf_loss: 16.01416778564453
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012781440455000848
        entropy: 1.1098078489303589
        entropy_coeff: 0.0017600000137463212
        kl: 0.006840295158326626
        model: {}
        policy_loss: -0.020815255120396614
        total_loss: -0.01964152231812477
        vf_explained_var: -0.0028256624937057495
        vf_loss: 17.589340209960938
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012781440455000848
        entropy: 1.093069314956665
        entropy_coeff: 0.0017600000137463212
        kl: 0.006418722216039896
        model: {}
        policy_loss: -0.01808544434607029
        total_loss: -0.017132679000496864
        vf_explained_var: 0.09111478924751282
        vf_loss: 15.92821216583252
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012781440455000848
        entropy: 0.6281044483184814
        entropy_coeff: 0.0017600000137463212
        kl: 0.005437458865344524
        model: {}
        policy_loss: -0.013814490288496017
        total_loss: -0.012994840741157532
        vf_explained_var: 0.21275657415390015
        vf_loss: 13.813661575317383
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012781440455000848
        entropy: 1.0072245597839355
        entropy_coeff: 0.0017600000137463212
        kl: 0.006417912431061268
        model: {}
        policy_loss: -0.0206251572817564
        total_loss: -0.01956198737025261
        vf_explained_var: 0.11516638100147247
        vf_loss: 15.52303695678711
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012781440455000848
        entropy: 0.9452983736991882
        entropy_coeff: 0.0017600000137463212
        kl: 0.0062616001814603806
        model: {}
        policy_loss: -0.019033219665288925
        total_loss: -0.01802569068968296
        vf_explained_var: 0.1911485642194748
        vf_loss: 14.189325332641602
    load_time_ms: 19042.225
    num_steps_sampled: 18240000
    num_steps_trained: 18240000
    sample_time_ms: 127593.721
    update_time_ms: 85.676
  iterations_since_restore: 110
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.35714285714286
    ram_util_percent: 14.86205357142857
  pid: 14340
  policy_reward_max:
    agent-0: 151.00000000000014
    agent-1: 151.00000000000014
    agent-2: 151.00000000000014
    agent-3: 151.00000000000014
    agent-4: 151.00000000000014
    agent-5: 151.00000000000014
  policy_reward_mean:
    agent-0: 111.6650000000003
    agent-1: 111.6650000000003
    agent-2: 111.6650000000003
    agent-3: 111.6650000000003
    agent-4: 111.6650000000003
    agent-5: 111.6650000000003
  policy_reward_min:
    agent-0: 37.666666666666664
    agent-1: 37.666666666666664
    agent-2: 37.666666666666664
    agent-3: 37.666666666666664
    agent-4: 37.666666666666664
    agent-5: 37.666666666666664
  sampler_perf:
    mean_env_wait_ms: 30.51542379526836
    mean_inference_ms: 14.587183341976706
    mean_processing_ms: 65.69587440373431
  time_since_restore: 18424.063777446747
  time_this_iter_s: 156.58539867401123
  time_total_s: 30974.880648851395
  timestamp: 1637053572
  timesteps_since_restore: 10560000
  timesteps_this_iter: 96000
  timesteps_total: 18240000
  training_iteration: 190
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    190 |          30974.9 | 18240000 |   669.99 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 3.37
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 24.61
    apples_agent-1_min: 0
    apples_agent-2_max: 100
    apples_agent-2_mean: 5.47
    apples_agent-2_min: 0
    apples_agent-3_max: 139
    apples_agent-3_mean: 89.03
    apples_agent-3_min: 21
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.31
    apples_agent-4_min: 0
    apples_agent-5_max: 218
    apples_agent-5_mean: 104.42
    apples_agent-5_min: 14
    cleaning_beam_agent-0_max: 450
    cleaning_beam_agent-0_mean: 299.3
    cleaning_beam_agent-0_min: 98
    cleaning_beam_agent-1_max: 341
    cleaning_beam_agent-1_mean: 195.45
    cleaning_beam_agent-1_min: 78
    cleaning_beam_agent-2_max: 545
    cleaning_beam_agent-2_mean: 313.16
    cleaning_beam_agent-2_min: 150
    cleaning_beam_agent-3_max: 225
    cleaning_beam_agent-3_mean: 48.55
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 439
    cleaning_beam_agent-4_mean: 341.64
    cleaning_beam_agent-4_min: 151
    cleaning_beam_agent-5_max: 281
    cleaning_beam_agent-5_mean: 59.06
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-09-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 900.9999999999787
  episode_reward_mean: 691.2999999999923
  episode_reward_min: 145.00000000000108
  episodes_this_iter: 96
  episodes_total: 18336
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13096.513
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012182399950688705
        entropy: 1.1569693088531494
        entropy_coeff: 0.0017600000137463212
        kl: 0.005576319061219692
        model: {}
        policy_loss: -0.01709936372935772
        total_loss: -0.016552304849028587
        vf_explained_var: 0.11283518373966217
        vf_loss: 14.680614471435547
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012182399950688705
        entropy: 1.1046262979507446
        entropy_coeff: 0.0017600000137463212
        kl: 0.006973850540816784
        model: {}
        policy_loss: -0.020305098965764046
        total_loss: -0.019224433228373528
        vf_explained_var: 0.016311392188072205
        vf_loss: 16.30040168762207
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012182399950688705
        entropy: 1.1005523204803467
        entropy_coeff: 0.0017600000137463212
        kl: 0.006357369478791952
        model: {}
        policy_loss: -0.018042393028736115
        total_loss: -0.017209038138389587
        vf_explained_var: 0.09247313439846039
        vf_loss: 14.988502502441406
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012182399950688705
        entropy: 0.6294434666633606
        entropy_coeff: 0.0017600000137463212
        kl: 0.005272537469863892
        model: {}
        policy_loss: -0.014032980427145958
        total_loss: -0.013228603638708591
        vf_explained_var: 0.16296429932117462
        vf_loss: 13.849398612976074
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012182399950688705
        entropy: 1.0191656351089478
        entropy_coeff: 0.0017600000137463212
        kl: 0.006258413195610046
        model: {}
        policy_loss: -0.01961422897875309
        total_loss: -0.018630728125572205
        vf_explained_var: 0.07884801924228668
        vf_loss: 15.255497932434082
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012182399950688705
        entropy: 0.9328050017356873
        entropy_coeff: 0.0017600000137463212
        kl: 0.006103689782321453
        model: {}
        policy_loss: -0.0185170266777277
        total_loss: -0.01757168583571911
        vf_explained_var: 0.17348425090312958
        vf_loss: 13.663405418395996
    load_time_ms: 20931.848
    num_steps_sampled: 18336000
    num_steps_trained: 18336000
    sample_time_ms: 127747.388
    update_time_ms: 79.448
  iterations_since_restore: 111
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.27
    ram_util_percent: 15.108799999999999
  pid: 14340
  policy_reward_max:
    agent-0: 150.1666666666667
    agent-1: 150.1666666666667
    agent-2: 150.1666666666667
    agent-3: 150.1666666666667
    agent-4: 150.1666666666667
    agent-5: 150.1666666666667
  policy_reward_mean:
    agent-0: 115.21666666666702
    agent-1: 115.21666666666702
    agent-2: 115.21666666666702
    agent-3: 115.21666666666702
    agent-4: 115.21666666666702
    agent-5: 115.21666666666702
  policy_reward_min:
    agent-0: 24.166666666666686
    agent-1: 24.166666666666686
    agent-2: 24.166666666666686
    agent-3: 24.166666666666686
    agent-4: 24.166666666666686
    agent-5: 24.166666666666686
  sampler_perf:
    mean_env_wait_ms: 30.51375039235078
    mean_inference_ms: 14.586357855024817
    mean_processing_ms: 65.6960862189079
  time_since_restore: 18599.508428812027
  time_this_iter_s: 175.44465136528015
  time_total_s: 31150.325300216675
  timestamp: 1637053748
  timesteps_since_restore: 10656000
  timesteps_this_iter: 96000
  timesteps_total: 18336000
  training_iteration: 191
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    191 |          31150.3 | 18336000 |    691.3 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 5.52
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 25.68
    apples_agent-1_min: 0
    apples_agent-2_max: 126
    apples_agent-2_mean: 4.36
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 87.48
    apples_agent-3_min: 42
    apples_agent-4_max: 28
    apples_agent-4_mean: 1.2
    apples_agent-4_min: 0
    apples_agent-5_max: 169
    apples_agent-5_mean: 93.97
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 410
    cleaning_beam_agent-0_mean: 286.71
    cleaning_beam_agent-0_min: 130
    cleaning_beam_agent-1_max: 332
    cleaning_beam_agent-1_mean: 189.57
    cleaning_beam_agent-1_min: 77
    cleaning_beam_agent-2_max: 520
    cleaning_beam_agent-2_mean: 310.4
    cleaning_beam_agent-2_min: 138
    cleaning_beam_agent-3_max: 152
    cleaning_beam_agent-3_mean: 45.38
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 469
    cleaning_beam_agent-4_mean: 347.31
    cleaning_beam_agent-4_min: 204
    cleaning_beam_agent-5_max: 573
    cleaning_beam_agent-5_mean: 77.14
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-11-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 906.9999999999847
  episode_reward_mean: 653.969999999996
  episode_reward_min: 284.9999999999987
  episodes_this_iter: 96
  episodes_total: 18432
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13071.125
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00011583360173972324
        entropy: 1.1548486948013306
        entropy_coeff: 0.0017600000137463212
        kl: 0.005141657777130604
        model: {}
        policy_loss: -0.01618434116244316
        total_loss: -0.01557867880910635
        vf_explained_var: 0.0871722549200058
        vf_loss: 16.098644256591797
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00011583360173972324
        entropy: 1.113921880722046
        entropy_coeff: 0.0017600000137463212
        kl: 0.007249647751450539
        model: {}
        policy_loss: -0.02057342603802681
        total_loss: -0.019318578764796257
        vf_explained_var: 0.001333877444267273
        vf_loss: 17.654197692871094
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00011583360173972324
        entropy: 1.0988997220993042
        entropy_coeff: 0.0017600000137463212
        kl: 0.006470667198300362
        model: {}
        policy_loss: -0.018061960116028786
        total_loss: -0.017102694138884544
        vf_explained_var: 0.09556283056735992
        vf_loss: 15.991948127746582
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00011583360173972324
        entropy: 0.6661987900733948
        entropy_coeff: 0.0017600000137463212
        kl: 0.005526629276573658
        model: {}
        policy_loss: -0.014079786837100983
        total_loss: -0.013305766507983208
        vf_explained_var: 0.20993320643901825
        vf_loss: 13.938664436340332
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00011583360173972324
        entropy: 1.0177428722381592
        entropy_coeff: 0.0017600000137463212
        kl: 0.006134511902928352
        model: {}
        policy_loss: -0.019543718546628952
        total_loss: -0.01854359731078148
        vf_explained_var: 0.11370019614696503
        vf_loss: 15.644444465637207
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00011583360173972324
        entropy: 0.9572402834892273
        entropy_coeff: 0.0017600000137463212
        kl: 0.006146939471364021
        model: {}
        policy_loss: -0.018514681607484818
        total_loss: -0.017517821863293648
        vf_explained_var: 0.17784623801708221
        vf_loss: 14.522135734558105
    load_time_ms: 20946.756
    num_steps_sampled: 18432000
    num_steps_trained: 18432000
    sample_time_ms: 128138.901
    update_time_ms: 85.815
  iterations_since_restore: 112
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.295154185022028
    ram_util_percent: 14.831277533039646
  pid: 14340
  policy_reward_max:
    agent-0: 151.16666666666694
    agent-1: 151.16666666666694
    agent-2: 151.16666666666694
    agent-3: 151.16666666666694
    agent-4: 151.16666666666694
    agent-5: 151.16666666666694
  policy_reward_mean:
    agent-0: 108.99500000000033
    agent-1: 108.99500000000033
    agent-2: 108.99500000000033
    agent-3: 108.99500000000033
    agent-4: 108.99500000000033
    agent-5: 108.99500000000033
  policy_reward_min:
    agent-0: 47.4999999999999
    agent-1: 47.4999999999999
    agent-2: 47.4999999999999
    agent-3: 47.4999999999999
    agent-4: 47.4999999999999
    agent-5: 47.4999999999999
  sampler_perf:
    mean_env_wait_ms: 30.51025821620847
    mean_inference_ms: 14.58569821551067
    mean_processing_ms: 65.6973811694296
  time_since_restore: 18758.887066841125
  time_this_iter_s: 159.3786380290985
  time_total_s: 31309.703938245773
  timestamp: 1637053908
  timesteps_since_restore: 10752000
  timesteps_this_iter: 96000
  timesteps_total: 18432000
  training_iteration: 192
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    192 |          31309.7 | 18432000 |   653.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 5.13
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 29.57
    apples_agent-1_min: 0
    apples_agent-2_max: 148
    apples_agent-2_mean: 6.62
    apples_agent-2_min: 0
    apples_agent-3_max: 138
    apples_agent-3_mean: 86.53
    apples_agent-3_min: 42
    apples_agent-4_max: 40
    apples_agent-4_mean: 1.34
    apples_agent-4_min: 0
    apples_agent-5_max: 215
    apples_agent-5_mean: 98.38
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 433
    cleaning_beam_agent-0_mean: 284.85
    cleaning_beam_agent-0_min: 102
    cleaning_beam_agent-1_max: 291
    cleaning_beam_agent-1_mean: 186.98
    cleaning_beam_agent-1_min: 66
    cleaning_beam_agent-2_max: 450
    cleaning_beam_agent-2_mean: 302.71
    cleaning_beam_agent-2_min: 126
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 45.82
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 448
    cleaning_beam_agent-4_mean: 353.11
    cleaning_beam_agent-4_min: 231
    cleaning_beam_agent-5_max: 272
    cleaning_beam_agent-5_mean: 67.3
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 3
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 24
    fire_beam_agent-4_mean: 0.24
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-14-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 893.9999999999773
  episode_reward_mean: 669.0599999999944
  episode_reward_min: 30.999999999998774
  episodes_this_iter: 96
  episodes_total: 18528
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12981.009
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00010984319669660181
        entropy: 1.1599982976913452
        entropy_coeff: 0.0017600000137463212
        kl: 0.005631007719784975
        model: {}
        policy_loss: -0.015759142115712166
        total_loss: -0.01461181789636612
        vf_explained_var: 0.06801600754261017
        vf_loss: 20.627195358276367
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00010984319669660181
        entropy: 1.1121742725372314
        entropy_coeff: 0.0017600000137463212
        kl: 0.006227373145520687
        model: {}
        policy_loss: -0.01769324392080307
        total_loss: -0.016214020550251007
        vf_explained_var: 0.010128647089004517
        vf_loss: 21.911724090576172
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00010984319669660181
        entropy: 1.1038062572479248
        entropy_coeff: 0.0017600000137463212
        kl: 0.005550007801502943
        model: {}
        policy_loss: -0.014934103935956955
        total_loss: -0.013720281422138214
        vf_explained_var: 0.07684442400932312
        vf_loss: 20.46519660949707
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00010984319669660181
        entropy: 0.6649563312530518
        entropy_coeff: 0.0017600000137463212
        kl: 0.005358334165066481
        model: {}
        policy_loss: -0.01268845982849598
        total_loss: -0.011466439813375473
        vf_explained_var: 0.16484302282333374
        vf_loss: 18.56507110595703
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00010984319669660181
        entropy: 0.9993960857391357
        entropy_coeff: 0.0017600000137463212
        kl: 0.006036132574081421
        model: {}
        policy_loss: -0.017440570518374443
        total_loss: -0.0159823726862669
        vf_explained_var: 0.08615978062152863
        vf_loss: 20.099103927612305
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00010984319669660181
        entropy: 0.9337379932403564
        entropy_coeff: 0.0017600000137463212
        kl: 0.005290498491376638
        model: {}
        policy_loss: -0.015996288508176804
        total_loss: -0.014767074957489967
        vf_explained_var: 0.17904673516750336
        vf_loss: 18.144899368286133
    load_time_ms: 22804.359
    num_steps_sampled: 18528000
    num_steps_trained: 18528000
    sample_time_ms: 127942.871
    update_time_ms: 120.088
  iterations_since_restore: 113
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.665182186234816
    ram_util_percent: 15.025506072874492
  pid: 14340
  policy_reward_max:
    agent-0: 149.00000000000034
    agent-1: 149.00000000000034
    agent-2: 149.00000000000034
    agent-3: 149.00000000000034
    agent-4: 149.00000000000034
    agent-5: 149.00000000000034
  policy_reward_mean:
    agent-0: 111.51000000000036
    agent-1: 111.51000000000036
    agent-2: 111.51000000000036
    agent-3: 111.51000000000036
    agent-4: 111.51000000000036
    agent-5: 111.51000000000036
  policy_reward_min:
    agent-0: 5.166666666666627
    agent-1: 5.166666666666627
    agent-2: 5.166666666666627
    agent-3: 5.166666666666627
    agent-4: 5.166666666666627
    agent-5: 5.166666666666627
  sampler_perf:
    mean_env_wait_ms: 30.507144270695804
    mean_inference_ms: 14.585433559335407
    mean_processing_ms: 65.6962435582463
  time_since_restore: 18931.844110250473
  time_this_iter_s: 172.95704340934753
  time_total_s: 31482.66098165512
  timestamp: 1637054081
  timesteps_since_restore: 10848000
  timesteps_this_iter: 96000
  timesteps_total: 18528000
  training_iteration: 193
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    193 |          31482.7 | 18528000 |   669.06 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 3.28
    apples_agent-0_min: 0
    apples_agent-1_max: 132
    apples_agent-1_mean: 27.18
    apples_agent-1_min: 0
    apples_agent-2_max: 101
    apples_agent-2_mean: 5.45
    apples_agent-2_min: 0
    apples_agent-3_max: 166
    apples_agent-3_mean: 89.78
    apples_agent-3_min: 31
    apples_agent-4_max: 40
    apples_agent-4_mean: 2.15
    apples_agent-4_min: 0
    apples_agent-5_max: 170
    apples_agent-5_mean: 100.08
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 495
    cleaning_beam_agent-0_mean: 285.3
    cleaning_beam_agent-0_min: 127
    cleaning_beam_agent-1_max: 341
    cleaning_beam_agent-1_mean: 197.12
    cleaning_beam_agent-1_min: 72
    cleaning_beam_agent-2_max: 644
    cleaning_beam_agent-2_mean: 309.08
    cleaning_beam_agent-2_min: 122
    cleaning_beam_agent-3_max: 109
    cleaning_beam_agent-3_mean: 45.3
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 491
    cleaning_beam_agent-4_mean: 358.49
    cleaning_beam_agent-4_min: 153
    cleaning_beam_agent-5_max: 465
    cleaning_beam_agent-5_mean: 63.63
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-17-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 879.9999999999739
  episode_reward_mean: 681.559999999992
  episode_reward_min: 210.99999999999827
  episodes_this_iter: 96
  episodes_total: 18624
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13032.89
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000103852798929438
        entropy: 1.1455765962600708
        entropy_coeff: 0.0017600000137463212
        kl: 0.005384788848459721
        model: {}
        policy_loss: -0.015886468812823296
        total_loss: -0.015256186947226524
        vf_explained_var: 0.07344524562358856
        vf_loss: 15.6953706741333
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000103852798929438
        entropy: 1.1296675205230713
        entropy_coeff: 0.0017600000137463212
        kl: 0.0063975295051932335
        model: {}
        policy_loss: -0.019137132912874222
        total_loss: -0.018157027661800385
        vf_explained_var: 0.0039579421281814575
        vf_loss: 16.88813591003418
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000103852798929438
        entropy: 1.0954076051712036
        entropy_coeff: 0.0017600000137463212
        kl: 0.0057564470916986465
        model: {}
        policy_loss: -0.01718742400407791
        total_loss: -0.016419876366853714
        vf_explained_var: 0.08908715844154358
        vf_loss: 15.441764831542969
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.000103852798929438
        entropy: 0.6481244564056396
        entropy_coeff: 0.0017600000137463212
        kl: 0.005291495472192764
        model: {}
        policy_loss: -0.013441726565361023
        total_loss: -0.012703927233815193
        vf_explained_var: 0.20363196730613708
        vf_loss: 13.493459701538086
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000103852798929438
        entropy: 0.9952236413955688
        entropy_coeff: 0.0017600000137463212
        kl: 0.0058304313570261
        model: {}
        policy_loss: -0.018211908638477325
        total_loss: -0.017257479950785637
        vf_explained_var: 0.09216800332069397
        vf_loss: 15.399334907531738
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000103852798929438
        entropy: 0.9320158362388611
        entropy_coeff: 0.0017600000137463212
        kl: 0.005455551203340292
        model: {}
        policy_loss: -0.017314746975898743
        total_loss: -0.01644831895828247
        vf_explained_var: 0.16537955403327942
        vf_loss: 14.156625747680664
    load_time_ms: 21884.252
    num_steps_sampled: 18624000
    num_steps_trained: 18624000
    sample_time_ms: 128398.573
    update_time_ms: 115.178
  iterations_since_restore: 114
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.241409691629954
    ram_util_percent: 14.776651982378857
  pid: 14340
  policy_reward_max:
    agent-0: 146.66666666666674
    agent-1: 146.66666666666674
    agent-2: 146.66666666666674
    agent-3: 146.66666666666674
    agent-4: 146.66666666666674
    agent-5: 146.66666666666674
  policy_reward_mean:
    agent-0: 113.59333333333369
    agent-1: 113.59333333333369
    agent-2: 113.59333333333369
    agent-3: 113.59333333333369
    agent-4: 113.59333333333369
    agent-5: 113.59333333333369
  policy_reward_min:
    agent-0: 35.16666666666672
    agent-1: 35.16666666666672
    agent-2: 35.16666666666672
    agent-3: 35.16666666666672
    agent-4: 35.16666666666672
    agent-5: 35.16666666666672
  sampler_perf:
    mean_env_wait_ms: 30.504278181299597
    mean_inference_ms: 14.585159661746012
    mean_processing_ms: 65.69338443444549
  time_since_restore: 19091.058080911636
  time_this_iter_s: 159.21397066116333
  time_total_s: 31641.874952316284
  timestamp: 1637054240
  timesteps_since_restore: 10944000
  timesteps_this_iter: 96000
  timesteps_total: 18624000
  training_iteration: 194
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    194 |          31641.9 | 18624000 |   681.56 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 1.92
    apples_agent-0_min: 0
    apples_agent-1_max: 115
    apples_agent-1_mean: 26.12
    apples_agent-1_min: 0
    apples_agent-2_max: 123
    apples_agent-2_mean: 8.07
    apples_agent-2_min: 0
    apples_agent-3_max: 174
    apples_agent-3_mean: 91.17
    apples_agent-3_min: 32
    apples_agent-4_max: 74
    apples_agent-4_mean: 2.76
    apples_agent-4_min: 0
    apples_agent-5_max: 175
    apples_agent-5_mean: 105.86
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 522
    cleaning_beam_agent-0_mean: 297.92
    cleaning_beam_agent-0_min: 107
    cleaning_beam_agent-1_max: 342
    cleaning_beam_agent-1_mean: 199.26
    cleaning_beam_agent-1_min: 74
    cleaning_beam_agent-2_max: 458
    cleaning_beam_agent-2_mean: 300.98
    cleaning_beam_agent-2_min: 107
    cleaning_beam_agent-3_max: 105
    cleaning_beam_agent-3_mean: 45.09
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 482
    cleaning_beam_agent-4_mean: 364.56
    cleaning_beam_agent-4_min: 191
    cleaning_beam_agent-5_max: 752
    cleaning_beam_agent-5_mean: 75.01
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-19-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 932.9999999999802
  episode_reward_mean: 704.189999999993
  episode_reward_min: 255.99999999999667
  episodes_this_iter: 96
  episodes_total: 18720
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13016.782
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.786240116227418e-05
        entropy: 1.1612460613250732
        entropy_coeff: 0.0017600000137463212
        kl: 0.00555782113224268
        model: {}
        policy_loss: -0.015430696308612823
        total_loss: -0.01474931463599205
        vf_explained_var: 0.06828945875167847
        vf_loss: 16.136123657226562
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.786240116227418e-05
        entropy: 1.1130311489105225
        entropy_coeff: 0.0017600000137463212
        kl: 0.006362168118357658
        model: {}
        policy_loss: -0.018591808155179024
        total_loss: -0.017549192532896996
        vf_explained_var: 0.00786939263343811
        vf_loss: 17.291141510009766
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.786240116227418e-05
        entropy: 1.1091651916503906
        entropy_coeff: 0.0017600000137463212
        kl: 0.0055764056742191315
        model: {}
        policy_loss: -0.0152767738327384
        total_loss: -0.01454117801040411
        vf_explained_var: 0.09603923559188843
        vf_loss: 15.724432945251465
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.786240116227418e-05
        entropy: 0.6231518983840942
        entropy_coeff: 0.0017600000137463212
        kl: 0.0043756989762187
        model: {}
        policy_loss: -0.011961039155721664
        total_loss: -0.011164742521941662
        vf_explained_var: 0.16083845496177673
        vf_loss: 14.5547456741333
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.786240116227418e-05
        entropy: 0.9984619617462158
        entropy_coeff: 0.0017600000137463212
        kl: 0.005656037479639053
        model: {}
        policy_loss: -0.018314335495233536
        total_loss: -0.017382608726620674
        vf_explained_var: 0.10588677227497101
        vf_loss: 15.578158378601074
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.786240116227418e-05
        entropy: 0.9202456474304199
        entropy_coeff: 0.0017600000137463212
        kl: 0.005370961502194405
        model: {}
        policy_loss: -0.01685597002506256
        total_loss: -0.015930062159895897
        vf_explained_var: 0.15250223875045776
        vf_loss: 14.7135009765625
    load_time_ms: 21816.172
    num_steps_sampled: 18720000
    num_steps_trained: 18720000
    sample_time_ms: 128347.069
    update_time_ms: 119.124
  iterations_since_restore: 115
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.658181818181816
    ram_util_percent: 14.765000000000002
  pid: 14340
  policy_reward_max:
    agent-0: 155.50000000000009
    agent-1: 155.50000000000009
    agent-2: 155.50000000000009
    agent-3: 155.50000000000009
    agent-4: 155.50000000000009
    agent-5: 155.50000000000009
  policy_reward_mean:
    agent-0: 117.36500000000032
    agent-1: 117.36500000000032
    agent-2: 117.36500000000032
    agent-3: 117.36500000000032
    agent-4: 117.36500000000032
    agent-5: 117.36500000000032
  policy_reward_min:
    agent-0: 42.66666666666667
    agent-1: 42.66666666666667
    agent-2: 42.66666666666667
    agent-3: 42.66666666666667
    agent-4: 42.66666666666667
    agent-5: 42.66666666666667
  sampler_perf:
    mean_env_wait_ms: 30.50185536002298
    mean_inference_ms: 14.584755279905423
    mean_processing_ms: 65.69316247294891
  time_since_restore: 19245.65504217148
  time_this_iter_s: 154.59696125984192
  time_total_s: 31796.471913576126
  timestamp: 1637054395
  timesteps_since_restore: 11040000
  timesteps_this_iter: 96000
  timesteps_total: 18720000
  training_iteration: 195
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    195 |          31796.5 | 18720000 |   704.19 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 1.84
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 26.07
    apples_agent-1_min: 0
    apples_agent-2_max: 95
    apples_agent-2_mean: 4.97
    apples_agent-2_min: 0
    apples_agent-3_max: 156
    apples_agent-3_mean: 94.27
    apples_agent-3_min: 32
    apples_agent-4_max: 34
    apples_agent-4_mean: 1.1
    apples_agent-4_min: 0
    apples_agent-5_max: 164
    apples_agent-5_mean: 107.47
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 522
    cleaning_beam_agent-0_mean: 308.84
    cleaning_beam_agent-0_min: 164
    cleaning_beam_agent-1_max: 329
    cleaning_beam_agent-1_mean: 187.88
    cleaning_beam_agent-1_min: 53
    cleaning_beam_agent-2_max: 465
    cleaning_beam_agent-2_mean: 299.0
    cleaning_beam_agent-2_min: 134
    cleaning_beam_agent-3_max: 123
    cleaning_beam_agent-3_mean: 46.1
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 470
    cleaning_beam_agent-4_mean: 377.62
    cleaning_beam_agent-4_min: 268
    cleaning_beam_agent-5_max: 752
    cleaning_beam_agent-5_mean: 57.29
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-22-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 946.9999999999702
  episode_reward_mean: 738.2899999999895
  episode_reward_min: 285.99999999999886
  episodes_this_iter: 96
  episodes_total: 18816
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13146.328
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.187200339511037e-05
        entropy: 1.1570885181427002
        entropy_coeff: 0.0017600000137463212
        kl: 0.005099156405776739
        model: {}
        policy_loss: -0.014229390770196915
        total_loss: -0.01368258148431778
        vf_explained_var: 0.042572274804115295
        vf_loss: 15.634498596191406
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.187200339511037e-05
        entropy: 1.1313445568084717
        entropy_coeff: 0.0017600000137463212
        kl: 0.006202667020261288
        model: {}
        policy_loss: -0.01771588809788227
        total_loss: -0.016849420964717865
        vf_explained_var: 0.015229403972625732
        vf_loss: 16.1710205078125
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.187200339511037e-05
        entropy: 1.111142873764038
        entropy_coeff: 0.0017600000137463212
        kl: 0.005311255343258381
        model: {}
        policy_loss: -0.01457164715975523
        total_loss: -0.014016159810125828
        vf_explained_var: 0.11592485010623932
        vf_loss: 14.48848819732666
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 9.187200339511037e-05
        entropy: 0.6202703714370728
        entropy_coeff: 0.0017600000137463212
        kl: 0.004467183258384466
        model: {}
        policy_loss: -0.011186271905899048
        total_loss: -0.010600144974887371
        vf_explained_var: 0.110277459025383
        vf_loss: 14.54443359375
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.187200339511037e-05
        entropy: 0.9891163110733032
        entropy_coeff: 0.0017600000137463212
        kl: 0.005536519922316074
        model: {}
        policy_loss: -0.01682043820619583
        total_loss: -0.01589192822575569
        vf_explained_var: 0.04419530928134918
        vf_loss: 15.620491027832031
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.187200339511037e-05
        entropy: 0.9066833257675171
        entropy_coeff: 0.0017600000137463212
        kl: 0.0049252985045313835
        model: {}
        policy_loss: -0.01553352177143097
        total_loss: -0.014754744246602058
        vf_explained_var: 0.14772570133209229
        vf_loss: 13.894755363464355
    load_time_ms: 20813.901
    num_steps_sampled: 18816000
    num_steps_trained: 18816000
    sample_time_ms: 128429.421
    update_time_ms: 110.405
  iterations_since_restore: 116
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.470222222222223
    ram_util_percent: 14.869777777777777
  pid: 14340
  policy_reward_max:
    agent-0: 157.83333333333312
    agent-1: 157.83333333333312
    agent-2: 157.83333333333312
    agent-3: 157.83333333333312
    agent-4: 157.83333333333312
    agent-5: 157.83333333333312
  policy_reward_mean:
    agent-0: 123.04833333333369
    agent-1: 123.04833333333369
    agent-2: 123.04833333333369
    agent-3: 123.04833333333369
    agent-4: 123.04833333333369
    agent-5: 123.04833333333369
  policy_reward_min:
    agent-0: 47.66666666666657
    agent-1: 47.66666666666657
    agent-2: 47.66666666666657
    agent-3: 47.66666666666657
    agent-4: 47.66666666666657
    agent-5: 47.66666666666657
  sampler_perf:
    mean_env_wait_ms: 30.49935377850185
    mean_inference_ms: 14.598332425304918
    mean_processing_ms: 65.69518629972931
  time_since_restore: 19403.403593301773
  time_this_iter_s: 157.7485511302948
  time_total_s: 31954.22046470642
  timestamp: 1637054554
  timesteps_since_restore: 11136000
  timesteps_this_iter: 96000
  timesteps_total: 18816000
  training_iteration: 196
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    196 |          31954.2 | 18816000 |   738.29 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 3.67
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 25.37
    apples_agent-1_min: 0
    apples_agent-2_max: 64
    apples_agent-2_mean: 6.65
    apples_agent-2_min: 0
    apples_agent-3_max: 156
    apples_agent-3_mean: 86.34
    apples_agent-3_min: 24
    apples_agent-4_max: 27
    apples_agent-4_mean: 1.76
    apples_agent-4_min: 0
    apples_agent-5_max: 186
    apples_agent-5_mean: 105.76
    apples_agent-5_min: 12
    cleaning_beam_agent-0_max: 514
    cleaning_beam_agent-0_mean: 290.41
    cleaning_beam_agent-0_min: 83
    cleaning_beam_agent-1_max: 328
    cleaning_beam_agent-1_mean: 187.37
    cleaning_beam_agent-1_min: 68
    cleaning_beam_agent-2_max: 413
    cleaning_beam_agent-2_mean: 295.0
    cleaning_beam_agent-2_min: 112
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 47.15
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 477
    cleaning_beam_agent-4_mean: 367.52
    cleaning_beam_agent-4_min: 195
    cleaning_beam_agent-5_max: 322
    cleaning_beam_agent-5_mean: 57.14
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-25-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 910.9999999999952
  episode_reward_mean: 699.9199999999921
  episode_reward_min: 178.99999999999997
  episodes_this_iter: 96
  episodes_total: 18912
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13171.664
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 8.588159835198894e-05
        entropy: 1.151196002960205
        entropy_coeff: 0.0017600000137463212
        kl: 0.005061683245003223
        model: {}
        policy_loss: -0.014068592339754105
        total_loss: -0.01335748191922903
        vf_explained_var: 0.09221942722797394
        vf_loss: 17.248783111572266
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 8.588159835198894e-05
        entropy: 1.1374872922897339
        entropy_coeff: 0.0017600000137463212
        kl: 0.0060400767251849174
        model: {}
        policy_loss: -0.01696396805346012
        total_loss: -0.015889368951320648
        vf_explained_var: 0.01697787642478943
        vf_loss: 18.685617446899414
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 8.588159835198894e-05
        entropy: 1.1095703840255737
        entropy_coeff: 0.0017600000137463212
        kl: 0.005188708193600178
        model: {}
        policy_loss: -0.014694157056510448
        total_loss: -0.01391196995973587
        vf_explained_var: 0.10736477375030518
        vf_loss: 16.97288703918457
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 8.588159835198894e-05
        entropy: 0.6511855125427246
        entropy_coeff: 0.0017600000137463212
        kl: 0.004889790900051594
        model: {}
        policy_loss: -0.012166786938905716
        total_loss: -0.011715132743120193
        vf_explained_var: 0.22313421964645386
        vf_loss: 14.754973411560059
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 8.588159835198894e-05
        entropy: 0.9844095706939697
        entropy_coeff: 0.0017600000137463212
        kl: 0.005350513383746147
        model: {}
        policy_loss: -0.01681314967572689
        total_loss: -0.015838736668229103
        vf_explained_var: 0.13912789523601532
        vf_loss: 16.368717193603516
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 8.588159835198894e-05
        entropy: 0.9352496862411499
        entropy_coeff: 0.0017600000137463212
        kl: 0.005348219070583582
        model: {}
        policy_loss: -0.015605771914124489
        total_loss: -0.015178161673247814
        vf_explained_var: 0.19059963524341583
        vf_loss: 15.388270378112793
    load_time_ms: 19062.057
    num_steps_sampled: 18912000
    num_steps_trained: 18912000
    sample_time_ms: 128102.374
    update_time_ms: 116.463
  iterations_since_restore: 117
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.41576576576577
    ram_util_percent: 14.87297297297297
  pid: 14340
  policy_reward_max:
    agent-0: 151.83333333333323
    agent-1: 151.83333333333323
    agent-2: 151.83333333333323
    agent-3: 151.83333333333323
    agent-4: 151.83333333333323
    agent-5: 151.83333333333323
  policy_reward_mean:
    agent-0: 116.6533333333336
    agent-1: 116.6533333333336
    agent-2: 116.6533333333336
    agent-3: 116.6533333333336
    agent-4: 116.6533333333336
    agent-5: 116.6533333333336
  policy_reward_min:
    agent-0: 29.83333333333334
    agent-1: 29.83333333333334
    agent-2: 29.83333333333334
    agent-3: 29.83333333333334
    agent-4: 29.83333333333334
    agent-5: 29.83333333333334
  sampler_perf:
    mean_env_wait_ms: 30.496083521605016
    mean_inference_ms: 14.597506792631764
    mean_processing_ms: 65.6935876608672
  time_since_restore: 19559.42750787735
  time_this_iter_s: 156.02391457557678
  time_total_s: 32110.244379281998
  timestamp: 1637054710
  timesteps_since_restore: 11232000
  timesteps_this_iter: 96000
  timesteps_total: 18912000
  training_iteration: 197
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    197 |          32110.2 | 18912000 |   699.92 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 5.07
    apples_agent-0_min: 0
    apples_agent-1_max: 129
    apples_agent-1_mean: 25.39
    apples_agent-1_min: 0
    apples_agent-2_max: 253
    apples_agent-2_mean: 9.33
    apples_agent-2_min: 0
    apples_agent-3_max: 143
    apples_agent-3_mean: 88.71
    apples_agent-3_min: 1
    apples_agent-4_max: 37
    apples_agent-4_mean: 1.86
    apples_agent-4_min: 0
    apples_agent-5_max: 178
    apples_agent-5_mean: 102.24
    apples_agent-5_min: 27
    cleaning_beam_agent-0_max: 415
    cleaning_beam_agent-0_mean: 292.81
    cleaning_beam_agent-0_min: 122
    cleaning_beam_agent-1_max: 313
    cleaning_beam_agent-1_mean: 187.42
    cleaning_beam_agent-1_min: 68
    cleaning_beam_agent-2_max: 451
    cleaning_beam_agent-2_mean: 292.76
    cleaning_beam_agent-2_min: 124
    cleaning_beam_agent-3_max: 235
    cleaning_beam_agent-3_mean: 51.41
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 487
    cleaning_beam_agent-4_mean: 377.75
    cleaning_beam_agent-4_min: 215
    cleaning_beam_agent-5_max: 332
    cleaning_beam_agent-5_mean: 52.65
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-27-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 906.9999999999812
  episode_reward_mean: 696.559999999992
  episode_reward_min: 310.0000000000012
  episodes_this_iter: 96
  episodes_total: 19008
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13132.838
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 7.989120058482513e-05
        entropy: 1.1350185871124268
        entropy_coeff: 0.0017600000137463212
        kl: 0.004238431807607412
        model: {}
        policy_loss: -0.012822845950722694
        total_loss: -0.012298041954636574
        vf_explained_var: 0.11710335314273834
        vf_loss: 16.747474670410156
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 7.989120058482513e-05
        entropy: 1.1445934772491455
        entropy_coeff: 0.0017600000137463212
        kl: 0.005449135322123766
        model: {}
        policy_loss: -0.016267403960227966
        total_loss: -0.015307162888348103
        vf_explained_var: 0.005443081259727478
        vf_loss: 18.84896469116211
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 7.989120058482513e-05
        entropy: 1.1107009649276733
        entropy_coeff: 0.0017600000137463212
        kl: 0.004717352334409952
        model: {}
        policy_loss: -0.014139845967292786
        total_loss: -0.01341567188501358
        vf_explained_var: 0.08391506969928741
        vf_loss: 17.355377197265625
      agent-3:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 7.989120058482513e-05
        entropy: 0.6571497321128845
        entropy_coeff: 0.0017600000137463212
        kl: 0.005110197700560093
        model: {}
        policy_loss: -0.011664735153317451
        total_loss: -0.011266134679317474
        vf_explained_var: 0.2148287445306778
        vf_loss: 14.913091659545898
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 7.989120058482513e-05
        entropy: 0.9862989783287048
        entropy_coeff: 0.0017600000137463212
        kl: 0.004914429970085621
        model: {}
        policy_loss: -0.015583164989948273
        total_loss: -0.014596221968531609
        vf_explained_var: 0.08127515017986298
        vf_loss: 17.3994197845459
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.989120058482513e-05
        entropy: 0.9223811030387878
        entropy_coeff: 0.0017600000137463212
        kl: 0.00557366106659174
        model: {}
        policy_loss: -0.014941981993615627
        total_loss: -0.014460654929280281
        vf_explained_var: 0.18353809416294098
        vf_loss: 15.473546981811523
    load_time_ms: 19015.592
    num_steps_sampled: 19008000
    num_steps_trained: 19008000
    sample_time_ms: 127978.62
    update_time_ms: 111.753
  iterations_since_restore: 118
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.404
    ram_util_percent: 14.810222222222222
  pid: 14340
  policy_reward_max:
    agent-0: 151.16666666666725
    agent-1: 151.16666666666725
    agent-2: 151.16666666666725
    agent-3: 151.16666666666725
    agent-4: 151.16666666666725
    agent-5: 151.16666666666725
  policy_reward_mean:
    agent-0: 116.09333333333366
    agent-1: 116.09333333333366
    agent-2: 116.09333333333366
    agent-3: 116.09333333333366
    agent-4: 116.09333333333366
    agent-5: 116.09333333333366
  policy_reward_min:
    agent-0: 51.66666666666652
    agent-1: 51.66666666666652
    agent-2: 51.66666666666652
    agent-3: 51.66666666666652
    agent-4: 51.66666666666652
    agent-5: 51.66666666666652
  sampler_perf:
    mean_env_wait_ms: 30.493770807830238
    mean_inference_ms: 14.597739540819255
    mean_processing_ms: 65.69328877913372
  time_since_restore: 19715.151911973953
  time_this_iter_s: 155.7244040966034
  time_total_s: 32265.9687833786
  timestamp: 1637054869
  timesteps_since_restore: 11328000
  timesteps_this_iter: 96000
  timesteps_total: 19008000
  training_iteration: 198
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    198 |            32266 | 19008000 |   696.56 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 4.92
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 22.81
    apples_agent-1_min: 0
    apples_agent-2_max: 201
    apples_agent-2_mean: 5.6
    apples_agent-2_min: 0
    apples_agent-3_max: 163
    apples_agent-3_mean: 92.98
    apples_agent-3_min: 24
    apples_agent-4_max: 54
    apples_agent-4_mean: 2.53
    apples_agent-4_min: 0
    apples_agent-5_max: 181
    apples_agent-5_mean: 102.27
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 450
    cleaning_beam_agent-0_mean: 297.15
    cleaning_beam_agent-0_min: 98
    cleaning_beam_agent-1_max: 358
    cleaning_beam_agent-1_mean: 194.48
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 446
    cleaning_beam_agent-2_mean: 302.79
    cleaning_beam_agent-2_min: 74
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 43.85
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 485
    cleaning_beam_agent-4_mean: 362.87
    cleaning_beam_agent-4_min: 149
    cleaning_beam_agent-5_max: 332
    cleaning_beam_agent-5_mean: 56.99
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-30-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 942.9999999999695
  episode_reward_mean: 702.5099999999916
  episode_reward_min: 138.00000000000085
  episodes_this_iter: 96
  episodes_total: 19104
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13211.161
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.390080281766132e-05
        entropy: 1.1321594715118408
        entropy_coeff: 0.0017600000137463212
        kl: 0.005038164556026459
        model: {}
        policy_loss: -0.013038475066423416
        total_loss: -0.01283982302993536
        vf_explained_var: 0.11365211009979248
        vf_loss: 16.874372482299805
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 7.390080281766132e-05
        entropy: 1.1398794651031494
        entropy_coeff: 0.0017600000137463212
        kl: 0.0052669476717710495
        model: {}
        policy_loss: -0.015352762304246426
        total_loss: -0.01441975124180317
        vf_explained_var: 0.008550956845283508
        vf_loss: 18.85809326171875
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.390080281766132e-05
        entropy: 1.1164941787719727
        entropy_coeff: 0.0017600000137463212
        kl: 0.005899096839129925
        model: {}
        policy_loss: -0.013960834592580795
        total_loss: -0.013560968451201916
        vf_explained_var: 0.06656737625598907
        vf_loss: 17.749862670898438
      agent-3:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 7.390080281766132e-05
        entropy: 0.6419860124588013
        entropy_coeff: 0.0017600000137463212
        kl: 0.0045150574296712875
        model: {}
        policy_loss: -0.010936904698610306
        total_loss: -0.010526531375944614
        vf_explained_var: 0.21977372467517853
        vf_loss: 14.838315963745117
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.390080281766132e-05
        entropy: 0.9906286597251892
        entropy_coeff: 0.0017600000137463212
        kl: 0.005908166989684105
        model: {}
        policy_loss: -0.015256747603416443
        total_loss: -0.014678153209388256
        vf_explained_var: 0.09035268425941467
        vf_loss: 17.312835693359375
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.390080281766132e-05
        entropy: 0.9270002245903015
        entropy_coeff: 0.0017600000137463212
        kl: 0.00512248370796442
        model: {}
        policy_loss: -0.013815696351230145
        total_loss: -0.013379689306020737
        vf_explained_var: 0.18329185247421265
        vf_loss: 15.552783012390137
    load_time_ms: 19007.046
    num_steps_sampled: 19104000
    num_steps_trained: 19104000
    sample_time_ms: 128041.188
    update_time_ms: 99.771
  iterations_since_restore: 119
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.416964285714283
    ram_util_percent: 14.898214285714285
  pid: 14340
  policy_reward_max:
    agent-0: 157.1666666666666
    agent-1: 157.1666666666666
    agent-2: 157.1666666666666
    agent-3: 157.1666666666666
    agent-4: 157.1666666666666
    agent-5: 157.1666666666666
  policy_reward_mean:
    agent-0: 117.08500000000024
    agent-1: 117.08500000000024
    agent-2: 117.08500000000024
    agent-3: 117.08500000000024
    agent-4: 117.08500000000024
    agent-5: 117.08500000000024
  policy_reward_min:
    agent-0: 23.00000000000001
    agent-1: 23.00000000000001
    agent-2: 23.00000000000001
    agent-3: 23.00000000000001
    agent-4: 23.00000000000001
    agent-5: 23.00000000000001
  sampler_perf:
    mean_env_wait_ms: 30.491411929459083
    mean_inference_ms: 14.597428655429974
    mean_processing_ms: 65.69246958110962
  time_since_restore: 19872.194647312164
  time_this_iter_s: 157.04273533821106
  time_total_s: 32423.011518716812
  timestamp: 1637055026
  timesteps_since_restore: 11424000
  timesteps_this_iter: 96000
  timesteps_total: 19104000
  training_iteration: 199
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    199 |            32423 | 19104000 |   702.51 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 3.21
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 24.35
    apples_agent-1_min: 0
    apples_agent-2_max: 141
    apples_agent-2_mean: 4.31
    apples_agent-2_min: 0
    apples_agent-3_max: 162
    apples_agent-3_mean: 91.96
    apples_agent-3_min: 36
    apples_agent-4_max: 82
    apples_agent-4_mean: 4.13
    apples_agent-4_min: 0
    apples_agent-5_max: 154
    apples_agent-5_mean: 98.06
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 432
    cleaning_beam_agent-0_mean: 293.24
    cleaning_beam_agent-0_min: 89
    cleaning_beam_agent-1_max: 380
    cleaning_beam_agent-1_mean: 193.41
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 479
    cleaning_beam_agent-2_mean: 302.5
    cleaning_beam_agent-2_min: 86
    cleaning_beam_agent-3_max: 118
    cleaning_beam_agent-3_mean: 40.29
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 504
    cleaning_beam_agent-4_mean: 375.2
    cleaning_beam_agent-4_min: 119
    cleaning_beam_agent-5_max: 418
    cleaning_beam_agent-5_mean: 58.02
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-33-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 921.9999999999834
  episode_reward_mean: 701.4199999999915
  episode_reward_min: 276.9999999999987
  episodes_this_iter: 96
  episodes_total: 19200
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13190.116
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.791039777453989e-05
        entropy: 1.1451886892318726
        entropy_coeff: 0.0017600000137463212
        kl: 0.005058526061475277
        model: {}
        policy_loss: -0.012720838189125061
        total_loss: -0.012492778711020947
        vf_explained_var: 0.0625801682472229
        vf_loss: 17.377418518066406
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 6.791039777453989e-05
        entropy: 1.136899471282959
        entropy_coeff: 0.0017600000137463212
        kl: 0.005093153566122055
        model: {}
        policy_loss: -0.014720282517373562
        total_loss: -0.013830209150910378
        vf_explained_var: -0.010225802659988403
        vf_loss: 18.723800659179688
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.791039777453989e-05
        entropy: 1.0971665382385254
        entropy_coeff: 0.0017600000137463212
        kl: 0.004925523418933153
        model: {}
        policy_loss: -0.012975256890058517
        total_loss: -0.012670034542679787
        vf_explained_var: 0.05919960141181946
        vf_loss: 17.43684196472168
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 6.791039777453989e-05
        entropy: 0.6545100808143616
        entropy_coeff: 0.0017600000137463212
        kl: 0.004582406021654606
        model: {}
        policy_loss: -0.01019892655313015
        total_loss: -0.009831654839217663
        vf_explained_var: 0.19530341029167175
        vf_loss: 14.905722618103027
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.791039777453989e-05
        entropy: 0.9879570603370667
        entropy_coeff: 0.0017600000137463212
        kl: 0.005205215886235237
        model: {}
        policy_loss: -0.01410846970975399
        total_loss: -0.013688781298696995
        vf_explained_var: 0.11632823944091797
        vf_loss: 16.37972640991211
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.791039777453989e-05
        entropy: 0.925257682800293
        entropy_coeff: 0.0017600000137463212
        kl: 0.004605486989021301
        model: {}
        policy_loss: -0.013279292732477188
        total_loss: -0.01297411322593689
        vf_explained_var: 0.2048252373933792
        vf_loss: 14.73083782196045
    load_time_ms: 20782.313
    num_steps_sampled: 19200000
    num_steps_trained: 19200000
    sample_time_ms: 128098.259
    update_time_ms: 91.334
  iterations_since_restore: 120
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.352
    ram_util_percent: 15.0968
  pid: 14340
  policy_reward_max:
    agent-0: 153.66666666666669
    agent-1: 153.66666666666669
    agent-2: 153.66666666666669
    agent-3: 153.66666666666669
    agent-4: 153.66666666666669
    agent-5: 153.66666666666669
  policy_reward_mean:
    agent-0: 116.90333333333365
    agent-1: 116.90333333333365
    agent-2: 116.90333333333365
    agent-3: 116.90333333333365
    agent-4: 116.90333333333365
    agent-5: 116.90333333333365
  policy_reward_min:
    agent-0: 46.16666666666662
    agent-1: 46.16666666666662
    agent-2: 46.16666666666662
    agent-3: 46.16666666666662
    agent-4: 46.16666666666662
    agent-5: 46.16666666666662
  sampler_perf:
    mean_env_wait_ms: 30.490091255741767
    mean_inference_ms: 14.597527695908282
    mean_processing_ms: 65.6920548227079
  time_since_restore: 20046.8581199646
  time_this_iter_s: 174.6634726524353
  time_total_s: 32597.674991369247
  timestamp: 1637055201
  timesteps_since_restore: 11520000
  timesteps_this_iter: 96000
  timesteps_total: 19200000
  training_iteration: 200
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    200 |          32597.7 | 19200000 |   701.42 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 2.49
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 20.71
    apples_agent-1_min: 0
    apples_agent-2_max: 78
    apples_agent-2_mean: 6.37
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 94.25
    apples_agent-3_min: 42
    apples_agent-4_max: 65
    apples_agent-4_mean: 2.98
    apples_agent-4_min: 0
    apples_agent-5_max: 188
    apples_agent-5_mean: 101.7
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 449
    cleaning_beam_agent-0_mean: 308.51
    cleaning_beam_agent-0_min: 155
    cleaning_beam_agent-1_max: 356
    cleaning_beam_agent-1_mean: 197.08
    cleaning_beam_agent-1_min: 77
    cleaning_beam_agent-2_max: 465
    cleaning_beam_agent-2_mean: 301.04
    cleaning_beam_agent-2_min: 158
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 40.97
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 478
    cleaning_beam_agent-4_mean: 376.62
    cleaning_beam_agent-4_min: 214
    cleaning_beam_agent-5_max: 490
    cleaning_beam_agent-5_mean: 67.8
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-36-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 950.9999999999915
  episode_reward_mean: 720.5699999999915
  episode_reward_min: 279.0000000000001
  episodes_this_iter: 96
  episodes_total: 19296
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13187.47
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.192000000737607e-05
        entropy: 1.1451780796051025
        entropy_coeff: 0.0017600000137463212
        kl: 0.0046890671364963055
        model: {}
        policy_loss: -0.011902926489710808
        total_loss: -0.01172560453414917
        vf_explained_var: 0.037933945655822754
        vf_loss: 17.23926544189453
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 6.192000000737607e-05
        entropy: 1.1454479694366455
        entropy_coeff: 0.0017600000137463212
        kl: 0.004529968835413456
        model: {}
        policy_loss: -0.013591073453426361
        total_loss: -0.012912692502140999
        vf_explained_var: 0.003916159272193909
        vf_loss: 17.88376235961914
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.192000000737607e-05
        entropy: 1.093968391418457
        entropy_coeff: 0.0017600000137463212
        kl: 0.005478312727063894
        model: {}
        policy_loss: -0.012073823250830173
        total_loss: -0.012017681263387203
        vf_explained_var: 0.047122836112976074
        vf_loss: 17.07611083984375
      agent-3:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 6.192000000737607e-05
        entropy: 0.6316556930541992
        entropy_coeff: 0.0017600000137463212
        kl: 0.003971764352172613
        model: {}
        policy_loss: -0.009671367704868317
        total_loss: -0.009297516196966171
        vf_explained_var: 0.17722147703170776
        vf_loss: 14.731536865234375
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.192000000737607e-05
        entropy: 0.9802874326705933
        entropy_coeff: 0.0017600000137463212
        kl: 0.004725820384919643
        model: {}
        policy_loss: -0.01336576696485281
        total_loss: -0.013084767386317253
        vf_explained_var: 0.1467701941728592
        vf_loss: 15.337248802185059
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.192000000737607e-05
        entropy: 0.9014172554016113
        entropy_coeff: 0.0017600000137463212
        kl: 0.004851638805121183
        model: {}
        policy_loss: -0.012454863637685776
        total_loss: -0.012275266461074352
        vf_explained_var: 0.14861777424812317
        vf_loss: 15.23512077331543
    load_time_ms: 18933.988
    num_steps_sampled: 19296000
    num_steps_trained: 19296000
    sample_time_ms: 128499.633
    update_time_ms: 81.887
  iterations_since_restore: 121
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.896956521739128
    ram_util_percent: 14.784347826086957
  pid: 14340
  policy_reward_max:
    agent-0: 158.49999999999997
    agent-1: 158.49999999999997
    agent-2: 158.49999999999997
    agent-3: 158.49999999999997
    agent-4: 158.49999999999997
    agent-5: 158.49999999999997
  policy_reward_mean:
    agent-0: 120.09500000000031
    agent-1: 120.09500000000031
    agent-2: 120.09500000000031
    agent-3: 120.09500000000031
    agent-4: 120.09500000000031
    agent-5: 120.09500000000031
  policy_reward_min:
    agent-0: 46.4999999999999
    agent-1: 46.4999999999999
    agent-2: 46.4999999999999
    agent-3: 46.4999999999999
    agent-4: 46.4999999999999
    agent-5: 46.4999999999999
  sampler_perf:
    mean_env_wait_ms: 30.487939539715157
    mean_inference_ms: 14.59661768669392
    mean_processing_ms: 65.69097098556603
  time_since_restore: 20207.633855581284
  time_this_iter_s: 160.77573561668396
  time_total_s: 32758.45072698593
  timestamp: 1637055363
  timesteps_since_restore: 11616000
  timesteps_this_iter: 96000
  timesteps_total: 19296000
  training_iteration: 201
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    201 |          32758.5 | 19296000 |   720.57 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 2.56
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 25.8
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 5.65
    apples_agent-2_min: 0
    apples_agent-3_max: 154
    apples_agent-3_mean: 97.22
    apples_agent-3_min: 26
    apples_agent-4_max: 52
    apples_agent-4_mean: 2.54
    apples_agent-4_min: 0
    apples_agent-5_max: 178
    apples_agent-5_mean: 107.4
    apples_agent-5_min: 26
    cleaning_beam_agent-0_max: 482
    cleaning_beam_agent-0_mean: 318.45
    cleaning_beam_agent-0_min: 186
    cleaning_beam_agent-1_max: 314
    cleaning_beam_agent-1_mean: 193.92
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 464
    cleaning_beam_agent-2_mean: 317.53
    cleaning_beam_agent-2_min: 149
    cleaning_beam_agent-3_max: 114
    cleaning_beam_agent-3_mean: 36.72
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 486
    cleaning_beam_agent-4_mean: 385.59
    cleaning_beam_agent-4_min: 138
    cleaning_beam_agent-5_max: 259
    cleaning_beam_agent-5_mean: 55.58
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-38-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 945.9999999999798
  episode_reward_mean: 751.3499999999897
  episode_reward_min: 159.9999999999996
  episodes_this_iter: 96
  episodes_total: 19392
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13160.428
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 5.5929598602233455e-05
        entropy: 1.1357308626174927
        entropy_coeff: 0.0017600000137463212
        kl: 0.004655149299651384
        model: {}
        policy_loss: -0.011062168516218662
        total_loss: -0.011011585593223572
        vf_explained_var: 0.02521449327468872
        vf_loss: 18.16710090637207
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 5.5929598602233455e-05
        entropy: 1.1432994604110718
        entropy_coeff: 0.0017600000137463212
        kl: 0.0046571106649935246
        model: {}
        policy_loss: -0.012578962370753288
        total_loss: -0.012226477265357971
        vf_explained_var: -0.0077024102210998535
        vf_loss: 18.989830017089844
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 5.5929598602233455e-05
        entropy: 1.1085963249206543
        entropy_coeff: 0.0017600000137463212
        kl: 0.004392973612993956
        model: {}
        policy_loss: -0.011463562957942486
        total_loss: -0.011492371559143066
        vf_explained_var: 0.09743067622184753
        vf_loss: 17.026687622070312
      agent-3:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 5.5929598602233455e-05
        entropy: 0.6081735491752625
        entropy_coeff: 0.0017600000137463212
        kl: 0.0038934345357120037
        model: {}
        policy_loss: -0.009092903696000576
        total_loss: -0.008642107248306274
        vf_explained_var: 0.18694902956485748
        vf_loss: 15.151015281677246
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 5.5929598602233455e-05
        entropy: 0.9632184505462646
        entropy_coeff: 0.0017600000137463212
        kl: 0.005218589678406715
        model: {}
        policy_loss: -0.01348461676388979
        total_loss: -0.013306266628205776
        vf_explained_var: 0.14178070425987244
        vf_loss: 16.12686538696289
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 5.5929598602233455e-05
        entropy: 0.9181737899780273
        entropy_coeff: 0.0017600000137463212
        kl: 0.00451304204761982
        model: {}
        policy_loss: -0.011614227667450905
        total_loss: -0.011559966951608658
        vf_explained_var: 0.1662328541278839
        vf_loss: 15.574188232421875
    load_time_ms: 18932.688
    num_steps_sampled: 19392000
    num_steps_trained: 19392000
    sample_time_ms: 128048.857
    update_time_ms: 96.843
  iterations_since_restore: 122
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.685454545454547
    ram_util_percent: 14.775
  pid: 14340
  policy_reward_max:
    agent-0: 157.66666666666657
    agent-1: 157.66666666666657
    agent-2: 157.66666666666657
    agent-3: 157.66666666666657
    agent-4: 157.66666666666657
    agent-5: 157.66666666666657
  policy_reward_mean:
    agent-0: 125.22500000000029
    agent-1: 125.22500000000029
    agent-2: 125.22500000000029
    agent-3: 125.22500000000029
    agent-4: 125.22500000000029
    agent-5: 125.22500000000029
  policy_reward_min:
    agent-0: 26.66666666666672
    agent-1: 26.66666666666672
    agent-2: 26.66666666666672
    agent-3: 26.66666666666672
    agent-4: 26.66666666666672
    agent-5: 26.66666666666672
  sampler_perf:
    mean_env_wait_ms: 30.487092315872687
    mean_inference_ms: 14.596294920799064
    mean_processing_ms: 65.69099439784073
  time_since_restore: 20362.371643304825
  time_this_iter_s: 154.73778772354126
  time_total_s: 32913.18851470947
  timestamp: 1637055518
  timesteps_since_restore: 11712000
  timesteps_this_iter: 96000
  timesteps_total: 19392000
  training_iteration: 202
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    202 |          32913.2 | 19392000 |   751.35 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 64
    apples_agent-0_mean: 4.42
    apples_agent-0_min: 0
    apples_agent-1_max: 125
    apples_agent-1_mean: 25.64
    apples_agent-1_min: 0
    apples_agent-2_max: 141
    apples_agent-2_mean: 7.67
    apples_agent-2_min: 0
    apples_agent-3_max: 171
    apples_agent-3_mean: 98.4
    apples_agent-3_min: 23
    apples_agent-4_max: 25
    apples_agent-4_mean: 1.3
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 98.21
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 451
    cleaning_beam_agent-0_mean: 316.33
    cleaning_beam_agent-0_min: 138
    cleaning_beam_agent-1_max: 361
    cleaning_beam_agent-1_mean: 197.95
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 531
    cleaning_beam_agent-2_mean: 322.68
    cleaning_beam_agent-2_min: 176
    cleaning_beam_agent-3_max: 109
    cleaning_beam_agent-3_mean: 34.22
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 517
    cleaning_beam_agent-4_mean: 397.52
    cleaning_beam_agent-4_min: 228
    cleaning_beam_agent-5_max: 430
    cleaning_beam_agent-5_mean: 71.41
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-41-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 954.9999999999808
  episode_reward_mean: 746.1499999999894
  episode_reward_min: 353.0000000000062
  episodes_this_iter: 96
  episodes_total: 19488
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13158.515
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 4.993920083506964e-05
        entropy: 1.1381601095199585
        entropy_coeff: 0.0017600000137463212
        kl: 0.004470651037991047
        model: {}
        policy_loss: -0.009846750646829605
        total_loss: -0.010076148435473442
        vf_explained_var: 0.03157705068588257
        vf_loss: 16.619991302490234
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 4.993920083506964e-05
        entropy: 1.1355226039886475
        entropy_coeff: 0.0017600000137463212
        kl: 0.005266197491437197
        model: {}
        policy_loss: -0.011783817782998085
        total_loss: -0.011792140081524849
        vf_explained_var: -0.0014714300632476807
        vf_loss: 17.268861770629883
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 4.993920083506964e-05
        entropy: 1.101296067237854
        entropy_coeff: 0.0017600000137463212
        kl: 0.005165386479347944
        model: {}
        policy_loss: -0.011089715175330639
        total_loss: -0.01132240705192089
        vf_explained_var: 0.08429530262947083
        vf_loss: 15.764537811279297
      agent-3:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 4.993920083506964e-05
        entropy: 0.5964011549949646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0036293137818574905
        model: {}
        policy_loss: -0.008105532266199589
        total_loss: -0.00766382459551096
        vf_explained_var: 0.13126587867736816
        vf_loss: 14.885396957397461
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 4.993920083506964e-05
        entropy: 0.9498564004898071
        entropy_coeff: 0.0017600000137463212
        kl: 0.00535294646397233
        model: {}
        policy_loss: -0.012049725279211998
        total_loss: -0.01187941711395979
        vf_explained_var: 0.08552902936935425
        vf_loss: 15.744095802307129
      agent-5:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 4.993920083506964e-05
        entropy: 0.8961429595947266
        entropy_coeff: 0.0017600000137463212
        kl: 0.004671304486691952
        model: {}
        policy_loss: -0.011032354086637497
        total_loss: -0.01110704056918621
        vf_explained_var: 0.15957193076610565
        vf_loss: 14.441324234008789
    load_time_ms: 19162.538
    num_steps_sampled: 19488000
    num_steps_trained: 19488000
    sample_time_ms: 128544.696
    update_time_ms: 66.915
  iterations_since_restore: 123
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.158139534883723
    ram_util_percent: 15.029457364341086
  pid: 14340
  policy_reward_max:
    agent-0: 159.1666666666663
    agent-1: 159.1666666666663
    agent-2: 159.1666666666663
    agent-3: 159.1666666666663
    agent-4: 159.1666666666663
    agent-5: 159.1666666666663
  policy_reward_mean:
    agent-0: 124.35833333333366
    agent-1: 124.35833333333366
    agent-2: 124.35833333333366
    agent-3: 124.35833333333366
    agent-4: 124.35833333333366
    agent-5: 124.35833333333366
  policy_reward_min:
    agent-0: 58.83333333333313
    agent-1: 58.83333333333313
    agent-2: 58.83333333333313
    agent-3: 58.83333333333313
    agent-4: 58.83333333333313
    agent-5: 58.83333333333313
  sampler_perf:
    mean_env_wait_ms: 30.48752755237095
    mean_inference_ms: 14.595468269342405
    mean_processing_ms: 65.69040524889095
  time_since_restore: 20542.270046949387
  time_this_iter_s: 179.89840364456177
  time_total_s: 33093.086918354034
  timestamp: 1637055699
  timesteps_since_restore: 11808000
  timesteps_this_iter: 96000
  timesteps_total: 19488000
  training_iteration: 203
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    203 |          33093.1 | 19488000 |   746.15 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 3.49
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 24.43
    apples_agent-1_min: 0
    apples_agent-2_max: 133
    apples_agent-2_mean: 4.59
    apples_agent-2_min: 0
    apples_agent-3_max: 146
    apples_agent-3_mean: 98.48
    apples_agent-3_min: 29
    apples_agent-4_max: 41
    apples_agent-4_mean: 1.5
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 105.82
    apples_agent-5_min: 12
    cleaning_beam_agent-0_max: 480
    cleaning_beam_agent-0_mean: 323.99
    cleaning_beam_agent-0_min: 128
    cleaning_beam_agent-1_max: 307
    cleaning_beam_agent-1_mean: 206.53
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 457
    cleaning_beam_agent-2_mean: 323.57
    cleaning_beam_agent-2_min: 182
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 30.82
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 594
    cleaning_beam_agent-4_mean: 406.79
    cleaning_beam_agent-4_min: 199
    cleaning_beam_agent-5_max: 329
    cleaning_beam_agent-5_mean: 54.99
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-44-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 964.999999999982
  episode_reward_mean: 758.8799999999908
  episode_reward_min: 170.99999999999912
  episodes_this_iter: 96
  episodes_total: 19584
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13103.96
    learner:
      agent-0:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 4.394879942992702e-05
        entropy: 1.1420578956604004
        entropy_coeff: 0.0017600000137463212
        kl: 0.00425527710467577
        model: {}
        policy_loss: -0.008939926512539387
        total_loss: -0.009178107604384422
        vf_explained_var: 0.07576695084571838
        vf_loss: 17.18650245666504
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 4.394879942992702e-05
        entropy: 1.128228783607483
        entropy_coeff: 0.0017600000137463212
        kl: 0.004459740594029427
        model: {}
        policy_loss: -0.01104726828634739
        total_loss: -0.010892930440604687
        vf_explained_var: -0.0230463445186615
        vf_loss: 19.170347213745117
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 4.394879942992702e-05
        entropy: 1.1040321588516235
        entropy_coeff: 0.0017600000137463212
        kl: 0.004780464805662632
        model: {}
        policy_loss: -0.010752958245575428
        total_loss: -0.010784091427922249
        vf_explained_var: 0.040742576122283936
        vf_loss: 17.924543380737305
      agent-3:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 4.394879942992702e-05
        entropy: 0.5900014638900757
        entropy_coeff: 0.0017600000137463212
        kl: 0.003264315892010927
        model: {}
        policy_loss: -0.007459766697138548
        total_loss: -0.007017132360488176
        vf_explained_var: 0.20355059206485748
        vf_loss: 14.797616958618164
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 4.394879942992702e-05
        entropy: 0.9523855447769165
        entropy_coeff: 0.0017600000137463212
        kl: 0.0040492527186870575
        model: {}
        policy_loss: -0.011007342487573624
        total_loss: -0.010808389633893967
        vf_explained_var: 0.1088760495185852
        vf_loss: 16.72688102722168
      agent-5:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 4.394879942992702e-05
        entropy: 0.8962852358818054
        entropy_coeff: 0.0017600000137463212
        kl: 0.004234627820551395
        model: {}
        policy_loss: -0.00983191654086113
        total_loss: -0.009808674454689026
        vf_explained_var: 0.15397395193576813
        vf_loss: 15.742383003234863
    load_time_ms: 19098.237
    num_steps_sampled: 19584000
    num_steps_trained: 19584000
    sample_time_ms: 128353.348
    update_time_ms: 70.608
  iterations_since_restore: 124
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.706756756756754
    ram_util_percent: 14.757657657657662
  pid: 14340
  policy_reward_max:
    agent-0: 160.8333333333332
    agent-1: 160.8333333333332
    agent-2: 160.8333333333332
    agent-3: 160.8333333333332
    agent-4: 160.8333333333332
    agent-5: 160.8333333333332
  policy_reward_mean:
    agent-0: 126.48000000000022
    agent-1: 126.48000000000022
    agent-2: 126.48000000000022
    agent-3: 126.48000000000022
    agent-4: 126.48000000000022
    agent-5: 126.48000000000022
  policy_reward_min:
    agent-0: 28.50000000000006
    agent-1: 28.50000000000006
    agent-2: 28.50000000000006
    agent-3: 28.50000000000006
    agent-4: 28.50000000000006
    agent-5: 28.50000000000006
  sampler_perf:
    mean_env_wait_ms: 30.489077423241852
    mean_inference_ms: 14.595027865740771
    mean_processing_ms: 65.69270091593049
  time_since_restore: 20698.418305635452
  time_this_iter_s: 156.14825868606567
  time_total_s: 33249.2351770401
  timestamp: 1637055855
  timesteps_since_restore: 11904000
  timesteps_this_iter: 96000
  timesteps_total: 19584000
  training_iteration: 204
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    204 |          33249.2 | 19584000 |   758.88 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 3.95
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 22.67
    apples_agent-1_min: 0
    apples_agent-2_max: 356
    apples_agent-2_mean: 8.03
    apples_agent-2_min: 0
    apples_agent-3_max: 358
    apples_agent-3_mean: 103.27
    apples_agent-3_min: 20
    apples_agent-4_max: 52
    apples_agent-4_mean: 1.74
    apples_agent-4_min: 0
    apples_agent-5_max: 158
    apples_agent-5_mean: 104.47
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 470
    cleaning_beam_agent-0_mean: 314.77
    cleaning_beam_agent-0_min: 107
    cleaning_beam_agent-1_max: 378
    cleaning_beam_agent-1_mean: 206.85
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 467
    cleaning_beam_agent-2_mean: 330.72
    cleaning_beam_agent-2_min: 71
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 32.79
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 496
    cleaning_beam_agent-4_mean: 403.69
    cleaning_beam_agent-4_min: 177
    cleaning_beam_agent-5_max: 274
    cleaning_beam_agent-5_mean: 56.33
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-46-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 973.9999999999847
  episode_reward_mean: 765.5599999999885
  episode_reward_min: 282.9999999999986
  episodes_this_iter: 96
  episodes_total: 19680
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13203.521
    learner:
      agent-0:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 3.795840166276321e-05
        entropy: 1.1481671333312988
        entropy_coeff: 0.0017600000137463212
        kl: 0.005196720827370882
        model: {}
        policy_loss: -0.008574943989515305
        total_loss: -0.008790419436991215
        vf_explained_var: 0.04054646193981171
        vf_loss: 17.72819709777832
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 3.795840166276321e-05
        entropy: 1.1383135318756104
        entropy_coeff: 0.0017600000137463212
        kl: 0.003877414623275399
        model: {}
        policy_loss: -0.010312745347619057
        total_loss: -0.010299298912286758
        vf_explained_var: -0.02852797508239746
        vf_loss: 19.19941520690918
      agent-2:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 3.795840166276321e-05
        entropy: 1.0847022533416748
        entropy_coeff: 0.0017600000137463212
        kl: 0.004386964254081249
        model: {}
        policy_loss: -0.00882894080132246
        total_loss: -0.008948912844061852
        vf_explained_var: 0.06962805986404419
        vf_loss: 17.342676162719727
      agent-3:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 3.795840166276321e-05
        entropy: 0.5616407990455627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0027659349143505096
        model: {}
        policy_loss: -0.00658324034884572
        total_loss: -0.0060142632573843
        vf_explained_var: 0.15679262578487396
        vf_loss: 15.569284439086914
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 3.795840166276321e-05
        entropy: 0.9612375497817993
        entropy_coeff: 0.0017600000137463212
        kl: 0.004485012963414192
        model: {}
        policy_loss: -0.010205218568444252
        total_loss: -0.010124505497515202
        vf_explained_var: 0.10836638510227203
        vf_loss: 16.603662490844727
      agent-5:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 3.795840166276321e-05
        entropy: 0.8976072669029236
        entropy_coeff: 0.0017600000137463212
        kl: 0.0034981640055775642
        model: {}
        policy_loss: -0.009279167279601097
        total_loss: -0.00923544354736805
        vf_explained_var: 0.12698401510715485
        vf_loss: 16.12580680847168
    load_time_ms: 19108.523
    num_steps_sampled: 19680000
    num_steps_trained: 19680000
    sample_time_ms: 128384.543
    update_time_ms: 74.675
  iterations_since_restore: 125
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.548214285714288
    ram_util_percent: 14.76339285714286
  pid: 14340
  policy_reward_max:
    agent-0: 162.33333333333303
    agent-1: 162.33333333333303
    agent-2: 162.33333333333303
    agent-3: 162.33333333333303
    agent-4: 162.33333333333303
    agent-5: 162.33333333333303
  policy_reward_mean:
    agent-0: 127.59333333333359
    agent-1: 127.59333333333359
    agent-2: 127.59333333333359
    agent-3: 127.59333333333359
    agent-4: 127.59333333333359
    agent-5: 127.59333333333359
  policy_reward_min:
    agent-0: 47.16666666666657
    agent-1: 47.16666666666657
    agent-2: 47.16666666666657
    agent-3: 47.16666666666657
    agent-4: 47.16666666666657
    agent-5: 47.16666666666657
  sampler_perf:
    mean_env_wait_ms: 30.490194962413923
    mean_inference_ms: 14.594449478264997
    mean_processing_ms: 65.69073928081937
  time_since_restore: 20854.747574567795
  time_this_iter_s: 156.32926893234253
  time_total_s: 33405.56444597244
  timestamp: 1637056013
  timesteps_since_restore: 12000000
  timesteps_this_iter: 96000
  timesteps_total: 19680000
  training_iteration: 205
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    205 |          33405.6 | 19680000 |   765.56 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 3.73
    apples_agent-0_min: 0
    apples_agent-1_max: 111
    apples_agent-1_mean: 25.74
    apples_agent-1_min: 0
    apples_agent-2_max: 78
    apples_agent-2_mean: 5.25
    apples_agent-2_min: 0
    apples_agent-3_max: 163
    apples_agent-3_mean: 98.11
    apples_agent-3_min: 20
    apples_agent-4_max: 40
    apples_agent-4_mean: 2.77
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 101.46
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 492
    cleaning_beam_agent-0_mean: 294.73
    cleaning_beam_agent-0_min: 96
    cleaning_beam_agent-1_max: 350
    cleaning_beam_agent-1_mean: 196.06
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 518
    cleaning_beam_agent-2_mean: 329.39
    cleaning_beam_agent-2_min: 68
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 33.93
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 486
    cleaning_beam_agent-4_mean: 390.22
    cleaning_beam_agent-4_min: 150
    cleaning_beam_agent-5_max: 432
    cleaning_beam_agent-5_mean: 62.95
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-49-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 949.9999999999716
  episode_reward_mean: 733.0499999999895
  episode_reward_min: 205.99999999999793
  episodes_this_iter: 96
  episodes_total: 19776
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13080.173
    learner:
      agent-0:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 3.196800025762059e-05
        entropy: 1.1359727382659912
        entropy_coeff: 0.0017600000137463212
        kl: 0.003975057043135166
        model: {}
        policy_loss: -0.006936904974281788
        total_loss: -0.0071682557463645935
        vf_explained_var: 0.04712629318237305
        vf_loss: 17.43113899230957
      agent-1:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 3.196800025762059e-05
        entropy: 1.1493394374847412
        entropy_coeff: 0.0017600000137463212
        kl: 0.003849379252642393
        model: {}
        policy_loss: -0.009090347215533257
        total_loss: -0.009273776784539223
        vf_explained_var: 0.0204341858625412
        vf_loss: 17.91288185119629
      agent-2:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 3.196800025762059e-05
        entropy: 1.104231357574463
        entropy_coeff: 0.0017600000137463212
        kl: 0.0034859327133744955
        model: {}
        policy_loss: -0.007840195670723915
        total_loss: -0.00804767943918705
        vf_explained_var: 0.06303340196609497
        vf_loss: 17.141765594482422
      agent-3:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 3.196800025762059e-05
        entropy: 0.6018900871276855
        entropy_coeff: 0.0017600000137463212
        kl: 0.002573237521573901
        model: {}
        policy_loss: -0.006172829307615757
        total_loss: -0.0057396250776946545
        vf_explained_var: 0.1842685490846634
        vf_loss: 14.9227933883667
      agent-4:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 3.196800025762059e-05
        entropy: 0.9776746034622192
        entropy_coeff: 0.0017600000137463212
        kl: 0.0036810170859098434
        model: {}
        policy_loss: -0.0087332958355546
        total_loss: -0.008832653053104877
        vf_explained_var: 0.13908660411834717
        vf_loss: 15.753373146057129
      agent-5:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 3.196800025762059e-05
        entropy: 0.9070942997932434
        entropy_coeff: 0.0017600000137463212
        kl: 0.003458204446360469
        model: {}
        policy_loss: -0.008577088825404644
        total_loss: -0.00863921269774437
        vf_explained_var: 0.16359354555606842
        vf_loss: 15.289623260498047
    load_time_ms: 20145.696
    num_steps_sampled: 19776000
    num_steps_trained: 19776000
    sample_time_ms: 128084.962
    update_time_ms: 74.983
  iterations_since_restore: 126
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 31.429707112970714
    ram_util_percent: 15.49037656903766
  pid: 14340
  policy_reward_max:
    agent-0: 158.33333333333306
    agent-1: 158.33333333333306
    agent-2: 158.33333333333306
    agent-3: 158.33333333333306
    agent-4: 158.33333333333306
    agent-5: 158.33333333333306
  policy_reward_mean:
    agent-0: 122.17500000000031
    agent-1: 122.17500000000031
    agent-2: 122.17500000000031
    agent-3: 122.17500000000031
    agent-4: 122.17500000000031
    agent-5: 122.17500000000031
  policy_reward_min:
    agent-0: 34.33333333333339
    agent-1: 34.33333333333339
    agent-2: 34.33333333333339
    agent-3: 34.33333333333339
    agent-4: 34.33333333333339
    agent-5: 34.33333333333339
  sampler_perf:
    mean_env_wait_ms: 30.486791677749217
    mean_inference_ms: 14.590806560737047
    mean_processing_ms: 65.68653436349136
  time_since_restore: 21018.59933757782
  time_this_iter_s: 163.85176301002502
  time_total_s: 33569.41620898247
  timestamp: 1637056180
  timesteps_since_restore: 12096000
  timesteps_this_iter: 96000
  timesteps_total: 19776000
  training_iteration: 206
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    206 |          33569.4 | 19776000 |   733.05 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 3.33
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 22.78
    apples_agent-1_min: 0
    apples_agent-2_max: 201
    apples_agent-2_mean: 10.6
    apples_agent-2_min: 0
    apples_agent-3_max: 153
    apples_agent-3_mean: 99.95
    apples_agent-3_min: 37
    apples_agent-4_max: 40
    apples_agent-4_mean: 1.53
    apples_agent-4_min: 0
    apples_agent-5_max: 201
    apples_agent-5_mean: 112.44
    apples_agent-5_min: 9
    cleaning_beam_agent-0_max: 473
    cleaning_beam_agent-0_mean: 313.38
    cleaning_beam_agent-0_min: 88
    cleaning_beam_agent-1_max: 330
    cleaning_beam_agent-1_mean: 202.04
    cleaning_beam_agent-1_min: 66
    cleaning_beam_agent-2_max: 508
    cleaning_beam_agent-2_mean: 328.89
    cleaning_beam_agent-2_min: 94
    cleaning_beam_agent-3_max: 80
    cleaning_beam_agent-3_mean: 35.34
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 513
    cleaning_beam_agent-4_mean: 391.28
    cleaning_beam_agent-4_min: 247
    cleaning_beam_agent-5_max: 392
    cleaning_beam_agent-5_mean: 54.01
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-52-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 961.9999999999815
  episode_reward_mean: 768.0999999999891
  episode_reward_min: 374.00000000000637
  episodes_this_iter: 96
  episodes_total: 19872
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13059.362
    learner:
      agent-0:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 2.597760067146737e-05
        entropy: 1.1325569152832031
        entropy_coeff: 0.0017600000137463212
        kl: 0.0037713912315666676
        model: {}
        policy_loss: -0.00690871849656105
        total_loss: -0.007223891094326973
        vf_explained_var: 0.03226713836193085
        vf_loss: 16.663436889648438
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 2.597760067146737e-05
        entropy: 1.1454650163650513
        entropy_coeff: 0.0017600000137463212
        kl: 0.003536115400493145
        model: {}
        policy_loss: -0.00803840160369873
        total_loss: -0.00825862679630518
        vf_explained_var: -0.02324002981185913
        vf_loss: 17.73689079284668
      agent-2:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 2.597760067146737e-05
        entropy: 1.0867013931274414
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030836816877126694
        model: {}
        policy_loss: -0.006848951335996389
        total_loss: -0.007125501986593008
        vf_explained_var: 0.07110412418842316
        vf_loss: 16.264083862304688
      agent-3:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 2.597760067146737e-05
        entropy: 0.575387716293335
        entropy_coeff: 0.0017600000137463212
        kl: 0.002829197095707059
        model: {}
        policy_loss: -0.005065759178251028
        total_loss: -0.004569916054606438
        vf_explained_var: 0.12872357666492462
        vf_loss: 15.083883285522461
      agent-4:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 2.597760067146737e-05
        entropy: 0.963717520236969
        entropy_coeff: 0.0017600000137463212
        kl: 0.0032811707351356745
        model: {}
        policy_loss: -0.007443252019584179
        total_loss: -0.007485090754926205
        vf_explained_var: 0.05589756369590759
        vf_loss: 16.337947845458984
      agent-5:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 2.597760067146737e-05
        entropy: 0.8948193192481995
        entropy_coeff: 0.0017600000137463212
        kl: 0.002908281981945038
        model: {}
        policy_loss: -0.006862705107778311
        total_loss: -0.0069167716428637505
        vf_explained_var: 0.12193818390369415
        vf_loss: 15.185417175292969
    load_time_ms: 21336.78
    num_steps_sampled: 19872000
    num_steps_trained: 19872000
    sample_time_ms: 127758.697
    update_time_ms: 69.271
  iterations_since_restore: 127
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.442307692307693
    ram_util_percent: 15.714957264957263
  pid: 14340
  policy_reward_max:
    agent-0: 160.33333333333326
    agent-1: 160.33333333333326
    agent-2: 160.33333333333326
    agent-3: 160.33333333333326
    agent-4: 160.33333333333326
    agent-5: 160.33333333333326
  policy_reward_mean:
    agent-0: 128.01666666666696
    agent-1: 128.01666666666696
    agent-2: 128.01666666666696
    agent-3: 128.01666666666696
    agent-4: 128.01666666666696
    agent-5: 128.01666666666696
  policy_reward_min:
    agent-0: 62.33333333333309
    agent-1: 62.33333333333309
    agent-2: 62.33333333333309
    agent-3: 62.33333333333309
    agent-4: 62.33333333333309
    agent-5: 62.33333333333309
  sampler_perf:
    mean_env_wait_ms: 30.48036141272329
    mean_inference_ms: 14.586854616726434
    mean_processing_ms: 65.67182837119103
  time_since_restore: 21182.849340438843
  time_this_iter_s: 164.25000286102295
  time_total_s: 33733.66621184349
  timestamp: 1637056345
  timesteps_since_restore: 12192000
  timesteps_this_iter: 96000
  timesteps_total: 19872000
  training_iteration: 207
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    207 |          33733.7 | 19872000 |    768.1 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 3.3
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 25.61
    apples_agent-1_min: 0
    apples_agent-2_max: 90
    apples_agent-2_mean: 3.62
    apples_agent-2_min: 0
    apples_agent-3_max: 206
    apples_agent-3_mean: 97.14
    apples_agent-3_min: 19
    apples_agent-4_max: 42
    apples_agent-4_mean: 2.4
    apples_agent-4_min: 0
    apples_agent-5_max: 207
    apples_agent-5_mean: 113.31
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 469
    cleaning_beam_agent-0_mean: 316.53
    cleaning_beam_agent-0_min: 170
    cleaning_beam_agent-1_max: 389
    cleaning_beam_agent-1_mean: 205.88
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 489
    cleaning_beam_agent-2_mean: 331.58
    cleaning_beam_agent-2_min: 156
    cleaning_beam_agent-3_max: 108
    cleaning_beam_agent-3_mean: 36.97
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 527
    cleaning_beam_agent-4_mean: 393.74
    cleaning_beam_agent-4_min: 265
    cleaning_beam_agent-5_max: 351
    cleaning_beam_agent-5_mean: 48.4
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 3
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-55-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 966.9999999999878
  episode_reward_mean: 760.3699999999891
  episode_reward_min: 182.99999999999883
  episodes_this_iter: 96
  episodes_total: 19968
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13063.386
    learner:
      agent-0:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.998719926632475e-05
        entropy: 1.1349319219589233
        entropy_coeff: 0.0017600000137463212
        kl: 0.003087309654802084
        model: {}
        policy_loss: -0.005287264473736286
        total_loss: -0.005559287965297699
        vf_explained_var: 0.02512507140636444
        vf_loss: 17.20635986328125
      agent-1:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.998719926632475e-05
        entropy: 1.1479049921035767
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025127027183771133
        model: {}
        policy_loss: -0.006647908594459295
        total_loss: -0.006882102228701115
        vf_explained_var: -0.006001576781272888
        vf_loss: 17.78264617919922
      agent-2:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.998719926632475e-05
        entropy: 1.110432744026184
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023364434018731117
        model: {}
        policy_loss: -0.005805542692542076
        total_loss: -0.0061496407724916935
        vf_explained_var: 0.09257727861404419
        vf_loss: 16.06614875793457
      agent-3:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.998719926632475e-05
        entropy: 0.5827904343605042
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018000266281887889
        model: {}
        policy_loss: -0.004358991980552673
        total_loss: -0.0038728767540305853
        vf_explained_var: 0.14403046667575836
        vf_loss: 15.11788272857666
      agent-4:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.998719926632475e-05
        entropy: 0.9591534733772278
        entropy_coeff: 0.0017600000137463212
        kl: 0.002645135624334216
        model: {}
        policy_loss: -0.006513836327940226
        total_loss: -0.006574293598532677
        vf_explained_var: 0.08597883582115173
        vf_loss: 16.193889617919922
      agent-5:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.998719926632475e-05
        entropy: 0.9056861400604248
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021716293413192034
        model: {}
        policy_loss: -0.005773881450295448
        total_loss: -0.005837056785821915
        vf_explained_var: 0.13317687809467316
        vf_loss: 15.299871444702148
    load_time_ms: 23757.056
    num_steps_sampled: 19968000
    num_steps_trained: 19968000
    sample_time_ms: 127461.987
    update_time_ms: 71.257
  iterations_since_restore: 128
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.64541832669323
    ram_util_percent: 16.164940239043826
  pid: 14340
  policy_reward_max:
    agent-0: 161.1666666666665
    agent-1: 161.1666666666665
    agent-2: 161.1666666666665
    agent-3: 161.1666666666665
    agent-4: 161.1666666666665
    agent-5: 161.1666666666665
  policy_reward_mean:
    agent-0: 126.7283333333336
    agent-1: 126.7283333333336
    agent-2: 126.7283333333336
    agent-3: 126.7283333333336
    agent-4: 126.7283333333336
    agent-5: 126.7283333333336
  policy_reward_min:
    agent-0: 30.500000000000032
    agent-1: 30.500000000000032
    agent-2: 30.500000000000032
    agent-3: 30.500000000000032
    agent-4: 30.500000000000032
    agent-5: 30.500000000000032
  sampler_perf:
    mean_env_wait_ms: 30.477064747016907
    mean_inference_ms: 14.583932459666103
    mean_processing_ms: 65.70709834687833
  time_since_restore: 21359.892733335495
  time_this_iter_s: 177.04339289665222
  time_total_s: 33910.70960474014
  timestamp: 1637056522
  timesteps_since_restore: 12288000
  timesteps_this_iter: 96000
  timesteps_total: 19968000
  training_iteration: 208
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    208 |          33910.7 | 19968000 |   760.37 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 5.04
    apples_agent-0_min: 0
    apples_agent-1_max: 113
    apples_agent-1_mean: 27.17
    apples_agent-1_min: 0
    apples_agent-2_max: 218
    apples_agent-2_mean: 7.29
    apples_agent-2_min: 0
    apples_agent-3_max: 175
    apples_agent-3_mean: 99.07
    apples_agent-3_min: 45
    apples_agent-4_max: 39
    apples_agent-4_mean: 1.36
    apples_agent-4_min: 0
    apples_agent-5_max: 179
    apples_agent-5_mean: 107.75
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 513
    cleaning_beam_agent-0_mean: 332.29
    cleaning_beam_agent-0_min: 152
    cleaning_beam_agent-1_max: 358
    cleaning_beam_agent-1_mean: 209.55
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 458
    cleaning_beam_agent-2_mean: 327.85
    cleaning_beam_agent-2_min: 85
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 32.53
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 532
    cleaning_beam_agent-4_mean: 395.73
    cleaning_beam_agent-4_min: 199
    cleaning_beam_agent-5_max: 391
    cleaning_beam_agent-5_mean: 58.34
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-58-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 965.9999999999824
  episode_reward_mean: 765.0699999999889
  episode_reward_min: 286.99999999999903
  episodes_this_iter: 96
  episodes_total: 20064
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12988.555
    learner:
      agent-0:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.3996799680171534e-05
        entropy: 1.11662757396698
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014989306218922138
        model: {}
        policy_loss: -0.0038162367418408394
        total_loss: -0.004145476035773754
        vf_explained_var: 0.036196351051330566
        vf_loss: 16.348522186279297
      agent-1:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.3996799680171534e-05
        entropy: 1.1392037868499756
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026628340128809214
        model: {}
        policy_loss: -0.005089380778372288
        total_loss: -0.005348743870854378
        vf_explained_var: -0.021655648946762085
        vf_loss: 17.41476821899414
      agent-2:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.3996799680171534e-05
        entropy: 1.1042170524597168
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016054933657869697
        model: {}
        policy_loss: -0.003930751234292984
        total_loss: -0.0042048003524541855
        vf_explained_var: 0.02366006374359131
        vf_loss: 16.68117332458496
      agent-3:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.3996799680171534e-05
        entropy: 0.5686005353927612
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016045290976762772
        model: {}
        policy_loss: -0.0032347901724278927
        total_loss: -0.002738713286817074
        vf_explained_var: 0.1217290610074997
        vf_loss: 14.967949867248535
      agent-4:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.3996799680171534e-05
        entropy: 0.9577096104621887
        entropy_coeff: 0.0017600000137463212
        kl: 0.002479385817423463
        model: {}
        policy_loss: -0.00538130197674036
        total_loss: -0.00545177236199379
        vf_explained_var: 0.05246870219707489
        vf_loss: 16.112258911132812
      agent-5:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.3996799680171534e-05
        entropy: 0.892906904220581
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013010537950322032
        model: {}
        policy_loss: -0.0042774914763867855
        total_loss: -0.004382379353046417
        vf_explained_var: 0.14115653932094574
        vf_loss: 14.663768768310547
    load_time_ms: 24762.117
    num_steps_sampled: 20064000
    num_steps_trained: 20064000
    sample_time_ms: 127337.84
    update_time_ms: 68.97
  iterations_since_restore: 129
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.683050847457626
    ram_util_percent: 16.068644067796612
  pid: 14340
  policy_reward_max:
    agent-0: 161.00000000000009
    agent-1: 161.00000000000009
    agent-2: 161.00000000000009
    agent-3: 161.00000000000009
    agent-4: 161.00000000000009
    agent-5: 161.00000000000009
  policy_reward_mean:
    agent-0: 127.51166666666695
    agent-1: 127.51166666666695
    agent-2: 127.51166666666695
    agent-3: 127.51166666666695
    agent-4: 127.51166666666695
    agent-5: 127.51166666666695
  policy_reward_min:
    agent-0: 47.83333333333322
    agent-1: 47.83333333333322
    agent-2: 47.83333333333322
    agent-3: 47.83333333333322
    agent-4: 47.83333333333322
    agent-5: 47.83333333333322
  sampler_perf:
    mean_env_wait_ms: 30.476002587135866
    mean_inference_ms: 14.581304147064834
    mean_processing_ms: 65.7071572976611
  time_since_restore: 21525.05135989189
  time_this_iter_s: 165.15862655639648
  time_total_s: 34075.86823129654
  timestamp: 1637056687
  timesteps_since_restore: 12384000
  timesteps_this_iter: 96000
  timesteps_total: 20064000
  training_iteration: 209
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    209 |          34075.9 | 20064000 |   765.07 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 4.33
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 22.26
    apples_agent-1_min: 0
    apples_agent-2_max: 237
    apples_agent-2_mean: 4.44
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 92.67
    apples_agent-3_min: 43
    apples_agent-4_max: 61
    apples_agent-4_mean: 3.84
    apples_agent-4_min: 0
    apples_agent-5_max: 195
    apples_agent-5_mean: 110.67
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 562
    cleaning_beam_agent-0_mean: 344.91
    cleaning_beam_agent-0_min: 217
    cleaning_beam_agent-1_max: 321
    cleaning_beam_agent-1_mean: 213.58
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 463
    cleaning_beam_agent-2_mean: 321.26
    cleaning_beam_agent-2_min: 126
    cleaning_beam_agent-3_max: 136
    cleaning_beam_agent-3_mean: 39.0
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 512
    cleaning_beam_agent-4_mean: 402.0
    cleaning_beam_agent-4_min: 131
    cleaning_beam_agent-5_max: 261
    cleaning_beam_agent-5_mean: 44.56
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-00-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 938.9999999999864
  episode_reward_mean: 773.6399999999882
  episode_reward_min: 286.99999999999807
  episodes_this_iter: 96
  episodes_total: 20160
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12988.329
    learner:
      agent-0:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1111793518066406
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015804971335455775
        model: {}
        policy_loss: -0.003714017104357481
        total_loss: -0.004009354393929243
        vf_explained_var: 0.10307741165161133
        vf_loss: 16.597200393676758
      agent-1:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1499087810516357
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016020635375753045
        model: {}
        policy_loss: -0.004210454877465963
        total_loss: -0.004389256704598665
        vf_explained_var: 0.005773201584815979
        vf_loss: 18.437896728515625
      agent-2:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1186566352844238
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016330825164914131
        model: {}
        policy_loss: -0.0038642361760139465
        total_loss: -0.0040314653888344765
        vf_explained_var: 0.03235143423080444
        vf_loss: 18.009693145751953
      agent-3:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5603121519088745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017761742928996682
        model: {}
        policy_loss: -0.002895836252719164
        total_loss: -0.002409826498478651
        vf_explained_var: 0.2049589455127716
        vf_loss: 14.72154426574707
      agent-4:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9583051204681396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017254570266231894
        model: {}
        policy_loss: -0.004688207991421223
        total_loss: -0.004681909456849098
        vf_explained_var: 0.09614744782447815
        vf_loss: 16.91573143005371
      agent-5:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8894115686416626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013897998724132776
        model: {}
        policy_loss: -0.003922834061086178
        total_loss: -0.003941914066672325
        vf_explained_var: 0.16487398743629456
        vf_loss: 15.46153450012207
    load_time_ms: 23541.416
    num_steps_sampled: 20160000
    num_steps_trained: 20160000
    sample_time_ms: 127774.801
    update_time_ms: 64.087
  iterations_since_restore: 130
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.61814345991561
    ram_util_percent: 15.981434599156119
  pid: 14340
  policy_reward_max:
    agent-0: 156.50000000000037
    agent-1: 156.50000000000037
    agent-2: 156.50000000000037
    agent-3: 156.50000000000037
    agent-4: 156.50000000000037
    agent-5: 156.50000000000037
  policy_reward_mean:
    agent-0: 128.94000000000023
    agent-1: 128.94000000000023
    agent-2: 128.94000000000023
    agent-3: 128.94000000000023
    agent-4: 128.94000000000023
    agent-5: 128.94000000000023
  policy_reward_min:
    agent-0: 47.833333333333194
    agent-1: 47.833333333333194
    agent-2: 47.833333333333194
    agent-3: 47.833333333333194
    agent-4: 47.833333333333194
    agent-5: 47.833333333333194
  sampler_perf:
    mean_env_wait_ms: 30.477915471422524
    mean_inference_ms: 14.57968588182921
    mean_processing_ms: 65.70503051183628
  time_since_restore: 21691.77380347252
  time_this_iter_s: 166.72244358062744
  time_total_s: 34242.59067487717
  timestamp: 1637056854
  timesteps_since_restore: 12480000
  timesteps_this_iter: 96000
  timesteps_total: 20160000
  training_iteration: 210
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    210 |          34242.6 | 20160000 |   773.64 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 3.0
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 26.86
    apples_agent-1_min: 0
    apples_agent-2_max: 91
    apples_agent-2_mean: 4.8
    apples_agent-2_min: 0
    apples_agent-3_max: 143
    apples_agent-3_mean: 94.57
    apples_agent-3_min: 0
    apples_agent-4_max: 52
    apples_agent-4_mean: 2.7
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 106.2
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 471
    cleaning_beam_agent-0_mean: 354.13
    cleaning_beam_agent-0_min: 218
    cleaning_beam_agent-1_max: 412
    cleaning_beam_agent-1_mean: 209.15
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 504
    cleaning_beam_agent-2_mean: 307.68
    cleaning_beam_agent-2_min: 136
    cleaning_beam_agent-3_max: 112
    cleaning_beam_agent-3_mean: 39.09
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 488
    cleaning_beam_agent-4_mean: 398.98
    cleaning_beam_agent-4_min: 246
    cleaning_beam_agent-5_max: 300
    cleaning_beam_agent-5_mean: 52.06
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-03-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 987.999999999988
  episode_reward_mean: 757.5699999999904
  episode_reward_min: 344.99999999999824
  episodes_this_iter: 96
  episodes_total: 20256
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13009.149
    learner:
      agent-0:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1249477863311768
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018879026174545288
        model: {}
        policy_loss: -0.0037192469462752342
        total_loss: -0.003895264118909836
        vf_explained_var: 0.04261285066604614
        vf_loss: 18.035253524780273
      agent-1:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1459789276123047
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019470955012366176
        model: {}
        policy_loss: -0.004325998947024345
        total_loss: -0.0044100647792220116
        vf_explained_var: -0.023987680673599243
        vf_loss: 19.320981979370117
      agent-2:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1201092004776
        entropy_coeff: 0.0017600000137463212
        kl: 0.001708958763629198
        model: {}
        policy_loss: -0.003972724080085754
        total_loss: -0.004178208764642477
        vf_explained_var: 0.06621754169464111
        vf_loss: 17.65576934814453
      agent-3:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5854799747467041
        entropy_coeff: 0.0017600000137463212
        kl: 0.001040255301631987
        model: {}
        policy_loss: -0.003003292717039585
        total_loss: -0.0024911919608712196
        vf_explained_var: 0.18266668915748596
        vf_loss: 15.425472259521484
      agent-4:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.953307032585144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018480520229786634
        model: {}
        policy_loss: -0.004522115923464298
        total_loss: -0.004468727391213179
        vf_explained_var: 0.08599025011062622
        vf_loss: 17.30487823486328
      agent-5:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8896455764770508
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017012865282595158
        model: {}
        policy_loss: -0.0038850735872983932
        total_loss: -0.003832226851955056
        vf_explained_var: 0.141574427485466
        vf_loss: 16.185420989990234
    load_time_ms: 23586.555
    num_steps_sampled: 20256000
    num_steps_trained: 20256000
    sample_time_ms: 127305.308
    update_time_ms: 64.984
  iterations_since_restore: 131
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.72197309417041
    ram_util_percent: 15.991928251121076
  pid: 14340
  policy_reward_max:
    agent-0: 164.66666666666637
    agent-1: 164.66666666666637
    agent-2: 164.66666666666637
    agent-3: 164.66666666666637
    agent-4: 164.66666666666637
    agent-5: 164.66666666666637
  policy_reward_mean:
    agent-0: 126.26166666666694
    agent-1: 126.26166666666694
    agent-2: 126.26166666666694
    agent-3: 126.26166666666694
    agent-4: 126.26166666666694
    agent-5: 126.26166666666694
  policy_reward_min:
    agent-0: 57.49999999999988
    agent-1: 57.49999999999988
    agent-2: 57.49999999999988
    agent-3: 57.49999999999988
    agent-4: 57.49999999999988
    agent-5: 57.49999999999988
  sampler_perf:
    mean_env_wait_ms: 30.47652244777784
    mean_inference_ms: 14.577177480133493
    mean_processing_ms: 65.6992314508763
  time_since_restore: 21848.555661916733
  time_this_iter_s: 156.78185844421387
  time_total_s: 34399.37253332138
  timestamp: 1637057011
  timesteps_since_restore: 12576000
  timesteps_this_iter: 96000
  timesteps_total: 20256000
  training_iteration: 211
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    211 |          34399.4 | 20256000 |   757.57 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 3.29
    apples_agent-0_min: 0
    apples_agent-1_max: 72
    apples_agent-1_mean: 21.46
    apples_agent-1_min: 0
    apples_agent-2_max: 89
    apples_agent-2_mean: 7.72
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 96.18
    apples_agent-3_min: 42
    apples_agent-4_max: 59
    apples_agent-4_mean: 1.91
    apples_agent-4_min: 0
    apples_agent-5_max: 191
    apples_agent-5_mean: 109.72
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 538
    cleaning_beam_agent-0_mean: 345.73
    cleaning_beam_agent-0_min: 214
    cleaning_beam_agent-1_max: 364
    cleaning_beam_agent-1_mean: 214.14
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 472
    cleaning_beam_agent-2_mean: 322.18
    cleaning_beam_agent-2_min: 131
    cleaning_beam_agent-3_max: 133
    cleaning_beam_agent-3_mean: 38.78
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 506
    cleaning_beam_agent-4_mean: 398.22
    cleaning_beam_agent-4_min: 236
    cleaning_beam_agent-5_max: 325
    cleaning_beam_agent-5_mean: 58.51
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-06-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 992.9999999999775
  episode_reward_mean: 776.2599999999896
  episode_reward_min: 399.00000000000415
  episodes_this_iter: 96
  episodes_total: 20352
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13031.976
    learner:
      agent-0:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1211011409759521
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013781704474240541
        model: {}
        policy_loss: -0.0036805246490985155
        total_loss: -0.003959789872169495
        vf_explained_var: 0.0165693461894989
        vf_loss: 16.93740463256836
      agent-1:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1378599405288696
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019286039751023054
        model: {}
        policy_loss: -0.004504518583416939
        total_loss: -0.0047957077622413635
        vf_explained_var: 0.013201147317886353
        vf_loss: 17.110719680786133
      agent-2:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0992882251739502
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016262648859992623
        model: {}
        policy_loss: -0.004220052622258663
        total_loss: -0.004479539580643177
        vf_explained_var: 0.03828278183937073
        vf_loss: 16.75106430053711
      agent-3:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5755338668823242
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014067088486626744
        model: {}
        policy_loss: -0.002946537919342518
        total_loss: -0.0024723238311707973
        vf_explained_var: 0.1368003785610199
        vf_loss: 14.871530532836914
      agent-4:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.961739718914032
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016317099798470736
        model: {}
        policy_loss: -0.004568392876535654
        total_loss: -0.004640977364033461
        vf_explained_var: 0.06343348324298859
        vf_loss: 16.197629928588867
      agent-5:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8778557777404785
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012201388599351048
        model: {}
        policy_loss: -0.0037372028455138206
        total_loss: -0.0037924880161881447
        vf_explained_var: 0.13745318353176117
        vf_loss: 14.89718246459961
    load_time_ms: 23625.59
    num_steps_sampled: 20352000
    num_steps_trained: 20352000
    sample_time_ms: 127288.665
    update_time_ms: 63.511
  iterations_since_restore: 132
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.251131221719458
    ram_util_percent: 15.860633484162891
  pid: 14340
  policy_reward_max:
    agent-0: 165.49999999999966
    agent-1: 165.49999999999966
    agent-2: 165.49999999999966
    agent-3: 165.49999999999966
    agent-4: 165.49999999999966
    agent-5: 165.49999999999966
  policy_reward_mean:
    agent-0: 129.37666666666692
    agent-1: 129.37666666666692
    agent-2: 129.37666666666692
    agent-3: 129.37666666666692
    agent-4: 129.37666666666692
    agent-5: 129.37666666666692
  policy_reward_min:
    agent-0: 66.49999999999982
    agent-1: 66.49999999999982
    agent-2: 66.49999999999982
    agent-3: 66.49999999999982
    agent-4: 66.49999999999982
    agent-5: 66.49999999999982
  sampler_perf:
    mean_env_wait_ms: 30.476986886295826
    mean_inference_ms: 14.574963465511665
    mean_processing_ms: 65.69369335728427
  time_since_restore: 22003.761836767197
  time_this_iter_s: 155.20617485046387
  time_total_s: 34554.578708171844
  timestamp: 1637057166
  timesteps_since_restore: 12672000
  timesteps_this_iter: 96000
  timesteps_total: 20352000
  training_iteration: 212
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    212 |          34554.6 | 20352000 |   776.26 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 4.05
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 23.41
    apples_agent-1_min: 0
    apples_agent-2_max: 40
    apples_agent-2_mean: 2.05
    apples_agent-2_min: 0
    apples_agent-3_max: 174
    apples_agent-3_mean: 97.91
    apples_agent-3_min: 33
    apples_agent-4_max: 71
    apples_agent-4_mean: 3.21
    apples_agent-4_min: 0
    apples_agent-5_max: 160
    apples_agent-5_mean: 108.43
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 516
    cleaning_beam_agent-0_mean: 342.78
    cleaning_beam_agent-0_min: 132
    cleaning_beam_agent-1_max: 371
    cleaning_beam_agent-1_mean: 208.61
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 476
    cleaning_beam_agent-2_mean: 337.61
    cleaning_beam_agent-2_min: 197
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 41.62
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 564
    cleaning_beam_agent-4_mean: 393.35
    cleaning_beam_agent-4_min: 216
    cleaning_beam_agent-5_max: 287
    cleaning_beam_agent-5_mean: 57.17
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-08-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 996.9999999999801
  episode_reward_mean: 766.1399999999896
  episode_reward_min: 314.0
  episodes_this_iter: 96
  episodes_total: 20448
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13074.94
    learner:
      agent-0:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1158242225646973
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011159345740452409
        model: {}
        policy_loss: -0.003648891346529126
        total_loss: -0.003815700765699148
        vf_explained_var: 0.08518171310424805
        vf_loss: 17.969924926757812
      agent-1:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.149115800857544
        entropy_coeff: 0.0017600000137463212
        kl: 0.001529740053229034
        model: {}
        policy_loss: -0.0044744499027729034
        total_loss: -0.004506983328610659
        vf_explained_var: -0.01660740375518799
        vf_loss: 19.897642135620117
      agent-2:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1017653942108154
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018534660339355469
        model: {}
        policy_loss: -0.004211917985230684
        total_loss: -0.004276243504136801
        vf_explained_var: 0.04071721434593201
        vf_loss: 18.746938705444336
      agent-3:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5844072103500366
        entropy_coeff: 0.0017600000137463212
        kl: 0.001592662651091814
        model: {}
        policy_loss: -0.0031567178666591644
        total_loss: -0.002602554624900222
        vf_explained_var: 0.1919519156217575
        vf_loss: 15.827186584472656
      agent-4:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9673926830291748
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014600660651922226
        model: {}
        policy_loss: -0.004317947663366795
        total_loss: -0.004315025173127651
        vf_explained_var: 0.13359546661376953
        vf_loss: 17.053966522216797
      agent-5:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8791975975036621
        entropy_coeff: 0.0017600000137463212
        kl: 0.001019281568005681
        model: {}
        policy_loss: -0.0037669651210308075
        total_loss: -0.0036815619096159935
        vf_explained_var: 0.16959531605243683
        vf_loss: 16.327796936035156
    load_time_ms: 21612.859
    num_steps_sampled: 20448000
    num_steps_trained: 20448000
    sample_time_ms: 127021.993
    update_time_ms: 60.398
  iterations_since_restore: 133
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.403478260869562
    ram_util_percent: 14.778260869565218
  pid: 14340
  policy_reward_max:
    agent-0: 166.16666666666634
    agent-1: 166.16666666666634
    agent-2: 166.16666666666634
    agent-3: 166.16666666666634
    agent-4: 166.16666666666634
    agent-5: 166.16666666666634
  policy_reward_mean:
    agent-0: 127.69000000000025
    agent-1: 127.69000000000025
    agent-2: 127.69000000000025
    agent-3: 127.69000000000025
    agent-4: 127.69000000000025
    agent-5: 127.69000000000025
  policy_reward_min:
    agent-0: 52.33333333333329
    agent-1: 52.33333333333329
    agent-2: 52.33333333333329
    agent-3: 52.33333333333329
    agent-4: 52.33333333333329
    agent-5: 52.33333333333329
  sampler_perf:
    mean_env_wait_ms: 30.480880892440794
    mean_inference_ms: 14.575353481785408
    mean_processing_ms: 65.69950406421421
  time_since_restore: 22161.22633600235
  time_this_iter_s: 157.4644992351532
  time_total_s: 34712.043207407
  timestamp: 1637057328
  timesteps_since_restore: 12768000
  timesteps_this_iter: 96000
  timesteps_total: 20448000
  training_iteration: 213
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    213 |            34712 | 20448000 |   766.14 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 72
    apples_agent-0_mean: 5.72
    apples_agent-0_min: 0
    apples_agent-1_max: 128
    apples_agent-1_mean: 25.5
    apples_agent-1_min: 0
    apples_agent-2_max: 162
    apples_agent-2_mean: 9.87
    apples_agent-2_min: 0
    apples_agent-3_max: 148
    apples_agent-3_mean: 102.94
    apples_agent-3_min: 57
    apples_agent-4_max: 33
    apples_agent-4_mean: 0.94
    apples_agent-4_min: 0
    apples_agent-5_max: 175
    apples_agent-5_mean: 110.4
    apples_agent-5_min: 27
    cleaning_beam_agent-0_max: 510
    cleaning_beam_agent-0_mean: 337.28
    cleaning_beam_agent-0_min: 177
    cleaning_beam_agent-1_max: 370
    cleaning_beam_agent-1_mean: 222.66
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 531
    cleaning_beam_agent-2_mean: 329.02
    cleaning_beam_agent-2_min: 112
    cleaning_beam_agent-3_max: 83
    cleaning_beam_agent-3_mean: 37.07
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 495
    cleaning_beam_agent-4_mean: 405.04
    cleaning_beam_agent-4_min: 308
    cleaning_beam_agent-5_max: 367
    cleaning_beam_agent-5_mean: 53.27
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-11-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 996.9999999999801
  episode_reward_mean: 797.9399999999871
  episode_reward_min: 430.000000000005
  episodes_this_iter: 96
  episodes_total: 20544
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13093.917
    learner:
      agent-0:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1191399097442627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018960076849907637
        model: {}
        policy_loss: -0.0038447666447609663
        total_loss: -0.004068240523338318
        vf_explained_var: 0.012801438570022583
        vf_loss: 17.46173095703125
      agent-1:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1530382633209229
        entropy_coeff: 0.0017600000137463212
        kl: 0.002394033595919609
        model: {}
        policy_loss: -0.004706345498561859
        total_loss: -0.0049645183607935905
        vf_explained_var: -0.006133973598480225
        vf_loss: 17.710634231567383
      agent-2:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0962908267974854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017859289655461907
        model: {}
        policy_loss: -0.004078668542206287
        total_loss: -0.004308472853153944
        vf_explained_var: 0.04586748778820038
        vf_loss: 16.9963436126709
      agent-3:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5718913078308105
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014337016036733985
        model: {}
        policy_loss: -0.0029553857166320086
        total_loss: -0.0024533746764063835
        vf_explained_var: 0.13410864770412445
        vf_loss: 15.085382461547852
      agent-4:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9484301209449768
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020214642863720655
        model: {}
        policy_loss: -0.004271847661584616
        total_loss: -0.004277884028851986
        vf_explained_var: 0.04262822866439819
        vf_loss: 16.631092071533203
      agent-5:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8781605958938599
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012259725481271744
        model: {}
        policy_loss: -0.0038247681222856045
        total_loss: -0.00386361638084054
        vf_explained_var: 0.13371892273426056
        vf_loss: 15.067070960998535
    load_time_ms: 23555.709
    num_steps_sampled: 20544000
    num_steps_trained: 20544000
    sample_time_ms: 127045.58
    update_time_ms: 69.722
  iterations_since_restore: 134
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.419444444444444
    ram_util_percent: 15.032936507936506
  pid: 14340
  policy_reward_max:
    agent-0: 166.16666666666634
    agent-1: 166.16666666666634
    agent-2: 166.16666666666634
    agent-3: 166.16666666666634
    agent-4: 166.16666666666634
    agent-5: 166.16666666666634
  policy_reward_mean:
    agent-0: 132.9900000000002
    agent-1: 132.9900000000002
    agent-2: 132.9900000000002
    agent-3: 132.9900000000002
    agent-4: 132.9900000000002
    agent-5: 132.9900000000002
  policy_reward_min:
    agent-0: 71.66666666666659
    agent-1: 71.66666666666659
    agent-2: 71.66666666666659
    agent-3: 71.66666666666659
    agent-4: 71.66666666666659
    agent-5: 71.66666666666659
  sampler_perf:
    mean_env_wait_ms: 30.483972781750097
    mean_inference_ms: 14.57536052285408
    mean_processing_ms: 65.69760129794616
  time_since_restore: 22337.349454402924
  time_this_iter_s: 176.12311840057373
  time_total_s: 34888.16632580757
  timestamp: 1637057504
  timesteps_since_restore: 12864000
  timesteps_this_iter: 96000
  timesteps_total: 20544000
  training_iteration: 214
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    214 |          34888.2 | 20544000 |   797.94 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 3.63
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 28.04
    apples_agent-1_min: 0
    apples_agent-2_max: 62
    apples_agent-2_mean: 3.21
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 98.64
    apples_agent-3_min: 7
    apples_agent-4_max: 50
    apples_agent-4_mean: 1.71
    apples_agent-4_min: 0
    apples_agent-5_max: 194
    apples_agent-5_mean: 118.18
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 477
    cleaning_beam_agent-0_mean: 344.87
    cleaning_beam_agent-0_min: 240
    cleaning_beam_agent-1_max: 357
    cleaning_beam_agent-1_mean: 220.03
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 467
    cleaning_beam_agent-2_mean: 331.54
    cleaning_beam_agent-2_min: 174
    cleaning_beam_agent-3_max: 118
    cleaning_beam_agent-3_mean: 40.5
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 527
    cleaning_beam_agent-4_mean: 406.22
    cleaning_beam_agent-4_min: 282
    cleaning_beam_agent-5_max: 298
    cleaning_beam_agent-5_mean: 44.03
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-14-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 995.9999999999791
  episode_reward_mean: 793.719999999988
  episode_reward_min: 426.0000000000099
  episodes_this_iter: 96
  episodes_total: 20640
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13048.148
    learner:
      agent-0:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1161354780197144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018628935795277357
        model: {}
        policy_loss: -0.0037120208144187927
        total_loss: -0.003938160836696625
        vf_explained_var: 0.020010218024253845
        vf_loss: 17.38239860534668
      agent-1:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1333695650100708
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019476768793538213
        model: {}
        policy_loss: -0.004187337588518858
        total_loss: -0.004412250593304634
        vf_explained_var: 6.74128532409668e-05
        vf_loss: 17.6977481842041
      agent-2:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0981166362762451
        entropy_coeff: 0.0017600000137463212
        kl: 0.00175930792465806
        model: {}
        policy_loss: -0.003898054361343384
        total_loss: -0.004092567600309849
        vf_explained_var: 0.013920918107032776
        vf_loss: 17.381532669067383
      agent-3:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5778005123138428
        entropy_coeff: 0.0017600000137463212
        kl: 0.001272788504138589
        model: {}
        policy_loss: -0.0027899115812033415
        total_loss: -0.0022348081693053246
        vf_explained_var: 0.11493386328220367
        vf_loss: 15.720329284667969
      agent-4:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9539744853973389
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013358616270124912
        model: {}
        policy_loss: -0.004224768374115229
        total_loss: -0.004224673379212618
        vf_explained_var: 0.05475141108036041
        vf_loss: 16.79062271118164
      agent-5:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8645827174186707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014171453658491373
        model: {}
        policy_loss: -0.0036607193760573864
        total_loss: -0.003626470919698477
        vf_explained_var: 0.12154035270214081
        vf_loss: 15.559103012084961
    load_time_ms: 23821.646
    num_steps_sampled: 20640000
    num_steps_trained: 20640000
    sample_time_ms: 127312.289
    update_time_ms: 58.117
  iterations_since_restore: 135
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.215283842794765
    ram_util_percent: 14.823580786026202
  pid: 14340
  policy_reward_max:
    agent-0: 165.9999999999997
    agent-1: 165.9999999999997
    agent-2: 165.9999999999997
    agent-3: 165.9999999999997
    agent-4: 165.9999999999997
    agent-5: 165.9999999999997
  policy_reward_mean:
    agent-0: 132.2866666666669
    agent-1: 132.2866666666669
    agent-2: 132.2866666666669
    agent-3: 132.2866666666669
    agent-4: 132.2866666666669
    agent-5: 132.2866666666669
  policy_reward_min:
    agent-0: 70.9999999999999
    agent-1: 70.9999999999999
    agent-2: 70.9999999999999
    agent-3: 70.9999999999999
    agent-4: 70.9999999999999
    agent-5: 70.9999999999999
  sampler_perf:
    mean_env_wait_ms: 30.486500173337554
    mean_inference_ms: 14.574848858195026
    mean_processing_ms: 65.69740242795055
  time_since_restore: 22498.15140104294
  time_this_iter_s: 160.80194664001465
  time_total_s: 35048.968272447586
  timestamp: 1637057665
  timesteps_since_restore: 12960000
  timesteps_this_iter: 96000
  timesteps_total: 20640000
  training_iteration: 215
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    215 |            35049 | 20640000 |   793.72 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 2.99
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 27.16
    apples_agent-1_min: 0
    apples_agent-2_max: 105
    apples_agent-2_mean: 5.29
    apples_agent-2_min: 0
    apples_agent-3_max: 161
    apples_agent-3_mean: 98.34
    apples_agent-3_min: 27
    apples_agent-4_max: 51
    apples_agent-4_mean: 1.65
    apples_agent-4_min: 0
    apples_agent-5_max: 185
    apples_agent-5_mean: 109.87
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 355.57
    cleaning_beam_agent-0_min: 167
    cleaning_beam_agent-1_max: 382
    cleaning_beam_agent-1_mean: 226.98
    cleaning_beam_agent-1_min: 89
    cleaning_beam_agent-2_max: 553
    cleaning_beam_agent-2_mean: 336.39
    cleaning_beam_agent-2_min: 160
    cleaning_beam_agent-3_max: 105
    cleaning_beam_agent-3_mean: 40.42
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 557
    cleaning_beam_agent-4_mean: 410.73
    cleaning_beam_agent-4_min: 257
    cleaning_beam_agent-5_max: 437
    cleaning_beam_agent-5_mean: 52.27
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-17-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 983.9999999999783
  episode_reward_mean: 798.3099999999874
  episode_reward_min: 353.00000000000244
  episodes_this_iter: 96
  episodes_total: 20736
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13131.709
    learner:
      agent-0:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1141685247421265
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017194487154483795
        model: {}
        policy_loss: -0.0034962871577590704
        total_loss: -0.0036177008878439665
        vf_explained_var: 0.04887634515762329
        vf_loss: 18.395153045654297
      agent-1:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1437958478927612
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019820518791675568
        model: {}
        policy_loss: -0.004295524675399065
        total_loss: -0.004358888138085604
        vf_explained_var: -0.004169940948486328
        vf_loss: 19.496967315673828
      agent-2:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1123782396316528
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018462184816598892
        model: {}
        policy_loss: -0.0038389558903872967
        total_loss: -0.003952092491090298
        vf_explained_var: 0.0553356409072876
        vf_loss: 18.446426391601562
      agent-3:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5647208094596863
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011948752216994762
        model: {}
        policy_loss: -0.002670692279934883
        total_loss: -0.0020720171742141247
        vf_explained_var: 0.17584429681301117
        vf_loss: 15.925848007202148
      agent-4:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9441258311271667
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016028437530621886
        model: {}
        policy_loss: -0.0044545442797243595
        total_loss: -0.004306545481085777
        vf_explained_var: 0.06298166513442993
        vf_loss: 18.096420288085938
      agent-5:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8590078949928284
        entropy_coeff: 0.0017600000137463212
        kl: 0.001685066381469369
        model: {}
        policy_loss: -0.00394368777051568
        total_loss: -0.0038192416541278362
        vf_explained_var: 0.15366573631763458
        vf_loss: 16.36301040649414
    load_time_ms: 22733.584
    num_steps_sampled: 20736000
    num_steps_trained: 20736000
    sample_time_ms: 127626.032
    update_time_ms: 62.389
  iterations_since_restore: 136
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.421238938053097
    ram_util_percent: 14.789823008849558
  pid: 14340
  policy_reward_max:
    agent-0: 163.9999999999998
    agent-1: 163.9999999999998
    agent-2: 163.9999999999998
    agent-3: 163.9999999999998
    agent-4: 163.9999999999998
    agent-5: 163.9999999999998
  policy_reward_mean:
    agent-0: 133.0516666666669
    agent-1: 133.0516666666669
    agent-2: 133.0516666666669
    agent-3: 133.0516666666669
    agent-4: 133.0516666666669
    agent-5: 133.0516666666669
  policy_reward_min:
    agent-0: 58.8333333333332
    agent-1: 58.8333333333332
    agent-2: 58.8333333333332
    agent-3: 58.8333333333332
    agent-4: 58.8333333333332
    agent-5: 58.8333333333332
  sampler_perf:
    mean_env_wait_ms: 30.491055180910262
    mean_inference_ms: 14.574681602370672
    mean_processing_ms: 65.69803055394965
  time_since_restore: 22655.120885133743
  time_this_iter_s: 156.96948409080505
  time_total_s: 35205.93775653839
  timestamp: 1637057824
  timesteps_since_restore: 13056000
  timesteps_this_iter: 96000
  timesteps_total: 20736000
  training_iteration: 216
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    216 |          35205.9 | 20736000 |   798.31 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 3.07
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 23.27
    apples_agent-1_min: 0
    apples_agent-2_max: 147
    apples_agent-2_mean: 7.23
    apples_agent-2_min: 0
    apples_agent-3_max: 233
    apples_agent-3_mean: 96.39
    apples_agent-3_min: 35
    apples_agent-4_max: 60
    apples_agent-4_mean: 3.04
    apples_agent-4_min: 0
    apples_agent-5_max: 236
    apples_agent-5_mean: 109.32
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 538
    cleaning_beam_agent-0_mean: 358.7
    cleaning_beam_agent-0_min: 135
    cleaning_beam_agent-1_max: 415
    cleaning_beam_agent-1_mean: 224.04
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 521
    cleaning_beam_agent-2_mean: 322.36
    cleaning_beam_agent-2_min: 144
    cleaning_beam_agent-3_max: 89
    cleaning_beam_agent-3_mean: 41.04
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 496
    cleaning_beam_agent-4_mean: 399.43
    cleaning_beam_agent-4_min: 202
    cleaning_beam_agent-5_max: 736
    cleaning_beam_agent-5_mean: 61.07
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-19-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1023.99999999999
  episode_reward_mean: 788.5899999999883
  episode_reward_min: 323.99999999999585
  episodes_this_iter: 96
  episodes_total: 20832
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13198.486
    learner:
      agent-0:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0903105735778809
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017595981480553746
        model: {}
        policy_loss: -0.003359312191605568
        total_loss: -0.003527347929775715
        vf_explained_var: 0.002807021141052246
        vf_loss: 17.509078979492188
      agent-1:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1400763988494873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019537184853106737
        model: {}
        policy_loss: -0.004390728659927845
        total_loss: -0.004648839123547077
        vf_explained_var: 0.0060958415269851685
        vf_loss: 17.484161376953125
      agent-2:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1049962043762207
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018808565801009536
        model: {}
        policy_loss: -0.003942370414733887
        total_loss: -0.0041539836674928665
        vf_explained_var: 0.021918311715126038
        vf_loss: 17.331741333007812
      agent-3:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5623176097869873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013573432806879282
        model: {}
        policy_loss: -0.0030042361468076706
        total_loss: -0.002421585377305746
        vf_explained_var: 0.10545183718204498
        vf_loss: 15.723287582397461
      agent-4:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9612732529640198
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019487251993268728
        model: {}
        policy_loss: -0.004412660840898752
        total_loss: -0.0045072766952216625
        vf_explained_var: 0.09450048208236694
        vf_loss: 15.97217845916748
      agent-5:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8584772944450378
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013912224676460028
        model: {}
        policy_loss: -0.0038064047694206238
        total_loss: -0.003786268178373575
        vf_explained_var: 0.12750661373138428
        vf_loss: 15.310562133789062
    load_time_ms: 21503.295
    num_steps_sampled: 20832000
    num_steps_trained: 20832000
    sample_time_ms: 128143.938
    update_time_ms: 70.989
  iterations_since_restore: 137
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.18913043478261
    ram_util_percent: 14.778260869565218
  pid: 14340
  policy_reward_max:
    agent-0: 170.66666666666657
    agent-1: 170.66666666666657
    agent-2: 170.66666666666657
    agent-3: 170.66666666666657
    agent-4: 170.66666666666657
    agent-5: 170.66666666666657
  policy_reward_mean:
    agent-0: 131.43166666666693
    agent-1: 131.43166666666693
    agent-2: 131.43166666666693
    agent-3: 131.43166666666693
    agent-4: 131.43166666666693
    agent-5: 131.43166666666693
  policy_reward_min:
    agent-0: 53.999999999999886
    agent-1: 53.999999999999886
    agent-2: 53.999999999999886
    agent-3: 53.999999999999886
    agent-4: 53.999999999999886
    agent-5: 53.999999999999886
  sampler_perf:
    mean_env_wait_ms: 30.495741995388904
    mean_inference_ms: 14.574620279461238
    mean_processing_ms: 65.69880755612587
  time_since_restore: 22813.379924058914
  time_this_iter_s: 158.2590389251709
  time_total_s: 35364.19679546356
  timestamp: 1637057986
  timesteps_since_restore: 13152000
  timesteps_this_iter: 96000
  timesteps_total: 20832000
  training_iteration: 217
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    217 |          35364.2 | 20832000 |   788.59 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 79
    apples_agent-0_mean: 4.23
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 23.17
    apples_agent-1_min: 0
    apples_agent-2_max: 102
    apples_agent-2_mean: 4.2
    apples_agent-2_min: 0
    apples_agent-3_max: 173
    apples_agent-3_mean: 93.02
    apples_agent-3_min: 48
    apples_agent-4_max: 46
    apples_agent-4_mean: 2.6
    apples_agent-4_min: 0
    apples_agent-5_max: 183
    apples_agent-5_mean: 109.11
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 482
    cleaning_beam_agent-0_mean: 356.55
    cleaning_beam_agent-0_min: 154
    cleaning_beam_agent-1_max: 370
    cleaning_beam_agent-1_mean: 228.38
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 515
    cleaning_beam_agent-2_mean: 323.53
    cleaning_beam_agent-2_min: 117
    cleaning_beam_agent-3_max: 262
    cleaning_beam_agent-3_mean: 42.5
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 499
    cleaning_beam_agent-4_mean: 402.75
    cleaning_beam_agent-4_min: 198
    cleaning_beam_agent-5_max: 268
    cleaning_beam_agent-5_mean: 54.68
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-22-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 964.9999999999898
  episode_reward_mean: 789.9299999999878
  episode_reward_min: 383.0000000000071
  episodes_this_iter: 96
  episodes_total: 20928
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13255.347
    learner:
      agent-0:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1017484664916992
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014257049188017845
        model: {}
        policy_loss: -0.0036495430395007133
        total_loss: -0.003756239078938961
        vf_explained_var: 0.04883015155792236
        vf_loss: 18.323814392089844
      agent-1:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1309478282928467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014105400769039989
        model: {}
        policy_loss: -0.00422546174377203
        total_loss: -0.004270187113434076
        vf_explained_var: -0.009777367115020752
        vf_loss: 19.457406997680664
      agent-2:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1065363883972168
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019886898808181286
        model: {}
        policy_loss: -0.003736800979822874
        total_loss: -0.0038700560107827187
        vf_explained_var: 0.05473683774471283
        vf_loss: 18.142457962036133
      agent-3:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5566778182983398
        entropy_coeff: 0.0017600000137463212
        kl: 0.000991680659353733
        model: {}
        policy_loss: -0.0025272055063396692
        total_loss: -0.0018886053003370762
        vf_explained_var: 0.15916390717029572
        vf_loss: 16.18352699279785
      agent-4:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9517146348953247
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015722094103693962
        model: {}
        policy_loss: -0.0044088708236813545
        total_loss: -0.004327976144850254
        vf_explained_var: 0.09070149064064026
        vf_loss: 17.559053421020508
      agent-5:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.85982346534729
        entropy_coeff: 0.0017600000137463212
        kl: 0.001293327659368515
        model: {}
        policy_loss: -0.0037737605161964893
        total_loss: -0.0036380267702043056
        vf_explained_var: 0.14204849302768707
        vf_loss: 16.49025535583496
    load_time_ms: 19074.161
    num_steps_sampled: 20928000
    num_steps_trained: 20928000
    sample_time_ms: 128497.795
    update_time_ms: 71.263
  iterations_since_restore: 138
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.254148471615725
    ram_util_percent: 14.769432314410482
  pid: 14340
  policy_reward_max:
    agent-0: 160.83333333333343
    agent-1: 160.83333333333343
    agent-2: 160.83333333333343
    agent-3: 160.83333333333343
    agent-4: 160.83333333333343
    agent-5: 160.83333333333343
  policy_reward_mean:
    agent-0: 131.65500000000023
    agent-1: 131.65500000000023
    agent-2: 131.65500000000023
    agent-3: 131.65500000000023
    agent-4: 131.65500000000023
    agent-5: 131.65500000000023
  policy_reward_min:
    agent-0: 63.83333333333308
    agent-1: 63.83333333333308
    agent-2: 63.83333333333308
    agent-3: 63.83333333333308
    agent-4: 63.83333333333308
    agent-5: 63.83333333333308
  sampler_perf:
    mean_env_wait_ms: 30.499181011275038
    mean_inference_ms: 14.57493243782594
    mean_processing_ms: 65.69909786155208
  time_since_restore: 22970.339056015015
  time_this_iter_s: 156.95913195610046
  time_total_s: 35521.15592741966
  timestamp: 1637058147
  timesteps_since_restore: 13248000
  timesteps_this_iter: 96000
  timesteps_total: 20928000
  training_iteration: 218
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    218 |          35521.2 | 20928000 |   789.93 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 2.44
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 24.27
    apples_agent-1_min: 0
    apples_agent-2_max: 144
    apples_agent-2_mean: 12.52
    apples_agent-2_min: 0
    apples_agent-3_max: 161
    apples_agent-3_mean: 95.24
    apples_agent-3_min: 10
    apples_agent-4_max: 39
    apples_agent-4_mean: 1.04
    apples_agent-4_min: 0
    apples_agent-5_max: 183
    apples_agent-5_mean: 114.21
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 518
    cleaning_beam_agent-0_mean: 377.8
    cleaning_beam_agent-0_min: 210
    cleaning_beam_agent-1_max: 501
    cleaning_beam_agent-1_mean: 244.91
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 490
    cleaning_beam_agent-2_mean: 314.4
    cleaning_beam_agent-2_min: 114
    cleaning_beam_agent-3_max: 88
    cleaning_beam_agent-3_mean: 40.02
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 514
    cleaning_beam_agent-4_mean: 411.8
    cleaning_beam_agent-4_min: 272
    cleaning_beam_agent-5_max: 283
    cleaning_beam_agent-5_mean: 45.8
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-25-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1001.9999999999675
  episode_reward_mean: 793.6999999999882
  episode_reward_min: 431.0000000000039
  episodes_this_iter: 96
  episodes_total: 21024
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13271.352
    learner:
      agent-0:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0964057445526123
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021269710268825293
        model: {}
        policy_loss: -0.0037873191758990288
        total_loss: -0.003986336290836334
        vf_explained_var: 0.022004827857017517
        vf_loss: 17.306575775146484
      agent-1:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.133732795715332
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016502657672390342
        model: {}
        policy_loss: -0.004313876386731863
        total_loss: -0.004506367724388838
        vf_explained_var: -0.010697007179260254
        vf_loss: 18.028804779052734
      agent-2:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.094099521636963
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018928415374830365
        model: {}
        policy_loss: -0.0039009274914860725
        total_loss: -0.004076449666172266
        vf_explained_var: 0.030079171061515808
        vf_loss: 17.500926971435547
      agent-3:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5604334473609924
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011401887750253081
        model: {}
        policy_loss: -0.0029404349625110626
        total_loss: -0.0024102991446852684
        vf_explained_var: 0.1511046439409256
        vf_loss: 15.164952278137207
      agent-4:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9526472091674805
        entropy_coeff: 0.0017600000137463212
        kl: 0.002021677326411009
        model: {}
        policy_loss: -0.0044744983315467834
        total_loss: -0.004482105374336243
        vf_explained_var: 0.058518558740615845
        vf_loss: 16.690492630004883
      agent-5:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8609243035316467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011821106309071183
        model: {}
        policy_loss: -0.0036560811568051577
        total_loss: -0.003650013357400894
        vf_explained_var: 0.14121496677398682
        vf_loss: 15.21296215057373
    load_time_ms: 18048.29
    num_steps_sampled: 21024000
    num_steps_trained: 21024000
    sample_time_ms: 128723.798
    update_time_ms: 75.096
  iterations_since_restore: 139
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.20349344978166
    ram_util_percent: 14.805240174672488
  pid: 14340
  policy_reward_max:
    agent-0: 166.99999999999932
    agent-1: 166.99999999999932
    agent-2: 166.99999999999932
    agent-3: 166.99999999999932
    agent-4: 166.99999999999932
    agent-5: 166.99999999999932
  policy_reward_mean:
    agent-0: 132.28333333333356
    agent-1: 132.28333333333356
    agent-2: 132.28333333333356
    agent-3: 132.28333333333356
    agent-4: 132.28333333333356
    agent-5: 132.28333333333356
  policy_reward_min:
    agent-0: 71.83333333333312
    agent-1: 71.83333333333312
    agent-2: 71.83333333333312
    agent-3: 71.83333333333312
    agent-4: 71.83333333333312
    agent-5: 71.83333333333312
  sampler_perf:
    mean_env_wait_ms: 30.503580973085388
    mean_inference_ms: 14.574955339416931
    mean_processing_ms: 65.70444624479134
  time_since_restore: 23127.628568172455
  time_this_iter_s: 157.28951215744019
  time_total_s: 35678.4454395771
  timestamp: 1637058308
  timesteps_since_restore: 13344000
  timesteps_this_iter: 96000
  timesteps_total: 21024000
  training_iteration: 219
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    219 |          35678.4 | 21024000 |    793.7 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 4.44
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 24.73
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 4.88
    apples_agent-2_min: 0
    apples_agent-3_max: 197
    apples_agent-3_mean: 88.42
    apples_agent-3_min: 38
    apples_agent-4_max: 61
    apples_agent-4_mean: 3.62
    apples_agent-4_min: 0
    apples_agent-5_max: 187
    apples_agent-5_mean: 104.58
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 512
    cleaning_beam_agent-0_mean: 367.79
    cleaning_beam_agent-0_min: 160
    cleaning_beam_agent-1_max: 468
    cleaning_beam_agent-1_mean: 226.19
    cleaning_beam_agent-1_min: 56
    cleaning_beam_agent-2_max: 513
    cleaning_beam_agent-2_mean: 332.03
    cleaning_beam_agent-2_min: 124
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 42.38
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 500
    cleaning_beam_agent-4_mean: 386.91
    cleaning_beam_agent-4_min: 155
    cleaning_beam_agent-5_max: 246
    cleaning_beam_agent-5_mean: 51.52
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-27-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1005.9999999999759
  episode_reward_mean: 770.4099999999899
  episode_reward_min: 294.0000000000004
  episodes_this_iter: 96
  episodes_total: 21120
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13300.132
    learner:
      agent-0:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0902092456817627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018579522147774696
        model: {}
        policy_loss: -0.00354685727506876
        total_loss: -0.0036280169151723385
        vf_explained_var: 0.07694604992866516
        vf_loss: 18.376100540161133
      agent-1:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1304501295089722
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012883945601060987
        model: {}
        policy_loss: -0.004081211052834988
        total_loss: -0.004027619492262602
        vf_explained_var: -0.02616947889328003
        vf_loss: 20.43185043334961
      agent-2:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.111024260520935
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015205893432721496
        model: {}
        policy_loss: -0.0036705411039292812
        total_loss: -0.0037303895223885775
        vf_explained_var: 0.048735812306404114
        vf_loss: 18.95555305480957
      agent-3:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5758219957351685
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010297951521351933
        model: {}
        policy_loss: -0.0026476248167455196
        total_loss: -0.00203867070376873
        vf_explained_var: 0.18975283205509186
        vf_loss: 16.223966598510742
      agent-4:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9568755626678467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017038116930052638
        model: {}
        policy_loss: -0.004265651572495699
        total_loss: -0.004145103041082621
        vf_explained_var: 0.09567303955554962
        vf_loss: 18.046476364135742
      agent-5:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8723394274711609
        entropy_coeff: 0.0017600000137463212
        kl: 0.00116654671728611
        model: {}
        policy_loss: -0.003490202594548464
        total_loss: -0.0034368587657809258
        vf_explained_var: 0.20192039012908936
        vf_loss: 15.886617660522461
    load_time_ms: 17518.896
    num_steps_sampled: 21120000
    num_steps_trained: 21120000
    sample_time_ms: 128237.205
    update_time_ms: 89.691
  iterations_since_restore: 140
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.464
    ram_util_percent: 14.785333333333337
  pid: 14340
  policy_reward_max:
    agent-0: 167.66666666666632
    agent-1: 167.66666666666632
    agent-2: 167.66666666666632
    agent-3: 167.66666666666632
    agent-4: 167.66666666666632
    agent-5: 167.66666666666632
  policy_reward_mean:
    agent-0: 128.40166666666687
    agent-1: 128.40166666666687
    agent-2: 128.40166666666687
    agent-3: 128.40166666666687
    agent-4: 128.40166666666687
    agent-5: 128.40166666666687
  policy_reward_min:
    agent-0: 48.99999999999987
    agent-1: 48.99999999999987
    agent-2: 48.99999999999987
    agent-3: 48.99999999999987
    agent-4: 48.99999999999987
    agent-5: 48.99999999999987
  sampler_perf:
    mean_env_wait_ms: 30.506613063937824
    mean_inference_ms: 14.575500500377611
    mean_processing_ms: 65.7018198072044
  time_since_restore: 23284.669730901718
  time_this_iter_s: 157.0411627292633
  time_total_s: 35835.486602306366
  timestamp: 1637058466
  timesteps_since_restore: 13440000
  timesteps_this_iter: 96000
  timesteps_total: 21120000
  training_iteration: 220
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    220 |          35835.5 | 21120000 |   770.41 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 3.57
    apples_agent-0_min: 0
    apples_agent-1_max: 131
    apples_agent-1_mean: 25.26
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 5.09
    apples_agent-2_min: 0
    apples_agent-3_max: 152
    apples_agent-3_mean: 93.88
    apples_agent-3_min: 24
    apples_agent-4_max: 44
    apples_agent-4_mean: 1.64
    apples_agent-4_min: 0
    apples_agent-5_max: 193
    apples_agent-5_mean: 106.46
    apples_agent-5_min: 28
    cleaning_beam_agent-0_max: 564
    cleaning_beam_agent-0_mean: 379.85
    cleaning_beam_agent-0_min: 170
    cleaning_beam_agent-1_max: 448
    cleaning_beam_agent-1_mean: 235.54
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 513
    cleaning_beam_agent-2_mean: 338.45
    cleaning_beam_agent-2_min: 170
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 41.25
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 512
    cleaning_beam_agent-4_mean: 401.5
    cleaning_beam_agent-4_min: 235
    cleaning_beam_agent-5_max: 330
    cleaning_beam_agent-5_mean: 56.41
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-30-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1027.9999999999832
  episode_reward_mean: 794.9699999999867
  episode_reward_min: 263.999999999998
  episodes_this_iter: 96
  episodes_total: 21216
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13281.572
    learner:
      agent-0:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0828193426132202
        entropy_coeff: 0.0017600000137463212
        kl: 0.001756914658471942
        model: {}
        policy_loss: -0.0034457147121429443
        total_loss: -0.003501584753394127
        vf_explained_var: 0.05512365698814392
        vf_loss: 18.498897552490234
      agent-1:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1398835182189941
        entropy_coeff: 0.0017600000137463212
        kl: 0.001404573442414403
        model: {}
        policy_loss: -0.004378079902380705
        total_loss: -0.0044696638360619545
        vf_explained_var: 0.0196925550699234
        vf_loss: 19.146093368530273
      agent-2:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1011226177215576
        entropy_coeff: 0.0017600000137463212
        kl: 0.002775323810055852
        model: {}
        policy_loss: -0.004239036235958338
        total_loss: -0.004334491677582264
        vf_explained_var: 0.05449064075946808
        vf_loss: 18.425189971923828
      agent-3:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5618478655815125
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013479789486154914
        model: {}
        policy_loss: -0.0026998710818588734
        total_loss: -0.0020290985703468323
        vf_explained_var: 0.15471085906028748
        vf_loss: 16.596214294433594
      agent-4:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9607197046279907
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017734230495989323
        model: {}
        policy_loss: -0.004313596524298191
        total_loss: -0.004205646459013224
        vf_explained_var: 0.07774432003498077
        vf_loss: 17.9881591796875
      agent-5:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8534480929374695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015392100904136896
        model: {}
        policy_loss: -0.0037926500663161278
        total_loss: -0.0036947084590792656
        vf_explained_var: 0.17632783949375153
        vf_loss: 16.000112533569336
    load_time_ms: 19257.021
    num_steps_sampled: 21216000
    num_steps_trained: 21216000
    sample_time_ms: 128331.289
    update_time_ms: 88.909
  iterations_since_restore: 141
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.5904
    ram_util_percent: 15.012000000000002
  pid: 14340
  policy_reward_max:
    agent-0: 171.33333333333303
    agent-1: 171.33333333333303
    agent-2: 171.33333333333303
    agent-3: 171.33333333333303
    agent-4: 171.33333333333303
    agent-5: 171.33333333333303
  policy_reward_mean:
    agent-0: 132.4950000000002
    agent-1: 132.4950000000002
    agent-2: 132.4950000000002
    agent-3: 132.4950000000002
    agent-4: 132.4950000000002
    agent-5: 132.4950000000002
  policy_reward_min:
    agent-0: 43.99999999999999
    agent-1: 43.99999999999999
    agent-2: 43.99999999999999
    agent-3: 43.99999999999999
    agent-4: 43.99999999999999
    agent-5: 43.99999999999999
  sampler_perf:
    mean_env_wait_ms: 30.511861742209803
    mean_inference_ms: 14.575837281850868
    mean_processing_ms: 65.70188342774105
  time_since_restore: 23459.550838708878
  time_this_iter_s: 174.88110780715942
  time_total_s: 36010.367710113525
  timestamp: 1637058642
  timesteps_since_restore: 13536000
  timesteps_this_iter: 96000
  timesteps_total: 21216000
  training_iteration: 221
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    221 |          36010.4 | 21216000 |   794.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 70
    apples_agent-0_mean: 4.01
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 21.31
    apples_agent-1_min: 0
    apples_agent-2_max: 63
    apples_agent-2_mean: 4.85
    apples_agent-2_min: 0
    apples_agent-3_max: 162
    apples_agent-3_mean: 95.88
    apples_agent-3_min: 4
    apples_agent-4_max: 48
    apples_agent-4_mean: 1.99
    apples_agent-4_min: 0
    apples_agent-5_max: 200
    apples_agent-5_mean: 105.42
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 543
    cleaning_beam_agent-0_mean: 376.19
    cleaning_beam_agent-0_min: 171
    cleaning_beam_agent-1_max: 439
    cleaning_beam_agent-1_mean: 251.88
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 530
    cleaning_beam_agent-2_mean: 370.9
    cleaning_beam_agent-2_min: 181
    cleaning_beam_agent-3_max: 185
    cleaning_beam_agent-3_mean: 43.54
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 483
    cleaning_beam_agent-4_mean: 397.8
    cleaning_beam_agent-4_min: 194
    cleaning_beam_agent-5_max: 373
    cleaning_beam_agent-5_mean: 52.06
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-33-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1027.9999999999845
  episode_reward_mean: 808.7499999999866
  episode_reward_min: 98.0000000000006
  episodes_this_iter: 96
  episodes_total: 21312
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13297.918
    learner:
      agent-0:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0858030319213867
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012098010629415512
        model: {}
        policy_loss: -0.0031119505874812603
        total_loss: -0.003197100944817066
        vf_explained_var: 0.09103888273239136
        vf_loss: 18.25861930847168
      agent-1:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1196887493133545
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017418954521417618
        model: {}
        policy_loss: -0.004536765161901712
        total_loss: -0.004460083786398172
        vf_explained_var: -0.01216822862625122
        vf_loss: 20.473316192626953
      agent-2:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.082395076751709
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014661225723102689
        model: {}
        policy_loss: -0.0036641908809542656
        total_loss: -0.0036774715408682823
        vf_explained_var: 0.056942686438560486
        vf_loss: 18.917354583740234
      agent-3:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5694414377212524
        entropy_coeff: 0.0017600000137463212
        kl: 0.00122765451669693
        model: {}
        policy_loss: -0.002987740095704794
        total_loss: -0.0022429670207202435
        vf_explained_var: 0.14680401980876923
        vf_loss: 17.469905853271484
      agent-4:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9509413242340088
        entropy_coeff: 0.0017600000137463212
        kl: 0.001985399052500725
        model: {}
        policy_loss: -0.004523129668086767
        total_loss: -0.004360917489975691
        vf_explained_var: 0.08871890604496002
        vf_loss: 18.35869789123535
      agent-5:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8340210318565369
        entropy_coeff: 0.0017600000137463212
        kl: 0.001773008261807263
        model: {}
        policy_loss: -0.003640458919107914
        total_loss: -0.0034377186093479395
        vf_explained_var: 0.1667328178882599
        vf_loss: 16.70616912841797
    load_time_ms: 19171.65
    num_steps_sampled: 21312000
    num_steps_trained: 21312000
    sample_time_ms: 128659.172
    update_time_ms: 68.967
  iterations_since_restore: 142
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.533482142857142
    ram_util_percent: 14.766964285714286
  pid: 14340
  policy_reward_max:
    agent-0: 171.3333333333333
    agent-1: 171.3333333333333
    agent-2: 171.3333333333333
    agent-3: 171.3333333333333
    agent-4: 171.3333333333333
    agent-5: 171.3333333333333
  policy_reward_mean:
    agent-0: 134.79166666666686
    agent-1: 134.79166666666686
    agent-2: 134.79166666666686
    agent-3: 134.79166666666686
    agent-4: 134.79166666666686
    agent-5: 134.79166666666686
  policy_reward_min:
    agent-0: 16.33333333333331
    agent-1: 16.33333333333331
    agent-2: 16.33333333333331
    agent-3: 16.33333333333331
    agent-4: 16.33333333333331
    agent-5: 16.33333333333331
  sampler_perf:
    mean_env_wait_ms: 30.51702422053685
    mean_inference_ms: 14.575232945399582
    mean_processing_ms: 65.69919928988006
  time_since_restore: 23617.122885465622
  time_this_iter_s: 157.57204675674438
  time_total_s: 36167.93975687027
  timestamp: 1637058800
  timesteps_since_restore: 13632000
  timesteps_this_iter: 96000
  timesteps_total: 21312000
  training_iteration: 222
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    222 |          36167.9 | 21312000 |   808.75 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 3.76
    apples_agent-0_min: 0
    apples_agent-1_max: 155
    apples_agent-1_mean: 26.19
    apples_agent-1_min: 0
    apples_agent-2_max: 141
    apples_agent-2_mean: 6.79
    apples_agent-2_min: 0
    apples_agent-3_max: 164
    apples_agent-3_mean: 97.8
    apples_agent-3_min: 50
    apples_agent-4_max: 105
    apples_agent-4_mean: 3.74
    apples_agent-4_min: 0
    apples_agent-5_max: 198
    apples_agent-5_mean: 106.48
    apples_agent-5_min: 29
    cleaning_beam_agent-0_max: 512
    cleaning_beam_agent-0_mean: 371.09
    cleaning_beam_agent-0_min: 204
    cleaning_beam_agent-1_max: 428
    cleaning_beam_agent-1_mean: 235.03
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 543
    cleaning_beam_agent-2_mean: 359.72
    cleaning_beam_agent-2_min: 93
    cleaning_beam_agent-3_max: 116
    cleaning_beam_agent-3_mean: 46.04
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 479
    cleaning_beam_agent-4_mean: 394.33
    cleaning_beam_agent-4_min: 243
    cleaning_beam_agent-5_max: 373
    cleaning_beam_agent-5_mean: 55.07
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-35-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 994.99999999998
  episode_reward_mean: 810.8899999999849
  episode_reward_min: 323.0000000000001
  episodes_this_iter: 96
  episodes_total: 21408
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13285.678
    learner:
      agent-0:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0924369096755981
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018729866715148091
        model: {}
        policy_loss: -0.0036949114874005318
        total_loss: -0.0037758778780698776
        vf_explained_var: 0.012513667345046997
        vf_loss: 18.41720962524414
      agent-1:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.138679027557373
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016484714578837156
        model: {}
        policy_loss: -0.0045202807523310184
        total_loss: -0.004620515741407871
        vf_explained_var: -0.010100066661834717
        vf_loss: 19.038414001464844
      agent-2:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.081287145614624
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021630357950925827
        model: {}
        policy_loss: -0.00416912604123354
        total_loss: -0.0042537786066532135
        vf_explained_var: 0.030125051736831665
        vf_loss: 18.184154510498047
      agent-3:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5475698709487915
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013874624855816364
        model: {}
        policy_loss: -0.002700398676097393
        total_loss: -0.002016899175941944
        vf_explained_var: 0.12438890337944031
        vf_loss: 16.472200393676758
      agent-4:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9508389234542847
        entropy_coeff: 0.0017600000137463212
        kl: 0.002167707309126854
        model: {}
        policy_loss: -0.004138660617172718
        total_loss: -0.0041276682168245316
        vf_explained_var: 0.1020805835723877
        vf_loss: 16.844684600830078
      agent-5:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8468554019927979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014337600441649556
        model: {}
        policy_loss: -0.0035476000048220158
        total_loss: -0.0034051991533488035
        vf_explained_var: 0.12553364038467407
        vf_loss: 16.328672409057617
    load_time_ms: 19084.427
    num_steps_sampled: 21408000
    num_steps_trained: 21408000
    sample_time_ms: 128579.567
    update_time_ms: 73.537
  iterations_since_restore: 143
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.371300448430492
    ram_util_percent: 14.765470852017941
  pid: 14340
  policy_reward_max:
    agent-0: 165.83333333333346
    agent-1: 165.83333333333346
    agent-2: 165.83333333333346
    agent-3: 165.83333333333346
    agent-4: 165.83333333333346
    agent-5: 165.83333333333346
  policy_reward_mean:
    agent-0: 135.14833333333354
    agent-1: 135.14833333333354
    agent-2: 135.14833333333354
    agent-3: 135.14833333333354
    agent-4: 135.14833333333354
    agent-5: 135.14833333333354
  policy_reward_min:
    agent-0: 53.83333333333322
    agent-1: 53.83333333333322
    agent-2: 53.83333333333322
    agent-3: 53.83333333333322
    agent-4: 53.83333333333322
    agent-5: 53.83333333333322
  sampler_perf:
    mean_env_wait_ms: 30.520509292707157
    mean_inference_ms: 14.574767065780293
    mean_processing_ms: 65.69565861206826
  time_since_restore: 23772.91581583023
  time_this_iter_s: 155.79293036460876
  time_total_s: 36323.73268723488
  timestamp: 1637058956
  timesteps_since_restore: 13728000
  timesteps_this_iter: 96000
  timesteps_total: 21408000
  training_iteration: 223
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    223 |          36323.7 | 21408000 |   810.89 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 2.79
    apples_agent-0_min: 0
    apples_agent-1_max: 121
    apples_agent-1_mean: 24.61
    apples_agent-1_min: 0
    apples_agent-2_max: 681
    apples_agent-2_mean: 11.87
    apples_agent-2_min: 0
    apples_agent-3_max: 144
    apples_agent-3_mean: 95.21
    apples_agent-3_min: 7
    apples_agent-4_max: 102
    apples_agent-4_mean: 3.48
    apples_agent-4_min: 0
    apples_agent-5_max: 593
    apples_agent-5_mean: 115.67
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 502
    cleaning_beam_agent-0_mean: 353.65
    cleaning_beam_agent-0_min: 174
    cleaning_beam_agent-1_max: 419
    cleaning_beam_agent-1_mean: 231.62
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 490
    cleaning_beam_agent-2_mean: 358.6
    cleaning_beam_agent-2_min: 65
    cleaning_beam_agent-3_max: 177
    cleaning_beam_agent-3_mean: 42.3
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 525
    cleaning_beam_agent-4_mean: 412.53
    cleaning_beam_agent-4_min: 168
    cleaning_beam_agent-5_max: 326
    cleaning_beam_agent-5_mean: 43.43
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-38-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 992.999999999985
  episode_reward_mean: 830.8899999999858
  episode_reward_min: 130.00000000000153
  episodes_this_iter: 96
  episodes_total: 21504
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13266.062
    learner:
      agent-0:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.103838324546814
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013220962136983871
        model: {}
        policy_loss: -0.0032154605723917484
        total_loss: -0.0030640375334769487
        vf_explained_var: 0.00917041301727295
        vf_loss: 20.941787719726562
      agent-1:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1172559261322021
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012367048766463995
        model: {}
        policy_loss: -0.003981437534093857
        total_loss: -0.003757292404770851
        vf_explained_var: -0.02198411524295807
        vf_loss: 21.905147552490234
      agent-2:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0816618204116821
        entropy_coeff: 0.0017600000137463212
        kl: 0.001458187005482614
        model: {}
        policy_loss: -0.0037569724954664707
        total_loss: -0.0037453584372997284
        vf_explained_var: 0.10578550398349762
        vf_loss: 19.153404235839844
      agent-3:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5382077693939209
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008583185262978077
        model: {}
        policy_loss: -0.0023571052588522434
        total_loss: -0.0014548590406775475
        vf_explained_var: 0.1201307624578476
        vf_loss: 18.494897842407227
      agent-4:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9392509460449219
        entropy_coeff: 0.0017600000137463212
        kl: 0.001943429815582931
        model: {}
        policy_loss: -0.004410486202687025
        total_loss: -0.004133842419832945
        vf_explained_var: 0.08008670806884766
        vf_loss: 19.29725456237793
      agent-5:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8194838166236877
        entropy_coeff: 0.0017600000137463212
        kl: 0.002108559012413025
        model: {}
        policy_loss: -0.0033898111432790756
        total_loss: -0.0029896479099988937
        vf_explained_var: 0.120462566614151
        vf_loss: 18.424564361572266
    load_time_ms: 17170.995
    num_steps_sampled: 21504000
    num_steps_trained: 21504000
    sample_time_ms: 128549.483
    update_time_ms: 63.466
  iterations_since_restore: 144
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.767117117117117
    ram_util_percent: 14.742792792792795
  pid: 14340
  policy_reward_max:
    agent-0: 165.49999999999974
    agent-1: 165.49999999999974
    agent-2: 165.49999999999974
    agent-3: 165.49999999999974
    agent-4: 165.49999999999974
    agent-5: 165.49999999999974
  policy_reward_mean:
    agent-0: 138.4816666666668
    agent-1: 138.4816666666668
    agent-2: 138.4816666666668
    agent-3: 138.4816666666668
    agent-4: 138.4816666666668
    agent-5: 138.4816666666668
  policy_reward_min:
    agent-0: 21.66666666666669
    agent-1: 21.66666666666669
    agent-2: 21.66666666666669
    agent-3: 21.66666666666669
    agent-4: 21.66666666666669
    agent-5: 21.66666666666669
  sampler_perf:
    mean_env_wait_ms: 30.524888563938934
    mean_inference_ms: 14.574638779163067
    mean_processing_ms: 65.69600565527986
  time_since_restore: 23929.658507585526
  time_this_iter_s: 156.7426917552948
  time_total_s: 36480.47537899017
  timestamp: 1637059113
  timesteps_since_restore: 13824000
  timesteps_this_iter: 96000
  timesteps_total: 21504000
  training_iteration: 224
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    224 |          36480.5 | 21504000 |   830.89 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 72
    apples_agent-0_mean: 3.79
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 23.54
    apples_agent-1_min: 0
    apples_agent-2_max: 126
    apples_agent-2_mean: 6.01
    apples_agent-2_min: 0
    apples_agent-3_max: 195
    apples_agent-3_mean: 99.49
    apples_agent-3_min: 47
    apples_agent-4_max: 52
    apples_agent-4_mean: 1.18
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 107.82
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 522
    cleaning_beam_agent-0_mean: 345.04
    cleaning_beam_agent-0_min: 187
    cleaning_beam_agent-1_max: 353
    cleaning_beam_agent-1_mean: 228.62
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 619
    cleaning_beam_agent-2_mean: 369.3
    cleaning_beam_agent-2_min: 115
    cleaning_beam_agent-3_max: 105
    cleaning_beam_agent-3_mean: 42.5
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 514
    cleaning_beam_agent-4_mean: 424.6
    cleaning_beam_agent-4_min: 177
    cleaning_beam_agent-5_max: 306
    cleaning_beam_agent-5_mean: 48.82
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-41-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1000.9999999999637
  episode_reward_mean: 830.8299999999856
  episode_reward_min: 365.00000000000426
  episodes_this_iter: 96
  episodes_total: 21600
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13227.14
    learner:
      agent-0:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0954244136810303
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013773589162155986
        model: {}
        policy_loss: -0.003354766871780157
        total_loss: -0.0035341656766831875
        vf_explained_var: 0.04475538432598114
        vf_loss: 17.48548126220703
      agent-1:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1164374351501465
        entropy_coeff: 0.0017600000137463212
        kl: 0.001899250317364931
        model: {}
        policy_loss: -0.004566669464111328
        total_loss: -0.00462520495057106
        vf_explained_var: -0.015517115592956543
        vf_loss: 19.063915252685547
      agent-2:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0821424722671509
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015275483019649982
        model: {}
        policy_loss: -0.003753448836505413
        total_loss: -0.003892632434144616
        vf_explained_var: 0.04360078275203705
        vf_loss: 17.653894424438477
      agent-3:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5486446619033813
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009804824367165565
        model: {}
        policy_loss: -0.0026270225644111633
        total_loss: -0.0019518351182341576
        vf_explained_var: 0.11043131351470947
        vf_loss: 16.407991409301758
      agent-4:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9448590278625488
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020368993282318115
        model: {}
        policy_loss: -0.004258595872670412
        total_loss: -0.004252332728356123
        vf_explained_var: 0.08652505278587341
        vf_loss: 16.692157745361328
      agent-5:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8168576955795288
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015513724647462368
        model: {}
        policy_loss: -0.0035467089619487524
        total_loss: -0.0033539505675435066
        vf_explained_var: 0.10586684942245483
        vf_loss: 16.30428123474121
    load_time_ms: 18611.393
    num_steps_sampled: 21600000
    num_steps_trained: 21600000
    sample_time_ms: 128255.93
    update_time_ms: 70.677
  iterations_since_restore: 145
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.568979591836733
    ram_util_percent: 14.968163265306126
  pid: 14340
  policy_reward_max:
    agent-0: 166.83333333333297
    agent-1: 166.83333333333297
    agent-2: 166.83333333333297
    agent-3: 166.83333333333297
    agent-4: 166.83333333333297
    agent-5: 166.83333333333297
  policy_reward_mean:
    agent-0: 138.4716666666669
    agent-1: 138.4716666666669
    agent-2: 138.4716666666669
    agent-3: 138.4716666666669
    agent-4: 138.4716666666669
    agent-5: 138.4716666666669
  policy_reward_min:
    agent-0: 60.833333333333094
    agent-1: 60.833333333333094
    agent-2: 60.833333333333094
    agent-3: 60.833333333333094
    agent-4: 60.833333333333094
    agent-5: 60.833333333333094
  sampler_perf:
    mean_env_wait_ms: 30.528842486810646
    mean_inference_ms: 14.574373338982324
    mean_processing_ms: 65.69482021019073
  time_since_restore: 24101.640429973602
  time_this_iter_s: 171.98192238807678
  time_total_s: 36652.45730137825
  timestamp: 1637059285
  timesteps_since_restore: 13920000
  timesteps_this_iter: 96000
  timesteps_total: 21600000
  training_iteration: 225
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    225 |          36652.5 | 21600000 |   830.83 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 72
    apples_agent-0_mean: 3.72
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 28.17
    apples_agent-1_min: 0
    apples_agent-2_max: 95
    apples_agent-2_mean: 4.97
    apples_agent-2_min: 0
    apples_agent-3_max: 201
    apples_agent-3_mean: 99.06
    apples_agent-3_min: 44
    apples_agent-4_max: 52
    apples_agent-4_mean: 2.21
    apples_agent-4_min: 0
    apples_agent-5_max: 181
    apples_agent-5_mean: 110.49
    apples_agent-5_min: 57
    cleaning_beam_agent-0_max: 489
    cleaning_beam_agent-0_mean: 349.29
    cleaning_beam_agent-0_min: 220
    cleaning_beam_agent-1_max: 411
    cleaning_beam_agent-1_mean: 226.42
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 514
    cleaning_beam_agent-2_mean: 359.77
    cleaning_beam_agent-2_min: 152
    cleaning_beam_agent-3_max: 176
    cleaning_beam_agent-3_mean: 44.99
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 567
    cleaning_beam_agent-4_mean: 422.27
    cleaning_beam_agent-4_min: 226
    cleaning_beam_agent-5_max: 196
    cleaning_beam_agent-5_mean: 40.38
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-44-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1092.9999999999823
  episode_reward_mean: 842.3399999999867
  episode_reward_min: 392.0000000000071
  episodes_this_iter: 96
  episodes_total: 21696
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13229.256
    learner:
      agent-0:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1024500131607056
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016348926583305001
        model: {}
        policy_loss: -0.003362100338563323
        total_loss: -0.003423415357246995
        vf_explained_var: 0.012631624937057495
        vf_loss: 18.78998565673828
      agent-1:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1347140073776245
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015365664148703218
        model: {}
        policy_loss: -0.0039039633702486753
        total_loss: -0.003942503593862057
        vf_explained_var: -0.0010727494955062866
        vf_loss: 19.585580825805664
      agent-2:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0863178968429565
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012940085725858808
        model: {}
        policy_loss: -0.00374350743368268
        total_loss: -0.0038598899263888597
        vf_explained_var: 0.05793917179107666
        vf_loss: 17.955352783203125
      agent-3:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5589801669120789
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015224090311676264
        model: {}
        policy_loss: -0.002761395648121834
        total_loss: -0.0020307982340455055
        vf_explained_var: 0.11402101814746857
        vf_loss: 17.143997192382812
      agent-4:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.932633638381958
        entropy_coeff: 0.0017600000137463212
        kl: 0.001722076558507979
        model: {}
        policy_loss: -0.004359155427664518
        total_loss: -0.004218219313770533
        vf_explained_var: 0.05983152985572815
        vf_loss: 17.82373046875
      agent-5:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7951432466506958
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012074391124770045
        model: {}
        policy_loss: -0.003431614488363266
        total_loss: -0.003216845216229558
        vf_explained_var: 0.13649553060531616
        vf_loss: 16.142253875732422
    load_time_ms: 18584.264
    num_steps_sampled: 21696000
    num_steps_trained: 21696000
    sample_time_ms: 128326.82
    update_time_ms: 67.901
  iterations_since_restore: 146
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.618666666666666
    ram_util_percent: 14.712444444444445
  pid: 14340
  policy_reward_max:
    agent-0: 182.1666666666663
    agent-1: 182.1666666666663
    agent-2: 182.1666666666663
    agent-3: 182.1666666666663
    agent-4: 182.1666666666663
    agent-5: 182.1666666666663
  policy_reward_mean:
    agent-0: 140.3900000000001
    agent-1: 140.3900000000001
    agent-2: 140.3900000000001
    agent-3: 140.3900000000001
    agent-4: 140.3900000000001
    agent-5: 140.3900000000001
  policy_reward_min:
    agent-0: 65.33333333333309
    agent-1: 65.33333333333309
    agent-2: 65.33333333333309
    agent-3: 65.33333333333309
    agent-4: 65.33333333333309
    agent-5: 65.33333333333309
  sampler_perf:
    mean_env_wait_ms: 30.533581787761886
    mean_inference_ms: 14.574010662093848
    mean_processing_ms: 65.69462912144466
  time_since_restore: 24259.055224895477
  time_this_iter_s: 157.414794921875
  time_total_s: 36809.872096300125
  timestamp: 1637059443
  timesteps_since_restore: 14016000
  timesteps_this_iter: 96000
  timesteps_total: 21696000
  training_iteration: 226
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    226 |          36809.9 | 21696000 |   842.34 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 2.66
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 24.86
    apples_agent-1_min: 0
    apples_agent-2_max: 169
    apples_agent-2_mean: 4.53
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 98.3
    apples_agent-3_min: 18
    apples_agent-4_max: 86
    apples_agent-4_mean: 4.77
    apples_agent-4_min: 0
    apples_agent-5_max: 181
    apples_agent-5_mean: 104.84
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 498
    cleaning_beam_agent-0_mean: 359.68
    cleaning_beam_agent-0_min: 104
    cleaning_beam_agent-1_max: 411
    cleaning_beam_agent-1_mean: 219.55
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 566
    cleaning_beam_agent-2_mean: 366.73
    cleaning_beam_agent-2_min: 115
    cleaning_beam_agent-3_max: 148
    cleaning_beam_agent-3_mean: 44.23
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 555
    cleaning_beam_agent-4_mean: 427.21
    cleaning_beam_agent-4_min: 266
    cleaning_beam_agent-5_max: 246
    cleaning_beam_agent-5_mean: 43.58
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-46-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1098.9999999999877
  episode_reward_mean: 819.7299999999871
  episode_reward_min: 296.00000000000114
  episodes_this_iter: 96
  episodes_total: 21792
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13155.785
    learner:
      agent-0:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0801970958709717
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015965381171554327
        model: {}
        policy_loss: -0.0032306890934705734
        total_loss: -0.003073429688811302
        vf_explained_var: 0.047360196709632874
        vf_loss: 20.584075927734375
      agent-1:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1451904773712158
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025684803258627653
        model: {}
        policy_loss: -0.004780363757163286
        total_loss: -0.004545335657894611
        vf_explained_var: -0.030797898769378662
        vf_loss: 22.505611419677734
      agent-2:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0915508270263672
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016870801337063313
        model: {}
        policy_loss: -0.004028333351016045
        total_loss: -0.003787778550758958
        vf_explained_var: 0.007722780108451843
        vf_loss: 21.616836547851562
      agent-3:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.564845621585846
        entropy_coeff: 0.0017600000137463212
        kl: 0.001263764570467174
        model: {}
        policy_loss: -0.003053161548450589
        total_loss: -0.0022642940748482943
        vf_explained_var: 0.18301743268966675
        vf_loss: 17.82996940612793
      agent-4:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.94212406873703
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017686504870653152
        model: {}
        policy_loss: -0.004400028381496668
        total_loss: -0.0041801221668720245
        vf_explained_var: 0.13063229620456696
        vf_loss: 18.780487060546875
      agent-5:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8085576891899109
        entropy_coeff: 0.0017600000137463212
        kl: 0.001780603313818574
        model: {}
        policy_loss: -0.003410451114177704
        total_loss: -0.003066149540245533
        vf_explained_var: 0.17955805361270905
        vf_loss: 17.673648834228516
    load_time_ms: 20300.858
    num_steps_sampled: 21792000
    num_steps_trained: 21792000
    sample_time_ms: 128271.112
    update_time_ms: 63.795
  iterations_since_restore: 147
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.787044534412953
    ram_util_percent: 14.992307692307694
  pid: 14340
  policy_reward_max:
    agent-0: 183.16666666666643
    agent-1: 183.16666666666643
    agent-2: 183.16666666666643
    agent-3: 183.16666666666643
    agent-4: 183.16666666666643
    agent-5: 183.16666666666643
  policy_reward_mean:
    agent-0: 136.6216666666668
    agent-1: 136.6216666666668
    agent-2: 136.6216666666668
    agent-3: 136.6216666666668
    agent-4: 136.6216666666668
    agent-5: 136.6216666666668
  policy_reward_min:
    agent-0: 49.33333333333328
    agent-1: 49.33333333333328
    agent-2: 49.33333333333328
    agent-3: 49.33333333333328
    agent-4: 49.33333333333328
    agent-5: 49.33333333333328
  sampler_perf:
    mean_env_wait_ms: 30.539257232804502
    mean_inference_ms: 14.573900251078266
    mean_processing_ms: 65.69411219617722
  time_since_restore: 24432.750843048096
  time_this_iter_s: 173.6956181526184
  time_total_s: 36983.56771445274
  timestamp: 1637059617
  timesteps_since_restore: 14112000
  timesteps_this_iter: 96000
  timesteps_total: 21792000
  training_iteration: 227
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    227 |          36983.6 | 21792000 |   819.73 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 3.51
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 22.8
    apples_agent-1_min: 0
    apples_agent-2_max: 186
    apples_agent-2_mean: 6.05
    apples_agent-2_min: 0
    apples_agent-3_max: 168
    apples_agent-3_mean: 99.65
    apples_agent-3_min: 23
    apples_agent-4_max: 57
    apples_agent-4_mean: 2.6
    apples_agent-4_min: 0
    apples_agent-5_max: 173
    apples_agent-5_mean: 107.45
    apples_agent-5_min: 62
    cleaning_beam_agent-0_max: 511
    cleaning_beam_agent-0_mean: 361.79
    cleaning_beam_agent-0_min: 165
    cleaning_beam_agent-1_max: 421
    cleaning_beam_agent-1_mean: 218.53
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 582
    cleaning_beam_agent-2_mean: 368.6
    cleaning_beam_agent-2_min: 138
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 39.7
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 548
    cleaning_beam_agent-4_mean: 428.13
    cleaning_beam_agent-4_min: 199
    cleaning_beam_agent-5_max: 259
    cleaning_beam_agent-5_mean: 42.0
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-49-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1071.999999999996
  episode_reward_mean: 846.4499999999865
  episode_reward_min: 434.0000000000107
  episodes_this_iter: 96
  episodes_total: 21888
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13093.154
    learner:
      agent-0:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0701614618301392
        entropy_coeff: 0.0017600000137463212
        kl: 0.002401595702394843
        model: {}
        policy_loss: -0.003418202046304941
        total_loss: -0.0033051972277462482
        vf_explained_var: 0.0433545857667923
        vf_loss: 19.96490478515625
      agent-1:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1314136981964111
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016061882488429546
        model: {}
        policy_loss: -0.004443012643605471
        total_loss: -0.004245791584253311
        vf_explained_var: -0.02845066785812378
        vf_loss: 21.88511848449707
      agent-2:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.087174654006958
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018500383011996746
        model: {}
        policy_loss: -0.00406933156773448
        total_loss: -0.003987920470535755
        vf_explained_var: 0.0554497092962265
        vf_loss: 19.948373794555664
      agent-3:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5579960346221924
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015900987200438976
        model: {}
        policy_loss: -0.0028010287787765265
        total_loss: -0.0020109685137867928
        vf_explained_var: 0.15877877175807953
        vf_loss: 17.721323013305664
      agent-4:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9222215414047241
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014536867383867502
        model: {}
        policy_loss: -0.004210306331515312
        total_loss: -0.0038662198930978775
        vf_explained_var: 0.07026343047618866
        vf_loss: 19.67194175720215
      agent-5:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8077765107154846
        entropy_coeff: 0.0017600000137463212
        kl: 0.001275317627005279
        model: {}
        policy_loss: -0.003230027388781309
        total_loss: -0.0028831232339143753
        vf_explained_var: 0.14899668097496033
        vf_loss: 17.6859130859375
    load_time_ms: 22083.043
    num_steps_sampled: 21888000
    num_steps_trained: 21888000
    sample_time_ms: 128215.44
    update_time_ms: 59.478
  iterations_since_restore: 148
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.638709677419353
    ram_util_percent: 14.96491935483871
  pid: 14340
  policy_reward_max:
    agent-0: 178.66666666666632
    agent-1: 178.66666666666632
    agent-2: 178.66666666666632
    agent-3: 178.66666666666632
    agent-4: 178.66666666666632
    agent-5: 178.66666666666632
  policy_reward_mean:
    agent-0: 141.0750000000001
    agent-1: 141.0750000000001
    agent-2: 141.0750000000001
    agent-3: 141.0750000000001
    agent-4: 141.0750000000001
    agent-5: 141.0750000000001
  policy_reward_min:
    agent-0: 72.33333333333323
    agent-1: 72.33333333333323
    agent-2: 72.33333333333323
    agent-3: 72.33333333333323
    agent-4: 72.33333333333323
    agent-5: 72.33333333333323
  sampler_perf:
    mean_env_wait_ms: 30.543788884918925
    mean_inference_ms: 14.573361927806356
    mean_processing_ms: 65.69223284769187
  time_since_restore: 24606.222056627274
  time_this_iter_s: 173.47121357917786
  time_total_s: 37157.03892803192
  timestamp: 1637059790
  timesteps_since_restore: 14208000
  timesteps_this_iter: 96000
  timesteps_total: 21888000
  training_iteration: 228
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    228 |            37157 | 21888000 |   846.45 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 74
    apples_agent-0_mean: 3.67
    apples_agent-0_min: 0
    apples_agent-1_max: 113
    apples_agent-1_mean: 24.48
    apples_agent-1_min: 0
    apples_agent-2_max: 157
    apples_agent-2_mean: 3.4
    apples_agent-2_min: 0
    apples_agent-3_max: 156
    apples_agent-3_mean: 100.17
    apples_agent-3_min: 29
    apples_agent-4_max: 69
    apples_agent-4_mean: 3.85
    apples_agent-4_min: 0
    apples_agent-5_max: 166
    apples_agent-5_mean: 100.39
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 591
    cleaning_beam_agent-0_mean: 386.05
    cleaning_beam_agent-0_min: 137
    cleaning_beam_agent-1_max: 440
    cleaning_beam_agent-1_mean: 220.51
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 512
    cleaning_beam_agent-2_mean: 361.03
    cleaning_beam_agent-2_min: 147
    cleaning_beam_agent-3_max: 159
    cleaning_beam_agent-3_mean: 51.09
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 570
    cleaning_beam_agent-4_mean: 423.56
    cleaning_beam_agent-4_min: 199
    cleaning_beam_agent-5_max: 261
    cleaning_beam_agent-5_mean: 49.46
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-52-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1034.9999999999802
  episode_reward_mean: 816.2699999999875
  episode_reward_min: 194.9999999999991
  episodes_this_iter: 96
  episodes_total: 21984
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13072.569
    learner:
      agent-0:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0620639324188232
        entropy_coeff: 0.0017600000137463212
        kl: 0.002017183927819133
        model: {}
        policy_loss: -0.0035315146669745445
        total_loss: -0.0033873245120048523
        vf_explained_var: 0.0657365620136261
        vf_loss: 20.134212493896484
      agent-1:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1382668018341064
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013325922191143036
        model: {}
        policy_loss: -0.004291636403650045
        total_loss: -0.004137254785746336
        vf_explained_var: 0.008796468377113342
        vf_loss: 21.577306747436523
      agent-2:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0960121154785156
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012388790491968393
        model: {}
        policy_loss: -0.0037602605298161507
        total_loss: -0.0035657421685755253
        vf_explained_var: 0.024244025349617004
        vf_loss: 21.235015869140625
      agent-3:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5923341512680054
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015018533449620008
        model: {}
        policy_loss: -0.0030468995682895184
        total_loss: -0.0023638505954295397
        vf_explained_var: 0.2094046175479889
        vf_loss: 17.25556182861328
      agent-4:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9238845109939575
        entropy_coeff: 0.0017600000137463212
        kl: 0.002255974803119898
        model: {}
        policy_loss: -0.004522907547652721
        total_loss: -0.004303433932363987
        vf_explained_var: 0.14797155559062958
        vf_loss: 18.455074310302734
      agent-5:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8206644654273987
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017335301963612437
        model: {}
        policy_loss: -0.003568364540114999
        total_loss: -0.0032226049806922674
        vf_explained_var: 0.1694169044494629
        vf_loss: 17.901269912719727
    load_time_ms: 22049.381
    num_steps_sampled: 21984000
    num_steps_trained: 21984000
    sample_time_ms: 128361.092
    update_time_ms: 59.783
  iterations_since_restore: 149
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.56844444444444
    ram_util_percent: 14.672444444444444
  pid: 14340
  policy_reward_max:
    agent-0: 172.4999999999996
    agent-1: 172.4999999999996
    agent-2: 172.4999999999996
    agent-3: 172.4999999999996
    agent-4: 172.4999999999996
    agent-5: 172.4999999999996
  policy_reward_mean:
    agent-0: 136.04500000000007
    agent-1: 136.04500000000007
    agent-2: 136.04500000000007
    agent-3: 136.04500000000007
    agent-4: 136.04500000000007
    agent-5: 136.04500000000007
  policy_reward_min:
    agent-0: 32.50000000000005
    agent-1: 32.50000000000005
    agent-2: 32.50000000000005
    agent-3: 32.50000000000005
    agent-4: 32.50000000000005
    agent-5: 32.50000000000005
  sampler_perf:
    mean_env_wait_ms: 30.549658196554102
    mean_inference_ms: 14.573211477124088
    mean_processing_ms: 65.69325491367313
  time_since_restore: 24764.496707439423
  time_this_iter_s: 158.27465081214905
  time_total_s: 37315.31357884407
  timestamp: 1637059949
  timesteps_since_restore: 14304000
  timesteps_this_iter: 96000
  timesteps_total: 21984000
  training_iteration: 229
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    229 |          37315.3 | 21984000 |   816.27 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 4.75
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 22.35
    apples_agent-1_min: 0
    apples_agent-2_max: 110
    apples_agent-2_mean: 4.95
    apples_agent-2_min: 0
    apples_agent-3_max: 192
    apples_agent-3_mean: 99.0
    apples_agent-3_min: 50
    apples_agent-4_max: 71
    apples_agent-4_mean: 2.66
    apples_agent-4_min: 0
    apples_agent-5_max: 215
    apples_agent-5_mean: 104.96
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 517
    cleaning_beam_agent-0_mean: 359.0
    cleaning_beam_agent-0_min: 175
    cleaning_beam_agent-1_max: 414
    cleaning_beam_agent-1_mean: 225.74
    cleaning_beam_agent-1_min: 100
    cleaning_beam_agent-2_max: 566
    cleaning_beam_agent-2_mean: 367.22
    cleaning_beam_agent-2_min: 172
    cleaning_beam_agent-3_max: 135
    cleaning_beam_agent-3_mean: 48.02
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 555
    cleaning_beam_agent-4_mean: 444.14
    cleaning_beam_agent-4_min: 224
    cleaning_beam_agent-5_max: 367
    cleaning_beam_agent-5_mean: 53.47
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-55-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1050.9999999999823
  episode_reward_mean: 822.5899999999873
  episode_reward_min: 355.00000000000165
  episodes_this_iter: 96
  episodes_total: 22080
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13045.191
    learner:
      agent-0:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.078308343887329
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021406877785921097
        model: {}
        policy_loss: -0.003438729327172041
        total_loss: -0.0033908048644661903
        vf_explained_var: 0.07795499265193939
        vf_loss: 19.45747947692871
      agent-1:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1355053186416626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014622320886701345
        model: {}
        policy_loss: -0.003925273660570383
        total_loss: -0.003799750003963709
        vf_explained_var: -0.0004450380802154541
        vf_loss: 21.240123748779297
      agent-2:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.085331678390503
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013361836317926645
        model: {}
        policy_loss: -0.003534705378115177
        total_loss: -0.0034838439896702766
        vf_explained_var: 0.07321244478225708
        vf_loss: 19.61041831970215
      agent-3:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5860930681228638
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009142305934801698
        model: {}
        policy_loss: -0.0029160822741687298
        total_loss: -0.0021470077335834503
        vf_explained_var: 0.1509617269039154
        vf_loss: 18.005970001220703
      agent-4:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9113888740539551
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014163886662572622
        model: {}
        policy_loss: -0.004256586544215679
        total_loss: -0.003958512097597122
        vf_explained_var: 0.09460492432117462
        vf_loss: 19.02117156982422
      agent-5:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8159627914428711
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015303583350032568
        model: {}
        policy_loss: -0.0036075282841920853
        total_loss: -0.0033130617812275887
        vf_explained_var: 0.1766228824853897
        vf_loss: 17.305618286132812
    load_time_ms: 21972.238
    num_steps_sampled: 22080000
    num_steps_trained: 22080000
    sample_time_ms: 128571.069
    update_time_ms: 45.007
  iterations_since_restore: 150
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.421681415929203
    ram_util_percent: 14.74115044247788
  pid: 14340
  policy_reward_max:
    agent-0: 175.16666666666686
    agent-1: 175.16666666666686
    agent-2: 175.16666666666686
    agent-3: 175.16666666666686
    agent-4: 175.16666666666686
    agent-5: 175.16666666666686
  policy_reward_mean:
    agent-0: 137.09833333333344
    agent-1: 137.09833333333344
    agent-2: 137.09833333333344
    agent-3: 137.09833333333344
    agent-4: 137.09833333333344
    agent-5: 137.09833333333344
  policy_reward_min:
    agent-0: 59.166666666666536
    agent-1: 59.166666666666536
    agent-2: 59.166666666666536
    agent-3: 59.166666666666536
    agent-4: 59.166666666666536
    agent-5: 59.166666666666536
  sampler_perf:
    mean_env_wait_ms: 30.555291641535067
    mean_inference_ms: 14.573005682991548
    mean_processing_ms: 65.69366866718114
  time_since_restore: 24922.73259162903
  time_this_iter_s: 158.2358841896057
  time_total_s: 37473.549463033676
  timestamp: 1637060108
  timesteps_since_restore: 14400000
  timesteps_this_iter: 96000
  timesteps_total: 22080000
  training_iteration: 230
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    230 |          37473.5 | 22080000 |   822.59 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 2.73
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 28.8
    apples_agent-1_min: 0
    apples_agent-2_max: 105
    apples_agent-2_mean: 6.83
    apples_agent-2_min: 0
    apples_agent-3_max: 152
    apples_agent-3_mean: 101.95
    apples_agent-3_min: 26
    apples_agent-4_max: 41
    apples_agent-4_mean: 1.39
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 108.26
    apples_agent-5_min: 14
    cleaning_beam_agent-0_max: 527
    cleaning_beam_agent-0_mean: 364.75
    cleaning_beam_agent-0_min: 192
    cleaning_beam_agent-1_max: 414
    cleaning_beam_agent-1_mean: 226.39
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 545
    cleaning_beam_agent-2_mean: 349.97
    cleaning_beam_agent-2_min: 118
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 44.82
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 529
    cleaning_beam_agent-4_mean: 437.15
    cleaning_beam_agent-4_min: 164
    cleaning_beam_agent-5_max: 294
    cleaning_beam_agent-5_mean: 50.55
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-58-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1050.0000000000061
  episode_reward_mean: 829.3499999999873
  episode_reward_min: 260.99999999999756
  episodes_this_iter: 96
  episodes_total: 22176
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13042.44
    learner:
      agent-0:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0663307905197144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022534991148859262
        model: {}
        policy_loss: -0.0035584010183811188
        total_loss: -0.0035896901972591877
        vf_explained_var: 0.0735425055027008
        vf_loss: 18.45451545715332
      agent-1:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1288971900939941
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017660035518929362
        model: {}
        policy_loss: -0.004234928637742996
        total_loss: -0.004154597874730825
        vf_explained_var: -0.022443324327468872
        vf_loss: 20.67190933227539
      agent-2:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0958819389343262
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017214936669915915
        model: {}
        policy_loss: -0.0037864670157432556
        total_loss: -0.0038037621416151524
        vf_explained_var: 0.07002392411231995
        vf_loss: 19.114564895629883
      agent-3:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5863368511199951
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011149442289024591
        model: {}
        policy_loss: -0.0028003347106277943
        total_loss: -0.002125921193510294
        vf_explained_var: 0.15035714209079742
        vf_loss: 17.063642501831055
      agent-4:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9189008474349976
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012916645500808954
        model: {}
        policy_loss: -0.0037869159132242203
        total_loss: -0.0035332832485437393
        vf_explained_var: 0.05874177813529968
        vf_loss: 18.708988189697266
      agent-5:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8148612976074219
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013901026686653495
        model: {}
        policy_loss: -0.0036177262663841248
        total_loss: -0.0032783763017505407
        vf_explained_var: 0.11266490817070007
        vf_loss: 17.73508071899414
    load_time_ms: 22050.851
    num_steps_sampled: 22176000
    num_steps_trained: 22176000
    sample_time_ms: 128889.132
    update_time_ms: 56.266
  iterations_since_restore: 151
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.08125
    ram_util_percent: 15.010156250000001
  pid: 14340
  policy_reward_max:
    agent-0: 174.9999999999995
    agent-1: 174.9999999999995
    agent-2: 174.9999999999995
    agent-3: 174.9999999999995
    agent-4: 174.9999999999995
    agent-5: 174.9999999999995
  policy_reward_mean:
    agent-0: 138.22500000000008
    agent-1: 138.22500000000008
    agent-2: 138.22500000000008
    agent-3: 138.22500000000008
    agent-4: 138.22500000000008
    agent-5: 138.22500000000008
  policy_reward_min:
    agent-0: 43.500000000000036
    agent-1: 43.500000000000036
    agent-2: 43.500000000000036
    agent-3: 43.500000000000036
    agent-4: 43.500000000000036
    agent-5: 43.500000000000036
  sampler_perf:
    mean_env_wait_ms: 30.56044936345383
    mean_inference_ms: 14.586802299062347
    mean_processing_ms: 65.70660402840936
  time_since_restore: 25101.692754507065
  time_this_iter_s: 178.9601628780365
  time_total_s: 37652.50962591171
  timestamp: 1637060287
  timesteps_since_restore: 14496000
  timesteps_this_iter: 96000
  timesteps_total: 22176000
  training_iteration: 231
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    231 |          37652.5 | 22176000 |   829.35 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 2.8
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 23.47
    apples_agent-1_min: 0
    apples_agent-2_max: 86
    apples_agent-2_mean: 7.0
    apples_agent-2_min: 0
    apples_agent-3_max: 256
    apples_agent-3_mean: 104.04
    apples_agent-3_min: 10
    apples_agent-4_max: 74
    apples_agent-4_mean: 3.36
    apples_agent-4_min: 0
    apples_agent-5_max: 174
    apples_agent-5_mean: 103.93
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 577
    cleaning_beam_agent-0_mean: 382.86
    cleaning_beam_agent-0_min: 203
    cleaning_beam_agent-1_max: 482
    cleaning_beam_agent-1_mean: 233.64
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 585
    cleaning_beam_agent-2_mean: 354.99
    cleaning_beam_agent-2_min: 179
    cleaning_beam_agent-3_max: 109
    cleaning_beam_agent-3_mean: 45.89
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 558
    cleaning_beam_agent-4_mean: 437.55
    cleaning_beam_agent-4_min: 195
    cleaning_beam_agent-5_max: 386
    cleaning_beam_agent-5_mean: 49.38
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-00-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1013.9999999999893
  episode_reward_mean: 819.6799999999888
  episode_reward_min: 107.00000000000081
  episodes_this_iter: 96
  episodes_total: 22272
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12995.13
    learner:
      agent-0:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.059948444366455
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018278984352946281
        model: {}
        policy_loss: -0.00326137850061059
        total_loss: -0.003174430690705776
        vf_explained_var: 0.002713024616241455
        vf_loss: 19.52457046508789
      agent-1:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1195218563079834
        entropy_coeff: 0.0017600000137463212
        kl: 0.001417719293385744
        model: {}
        policy_loss: -0.003935609012842178
        total_loss: -0.0038709628861397505
        vf_explained_var: -0.03249049186706543
        vf_loss: 20.35003662109375
      agent-2:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0857207775115967
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014921508263796568
        model: {}
        policy_loss: -0.003787687048316002
        total_loss: -0.003761209547519684
        vf_explained_var: 0.025912240147590637
        vf_loss: 19.373483657836914
      agent-3:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5824173092842102
        entropy_coeff: 0.0017600000137463212
        kl: 0.001172803109511733
        model: {}
        policy_loss: -0.003182006999850273
        total_loss: -0.002552536316215992
        vf_explained_var: 0.15382498502731323
        vf_loss: 16.54522705078125
      agent-4:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9311403632164001
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017914276104420424
        model: {}
        policy_loss: -0.004095538519322872
        total_loss: -0.0039997827261686325
        vf_explained_var: 0.1159323900938034
        vf_loss: 17.345643997192383
      agent-5:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8242930173873901
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016551227308809757
        model: {}
        policy_loss: -0.003635580185800791
        total_loss: -0.0033801260869950056
        vf_explained_var: 0.13170073926448822
        vf_loss: 17.06206512451172
    load_time_ms: 22135.301
    num_steps_sampled: 22272000
    num_steps_trained: 22272000
    sample_time_ms: 129090.686
    update_time_ms: 56.874
  iterations_since_restore: 152
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.482894736842105
    ram_util_percent: 14.780263157894739
  pid: 14340
  policy_reward_max:
    agent-0: 169.00000000000017
    agent-1: 169.00000000000017
    agent-2: 169.00000000000017
    agent-3: 169.00000000000017
    agent-4: 169.00000000000017
    agent-5: 169.00000000000017
  policy_reward_mean:
    agent-0: 136.61333333333346
    agent-1: 136.61333333333346
    agent-2: 136.61333333333346
    agent-3: 136.61333333333346
    agent-4: 136.61333333333346
    agent-5: 136.61333333333346
  policy_reward_min:
    agent-0: 17.833333333333325
    agent-1: 17.833333333333325
    agent-2: 17.833333333333325
    agent-3: 17.833333333333325
    agent-4: 17.833333333333325
    agent-5: 17.833333333333325
  sampler_perf:
    mean_env_wait_ms: 30.566015269854514
    mean_inference_ms: 14.586466902927
    mean_processing_ms: 65.70911496204394
  time_since_restore: 25261.680905342102
  time_this_iter_s: 159.98815083503723
  time_total_s: 37812.49777674675
  timestamp: 1637060447
  timesteps_since_restore: 14592000
  timesteps_this_iter: 96000
  timesteps_total: 22272000
  training_iteration: 232
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    232 |          37812.5 | 22272000 |   819.68 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 71
    apples_agent-0_mean: 4.91
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 24.82
    apples_agent-1_min: 0
    apples_agent-2_max: 55
    apples_agent-2_mean: 2.63
    apples_agent-2_min: 0
    apples_agent-3_max: 178
    apples_agent-3_mean: 102.42
    apples_agent-3_min: 43
    apples_agent-4_max: 73
    apples_agent-4_mean: 1.94
    apples_agent-4_min: 0
    apples_agent-5_max: 217
    apples_agent-5_mean: 105.92
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 554
    cleaning_beam_agent-0_mean: 383.81
    cleaning_beam_agent-0_min: 192
    cleaning_beam_agent-1_max: 501
    cleaning_beam_agent-1_mean: 244.54
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 539
    cleaning_beam_agent-2_mean: 364.76
    cleaning_beam_agent-2_min: 216
    cleaning_beam_agent-3_max: 153
    cleaning_beam_agent-3_mean: 45.02
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 537
    cleaning_beam_agent-4_mean: 426.24
    cleaning_beam_agent-4_min: 256
    cleaning_beam_agent-5_max: 567
    cleaning_beam_agent-5_mean: 55.8
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-03-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 992.9999999999802
  episode_reward_mean: 826.0199999999875
  episode_reward_min: 427.00000000001074
  episodes_this_iter: 96
  episodes_total: 22368
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12963.684
    learner:
      agent-0:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.059139370918274
        entropy_coeff: 0.0017600000137463212
        kl: 0.00117070646956563
        model: {}
        policy_loss: -0.00301509746350348
        total_loss: -0.0030822032131254673
        vf_explained_var: 0.08584436774253845
        vf_loss: 17.969799041748047
      agent-1:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1118483543395996
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018659640336409211
        model: {}
        policy_loss: -0.00447623897343874
        total_loss: -0.004420093260705471
        vf_explained_var: -0.01651129126548767
        vf_loss: 20.129966735839844
      agent-2:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0923736095428467
        entropy_coeff: 0.0017600000137463212
        kl: 0.001877499627880752
        model: {}
        policy_loss: -0.003827482694759965
        total_loss: -0.003742679487913847
        vf_explained_var: -0.010521382093429565
        vf_loss: 20.07379913330078
      agent-3:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5703449249267578
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013545379042625427
        model: {}
        policy_loss: -0.002719930373132229
        total_loss: -0.0020809120032936335
        vf_explained_var: 0.16153326630592346
        vf_loss: 16.428239822387695
      agent-4:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9268724918365479
        entropy_coeff: 0.0017600000137463212
        kl: 0.002055969089269638
        model: {}
        policy_loss: -0.004132840782403946
        total_loss: -0.003901110030710697
        vf_explained_var: 0.04792734980583191
        vf_loss: 18.630294799804688
      agent-5:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8151251077651978
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012085463386029005
        model: {}
        policy_loss: -0.0034757833927869797
        total_loss: -0.0032568806782364845
        vf_explained_var: 0.15774084627628326
        vf_loss: 16.535192489624023
    load_time_ms: 24008.871
    num_steps_sampled: 22368000
    num_steps_trained: 22368000
    sample_time_ms: 129121.844
    update_time_ms: 66.141
  iterations_since_restore: 153
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.53493975903615
    ram_util_percent: 15.017269076305222
  pid: 14340
  policy_reward_max:
    agent-0: 165.4999999999996
    agent-1: 165.4999999999996
    agent-2: 165.4999999999996
    agent-3: 165.4999999999996
    agent-4: 165.4999999999996
    agent-5: 165.4999999999996
  policy_reward_mean:
    agent-0: 137.67000000000013
    agent-1: 137.67000000000013
    agent-2: 137.67000000000013
    agent-3: 137.67000000000013
    agent-4: 137.67000000000013
    agent-5: 137.67000000000013
  policy_reward_min:
    agent-0: 71.1666666666666
    agent-1: 71.1666666666666
    agent-2: 71.1666666666666
    agent-3: 71.1666666666666
    agent-4: 71.1666666666666
    agent-5: 71.1666666666666
  sampler_perf:
    mean_env_wait_ms: 30.571875269306503
    mean_inference_ms: 14.586178137759575
    mean_processing_ms: 65.70719527663
  time_since_restore: 25436.223916769028
  time_this_iter_s: 174.54301142692566
  time_total_s: 37987.040788173676
  timestamp: 1637060623
  timesteps_since_restore: 14688000
  timesteps_this_iter: 96000
  timesteps_total: 22368000
  training_iteration: 233
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    233 |            37987 | 22368000 |   826.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 4.14
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 24.82
    apples_agent-1_min: 0
    apples_agent-2_max: 83
    apples_agent-2_mean: 5.37
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 97.85
    apples_agent-3_min: 37
    apples_agent-4_max: 59
    apples_agent-4_mean: 3.03
    apples_agent-4_min: 0
    apples_agent-5_max: 164
    apples_agent-5_mean: 106.4
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 584
    cleaning_beam_agent-0_mean: 403.04
    cleaning_beam_agent-0_min: 192
    cleaning_beam_agent-1_max: 481
    cleaning_beam_agent-1_mean: 235.89
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 498
    cleaning_beam_agent-2_mean: 355.53
    cleaning_beam_agent-2_min: 161
    cleaning_beam_agent-3_max: 133
    cleaning_beam_agent-3_mean: 48.78
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 533
    cleaning_beam_agent-4_mean: 422.46
    cleaning_beam_agent-4_min: 261
    cleaning_beam_agent-5_max: 403
    cleaning_beam_agent-5_mean: 45.15
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-06-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1076.9999999999989
  episode_reward_mean: 815.7299999999877
  episode_reward_min: 401.00000000000165
  episodes_this_iter: 96
  episodes_total: 22464
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12960.644
    learner:
      agent-0:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0422673225402832
        entropy_coeff: 0.0017600000137463212
        kl: 0.002040745224803686
        model: {}
        policy_loss: -0.0034573879092931747
        total_loss: -0.003417943138629198
        vf_explained_var: 0.04913191497325897
        vf_loss: 18.73835563659668
      agent-1:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1173275709152222
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019241049885749817
        model: {}
        policy_loss: -0.004177479073405266
        total_loss: -0.004099659621715546
        vf_explained_var: -0.028445273637771606
        vf_loss: 20.44318389892578
      agent-2:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0920891761779785
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015098635340109468
        model: {}
        policy_loss: -0.0037346575409173965
        total_loss: -0.0037698601372539997
        vf_explained_var: 0.057224541902542114
        vf_loss: 18.86874771118164
      agent-3:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5898722410202026
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011859580408781767
        model: {}
        policy_loss: -0.002951353322714567
        total_loss: -0.002300508785992861
        vf_explained_var: 0.1495073139667511
        vf_loss: 16.89019203186035
      agent-4:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9306018352508545
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015814776998013258
        model: {}
        policy_loss: -0.0040708743035793304
        total_loss: -0.0038962028920650482
        vf_explained_var: 0.08610233664512634
        vf_loss: 18.125289916992188
      agent-5:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8000475764274597
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019179723458364606
        model: {}
        policy_loss: -0.0035895025357604027
        total_loss: -0.00333300419151783
        vf_explained_var: 0.15535350143909454
        vf_loss: 16.645830154418945
    load_time_ms: 23991.426
    num_steps_sampled: 22464000
    num_steps_trained: 22464000
    sample_time_ms: 129207.503
    update_time_ms: 63.055
  iterations_since_restore: 154
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.41973094170404
    ram_util_percent: 14.754708520179372
  pid: 14340
  policy_reward_max:
    agent-0: 179.49999999999994
    agent-1: 179.49999999999994
    agent-2: 179.49999999999994
    agent-3: 179.49999999999994
    agent-4: 179.49999999999994
    agent-5: 179.49999999999994
  policy_reward_mean:
    agent-0: 135.95500000000015
    agent-1: 135.95500000000015
    agent-2: 135.95500000000015
    agent-3: 135.95500000000015
    agent-4: 135.95500000000015
    agent-5: 135.95500000000015
  policy_reward_min:
    agent-0: 66.83333333333327
    agent-1: 66.83333333333327
    agent-2: 66.83333333333327
    agent-3: 66.83333333333327
    agent-4: 66.83333333333327
    agent-5: 66.83333333333327
  sampler_perf:
    mean_env_wait_ms: 30.577123739976237
    mean_inference_ms: 14.585958967491903
    mean_processing_ms: 65.70697490082685
  time_since_restore: 25593.172791719437
  time_this_iter_s: 156.94887495040894
  time_total_s: 38143.989663124084
  timestamp: 1637060780
  timesteps_since_restore: 14784000
  timesteps_this_iter: 96000
  timesteps_total: 22464000
  training_iteration: 234
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    234 |            38144 | 22464000 |   815.73 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 3.92
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 19.43
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 4.87
    apples_agent-2_min: 0
    apples_agent-3_max: 185
    apples_agent-3_mean: 105.81
    apples_agent-3_min: 52
    apples_agent-4_max: 54
    apples_agent-4_mean: 1.4
    apples_agent-4_min: 0
    apples_agent-5_max: 211
    apples_agent-5_mean: 108.75
    apples_agent-5_min: 38
    cleaning_beam_agent-0_max: 629
    cleaning_beam_agent-0_mean: 413.44
    cleaning_beam_agent-0_min: 139
    cleaning_beam_agent-1_max: 437
    cleaning_beam_agent-1_mean: 253.62
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 525
    cleaning_beam_agent-2_mean: 356.36
    cleaning_beam_agent-2_min: 167
    cleaning_beam_agent-3_max: 157
    cleaning_beam_agent-3_mean: 44.44
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 556
    cleaning_beam_agent-4_mean: 441.14
    cleaning_beam_agent-4_min: 261
    cleaning_beam_agent-5_max: 308
    cleaning_beam_agent-5_mean: 44.33
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-08-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1121.999999999994
  episode_reward_mean: 857.7799999999854
  episode_reward_min: 355.0000000000036
  episodes_this_iter: 96
  episodes_total: 22560
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12985.905
    learner:
      agent-0:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0426676273345947
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011096530361101031
        model: {}
        policy_loss: -0.0032359776087105274
        total_loss: -0.0031798621639609337
        vf_explained_var: 0.04989375174045563
        vf_loss: 18.912086486816406
      agent-1:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1117658615112305
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015905832406133413
        model: {}
        policy_loss: -0.003720439039170742
        total_loss: -0.003563827835023403
        vf_explained_var: -0.033578693866729736
        vf_loss: 21.133201599121094
      agent-2:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1070784330368042
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018608010141178966
        model: {}
        policy_loss: -0.003949298057705164
        total_loss: -0.0038734404370188713
        vf_explained_var: 0.0035924911499023438
        vf_loss: 20.243125915527344
      agent-3:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5542342662811279
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016746968030929565
        model: {}
        policy_loss: -0.0031313160434365273
        total_loss: -0.0023337742313742638
        vf_explained_var: 0.11012201011180878
        vf_loss: 17.72992706298828
      agent-4:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9239889979362488
        entropy_coeff: 0.0017600000137463212
        kl: 0.002003248780965805
        model: {}
        policy_loss: -0.004148772917687893
        total_loss: -0.003836442716419697
        vf_explained_var: 0.037076666951179504
        vf_loss: 19.385501861572266
      agent-5:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7785682082176208
        entropy_coeff: 0.0017600000137463212
        kl: 0.00112887192517519
        model: {}
        policy_loss: -0.003599882358685136
        total_loss: -0.003199939150363207
        vf_explained_var: 0.10782097280025482
        vf_loss: 17.70220947265625
    load_time_ms: 22321.986
    num_steps_sampled: 22560000
    num_steps_trained: 22560000
    sample_time_ms: 129385.621
    update_time_ms: 62.006
  iterations_since_restore: 155
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.664
    ram_util_percent: 14.710222222222225
  pid: 14340
  policy_reward_max:
    agent-0: 186.99999999999994
    agent-1: 186.99999999999994
    agent-2: 186.99999999999994
    agent-3: 186.99999999999994
    agent-4: 186.99999999999994
    agent-5: 186.99999999999994
  policy_reward_mean:
    agent-0: 142.96333333333342
    agent-1: 142.96333333333342
    agent-2: 142.96333333333342
    agent-3: 142.96333333333342
    agent-4: 142.96333333333342
    agent-5: 142.96333333333342
  policy_reward_min:
    agent-0: 59.1666666666665
    agent-1: 59.1666666666665
    agent-2: 59.1666666666665
    agent-3: 59.1666666666665
    agent-4: 59.1666666666665
    agent-5: 59.1666666666665
  sampler_perf:
    mean_env_wait_ms: 30.584367011630448
    mean_inference_ms: 14.585976882144863
    mean_processing_ms: 65.70816225283366
  time_since_restore: 25750.499367952347
  time_this_iter_s: 157.32657623291016
  time_total_s: 38301.316239356995
  timestamp: 1637060937
  timesteps_since_restore: 14880000
  timesteps_this_iter: 96000
  timesteps_total: 22560000
  training_iteration: 235
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    235 |          38301.3 | 22560000 |   857.78 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 2.11
    apples_agent-0_min: 0
    apples_agent-1_max: 114
    apples_agent-1_mean: 21.26
    apples_agent-1_min: 0
    apples_agent-2_max: 108
    apples_agent-2_mean: 7.15
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 104.03
    apples_agent-3_min: 42
    apples_agent-4_max: 53
    apples_agent-4_mean: 3.09
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 103.09
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 572
    cleaning_beam_agent-0_mean: 426.98
    cleaning_beam_agent-0_min: 266
    cleaning_beam_agent-1_max: 449
    cleaning_beam_agent-1_mean: 246.79
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 524
    cleaning_beam_agent-2_mean: 323.37
    cleaning_beam_agent-2_min: 133
    cleaning_beam_agent-3_max: 157
    cleaning_beam_agent-3_mean: 46.85
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 435.66
    cleaning_beam_agent-4_min: 201
    cleaning_beam_agent-5_max: 281
    cleaning_beam_agent-5_mean: 45.03
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-11-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1009.99999999998
  episode_reward_mean: 837.2399999999875
  episode_reward_min: 285.9999999999993
  episodes_this_iter: 96
  episodes_total: 22656
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12915.308
    learner:
      agent-0:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.04347562789917
        entropy_coeff: 0.0017600000137463212
        kl: 0.001096145948395133
        model: {}
        policy_loss: -0.003142506582662463
        total_loss: -0.003103130729869008
        vf_explained_var: 0.07433575391769409
        vf_loss: 18.758930206298828
      agent-1:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1223750114440918
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014821121003478765
        model: {}
        policy_loss: -0.004233658313751221
        total_loss: -0.004191478248685598
        vf_explained_var: 0.016993284225463867
        vf_loss: 20.17559242248535
      agent-2:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1058387756347656
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015679908683523536
        model: {}
        policy_loss: -0.003920762334018946
        total_loss: -0.0038165035657584667
        vf_explained_var: 0.010014563798904419
        vf_loss: 20.50533103942871
      agent-3:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.580075740814209
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011051237815991044
        model: {}
        policy_loss: -0.0028888434171676636
        total_loss: -0.002151189371943474
        vf_explained_var: 0.14065775275230408
        vf_loss: 17.585886001586914
      agent-4:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.926447868347168
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016871779225766659
        model: {}
        policy_loss: -0.003923339769244194
        total_loss: -0.0037195077165961266
        vf_explained_var: 0.11094598472118378
        vf_loss: 18.343801498413086
      agent-5:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7955788969993591
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013267097529023886
        model: {}
        policy_loss: -0.003448174335062504
        total_loss: -0.003151906654238701
        vf_explained_var: 0.16615140438079834
        vf_loss: 16.964921951293945
    load_time_ms: 23187.611
    num_steps_sampled: 22656000
    num_steps_trained: 22656000
    sample_time_ms: 129377.095
    update_time_ms: 64.564
  iterations_since_restore: 156
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.57584745762712
    ram_util_percent: 14.890677966101698
  pid: 14340
  policy_reward_max:
    agent-0: 168.33333333333306
    agent-1: 168.33333333333306
    agent-2: 168.33333333333306
    agent-3: 168.33333333333306
    agent-4: 168.33333333333306
    agent-5: 168.33333333333306
  policy_reward_mean:
    agent-0: 139.54000000000013
    agent-1: 139.54000000000013
    agent-2: 139.54000000000013
    agent-3: 139.54000000000013
    agent-4: 139.54000000000013
    agent-5: 139.54000000000013
  policy_reward_min:
    agent-0: 47.66666666666655
    agent-1: 47.66666666666655
    agent-2: 47.66666666666655
    agent-3: 47.66666666666655
    agent-4: 47.66666666666655
    agent-5: 47.66666666666655
  sampler_perf:
    mean_env_wait_ms: 30.589525378584057
    mean_inference_ms: 14.585194375295325
    mean_processing_ms: 65.70477276633615
  time_since_restore: 25915.76630139351
  time_this_iter_s: 165.2669334411621
  time_total_s: 38466.58317279816
  timestamp: 1637061103
  timesteps_since_restore: 14976000
  timesteps_this_iter: 96000
  timesteps_total: 22656000
  training_iteration: 236
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    236 |          38466.6 | 22656000 |   837.24 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 3.28
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 23.83
    apples_agent-1_min: 0
    apples_agent-2_max: 115
    apples_agent-2_mean: 5.31
    apples_agent-2_min: 0
    apples_agent-3_max: 216
    apples_agent-3_mean: 108.59
    apples_agent-3_min: 56
    apples_agent-4_max: 68
    apples_agent-4_mean: 1.36
    apples_agent-4_min: 0
    apples_agent-5_max: 201
    apples_agent-5_mean: 106.67
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 600
    cleaning_beam_agent-0_mean: 411.23
    cleaning_beam_agent-0_min: 167
    cleaning_beam_agent-1_max: 439
    cleaning_beam_agent-1_mean: 267.19
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 517
    cleaning_beam_agent-2_mean: 345.08
    cleaning_beam_agent-2_min: 184
    cleaning_beam_agent-3_max: 148
    cleaning_beam_agent-3_mean: 43.95
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 584
    cleaning_beam_agent-4_mean: 447.01
    cleaning_beam_agent-4_min: 253
    cleaning_beam_agent-5_max: 359
    cleaning_beam_agent-5_mean: 43.15
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-14-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1072.9999999999986
  episode_reward_mean: 875.3799999999864
  episode_reward_min: 470.0000000000064
  episodes_this_iter: 96
  episodes_total: 22752
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12988.166
    learner:
      agent-0:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0545803308486938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017078666714951396
        model: {}
        policy_loss: -0.0033982861787080765
        total_loss: -0.0033786464482545853
        vf_explained_var: 0.005159124732017517
        vf_loss: 18.756986618041992
      agent-1:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1026912927627563
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016969150165095925
        model: {}
        policy_loss: -0.004221558105200529
        total_loss: -0.004225360229611397
        vf_explained_var: -1.531839370727539e-05
        vf_loss: 19.369338989257812
      agent-2:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1014902591705322
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015569735551252961
        model: {}
        policy_loss: -0.0035751000978052616
        total_loss: -0.003596189897507429
        vf_explained_var: 0.022527799010276794
        vf_loss: 19.175342559814453
      agent-3:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5571069121360779
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014948660973459482
        model: {}
        policy_loss: -0.0028518205508589745
        total_loss: -0.002105511724948883
        vf_explained_var: 0.08304257690906525
        vf_loss: 17.268163681030273
      agent-4:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9078130722045898
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013017634628340602
        model: {}
        policy_loss: -0.0038737354334443808
        total_loss: -0.003640769049525261
        vf_explained_var: 0.030013814568519592
        vf_loss: 18.307178497314453
      agent-5:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7469354867935181
        entropy_coeff: 0.0017600000137463212
        kl: 0.001541024073958397
        model: {}
        policy_loss: -0.003375338390469551
        total_loss: -0.003015057183802128
        vf_explained_var: 0.10510975122451782
        vf_loss: 16.748878479003906
    load_time_ms: 21410.811
    num_steps_sampled: 22752000
    num_steps_trained: 22752000
    sample_time_ms: 129342.038
    update_time_ms: 64.405
  iterations_since_restore: 157
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.798648648648648
    ram_util_percent: 14.683333333333335
  pid: 14340
  policy_reward_max:
    agent-0: 178.83333333333297
    agent-1: 178.83333333333297
    agent-2: 178.83333333333297
    agent-3: 178.83333333333297
    agent-4: 178.83333333333297
    agent-5: 178.83333333333297
  policy_reward_mean:
    agent-0: 145.89666666666676
    agent-1: 145.89666666666676
    agent-2: 145.89666666666676
    agent-3: 145.89666666666676
    agent-4: 145.89666666666676
    agent-5: 145.89666666666676
  policy_reward_min:
    agent-0: 78.33333333333321
    agent-1: 78.33333333333321
    agent-2: 78.33333333333321
    agent-3: 78.33333333333321
    agent-4: 78.33333333333321
    agent-5: 78.33333333333321
  sampler_perf:
    mean_env_wait_ms: 30.596208969629988
    mean_inference_ms: 14.584585534921336
    mean_processing_ms: 65.70302575126196
  time_since_restore: 26072.08033156395
  time_this_iter_s: 156.31403017044067
  time_total_s: 38622.8972029686
  timestamp: 1637061260
  timesteps_since_restore: 15072000
  timesteps_this_iter: 96000
  timesteps_total: 22752000
  training_iteration: 237
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    237 |          38622.9 | 22752000 |   875.38 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 1.85
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 22.16
    apples_agent-1_min: 0
    apples_agent-2_max: 126
    apples_agent-2_mean: 7.19
    apples_agent-2_min: 0
    apples_agent-3_max: 198
    apples_agent-3_mean: 107.78
    apples_agent-3_min: 29
    apples_agent-4_max: 67
    apples_agent-4_mean: 2.49
    apples_agent-4_min: 0
    apples_agent-5_max: 169
    apples_agent-5_mean: 109.19
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 557
    cleaning_beam_agent-0_mean: 413.25
    cleaning_beam_agent-0_min: 254
    cleaning_beam_agent-1_max: 476
    cleaning_beam_agent-1_mean: 266.53
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 496
    cleaning_beam_agent-2_mean: 336.79
    cleaning_beam_agent-2_min: 145
    cleaning_beam_agent-3_max: 148
    cleaning_beam_agent-3_mean: 42.54
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 569
    cleaning_beam_agent-4_mean: 448.18
    cleaning_beam_agent-4_min: 156
    cleaning_beam_agent-5_max: 211
    cleaning_beam_agent-5_mean: 40.45
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-17-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1041.9999999999882
  episode_reward_mean: 870.2799999999858
  episode_reward_min: 349.0000000000009
  episodes_this_iter: 96
  episodes_total: 22848
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13015.248
    learner:
      agent-0:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0470632314682007
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015865042805671692
        model: {}
        policy_loss: -0.003136318176984787
        total_loss: -0.0030041898135095835
        vf_explained_var: 0.028239935636520386
        vf_loss: 19.749601364135742
      agent-1:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0986859798431396
        entropy_coeff: 0.0017600000137463212
        kl: 0.00202386686578393
        model: {}
        policy_loss: -0.004633622244000435
        total_loss: -0.004442019388079643
        vf_explained_var: -0.013726025819778442
        vf_loss: 21.252897262573242
      agent-2:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1098613739013672
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018693397287279367
        model: {}
        policy_loss: -0.0036993972025811672
        total_loss: -0.0036062835715711117
        vf_explained_var: 0.024641722440719604
        vf_loss: 20.464725494384766
      agent-3:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.529312014579773
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015826462768018246
        model: {}
        policy_loss: -0.002803688868880272
        total_loss: -0.0019504120573401451
        vf_explained_var: 0.11480604112148285
        vf_loss: 17.84865951538086
      agent-4:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9170585870742798
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014678294537588954
        model: {}
        policy_loss: -0.004079227335751057
        total_loss: -0.0038087840657681227
        vf_explained_var: 0.08726513385772705
        vf_loss: 18.84463882446289
      agent-5:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7796785831451416
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018737277714535594
        model: {}
        policy_loss: -0.003386985044926405
        total_loss: -0.003005082719027996
        vf_explained_var: 0.13818217813968658
        vf_loss: 17.541362762451172
    load_time_ms: 20027.818
    num_steps_sampled: 22848000
    num_steps_trained: 22848000
    sample_time_ms: 129447.245
    update_time_ms: 65.316
  iterations_since_restore: 158
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.306113537117906
    ram_util_percent: 14.821834061135375
  pid: 14340
  policy_reward_max:
    agent-0: 173.66666666666654
    agent-1: 173.66666666666654
    agent-2: 173.66666666666654
    agent-3: 173.66666666666654
    agent-4: 173.66666666666654
    agent-5: 173.66666666666654
  policy_reward_mean:
    agent-0: 145.04666666666668
    agent-1: 145.04666666666668
    agent-2: 145.04666666666668
    agent-3: 145.04666666666668
    agent-4: 145.04666666666668
    agent-5: 145.04666666666668
  policy_reward_min:
    agent-0: 58.166666666666515
    agent-1: 58.166666666666515
    agent-2: 58.166666666666515
    agent-3: 58.166666666666515
    agent-4: 58.166666666666515
    agent-5: 58.166666666666515
  sampler_perf:
    mean_env_wait_ms: 30.602889745371957
    mean_inference_ms: 14.584460681425565
    mean_processing_ms: 65.70185611186014
  time_since_restore: 26233.053557157516
  time_this_iter_s: 160.9732255935669
  time_total_s: 38783.870428562164
  timestamp: 1637061421
  timesteps_since_restore: 15168000
  timesteps_this_iter: 96000
  timesteps_total: 22848000
  training_iteration: 238
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    238 |          38783.9 | 22848000 |   870.28 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 73
    apples_agent-0_mean: 3.67
    apples_agent-0_min: 0
    apples_agent-1_max: 69
    apples_agent-1_mean: 24.13
    apples_agent-1_min: 0
    apples_agent-2_max: 121
    apples_agent-2_mean: 5.77
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 100.16
    apples_agent-3_min: 29
    apples_agent-4_max: 64
    apples_agent-4_mean: 3.36
    apples_agent-4_min: 0
    apples_agent-5_max: 171
    apples_agent-5_mean: 106.27
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 557
    cleaning_beam_agent-0_mean: 395.55
    cleaning_beam_agent-0_min: 119
    cleaning_beam_agent-1_max: 488
    cleaning_beam_agent-1_mean: 267.74
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 466
    cleaning_beam_agent-2_mean: 322.12
    cleaning_beam_agent-2_min: 145
    cleaning_beam_agent-3_max: 135
    cleaning_beam_agent-3_mean: 45.82
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 579
    cleaning_beam_agent-4_mean: 425.89
    cleaning_beam_agent-4_min: 225
    cleaning_beam_agent-5_max: 653
    cleaning_beam_agent-5_mean: 54.68
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-19-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1043.9999999999854
  episode_reward_mean: 825.5799999999883
  episode_reward_min: 308.99999999999676
  episodes_this_iter: 96
  episodes_total: 22944
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13032.79
    learner:
      agent-0:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0460892915725708
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012942927423864603
        model: {}
        policy_loss: -0.0031535555608570576
        total_loss: -0.0029311352409422398
        vf_explained_var: 0.10321718454360962
        vf_loss: 20.635379791259766
      agent-1:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1072454452514648
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017877843929454684
        model: {}
        policy_loss: -0.004152254667133093
        total_loss: -0.003810059279203415
        vf_explained_var: 0.00740446150302887
        vf_loss: 22.909488677978516
      agent-2:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.111887812614441
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012890277430415154
        model: {}
        policy_loss: -0.0037557538598775864
        total_loss: -0.003513399511575699
        vf_explained_var: 0.047714799642562866
        vf_loss: 21.992746353149414
      agent-3:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.566668689250946
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013567934511229396
        model: {}
        policy_loss: -0.0030540525913238525
        total_loss: -0.002210903912782669
        vf_explained_var: 0.20101310312747955
        vf_loss: 18.40484046936035
      agent-4:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9489182233810425
        entropy_coeff: 0.0017600000137463212
        kl: 0.001753014512360096
        model: {}
        policy_loss: -0.003885071724653244
        total_loss: -0.003433540929108858
        vf_explained_var: 0.08205392956733704
        vf_loss: 21.216276168823242
      agent-5:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7992210388183594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013221476692706347
        model: {}
        policy_loss: -0.003385039046406746
        total_loss: -0.0029571675695478916
        vf_explained_var: 0.20262353122234344
        vf_loss: 18.344999313354492
    load_time_ms: 20034.959
    num_steps_sampled: 22944000
    num_steps_trained: 22944000
    sample_time_ms: 129230.715
    update_time_ms: 61.332
  iterations_since_restore: 159
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.69865470852018
    ram_util_percent: 14.74887892376682
  pid: 14340
  policy_reward_max:
    agent-0: 173.9999999999997
    agent-1: 173.9999999999997
    agent-2: 173.9999999999997
    agent-3: 173.9999999999997
    agent-4: 173.9999999999997
    agent-5: 173.9999999999997
  policy_reward_mean:
    agent-0: 137.59666666666675
    agent-1: 137.59666666666675
    agent-2: 137.59666666666675
    agent-3: 137.59666666666675
    agent-4: 137.59666666666675
    agent-5: 137.59666666666675
  policy_reward_min:
    agent-0: 51.49999999999995
    agent-1: 51.49999999999995
    agent-2: 51.49999999999995
    agent-3: 51.49999999999995
    agent-4: 51.49999999999995
    agent-5: 51.49999999999995
  sampler_perf:
    mean_env_wait_ms: 30.60811104082096
    mean_inference_ms: 14.583549734132284
    mean_processing_ms: 65.70156572516785
  time_since_restore: 26389.278918743134
  time_this_iter_s: 156.22536158561707
  time_total_s: 38940.09579014778
  timestamp: 1637061577
  timesteps_since_restore: 15264000
  timesteps_this_iter: 96000
  timesteps_total: 22944000
  training_iteration: 239
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    239 |          38940.1 | 22944000 |   825.58 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 65
    apples_agent-0_mean: 4.14
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 22.82
    apples_agent-1_min: 0
    apples_agent-2_max: 141
    apples_agent-2_mean: 7.06
    apples_agent-2_min: 0
    apples_agent-3_max: 160
    apples_agent-3_mean: 100.41
    apples_agent-3_min: 37
    apples_agent-4_max: 63
    apples_agent-4_mean: 3.2
    apples_agent-4_min: 0
    apples_agent-5_max: 183
    apples_agent-5_mean: 105.04
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 523
    cleaning_beam_agent-0_mean: 393.16
    cleaning_beam_agent-0_min: 155
    cleaning_beam_agent-1_max: 451
    cleaning_beam_agent-1_mean: 266.79
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 504
    cleaning_beam_agent-2_mean: 318.73
    cleaning_beam_agent-2_min: 95
    cleaning_beam_agent-3_max: 163
    cleaning_beam_agent-3_mean: 44.16
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 525
    cleaning_beam_agent-4_mean: 427.6
    cleaning_beam_agent-4_min: 185
    cleaning_beam_agent-5_max: 248
    cleaning_beam_agent-5_mean: 49.92
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-22-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1031.9999999999914
  episode_reward_mean: 843.4299999999866
  episode_reward_min: 235.9999999999988
  episodes_this_iter: 96
  episodes_total: 23040
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13030.159
    learner:
      agent-0:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0473606586456299
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014237865107133985
        model: {}
        policy_loss: -0.0029675932601094246
        total_loss: -0.002929356414824724
        vf_explained_var: 0.08699613809585571
        vf_loss: 18.81593132019043
      agent-1:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.111615538597107
        entropy_coeff: 0.0017600000137463212
        kl: 0.00152830861043185
        model: {}
        policy_loss: -0.00423858780413866
        total_loss: -0.004099209327250719
        vf_explained_var: -0.005908414721488953
        vf_loss: 20.958240509033203
      agent-2:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.104478120803833
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014267617370933294
        model: {}
        policy_loss: -0.0038728504441678524
        total_loss: -0.003701879410073161
        vf_explained_var: -0.008559420704841614
        vf_loss: 21.148557662963867
      agent-3:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5535378456115723
        entropy_coeff: 0.0017600000137463212
        kl: 0.001049170969054103
        model: {}
        policy_loss: -0.002625871915370226
        total_loss: -0.0018532390240579844
        vf_explained_var: 0.14962151646614075
        vf_loss: 17.468595504760742
      agent-4:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9364587068557739
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017308087553828955
        model: {}
        policy_loss: -0.003958134446293116
        total_loss: -0.0036832932382822037
        vf_explained_var: 0.07587374746799469
        vf_loss: 19.230083465576172
      agent-5:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.789686381816864
        entropy_coeff: 0.0017600000137463212
        kl: 0.001466842251829803
        model: {}
        policy_loss: -0.0033446969464421272
        total_loss: -0.002986788284033537
        vf_explained_var: 0.15373077988624573
        vf_loss: 17.47756004333496
    load_time_ms: 20095.86
    num_steps_sampled: 23040000
    num_steps_trained: 23040000
    sample_time_ms: 129038.689
    update_time_ms: 69.833
  iterations_since_restore: 160
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.661607142857143
    ram_util_percent: 14.682589285714286
  pid: 14340
  policy_reward_max:
    agent-0: 171.9999999999997
    agent-1: 171.9999999999997
    agent-2: 171.9999999999997
    agent-3: 171.9999999999997
    agent-4: 171.9999999999997
    agent-5: 171.9999999999997
  policy_reward_mean:
    agent-0: 140.5716666666668
    agent-1: 140.5716666666668
    agent-2: 140.5716666666668
    agent-3: 140.5716666666668
    agent-4: 140.5716666666668
    agent-5: 140.5716666666668
  policy_reward_min:
    agent-0: 39.33333333333332
    agent-1: 39.33333333333332
    agent-2: 39.33333333333332
    agent-3: 39.33333333333332
    agent-4: 39.33333333333332
    agent-5: 39.33333333333332
  sampler_perf:
    mean_env_wait_ms: 30.61358059614215
    mean_inference_ms: 14.583902087047186
    mean_processing_ms: 65.70145163750222
  time_since_restore: 26546.386642694473
  time_this_iter_s: 157.10772395133972
  time_total_s: 39097.20351409912
  timestamp: 1637061735
  timesteps_since_restore: 15360000
  timesteps_this_iter: 96000
  timesteps_total: 23040000
  training_iteration: 240
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    240 |          39097.2 | 23040000 |   843.43 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 3.01
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 24.55
    apples_agent-1_min: 0
    apples_agent-2_max: 148
    apples_agent-2_mean: 5.94
    apples_agent-2_min: 0
    apples_agent-3_max: 197
    apples_agent-3_mean: 101.59
    apples_agent-3_min: 30
    apples_agent-4_max: 60
    apples_agent-4_mean: 3.79
    apples_agent-4_min: 0
    apples_agent-5_max: 221
    apples_agent-5_mean: 104.66
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 554
    cleaning_beam_agent-0_mean: 411.02
    cleaning_beam_agent-0_min: 222
    cleaning_beam_agent-1_max: 427
    cleaning_beam_agent-1_mean: 260.03
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 490
    cleaning_beam_agent-2_mean: 313.03
    cleaning_beam_agent-2_min: 151
    cleaning_beam_agent-3_max: 236
    cleaning_beam_agent-3_mean: 39.76
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 536
    cleaning_beam_agent-4_mean: 429.35
    cleaning_beam_agent-4_min: 256
    cleaning_beam_agent-5_max: 233
    cleaning_beam_agent-5_mean: 51.97
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 3
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-24-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1046.9999999999836
  episode_reward_mean: 839.3299999999875
  episode_reward_min: 293.99999999999966
  episodes_this_iter: 96
  episodes_total: 23136
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13033.196
    learner:
      agent-0:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.050430178642273
        entropy_coeff: 0.0017600000137463212
        kl: 0.001466870540753007
        model: {}
        policy_loss: -0.003039184957742691
        total_loss: -0.0027837343513965607
        vf_explained_var: 0.05747711658477783
        vf_loss: 21.042036056518555
      agent-1:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1101865768432617
        entropy_coeff: 0.0017600000137463212
        kl: 0.001354327891021967
        model: {}
        policy_loss: -0.0038762488402426243
        total_loss: -0.003479922190308571
        vf_explained_var: -0.039925068616867065
        vf_loss: 23.50253677368164
      agent-2:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1176722049713135
        entropy_coeff: 0.0017600000137463212
        kl: 0.002221893286332488
        model: {}
        policy_loss: -0.0042360275983810425
        total_loss: -0.0041162725538015366
        vf_explained_var: 0.07642684876918793
        vf_loss: 20.86859130859375
      agent-3:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5415533781051636
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010310433572158217
        model: {}
        policy_loss: -0.0029519195668399334
        total_loss: -0.00204283744096756
        vf_explained_var: 0.16904282569885254
        vf_loss: 18.622163772583008
      agent-4:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9426385760307312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015156849985942245
        model: {}
        policy_loss: -0.004149517975747585
        total_loss: -0.003820971352979541
        vf_explained_var: 0.11897136270999908
        vf_loss: 19.875885009765625
      agent-5:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7905583381652832
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008566100732423365
        model: {}
        policy_loss: -0.0032377373427152634
        total_loss: -0.002766889985650778
        vf_explained_var: 0.16802841424942017
        vf_loss: 18.622337341308594
    load_time_ms: 18199.771
    num_steps_sampled: 23136000
    num_steps_trained: 23136000
    sample_time_ms: 128738.182
    update_time_ms: 62.918
  iterations_since_restore: 161
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.17608695652174
    ram_util_percent: 14.745652173913046
  pid: 14340
  policy_reward_max:
    agent-0: 174.49999999999966
    agent-1: 174.49999999999966
    agent-2: 174.49999999999966
    agent-3: 174.49999999999966
    agent-4: 174.49999999999966
    agent-5: 174.49999999999966
  policy_reward_mean:
    agent-0: 139.88833333333338
    agent-1: 139.88833333333338
    agent-2: 139.88833333333338
    agent-3: 139.88833333333338
    agent-4: 139.88833333333338
    agent-5: 139.88833333333338
  policy_reward_min:
    agent-0: 48.9999999999999
    agent-1: 48.9999999999999
    agent-2: 48.9999999999999
    agent-3: 48.9999999999999
    agent-4: 48.9999999999999
    agent-5: 48.9999999999999
  sampler_perf:
    mean_env_wait_ms: 30.6194991030603
    mean_inference_ms: 14.583794165876787
    mean_processing_ms: 65.70162190269535
  time_since_restore: 26703.386446237564
  time_this_iter_s: 156.99980354309082
  time_total_s: 39254.20331764221
  timestamp: 1637061896
  timesteps_since_restore: 15456000
  timesteps_this_iter: 96000
  timesteps_total: 23136000
  training_iteration: 241
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    241 |          39254.2 | 23136000 |   839.33 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 133
    apples_agent-0_mean: 4.76
    apples_agent-0_min: 0
    apples_agent-1_max: 121
    apples_agent-1_mean: 24.51
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 3.56
    apples_agent-2_min: 0
    apples_agent-3_max: 252
    apples_agent-3_mean: 99.56
    apples_agent-3_min: 44
    apples_agent-4_max: 49
    apples_agent-4_mean: 0.97
    apples_agent-4_min: 0
    apples_agent-5_max: 317
    apples_agent-5_mean: 107.44
    apples_agent-5_min: 47
    cleaning_beam_agent-0_max: 587
    cleaning_beam_agent-0_mean: 418.16
    cleaning_beam_agent-0_min: 245
    cleaning_beam_agent-1_max: 492
    cleaning_beam_agent-1_mean: 273.47
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 544
    cleaning_beam_agent-2_mean: 332.5
    cleaning_beam_agent-2_min: 86
    cleaning_beam_agent-3_max: 93
    cleaning_beam_agent-3_mean: 36.14
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 530
    cleaning_beam_agent-4_mean: 439.2
    cleaning_beam_agent-4_min: 299
    cleaning_beam_agent-5_max: 280
    cleaning_beam_agent-5_mean: 43.21
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-27-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1034.9999999999934
  episode_reward_mean: 869.9599999999855
  episode_reward_min: 369.00000000000125
  episodes_this_iter: 96
  episodes_total: 23232
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13085.363
    learner:
      agent-0:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0545494556427002
        entropy_coeff: 0.0017600000137463212
        kl: 0.001453446806408465
        model: {}
        policy_loss: -0.00279714772477746
        total_loss: -0.0027539087459445
        vf_explained_var: 0.07059922814369202
        vf_loss: 18.992481231689453
      agent-1:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1017121076583862
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014477316290140152
        model: {}
        policy_loss: -0.004167548380792141
        total_loss: -0.003976822830736637
        vf_explained_var: -0.017549604177474976
        vf_loss: 21.297319412231445
      agent-2:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1185729503631592
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013402741169556975
        model: {}
        policy_loss: -0.0037107260432094336
        total_loss: -0.003767243353649974
        vf_explained_var: 0.07572880387306213
        vf_loss: 19.12167739868164
      agent-3:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5157560706138611
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008635552949272096
        model: {}
        policy_loss: -0.002582464599981904
        total_loss: -0.0017059791134670377
        vf_explained_var: 0.1288531869649887
        vf_loss: 17.842132568359375
      agent-4:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9292091131210327
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016033154679462314
        model: {}
        policy_loss: -0.0040596723556518555
        total_loss: -0.003736505750566721
        vf_explained_var: 0.061014324426651
        vf_loss: 19.58574676513672
      agent-5:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7757319211959839
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012732382165268064
        model: {}
        policy_loss: -0.0033071148209273815
        total_loss: -0.002903460059314966
        vf_explained_var: 0.13131585717201233
        vf_loss: 17.689453125
    load_time_ms: 19057.4
    num_steps_sampled: 23232000
    num_steps_trained: 23232000
    sample_time_ms: 128458.003
    update_time_ms: 62.47
  iterations_since_restore: 162
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.492405063291134
    ram_util_percent: 14.857805907172997
  pid: 14340
  policy_reward_max:
    agent-0: 172.49999999999994
    agent-1: 172.49999999999994
    agent-2: 172.49999999999994
    agent-3: 172.49999999999994
    agent-4: 172.49999999999994
    agent-5: 172.49999999999994
  policy_reward_mean:
    agent-0: 144.99333333333342
    agent-1: 144.99333333333342
    agent-2: 144.99333333333342
    agent-3: 144.99333333333342
    agent-4: 144.99333333333342
    agent-5: 144.99333333333342
  policy_reward_min:
    agent-0: 61.49999999999997
    agent-1: 61.49999999999997
    agent-2: 61.49999999999997
    agent-3: 61.49999999999997
    agent-4: 61.49999999999997
    agent-5: 61.49999999999997
  sampler_perf:
    mean_env_wait_ms: 30.625823747802446
    mean_inference_ms: 14.58705168978799
    mean_processing_ms: 65.70907476070094
  time_since_restore: 26869.632529973984
  time_this_iter_s: 166.24608373641968
  time_total_s: 39420.44940137863
  timestamp: 1637062063
  timesteps_since_restore: 15552000
  timesteps_this_iter: 96000
  timesteps_total: 23232000
  training_iteration: 242
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    242 |          39420.4 | 23232000 |   869.96 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 2.27
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 21.49
    apples_agent-1_min: 0
    apples_agent-2_max: 177
    apples_agent-2_mean: 6.19
    apples_agent-2_min: 0
    apples_agent-3_max: 172
    apples_agent-3_mean: 102.72
    apples_agent-3_min: 59
    apples_agent-4_max: 31
    apples_agent-4_mean: 1.07
    apples_agent-4_min: 0
    apples_agent-5_max: 161
    apples_agent-5_mean: 104.89
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 559
    cleaning_beam_agent-0_mean: 415.22
    cleaning_beam_agent-0_min: 208
    cleaning_beam_agent-1_max: 408
    cleaning_beam_agent-1_mean: 258.91
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 566
    cleaning_beam_agent-2_mean: 337.35
    cleaning_beam_agent-2_min: 172
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 37.89
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 437.39
    cleaning_beam_agent-4_min: 254
    cleaning_beam_agent-5_max: 463
    cleaning_beam_agent-5_mean: 51.65
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-30-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1054.9999999999761
  episode_reward_mean: 868.5699999999856
  episode_reward_min: 406.00000000000625
  episodes_this_iter: 96
  episodes_total: 23328
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13129.029
    learner:
      agent-0:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0498158931732178
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014839624054729939
        model: {}
        policy_loss: -0.002958702389150858
        total_loss: -0.002989604137837887
        vf_explained_var: 0.07706643640995026
        vf_loss: 18.167743682861328
      agent-1:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.124070644378662
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020353011786937714
        model: {}
        policy_loss: -0.0041730208322405815
        total_loss: -0.004114458337426186
        vf_explained_var: -0.015570089221000671
        vf_loss: 20.36927604675293
      agent-2:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1028828620910645
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017354448791593313
        model: {}
        policy_loss: -0.0038215145468711853
        total_loss: -0.0037911715917289257
        vf_explained_var: 0.009545519948005676
        vf_loss: 19.714153289794922
      agent-3:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5178195238113403
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007168250158429146
        model: {}
        policy_loss: -0.00249186297878623
        total_loss: -0.00161752849817276
        vf_explained_var: 0.08959518373012543
        vf_loss: 17.85696029663086
      agent-4:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9189122915267944
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017606158507987857
        model: {}
        policy_loss: -0.004133894108235836
        total_loss: -0.0038234470412135124
        vf_explained_var: 0.0400865375995636
        vf_loss: 19.277332305908203
      agent-5:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7651604413986206
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016526548424735665
        model: {}
        policy_loss: -0.003270323621109128
        total_loss: -0.002901701256632805
        vf_explained_var: 0.13142015039920807
        vf_loss: 17.153034210205078
    load_time_ms: 17220.248
    num_steps_sampled: 23328000
    num_steps_trained: 23328000
    sample_time_ms: 128551.005
    update_time_ms: 49.061
  iterations_since_restore: 163
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.777232142857144
    ram_util_percent: 14.717410714285716
  pid: 14340
  policy_reward_max:
    agent-0: 175.83333333333306
    agent-1: 175.83333333333306
    agent-2: 175.83333333333306
    agent-3: 175.83333333333306
    agent-4: 175.83333333333306
    agent-5: 175.83333333333306
  policy_reward_mean:
    agent-0: 144.7616666666667
    agent-1: 144.7616666666667
    agent-2: 144.7616666666667
    agent-3: 144.7616666666667
    agent-4: 144.7616666666667
    agent-5: 144.7616666666667
  policy_reward_min:
    agent-0: 67.66666666666646
    agent-1: 67.66666666666646
    agent-2: 67.66666666666646
    agent-3: 67.66666666666646
    agent-4: 67.66666666666646
    agent-5: 67.66666666666646
  sampler_perf:
    mean_env_wait_ms: 30.631369551803704
    mean_inference_ms: 14.587287979353327
    mean_processing_ms: 65.70691125557838
  time_since_restore: 27027.07550263405
  time_this_iter_s: 157.4429726600647
  time_total_s: 39577.892374038696
  timestamp: 1637062221
  timesteps_since_restore: 15648000
  timesteps_this_iter: 96000
  timesteps_total: 23328000
  training_iteration: 243
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    243 |          39577.9 | 23328000 |   868.57 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 178
    apples_agent-0_mean: 6.03
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 24.87
    apples_agent-1_min: 0
    apples_agent-2_max: 117
    apples_agent-2_mean: 3.87
    apples_agent-2_min: 0
    apples_agent-3_max: 189
    apples_agent-3_mean: 105.49
    apples_agent-3_min: 35
    apples_agent-4_max: 74
    apples_agent-4_mean: 1.84
    apples_agent-4_min: 0
    apples_agent-5_max: 196
    apples_agent-5_mean: 97.76
    apples_agent-5_min: 38
    cleaning_beam_agent-0_max: 593
    cleaning_beam_agent-0_mean: 434.28
    cleaning_beam_agent-0_min: 214
    cleaning_beam_agent-1_max: 518
    cleaning_beam_agent-1_mean: 286.02
    cleaning_beam_agent-1_min: 148
    cleaning_beam_agent-2_max: 502
    cleaning_beam_agent-2_mean: 339.78
    cleaning_beam_agent-2_min: 131
    cleaning_beam_agent-3_max: 141
    cleaning_beam_agent-3_mean: 35.63
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 549
    cleaning_beam_agent-4_mean: 448.85
    cleaning_beam_agent-4_min: 278
    cleaning_beam_agent-5_max: 266
    cleaning_beam_agent-5_mean: 49.43
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-32-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1062.9999999999945
  episode_reward_mean: 871.129999999985
  episode_reward_min: 525.0000000000059
  episodes_this_iter: 96
  episodes_total: 23424
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13134.569
    learner:
      agent-0:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.027979850769043
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014009091537445784
        model: {}
        policy_loss: -0.003144599497318268
        total_loss: -0.0030522216111421585
        vf_explained_var: 0.023845121264457703
        vf_loss: 19.016225814819336
      agent-1:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0986881256103516
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014264570781961083
        model: {}
        policy_loss: -0.003908553626388311
        total_loss: -0.003775419667363167
        vf_explained_var: -0.03262639045715332
        vf_loss: 20.668228149414062
      agent-2:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1026172637939453
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017859661020338535
        model: {}
        policy_loss: -0.0034055463038384914
        total_loss: -0.0033846735022962093
        vf_explained_var: 0.00798507034778595
        vf_loss: 19.61480140686035
      agent-3:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.516112208366394
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012806141749024391
        model: {}
        policy_loss: -0.0025965096428990364
        total_loss: -0.0017505204305052757
        vf_explained_var: 0.0999835729598999
        vf_loss: 17.543472290039062
      agent-4:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9181139469146729
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016150162555277348
        model: {}
        policy_loss: -0.0038794949650764465
        total_loss: -0.003620790783315897
        vf_explained_var: 0.056491076946258545
        vf_loss: 18.74588394165039
      agent-5:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7517778277397156
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008798002963885665
        model: {}
        policy_loss: -0.00336556788533926
        total_loss: -0.0029565058648586273
        vf_explained_var: 0.11292541027069092
        vf_loss: 17.32189178466797
    load_time_ms: 17291.939
    num_steps_sampled: 23424000
    num_steps_trained: 23424000
    sample_time_ms: 128450.945
    update_time_ms: 49.328
  iterations_since_restore: 164
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.602678571428573
    ram_util_percent: 14.688839285714286
  pid: 14340
  policy_reward_max:
    agent-0: 177.16666666666674
    agent-1: 177.16666666666674
    agent-2: 177.16666666666674
    agent-3: 177.16666666666674
    agent-4: 177.16666666666674
    agent-5: 177.16666666666674
  policy_reward_mean:
    agent-0: 145.18833333333345
    agent-1: 145.18833333333345
    agent-2: 145.18833333333345
    agent-3: 145.18833333333345
    agent-4: 145.18833333333345
    agent-5: 145.18833333333345
  policy_reward_min:
    agent-0: 87.5000000000001
    agent-1: 87.5000000000001
    agent-2: 87.5000000000001
    agent-3: 87.5000000000001
    agent-4: 87.5000000000001
    agent-5: 87.5000000000001
  sampler_perf:
    mean_env_wait_ms: 30.638870454664037
    mean_inference_ms: 14.587134252112548
    mean_processing_ms: 65.7062370027518
  time_since_restore: 27183.824993371964
  time_this_iter_s: 156.74949073791504
  time_total_s: 39734.64186477661
  timestamp: 1637062378
  timesteps_since_restore: 15744000
  timesteps_this_iter: 96000
  timesteps_total: 23424000
  training_iteration: 244
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    244 |          39734.6 | 23424000 |   871.13 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 64
    apples_agent-0_mean: 4.52
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 24.75
    apples_agent-1_min: 0
    apples_agent-2_max: 66
    apples_agent-2_mean: 6.0
    apples_agent-2_min: 0
    apples_agent-3_max: 185
    apples_agent-3_mean: 98.46
    apples_agent-3_min: 31
    apples_agent-4_max: 63
    apples_agent-4_mean: 3.31
    apples_agent-4_min: 0
    apples_agent-5_max: 224
    apples_agent-5_mean: 103.1
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 602
    cleaning_beam_agent-0_mean: 428.36
    cleaning_beam_agent-0_min: 234
    cleaning_beam_agent-1_max: 540
    cleaning_beam_agent-1_mean: 253.39
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 497
    cleaning_beam_agent-2_mean: 336.59
    cleaning_beam_agent-2_min: 155
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 35.09
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 555
    cleaning_beam_agent-4_mean: 452.94
    cleaning_beam_agent-4_min: 261
    cleaning_beam_agent-5_max: 271
    cleaning_beam_agent-5_mean: 46.47
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-35-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1042.999999999985
  episode_reward_mean: 854.7399999999885
  episode_reward_min: 263.99999999999665
  episodes_this_iter: 96
  episodes_total: 23520
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13172.422
    learner:
      agent-0:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0410127639770508
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017242204630747437
        model: {}
        policy_loss: -0.0032408887054771185
        total_loss: -0.003195343539118767
        vf_explained_var: 0.0720115602016449
        vf_loss: 18.77726173400879
      agent-1:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1160751581192017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016672060592100024
        model: {}
        policy_loss: -0.004024676512926817
        total_loss: -0.003942327108234167
        vf_explained_var: 0.0014563500881195068
        vf_loss: 20.46640396118164
      agent-2:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.097182273864746
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014775528106838465
        model: {}
        policy_loss: -0.0036726410035043955
        total_loss: -0.003547497559338808
        vf_explained_var: -0.0068926215171813965
        vf_loss: 20.561885833740234
      agent-3:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5263692140579224
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009827378671616316
        model: {}
        policy_loss: -0.00271266489289701
        total_loss: -0.0018867908511310816
        vf_explained_var: 0.13445229828357697
        vf_loss: 17.5228214263916
      agent-4:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9151648283004761
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014697563601657748
        model: {}
        policy_loss: -0.004224869422614574
        total_loss: -0.003989601042121649
        vf_explained_var: 0.09648974239826202
        vf_loss: 18.459552764892578
      agent-5:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7701395750045776
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011867056600749493
        model: {}
        policy_loss: -0.0033402624540030956
        total_loss: -0.0029754501301795244
        vf_explained_var: 0.15260246396064758
        vf_loss: 17.20257568359375
    load_time_ms: 17187.021
    num_steps_sampled: 23520000
    num_steps_trained: 23520000
    sample_time_ms: 128422.785
    update_time_ms: 56.622
  iterations_since_restore: 165
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.518834080717493
    ram_util_percent: 14.758295964125562
  pid: 14340
  policy_reward_max:
    agent-0: 173.83333333333323
    agent-1: 173.83333333333323
    agent-2: 173.83333333333323
    agent-3: 173.83333333333323
    agent-4: 173.83333333333323
    agent-5: 173.83333333333323
  policy_reward_mean:
    agent-0: 142.45666666666676
    agent-1: 142.45666666666676
    agent-2: 142.45666666666676
    agent-3: 142.45666666666676
    agent-4: 142.45666666666676
    agent-5: 142.45666666666676
  policy_reward_min:
    agent-0: 43.999999999999964
    agent-1: 43.999999999999964
    agent-2: 43.999999999999964
    agent-3: 43.999999999999964
    agent-4: 43.999999999999964
    agent-5: 43.999999999999964
  sampler_perf:
    mean_env_wait_ms: 30.644950389344505
    mean_inference_ms: 14.587427926226965
    mean_processing_ms: 65.70476342262153
  time_since_restore: 27340.228365421295
  time_this_iter_s: 156.40337204933167
  time_total_s: 39891.04523682594
  timestamp: 1637062535
  timesteps_since_restore: 15840000
  timesteps_this_iter: 96000
  timesteps_total: 23520000
  training_iteration: 245
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    245 |            39891 | 23520000 |   854.74 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 89
    apples_agent-0_mean: 2.81
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 21.6
    apples_agent-1_min: 0
    apples_agent-2_max: 91
    apples_agent-2_mean: 5.17
    apples_agent-2_min: 0
    apples_agent-3_max: 302
    apples_agent-3_mean: 104.26
    apples_agent-3_min: 48
    apples_agent-4_max: 38
    apples_agent-4_mean: 1.27
    apples_agent-4_min: 0
    apples_agent-5_max: 357
    apples_agent-5_mean: 101.35
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 610
    cleaning_beam_agent-0_mean: 454.33
    cleaning_beam_agent-0_min: 240
    cleaning_beam_agent-1_max: 471
    cleaning_beam_agent-1_mean: 263.46
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 658
    cleaning_beam_agent-2_mean: 353.84
    cleaning_beam_agent-2_min: 135
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 34.84
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 582
    cleaning_beam_agent-4_mean: 460.84
    cleaning_beam_agent-4_min: 305
    cleaning_beam_agent-5_max: 290
    cleaning_beam_agent-5_mean: 50.11
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-38-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1070.9999999999832
  episode_reward_mean: 878.2099999999863
  episode_reward_min: 485.00000000001086
  episodes_this_iter: 96
  episodes_total: 23616
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13258.287
    learner:
      agent-0:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0319865942001343
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014312549028545618
        model: {}
        policy_loss: -0.003254431765526533
        total_loss: -0.0030719111673533916
        vf_explained_var: 0.006751388311386108
        vf_loss: 19.988157272338867
      agent-1:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.105355978012085
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018526138737797737
        model: {}
        policy_loss: -0.004081794060766697
        total_loss: -0.0038783662021160126
        vf_explained_var: -0.037183016538619995
        vf_loss: 21.48851776123047
      agent-2:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0839321613311768
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019087179098278284
        model: {}
        policy_loss: -0.0036749662831425667
        total_loss: -0.003597619943320751
        vf_explained_var: 0.03081311285495758
        vf_loss: 19.85064697265625
      agent-3:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4956894814968109
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015680964570492506
        model: {}
        policy_loss: -0.0025536927860230207
        total_loss: -0.0016053764848038554
        vf_explained_var: 0.08934761583805084
        vf_loss: 18.207284927368164
      agent-4:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9193556904792786
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018030062783509493
        model: {}
        policy_loss: -0.0039015477523207664
        total_loss: -0.0036108794156461954
        vf_explained_var: 0.06601284444332123
        vf_loss: 19.087318420410156
      agent-5:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7436419129371643
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017198906280100346
        model: {}
        policy_loss: -0.003226565197110176
        total_loss: -0.0027473922818899155
        vf_explained_var: 0.11972576379776001
        vf_loss: 17.879791259765625
    load_time_ms: 16163.309
    num_steps_sampled: 23616000
    num_steps_trained: 23616000
    sample_time_ms: 128386.676
    update_time_ms: 52.018
  iterations_since_restore: 166
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.79594594594594
    ram_util_percent: 14.664864864864866
  pid: 14340
  policy_reward_max:
    agent-0: 178.49999999999994
    agent-1: 178.49999999999994
    agent-2: 178.49999999999994
    agent-3: 178.49999999999994
    agent-4: 178.49999999999994
    agent-5: 178.49999999999994
  policy_reward_mean:
    agent-0: 146.36833333333342
    agent-1: 146.36833333333342
    agent-2: 146.36833333333342
    agent-3: 146.36833333333342
    agent-4: 146.36833333333342
    agent-5: 146.36833333333342
  policy_reward_min:
    agent-0: 80.83333333333341
    agent-1: 80.83333333333341
    agent-2: 80.83333333333341
    agent-3: 80.83333333333341
    agent-4: 80.83333333333341
    agent-5: 80.83333333333341
  sampler_perf:
    mean_env_wait_ms: 30.65313380801177
    mean_inference_ms: 14.586808590057265
    mean_processing_ms: 65.70512744675885
  time_since_restore: 27495.702480793
  time_this_iter_s: 155.4741153717041
  time_total_s: 40046.51935219765
  timestamp: 1637062691
  timesteps_since_restore: 15936000
  timesteps_this_iter: 96000
  timesteps_total: 23616000
  training_iteration: 246
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    246 |          40046.5 | 23616000 |   878.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 2.19
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 28.41
    apples_agent-1_min: 0
    apples_agent-2_max: 122
    apples_agent-2_mean: 7.64
    apples_agent-2_min: 0
    apples_agent-3_max: 154
    apples_agent-3_mean: 99.78
    apples_agent-3_min: 49
    apples_agent-4_max: 75
    apples_agent-4_mean: 1.89
    apples_agent-4_min: 0
    apples_agent-5_max: 186
    apples_agent-5_mean: 107.1
    apples_agent-5_min: 19
    cleaning_beam_agent-0_max: 613
    cleaning_beam_agent-0_mean: 443.99
    cleaning_beam_agent-0_min: 282
    cleaning_beam_agent-1_max: 458
    cleaning_beam_agent-1_mean: 244.95
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 582
    cleaning_beam_agent-2_mean: 359.03
    cleaning_beam_agent-2_min: 122
    cleaning_beam_agent-3_max: 124
    cleaning_beam_agent-3_mean: 35.46
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 603
    cleaning_beam_agent-4_mean: 460.59
    cleaning_beam_agent-4_min: 289
    cleaning_beam_agent-5_max: 386
    cleaning_beam_agent-5_mean: 41.83
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-40-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1074.9999999999843
  episode_reward_mean: 878.739999999986
  episode_reward_min: 545.0000000000053
  episodes_this_iter: 96
  episodes_total: 23712
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13173.214
    learner:
      agent-0:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0399457216262817
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017471832688897848
        model: {}
        policy_loss: -0.00330977700650692
        total_loss: -0.0033546164631843567
        vf_explained_var: 0.04213950037956238
        vf_loss: 17.854623794555664
      agent-1:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1232621669769287
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012965667992830276
        model: {}
        policy_loss: -0.004138539545238018
        total_loss: -0.004140262026339769
        vf_explained_var: -0.030772149562835693
        vf_loss: 19.752201080322266
      agent-2:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0758764743804932
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013606180436909199
        model: {}
        policy_loss: -0.0036956253461539745
        total_loss: -0.00365908769890666
        vf_explained_var: -0.018719106912612915
        vf_loss: 19.30078887939453
      agent-3:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.508061408996582
        entropy_coeff: 0.0017600000137463212
        kl: 0.001749495160765946
        model: {}
        policy_loss: -0.002666953019797802
        total_loss: -0.0018183919601142406
        vf_explained_var: 0.06492657959461212
        vf_loss: 17.427499771118164
      agent-4:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9178907871246338
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017997701652348042
        model: {}
        policy_loss: -0.0038009630516171455
        total_loss: -0.0036135707050561905
        vf_explained_var: 0.045794472098350525
        vf_loss: 18.028779983520508
      agent-5:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7724454998970032
        entropy_coeff: 0.0017600000137463212
        kl: 0.002077295910567045
        model: {}
        policy_loss: -0.003643109230324626
        total_loss: -0.0033081346191465855
        vf_explained_var: 0.10209102928638458
        vf_loss: 16.944801330566406
    load_time_ms: 16052.076
    num_steps_sampled: 23712000
    num_steps_trained: 23712000
    sample_time_ms: 128438.608
    update_time_ms: 48.001
  iterations_since_restore: 167
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.87590909090909
    ram_util_percent: 14.654090909090911
  pid: 14340
  policy_reward_max:
    agent-0: 179.16666666666688
    agent-1: 179.16666666666688
    agent-2: 179.16666666666688
    agent-3: 179.16666666666688
    agent-4: 179.16666666666688
    agent-5: 179.16666666666688
  policy_reward_mean:
    agent-0: 146.45666666666676
    agent-1: 146.45666666666676
    agent-2: 146.45666666666676
    agent-3: 146.45666666666676
    agent-4: 146.45666666666676
    agent-5: 146.45666666666676
  policy_reward_min:
    agent-0: 90.83333333333361
    agent-1: 90.83333333333361
    agent-2: 90.83333333333361
    agent-3: 90.83333333333361
    agent-4: 90.83333333333361
    agent-5: 90.83333333333361
  sampler_perf:
    mean_env_wait_ms: 30.659874111715215
    mean_inference_ms: 14.586761624004005
    mean_processing_ms: 65.70344770970762
  time_since_restore: 27650.583242177963
  time_this_iter_s: 154.880761384964
  time_total_s: 40201.40011358261
  timestamp: 1637062846
  timesteps_since_restore: 16032000
  timesteps_this_iter: 96000
  timesteps_total: 23712000
  training_iteration: 247
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    247 |          40201.4 | 23712000 |   878.74 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 1.37
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 25.2
    apples_agent-1_min: 0
    apples_agent-2_max: 90
    apples_agent-2_mean: 7.39
    apples_agent-2_min: 0
    apples_agent-3_max: 173
    apples_agent-3_mean: 101.57
    apples_agent-3_min: 42
    apples_agent-4_max: 59
    apples_agent-4_mean: 1.43
    apples_agent-4_min: 0
    apples_agent-5_max: 154
    apples_agent-5_mean: 101.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 617
    cleaning_beam_agent-0_mean: 449.34
    cleaning_beam_agent-0_min: 262
    cleaning_beam_agent-1_max: 470
    cleaning_beam_agent-1_mean: 248.04
    cleaning_beam_agent-1_min: 149
    cleaning_beam_agent-2_max: 546
    cleaning_beam_agent-2_mean: 353.09
    cleaning_beam_agent-2_min: 181
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 34.17
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 635
    cleaning_beam_agent-4_mean: 473.39
    cleaning_beam_agent-4_min: 284
    cleaning_beam_agent-5_max: 564
    cleaning_beam_agent-5_mean: 57.5
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-43-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1048.9999999999775
  episode_reward_mean: 878.3999999999859
  episode_reward_min: 414.0000000000056
  episodes_this_iter: 96
  episodes_total: 23808
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13142.901
    learner:
      agent-0:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.043861985206604
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012847082689404488
        model: {}
        policy_loss: -0.003055357374250889
        total_loss: -0.002971363253891468
        vf_explained_var: -0.0071839988231658936
        vf_loss: 19.21188735961914
      agent-1:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1187498569488525
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019528490956872702
        model: {}
        policy_loss: -0.004334731958806515
        total_loss: -0.004276917316019535
        vf_explained_var: -0.025119245052337646
        vf_loss: 20.268142700195312
      agent-2:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0884792804718018
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015825991285964847
        model: {}
        policy_loss: -0.003508226713165641
        total_loss: -0.003504791297018528
        vf_explained_var: 0.013179704546928406
        vf_loss: 19.191558837890625
      agent-3:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4872438907623291
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013356675626710057
        model: {}
        policy_loss: -0.002445082413032651
        total_loss: -0.0016526146791875362
        vf_explained_var: 0.13342632353305817
        vf_loss: 16.50015640258789
      agent-4:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9112728238105774
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019912882708013058
        model: {}
        policy_loss: -0.004095800220966339
        total_loss: -0.0038482723757624626
        vf_explained_var: 0.0442071259021759
        vf_loss: 18.513710021972656
      agent-5:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7664484977722168
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013272069627419114
        model: {}
        policy_loss: -0.0032734579872339964
        total_loss: -0.0028473837301135063
        vf_explained_var: 0.0835227519273758
        vf_loss: 17.75022315979004
    load_time_ms: 15460.093
    num_steps_sampled: 23808000
    num_steps_trained: 23808000
    sample_time_ms: 128611.257
    update_time_ms: 47.293
  iterations_since_restore: 168
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.773991031390135
    ram_util_percent: 14.63766816143498
  pid: 14340
  policy_reward_max:
    agent-0: 174.83333333333294
    agent-1: 174.83333333333294
    agent-2: 174.83333333333294
    agent-3: 174.83333333333294
    agent-4: 174.83333333333294
    agent-5: 174.83333333333294
  policy_reward_mean:
    agent-0: 146.40000000000006
    agent-1: 146.40000000000006
    agent-2: 146.40000000000006
    agent-3: 146.40000000000006
    agent-4: 146.40000000000006
    agent-5: 146.40000000000006
  policy_reward_min:
    agent-0: 68.9999999999999
    agent-1: 68.9999999999999
    agent-2: 68.9999999999999
    agent-3: 68.9999999999999
    agent-4: 68.9999999999999
    agent-5: 68.9999999999999
  sampler_perf:
    mean_env_wait_ms: 30.66753264629218
    mean_inference_ms: 14.586637802665086
    mean_processing_ms: 65.7027996991735
  time_since_restore: 27807.014978647232
  time_this_iter_s: 156.4317364692688
  time_total_s: 40357.83185005188
  timestamp: 1637063002
  timesteps_since_restore: 16128000
  timesteps_this_iter: 96000
  timesteps_total: 23808000
  training_iteration: 248
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    248 |          40357.8 | 23808000 |    878.4 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 3.58
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 24.02
    apples_agent-1_min: 0
    apples_agent-2_max: 104
    apples_agent-2_mean: 5.28
    apples_agent-2_min: 0
    apples_agent-3_max: 184
    apples_agent-3_mean: 102.76
    apples_agent-3_min: 57
    apples_agent-4_max: 53
    apples_agent-4_mean: 1.66
    apples_agent-4_min: 0
    apples_agent-5_max: 170
    apples_agent-5_mean: 102.83
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 596
    cleaning_beam_agent-0_mean: 439.39
    cleaning_beam_agent-0_min: 251
    cleaning_beam_agent-1_max: 406
    cleaning_beam_agent-1_mean: 251.07
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 532
    cleaning_beam_agent-2_mean: 372.43
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 83
    cleaning_beam_agent-3_mean: 33.36
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 559
    cleaning_beam_agent-4_mean: 457.38
    cleaning_beam_agent-4_min: 268
    cleaning_beam_agent-5_max: 500
    cleaning_beam_agent-5_mean: 65.41
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-45-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1076.999999999995
  episode_reward_mean: 868.1499999999869
  episode_reward_min: 444.0000000000086
  episodes_this_iter: 96
  episodes_total: 23904
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13124.455
    learner:
      agent-0:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0387530326843262
        entropy_coeff: 0.0017600000137463212
        kl: 0.001134395133703947
        model: {}
        policy_loss: -0.0029866534750908613
        total_loss: -0.00292913313023746
        vf_explained_var: 0.049228549003601074
        vf_loss: 18.85724449157715
      agent-1:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1065855026245117
        entropy_coeff: 0.0017600000137463212
        kl: 0.001923400443047285
        model: {}
        policy_loss: -0.004325925372540951
        total_loss: -0.004178793169558048
        vf_explained_var: -0.0273686945438385
        vf_loss: 20.947216033935547
      agent-2:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.075226902961731
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015551639953628182
        model: {}
        policy_loss: -0.003792314324527979
        total_loss: -0.003690261160954833
        vf_explained_var: 0.005282804369926453
        vf_loss: 19.944496154785156
      agent-3:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4955747723579407
        entropy_coeff: 0.0017600000137463212
        kl: 0.001113644801080227
        model: {}
        policy_loss: -0.002664241474121809
        total_loss: -0.001782052218914032
        vf_explained_var: 0.11459055542945862
        vf_loss: 17.54400062561035
      agent-4:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9120844602584839
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012948636431246996
        model: {}
        policy_loss: -0.0039332956075668335
        total_loss: -0.003627926344051957
        vf_explained_var: 0.04951836168766022
        vf_loss: 19.106393814086914
      agent-5:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7746782898902893
        entropy_coeff: 0.0017600000137463212
        kl: 0.00116993288975209
        model: {}
        policy_loss: -0.0036335724871605635
        total_loss: -0.003230099566280842
        vf_explained_var: 0.1258762925863266
        vf_loss: 17.66909408569336
    load_time_ms: 15300.266
    num_steps_sampled: 23904000
    num_steps_trained: 23904000
    sample_time_ms: 128555.91
    update_time_ms: 46.858
  iterations_since_restore: 169
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.925454545454546
    ram_util_percent: 14.651363636363637
  pid: 14340
  policy_reward_max:
    agent-0: 179.4999999999993
    agent-1: 179.4999999999993
    agent-2: 179.4999999999993
    agent-3: 179.4999999999993
    agent-4: 179.4999999999993
    agent-5: 179.4999999999993
  policy_reward_mean:
    agent-0: 144.69166666666675
    agent-1: 144.69166666666675
    agent-2: 144.69166666666675
    agent-3: 144.69166666666675
    agent-4: 144.69166666666675
    agent-5: 144.69166666666675
  policy_reward_min:
    agent-0: 74.0
    agent-1: 74.0
    agent-2: 74.0
    agent-3: 74.0
    agent-4: 74.0
    agent-5: 74.0
  sampler_perf:
    mean_env_wait_ms: 30.67542247847322
    mean_inference_ms: 14.586221504400573
    mean_processing_ms: 65.70119585713543
  time_since_restore: 27960.93300318718
  time_this_iter_s: 153.9180245399475
  time_total_s: 40511.74987459183
  timestamp: 1637063156
  timesteps_since_restore: 16224000
  timesteps_this_iter: 96000
  timesteps_total: 23904000
  training_iteration: 249
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    249 |          40511.7 | 23904000 |   868.15 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 3.35
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 19.34
    apples_agent-1_min: 0
    apples_agent-2_max: 246
    apples_agent-2_mean: 10.52
    apples_agent-2_min: 0
    apples_agent-3_max: 183
    apples_agent-3_mean: 99.43
    apples_agent-3_min: 18
    apples_agent-4_max: 59
    apples_agent-4_mean: 2.62
    apples_agent-4_min: 0
    apples_agent-5_max: 247
    apples_agent-5_mean: 102.67
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 579
    cleaning_beam_agent-0_mean: 437.8
    cleaning_beam_agent-0_min: 178
    cleaning_beam_agent-1_max: 452
    cleaning_beam_agent-1_mean: 264.81
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 532
    cleaning_beam_agent-2_mean: 355.14
    cleaning_beam_agent-2_min: 89
    cleaning_beam_agent-3_max: 135
    cleaning_beam_agent-3_mean: 37.82
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 585
    cleaning_beam_agent-4_mean: 457.5
    cleaning_beam_agent-4_min: 323
    cleaning_beam_agent-5_max: 358
    cleaning_beam_agent-5_mean: 57.34
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-48-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1094.9999999999861
  episode_reward_mean: 843.2599999999885
  episode_reward_min: 202.99999999999744
  episodes_this_iter: 96
  episodes_total: 24000
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13133.145
    learner:
      agent-0:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.037890911102295
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014579987619072199
        model: {}
        policy_loss: -0.0028919170144945383
        total_loss: -0.0025433979462832212
        vf_explained_var: 0.08896927535533905
        vf_loss: 21.752046585083008
      agent-1:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.107200264930725
        entropy_coeff: 0.0017600000137463212
        kl: 0.001901144627481699
        model: {}
        policy_loss: -0.004408346023410559
        total_loss: -0.003853581380099058
        vf_explained_var: -0.0449848473072052
        vf_loss: 25.03437614440918
      agent-2:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.085606575012207
        entropy_coeff: 0.0017600000137463212
        kl: 0.001829376327805221
        model: {}
        policy_loss: -0.003609912469983101
        total_loss: -0.0033147246576845646
        vf_explained_var: 0.07916247844696045
        vf_loss: 22.05855941772461
      agent-3:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5240848064422607
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010345843620598316
        model: {}
        policy_loss: -0.0027291905134916306
        total_loss: -0.0016975845210254192
        vf_explained_var: 0.1818837970495224
        vf_loss: 19.539953231811523
      agent-4:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9264800548553467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015940150478854775
        model: {}
        policy_loss: -0.003972478210926056
        total_loss: -0.0033771879971027374
        vf_explained_var: 0.06877855956554413
        vf_loss: 22.258962631225586
      agent-5:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7829144597053528
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019428613595664501
        model: {}
        policy_loss: -0.0036129355430603027
        total_loss: -0.002983695361763239
        vf_explained_var: 0.1610705852508545
        vf_loss: 20.071704864501953
    load_time_ms: 15114.746
    num_steps_sampled: 24000000
    num_steps_trained: 24000000
    sample_time_ms: 128637.486
    update_time_ms: 38.371
  iterations_since_restore: 170
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.914479638009052
    ram_util_percent: 14.664253393665161
  pid: 14340
  policy_reward_max:
    agent-0: 182.49999999999986
    agent-1: 182.49999999999986
    agent-2: 182.49999999999986
    agent-3: 182.49999999999986
    agent-4: 182.49999999999986
    agent-5: 182.49999999999986
  policy_reward_mean:
    agent-0: 140.54333333333335
    agent-1: 140.54333333333335
    agent-2: 140.54333333333335
    agent-3: 140.54333333333335
    agent-4: 140.54333333333335
    agent-5: 140.54333333333335
  policy_reward_min:
    agent-0: 33.83333333333337
    agent-1: 33.83333333333337
    agent-2: 33.83333333333337
    agent-3: 33.83333333333337
    agent-4: 33.83333333333337
    agent-5: 33.83333333333337
  sampler_perf:
    mean_env_wait_ms: 30.683198437372837
    mean_inference_ms: 14.58619269782085
    mean_processing_ms: 65.70239740154899
  time_since_restore: 28116.6458837986
  time_this_iter_s: 155.71288061141968
  time_total_s: 40667.46275520325
  timestamp: 1637063312
  timesteps_since_restore: 16320000
  timesteps_this_iter: 96000
  timesteps_total: 24000000
  training_iteration: 250
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    250 |          40667.5 | 24000000 |   843.26 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 2.01
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 20.37
    apples_agent-1_min: 0
    apples_agent-2_max: 110
    apples_agent-2_mean: 6.8
    apples_agent-2_min: 0
    apples_agent-3_max: 330
    apples_agent-3_mean: 102.55
    apples_agent-3_min: 20
    apples_agent-4_max: 79
    apples_agent-4_mean: 3.0
    apples_agent-4_min: 0
    apples_agent-5_max: 294
    apples_agent-5_mean: 110.54
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 576
    cleaning_beam_agent-0_mean: 440.17
    cleaning_beam_agent-0_min: 231
    cleaning_beam_agent-1_max: 491
    cleaning_beam_agent-1_mean: 266.72
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 585
    cleaning_beam_agent-2_mean: 364.88
    cleaning_beam_agent-2_min: 157
    cleaning_beam_agent-3_max: 325
    cleaning_beam_agent-3_mean: 36.02
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 578
    cleaning_beam_agent-4_mean: 456.27
    cleaning_beam_agent-4_min: 197
    cleaning_beam_agent-5_max: 370
    cleaning_beam_agent-5_mean: 39.63
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-51-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1073.0000000000023
  episode_reward_mean: 893.599999999986
  episode_reward_min: 444.00000000000676
  episodes_this_iter: 96
  episodes_total: 24096
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13125.642
    learner:
      agent-0:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.036081314086914
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012913865502923727
        model: {}
        policy_loss: -0.0030281972140073776
        total_loss: -0.0028541351202875376
        vf_explained_var: -0.00021147727966308594
        vf_loss: 19.975624084472656
      agent-1:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1085174083709717
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016409261152148247
        model: {}
        policy_loss: -0.0040261996909976006
        total_loss: -0.0038315989077091217
        vf_explained_var: -0.03717079758644104
        vf_loss: 21.455923080444336
      agent-2:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0835115909576416
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015193126164376736
        model: {}
        policy_loss: -0.0035252366214990616
        total_loss: -0.003325904253870249
        vf_explained_var: -0.02822546660900116
        vf_loss: 21.063119888305664
      agent-3:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48645883798599243
        entropy_coeff: 0.0017600000137463212
        kl: 0.001059221918694675
        model: {}
        policy_loss: -0.0023208362981677055
        total_loss: -0.00136627908796072
        vf_explained_var: 0.09688781201839447
        vf_loss: 18.107215881347656
      agent-4:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9172489643096924
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019870903342962265
        model: {}
        policy_loss: -0.0040514105930924416
        total_loss: -0.0038044583052396774
        vf_explained_var: 0.0804409384727478
        vf_loss: 18.613107681274414
      agent-5:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7327648401260376
        entropy_coeff: 0.0017600000137463212
        kl: 0.002188118640333414
        model: {}
        policy_loss: -0.00309605710208416
        total_loss: -0.0026237955316901207
        vf_explained_var: 0.12897644937038422
        vf_loss: 17.619281768798828
    load_time_ms: 14969.035
    num_steps_sampled: 24096000
    num_steps_trained: 24096000
    sample_time_ms: 128682.833
    update_time_ms: 33.569
  iterations_since_restore: 171
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.85855855855856
    ram_util_percent: 14.632432432432434
  pid: 14340
  policy_reward_max:
    agent-0: 178.83333333333331
    agent-1: 178.83333333333331
    agent-2: 178.83333333333331
    agent-3: 178.83333333333331
    agent-4: 178.83333333333331
    agent-5: 178.83333333333331
  policy_reward_mean:
    agent-0: 148.93333333333337
    agent-1: 148.93333333333337
    agent-2: 148.93333333333337
    agent-3: 148.93333333333337
    agent-4: 148.93333333333337
    agent-5: 148.93333333333337
  policy_reward_min:
    agent-0: 73.99999999999997
    agent-1: 73.99999999999997
    agent-2: 73.99999999999997
    agent-3: 73.99999999999997
    agent-4: 73.99999999999997
    agent-5: 73.99999999999997
  sampler_perf:
    mean_env_wait_ms: 30.691043826916957
    mean_inference_ms: 14.586173605161262
    mean_processing_ms: 65.70132976040726
  time_since_restore: 28272.444754838943
  time_this_iter_s: 155.79887104034424
  time_total_s: 40823.26162624359
  timestamp: 1637063468
  timesteps_since_restore: 16416000
  timesteps_this_iter: 96000
  timesteps_total: 24096000
  training_iteration: 251
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    251 |          40823.3 | 24096000 |    893.6 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 1.63
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 22.49
    apples_agent-1_min: 0
    apples_agent-2_max: 110
    apples_agent-2_mean: 6.06
    apples_agent-2_min: 0
    apples_agent-3_max: 178
    apples_agent-3_mean: 105.19
    apples_agent-3_min: 20
    apples_agent-4_max: 56
    apples_agent-4_mean: 1.87
    apples_agent-4_min: 0
    apples_agent-5_max: 198
    apples_agent-5_mean: 106.78
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 586
    cleaning_beam_agent-0_mean: 439.52
    cleaning_beam_agent-0_min: 237
    cleaning_beam_agent-1_max: 443
    cleaning_beam_agent-1_mean: 281.81
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 545
    cleaning_beam_agent-2_mean: 357.19
    cleaning_beam_agent-2_min: 150
    cleaning_beam_agent-3_max: 325
    cleaning_beam_agent-3_mean: 35.92
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 578
    cleaning_beam_agent-4_mean: 468.14
    cleaning_beam_agent-4_min: 358
    cleaning_beam_agent-5_max: 193
    cleaning_beam_agent-5_mean: 40.97
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-53-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1094.9999999999927
  episode_reward_mean: 889.7199999999859
  episode_reward_min: 401.00000000000307
  episodes_this_iter: 96
  episodes_total: 24192
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13078.023
    learner:
      agent-0:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0572322607040405
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016272620996460319
        model: {}
        policy_loss: -0.00304573867470026
        total_loss: -0.0029746328946202993
        vf_explained_var: 0.03939194977283478
        vf_loss: 19.318336486816406
      agent-1:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0926433801651
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016508460976183414
        model: {}
        policy_loss: -0.004418788943439722
        total_loss: -0.004205076489597559
        vf_explained_var: -0.030855387449264526
        vf_loss: 21.367666244506836
      agent-2:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0947993993759155
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017336953897029161
        model: {}
        policy_loss: -0.0037893077824264765
        total_loss: -0.003572371555492282
        vf_explained_var: -0.041778236627578735
        vf_loss: 21.437835693359375
      agent-3:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4959562122821808
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014496081275865436
        model: {}
        policy_loss: -0.0025029792450368404
        total_loss: -0.001612731721252203
        vf_explained_var: 0.12477609515190125
        vf_loss: 17.63129234313965
      agent-4:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9114038944244385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018126852810382843
        model: {}
        policy_loss: -0.003950047306716442
        total_loss: -0.0036082081496715546
        vf_explained_var: 0.04922786355018616
        vf_loss: 19.459117889404297
      agent-5:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7430262565612793
        entropy_coeff: 0.0017600000137463212
        kl: 0.001130837481468916
        model: {}
        policy_loss: -0.0032315657008439302
        total_loss: -0.0027721296064555645
        vf_explained_var: 0.13005369901657104
        vf_loss: 17.671632766723633
    load_time_ms: 13907.981
    num_steps_sampled: 24192000
    num_steps_trained: 24192000
    sample_time_ms: 128574.543
    update_time_ms: 33.482
  iterations_since_restore: 172
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.95707762557078
    ram_util_percent: 14.643378995433793
  pid: 14340
  policy_reward_max:
    agent-0: 182.49999999999952
    agent-1: 182.49999999999952
    agent-2: 182.49999999999952
    agent-3: 182.49999999999952
    agent-4: 182.49999999999952
    agent-5: 182.49999999999952
  policy_reward_mean:
    agent-0: 148.28666666666672
    agent-1: 148.28666666666672
    agent-2: 148.28666666666672
    agent-3: 148.28666666666672
    agent-4: 148.28666666666672
    agent-5: 148.28666666666672
  policy_reward_min:
    agent-0: 66.83333333333331
    agent-1: 66.83333333333331
    agent-2: 66.83333333333331
    agent-3: 66.83333333333331
    agent-4: 66.83333333333331
    agent-5: 66.83333333333331
  sampler_perf:
    mean_env_wait_ms: 30.698905210487673
    mean_inference_ms: 14.586131695579027
    mean_processing_ms: 65.7012058089577
  time_since_restore: 28426.55235862732
  time_this_iter_s: 154.10760378837585
  time_total_s: 40977.36923003197
  timestamp: 1637063622
  timesteps_since_restore: 16512000
  timesteps_this_iter: 96000
  timesteps_total: 24192000
  training_iteration: 252
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    252 |          40977.4 | 24192000 |   889.72 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 2.89
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 25.62
    apples_agent-1_min: 0
    apples_agent-2_max: 191
    apples_agent-2_mean: 10.58
    apples_agent-2_min: 0
    apples_agent-3_max: 243
    apples_agent-3_mean: 103.34
    apples_agent-3_min: 52
    apples_agent-4_max: 72
    apples_agent-4_mean: 2.4
    apples_agent-4_min: 0
    apples_agent-5_max: 242
    apples_agent-5_mean: 101.97
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 597
    cleaning_beam_agent-0_mean: 438.81
    cleaning_beam_agent-0_min: 186
    cleaning_beam_agent-1_max: 545
    cleaning_beam_agent-1_mean: 281.52
    cleaning_beam_agent-1_min: 159
    cleaning_beam_agent-2_max: 540
    cleaning_beam_agent-2_mean: 343.9
    cleaning_beam_agent-2_min: 146
    cleaning_beam_agent-3_max: 149
    cleaning_beam_agent-3_mean: 35.06
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 587
    cleaning_beam_agent-4_mean: 478.13
    cleaning_beam_agent-4_min: 324
    cleaning_beam_agent-5_max: 324
    cleaning_beam_agent-5_mean: 50.32
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 3
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-56-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1079.999999999997
  episode_reward_mean: 870.6399999999882
  episode_reward_min: 327.000000000003
  episodes_this_iter: 96
  episodes_total: 24288
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13042.219
    learner:
      agent-0:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.030667781829834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014459880767390132
        model: {}
        policy_loss: -0.003278033807873726
        total_loss: -0.0031358925625681877
        vf_explained_var: 0.06666599214076996
        vf_loss: 19.561185836791992
      agent-1:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1024842262268066
        entropy_coeff: 0.0017600000137463212
        kl: 0.001600718591362238
        model: {}
        policy_loss: -0.004117175005376339
        total_loss: -0.0038895048201084137
        vf_explained_var: -0.024280250072479248
        vf_loss: 21.680419921875
      agent-2:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1005369424819946
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020781613420695066
        model: {}
        policy_loss: -0.0037741458509117365
        total_loss: -0.0036126170307397842
        vf_explained_var: 0.009730592370033264
        vf_loss: 20.98473358154297
      agent-3:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5117541551589966
        entropy_coeff: 0.0017600000137463212
        kl: 0.001048437086865306
        model: {}
        policy_loss: -0.002409684471786022
        total_loss: -0.0014776140451431274
        vf_explained_var: 0.1267699897289276
        vf_loss: 18.327606201171875
      agent-4:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8951810598373413
        entropy_coeff: 0.0017600000137463212
        kl: 0.002176657784730196
        model: {}
        policy_loss: -0.004199960734695196
        total_loss: -0.0037376529071480036
        vf_explained_var: 0.029612496495246887
        vf_loss: 20.378280639648438
      agent-5:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7618523240089417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013930873246863484
        model: {}
        policy_loss: -0.0028846473433077335
        total_loss: -0.0024222678039222956
        vf_explained_var: 0.14109520614147186
        vf_loss: 18.032424926757812
    load_time_ms: 13764.172
    num_steps_sampled: 24288000
    num_steps_trained: 24288000
    sample_time_ms: 128638.669
    update_time_ms: 34.03
  iterations_since_restore: 173
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.49955156950673
    ram_util_percent: 14.634529147982066
  pid: 14340
  policy_reward_max:
    agent-0: 179.99999999999946
    agent-1: 179.99999999999946
    agent-2: 179.99999999999946
    agent-3: 179.99999999999946
    agent-4: 179.99999999999946
    agent-5: 179.99999999999946
  policy_reward_mean:
    agent-0: 145.10666666666674
    agent-1: 145.10666666666674
    agent-2: 145.10666666666674
    agent-3: 145.10666666666674
    agent-4: 145.10666666666674
    agent-5: 145.10666666666674
  policy_reward_min:
    agent-0: 54.49999999999988
    agent-1: 54.49999999999988
    agent-2: 54.49999999999988
    agent-3: 54.49999999999988
    agent-4: 54.49999999999988
    agent-5: 54.49999999999988
  sampler_perf:
    mean_env_wait_ms: 30.706279751148248
    mean_inference_ms: 14.58585454400611
    mean_processing_ms: 65.70114434649659
  time_since_restore: 28582.858399152756
  time_this_iter_s: 156.3060405254364
  time_total_s: 41133.6752705574
  timestamp: 1637063779
  timesteps_since_restore: 16608000
  timesteps_this_iter: 96000
  timesteps_total: 24288000
  training_iteration: 253
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    253 |          41133.7 | 24288000 |   870.64 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 70
    apples_agent-0_mean: 4.3
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 19.4
    apples_agent-1_min: 0
    apples_agent-2_max: 191
    apples_agent-2_mean: 11.0
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 109.26
    apples_agent-3_min: 45
    apples_agent-4_max: 91
    apples_agent-4_mean: 3.01
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 99.11
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 613
    cleaning_beam_agent-0_mean: 434.0
    cleaning_beam_agent-0_min: 169
    cleaning_beam_agent-1_max: 500
    cleaning_beam_agent-1_mean: 291.38
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 559
    cleaning_beam_agent-2_mean: 328.3
    cleaning_beam_agent-2_min: 91
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 32.16
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 590
    cleaning_beam_agent-4_mean: 475.37
    cleaning_beam_agent-4_min: 279
    cleaning_beam_agent-5_max: 443
    cleaning_beam_agent-5_mean: 48.54
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-59-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1075.9999999999834
  episode_reward_mean: 884.6499999999852
  episode_reward_min: 392.00000000000614
  episodes_this_iter: 96
  episodes_total: 24384
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13037.247
    learner:
      agent-0:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0424091815948486
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016766093904152513
        model: {}
        policy_loss: -0.003101957030594349
        total_loss: -0.0029844650998711586
        vf_explained_var: 0.016276508569717407
        vf_loss: 19.5213565826416
      agent-1:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.094664454460144
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015743025578558445
        model: {}
        policy_loss: -0.004309738054871559
        total_loss: -0.004066023975610733
        vf_explained_var: -0.0681205689907074
        vf_loss: 21.703243255615234
      agent-2:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0987170934677124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013897796161472797
        model: {}
        policy_loss: -0.0034931940026581287
        total_loss: -0.0034867790527641773
        vf_explained_var: 0.046844467520713806
        vf_loss: 19.401580810546875
      agent-3:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49773645401000977
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010720717255026102
        model: {}
        policy_loss: -0.0024239597842097282
        total_loss: -0.00151168298907578
        vf_explained_var: 0.09527422487735748
        vf_loss: 17.882965087890625
      agent-4:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9119852781295776
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017389985732734203
        model: {}
        policy_loss: -0.00389004684984684
        total_loss: -0.003622024320065975
        vf_explained_var: 0.06445123255252838
        vf_loss: 18.731168746948242
      agent-5:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7504034638404846
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011665167985484004
        model: {}
        policy_loss: -0.0031679016537964344
        total_loss: -0.0027078124694526196
        vf_explained_var: 0.10878407955169678
        vf_loss: 17.80799102783203
    load_time_ms: 15525.215
    num_steps_sampled: 24384000
    num_steps_trained: 24384000
    sample_time_ms: 128598.741
    update_time_ms: 34.106
  iterations_since_restore: 174
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.766129032258064
    ram_util_percent: 14.977822580645162
  pid: 14340
  policy_reward_max:
    agent-0: 179.33333333333312
    agent-1: 179.33333333333312
    agent-2: 179.33333333333312
    agent-3: 179.33333333333312
    agent-4: 179.33333333333312
    agent-5: 179.33333333333312
  policy_reward_mean:
    agent-0: 147.4416666666667
    agent-1: 147.4416666666667
    agent-2: 147.4416666666667
    agent-3: 147.4416666666667
    agent-4: 147.4416666666667
    agent-5: 147.4416666666667
  policy_reward_min:
    agent-0: 65.33333333333309
    agent-1: 65.33333333333309
    agent-2: 65.33333333333309
    agent-3: 65.33333333333309
    agent-4: 65.33333333333309
    agent-5: 65.33333333333309
  sampler_perf:
    mean_env_wait_ms: 30.71273658370661
    mean_inference_ms: 14.585585933537537
    mean_processing_ms: 65.72337985255793
  time_since_restore: 28756.748186588287
  time_this_iter_s: 173.88978743553162
  time_total_s: 41307.565057992935
  timestamp: 1637063953
  timesteps_since_restore: 16704000
  timesteps_this_iter: 96000
  timesteps_total: 24384000
  training_iteration: 254
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    254 |          41307.6 | 24384000 |   884.65 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 1.98
    apples_agent-0_min: 0
    apples_agent-1_max: 114
    apples_agent-1_mean: 25.23
    apples_agent-1_min: 0
    apples_agent-2_max: 56
    apples_agent-2_mean: 5.47
    apples_agent-2_min: 0
    apples_agent-3_max: 191
    apples_agent-3_mean: 108.77
    apples_agent-3_min: 49
    apples_agent-4_max: 46
    apples_agent-4_mean: 1.43
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 97.94
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 576
    cleaning_beam_agent-0_mean: 425.65
    cleaning_beam_agent-0_min: 258
    cleaning_beam_agent-1_max: 463
    cleaning_beam_agent-1_mean: 282.47
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 484
    cleaning_beam_agent-2_mean: 321.13
    cleaning_beam_agent-2_min: 143
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 32.05
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 586
    cleaning_beam_agent-4_mean: 467.33
    cleaning_beam_agent-4_min: 308
    cleaning_beam_agent-5_max: 329
    cleaning_beam_agent-5_mean: 53.77
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-01-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1098.000000000004
  episode_reward_mean: 883.209999999986
  episode_reward_min: 391.0000000000026
  episodes_this_iter: 96
  episodes_total: 24480
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12983.646
    learner:
      agent-0:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0339739322662354
        entropy_coeff: 0.0017600000137463212
        kl: 0.001767866313457489
        model: {}
        policy_loss: -0.002861171728000045
        total_loss: -0.002736722119152546
        vf_explained_var: 0.04408101737499237
        vf_loss: 19.442428588867188
      agent-1:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.086944818496704
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011963493889197707
        model: {}
        policy_loss: -0.0040318844839930534
        total_loss: -0.0037750662304461002
        vf_explained_var: -0.04309743642807007
        vf_loss: 21.69841194152832
      agent-2:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.115588903427124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016511857975274324
        model: {}
        policy_loss: -0.003576291725039482
        total_loss: -0.003459676168859005
        vf_explained_var: -0.0014118105173110962
        vf_loss: 20.800514221191406
      agent-3:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5079619288444519
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011293214047327638
        model: {}
        policy_loss: -0.0024639545008540154
        total_loss: -0.0015745935961604118
        vf_explained_var: 0.12491144239902496
        vf_loss: 17.833749771118164
      agent-4:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9135732650756836
        entropy_coeff: 0.0017600000137463212
        kl: 0.001721610315144062
        model: {}
        policy_loss: -0.00395867507904768
        total_loss: -0.0036554280668497086
        vf_explained_var: 0.0661327987909317
        vf_loss: 19.111343383789062
      agent-5:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7686924934387207
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013537723571062088
        model: {}
        policy_loss: -0.003378942608833313
        total_loss: -0.002978862263262272
        vf_explained_var: 0.13897432386875153
        vf_loss: 17.52979850769043
    load_time_ms: 15602.883
    num_steps_sampled: 24480000
    num_steps_trained: 24480000
    sample_time_ms: 128747.232
    update_time_ms: 20.514
  iterations_since_restore: 175
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.69866666666667
    ram_util_percent: 14.779555555555557
  pid: 14340
  policy_reward_max:
    agent-0: 182.9999999999995
    agent-1: 182.9999999999995
    agent-2: 182.9999999999995
    agent-3: 182.9999999999995
    agent-4: 182.9999999999995
    agent-5: 182.9999999999995
  policy_reward_mean:
    agent-0: 147.2016666666667
    agent-1: 147.2016666666667
    agent-2: 147.2016666666667
    agent-3: 147.2016666666667
    agent-4: 147.2016666666667
    agent-5: 147.2016666666667
  policy_reward_min:
    agent-0: 65.16666666666663
    agent-1: 65.16666666666663
    agent-2: 65.16666666666663
    agent-3: 65.16666666666663
    agent-4: 65.16666666666663
    agent-5: 65.16666666666663
  sampler_perf:
    mean_env_wait_ms: 30.719438404690894
    mean_inference_ms: 14.585148652402218
    mean_processing_ms: 65.72768189814968
  time_since_restore: 28914.773472070694
  time_this_iter_s: 158.02528548240662
  time_total_s: 41465.59034347534
  timestamp: 1637064111
  timesteps_since_restore: 16800000
  timesteps_this_iter: 96000
  timesteps_total: 24480000
  training_iteration: 255
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    255 |          41465.6 | 24480000 |   883.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 2.73
    apples_agent-0_min: 0
    apples_agent-1_max: 139
    apples_agent-1_mean: 23.41
    apples_agent-1_min: 0
    apples_agent-2_max: 119
    apples_agent-2_mean: 7.0
    apples_agent-2_min: 0
    apples_agent-3_max: 223
    apples_agent-3_mean: 109.85
    apples_agent-3_min: 21
    apples_agent-4_max: 53
    apples_agent-4_mean: 1.99
    apples_agent-4_min: 0
    apples_agent-5_max: 201
    apples_agent-5_mean: 107.13
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 587
    cleaning_beam_agent-0_mean: 425.01
    cleaning_beam_agent-0_min: 252
    cleaning_beam_agent-1_max: 478
    cleaning_beam_agent-1_mean: 296.41
    cleaning_beam_agent-1_min: 154
    cleaning_beam_agent-2_max: 436
    cleaning_beam_agent-2_mean: 323.44
    cleaning_beam_agent-2_min: 110
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 31.59
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 564
    cleaning_beam_agent-4_mean: 468.02
    cleaning_beam_agent-4_min: 230
    cleaning_beam_agent-5_max: 278
    cleaning_beam_agent-5_mean: 37.3
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-04-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1131.00000000001
  episode_reward_mean: 881.9699999999852
  episode_reward_min: 216.99999999999835
  episodes_this_iter: 96
  episodes_total: 24576
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12941.633
    learner:
      agent-0:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0350744724273682
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014152558287605643
        model: {}
        policy_loss: -0.003111955476924777
        total_loss: -0.0029147162567824125
        vf_explained_var: 0.04130712151527405
        vf_loss: 20.189685821533203
      agent-1:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.083646297454834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016560336807742715
        model: {}
        policy_loss: -0.0041107237339019775
        total_loss: -0.003820439800620079
        vf_explained_var: -0.02820262312889099
        vf_loss: 21.975004196166992
      agent-2:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.108069658279419
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030378769151866436
        model: {}
        policy_loss: -0.004402291029691696
        total_loss: -0.0042353137396276
        vf_explained_var: 0.00902499258518219
        vf_loss: 21.171815872192383
      agent-3:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49639928340911865
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011144974268972874
        model: {}
        policy_loss: -0.00250692549161613
        total_loss: -0.001562068471685052
        vf_explained_var: 0.13507190346717834
        vf_loss: 18.185184478759766
      agent-4:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9125654101371765
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016476522432640195
        model: {}
        policy_loss: -0.004135506227612495
        total_loss: -0.0038358280435204506
        vf_explained_var: 0.09884370863437653
        vf_loss: 19.05791473388672
      agent-5:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7615473866462708
        entropy_coeff: 0.0017600000137463212
        kl: 0.001084502087906003
        model: {}
        policy_loss: -0.0032226773910224438
        total_loss: -0.0027348536532372236
        vf_explained_var: 0.12975700199604034
        vf_loss: 18.281471252441406
    load_time_ms: 15715.237
    num_steps_sampled: 24576000
    num_steps_trained: 24576000
    sample_time_ms: 128867.465
    update_time_ms: 33.702
  iterations_since_restore: 176
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.39688888888889
    ram_util_percent: 14.763111111111114
  pid: 14340
  policy_reward_max:
    agent-0: 188.49999999999935
    agent-1: 188.49999999999935
    agent-2: 188.49999999999935
    agent-3: 188.49999999999935
    agent-4: 188.49999999999935
    agent-5: 188.49999999999935
  policy_reward_mean:
    agent-0: 146.995
    agent-1: 146.995
    agent-2: 146.995
    agent-3: 146.995
    agent-4: 146.995
    agent-5: 146.995
  policy_reward_min:
    agent-0: 36.16666666666666
    agent-1: 36.16666666666666
    agent-2: 36.16666666666666
    agent-3: 36.16666666666666
    agent-4: 36.16666666666666
    agent-5: 36.16666666666666
  sampler_perf:
    mean_env_wait_ms: 30.725125506056706
    mean_inference_ms: 14.584486826087595
    mean_processing_ms: 65.72766103702278
  time_since_restore: 29072.310023784637
  time_this_iter_s: 157.53655171394348
  time_total_s: 41623.126895189285
  timestamp: 1637064269
  timesteps_since_restore: 16896000
  timesteps_this_iter: 96000
  timesteps_total: 24576000
  training_iteration: 256
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    256 |          41623.1 | 24576000 |   881.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 2.64
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 21.8
    apples_agent-1_min: 0
    apples_agent-2_max: 119
    apples_agent-2_mean: 6.67
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 106.48
    apples_agent-3_min: 24
    apples_agent-4_max: 100
    apples_agent-4_mean: 2.76
    apples_agent-4_min: 0
    apples_agent-5_max: 176
    apples_agent-5_mean: 101.07
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 617
    cleaning_beam_agent-0_mean: 424.24
    cleaning_beam_agent-0_min: 235
    cleaning_beam_agent-1_max: 578
    cleaning_beam_agent-1_mean: 307.48
    cleaning_beam_agent-1_min: 167
    cleaning_beam_agent-2_max: 519
    cleaning_beam_agent-2_mean: 350.82
    cleaning_beam_agent-2_min: 110
    cleaning_beam_agent-3_max: 124
    cleaning_beam_agent-3_mean: 33.14
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 573
    cleaning_beam_agent-4_mean: 461.93
    cleaning_beam_agent-4_min: 266
    cleaning_beam_agent-5_max: 246
    cleaning_beam_agent-5_mean: 46.8
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-07-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1085.999999999979
  episode_reward_mean: 894.5399999999858
  episode_reward_min: 321.0000000000007
  episodes_this_iter: 96
  episodes_total: 24672
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12940.688
    learner:
      agent-0:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.036667823791504
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017477736109867692
        model: {}
        policy_loss: -0.0034164884127676487
        total_loss: -0.0031732553616166115
        vf_explained_var: 0.03938129544258118
        vf_loss: 20.677690505981445
      agent-1:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0826516151428223
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013468465767800808
        model: {}
        policy_loss: -0.004038145300000906
        total_loss: -0.0036968784406781197
        vf_explained_var: -0.02363365888595581
        vf_loss: 22.46735191345215
      agent-2:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0977195501327515
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018888721242547035
        model: {}
        policy_loss: -0.0035920017398893833
        total_loss: -0.003496970050036907
        vf_explained_var: 0.06635014712810516
        vf_loss: 20.270198822021484
      agent-3:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4787733554840088
        entropy_coeff: 0.0017600000137463212
        kl: 0.001254066824913025
        model: {}
        policy_loss: -0.002521576127037406
        total_loss: -0.0015154352877289057
        vf_explained_var: 0.1338557004928589
        vf_loss: 18.487815856933594
      agent-4:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9182219505310059
        entropy_coeff: 0.0017600000137463212
        kl: 0.001309442799538374
        model: {}
        policy_loss: -0.0037983721122145653
        total_loss: -0.0034807855263352394
        vf_explained_var: 0.11284372210502625
        vf_loss: 19.33658218383789
      agent-5:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7596909403800964
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013019791804254055
        model: {}
        policy_loss: -0.0033094845712184906
        total_loss: -0.0028137434273958206
        vf_explained_var: 0.1501099020242691
        vf_loss: 18.32797622680664
    load_time_ms: 17660.024
    num_steps_sampled: 24672000
    num_steps_trained: 24672000
    sample_time_ms: 129054.788
    update_time_ms: 38.66
  iterations_since_restore: 177
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.58725099601594
    ram_util_percent: 14.935458167330678
  pid: 14340
  policy_reward_max:
    agent-0: 181.00000000000023
    agent-1: 181.00000000000023
    agent-2: 181.00000000000023
    agent-3: 181.00000000000023
    agent-4: 181.00000000000023
    agent-5: 181.00000000000023
  policy_reward_mean:
    agent-0: 149.09
    agent-1: 149.09
    agent-2: 149.09
    agent-3: 149.09
    agent-4: 149.09
    agent-5: 149.09
  policy_reward_min:
    agent-0: 53.499999999999844
    agent-1: 53.499999999999844
    agent-2: 53.499999999999844
    agent-3: 53.499999999999844
    agent-4: 53.499999999999844
    agent-5: 53.499999999999844
  sampler_perf:
    mean_env_wait_ms: 30.733195078766524
    mean_inference_ms: 14.584805287952356
    mean_processing_ms: 65.72578736422996
  time_since_restore: 29248.491150140762
  time_this_iter_s: 176.18112635612488
  time_total_s: 41799.30802154541
  timestamp: 1637064445
  timesteps_since_restore: 16992000
  timesteps_this_iter: 96000
  timesteps_total: 24672000
  training_iteration: 257
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    257 |          41799.3 | 24672000 |   894.54 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 2.67
    apples_agent-0_min: 0
    apples_agent-1_max: 136
    apples_agent-1_mean: 22.32
    apples_agent-1_min: 0
    apples_agent-2_max: 154
    apples_agent-2_mean: 10.58
    apples_agent-2_min: 0
    apples_agent-3_max: 151
    apples_agent-3_mean: 100.79
    apples_agent-3_min: 39
    apples_agent-4_max: 43
    apples_agent-4_mean: 2.48
    apples_agent-4_min: 0
    apples_agent-5_max: 202
    apples_agent-5_mean: 104.18
    apples_agent-5_min: 19
    cleaning_beam_agent-0_max: 654
    cleaning_beam_agent-0_mean: 437.53
    cleaning_beam_agent-0_min: 177
    cleaning_beam_agent-1_max: 535
    cleaning_beam_agent-1_mean: 319.91
    cleaning_beam_agent-1_min: 182
    cleaning_beam_agent-2_max: 565
    cleaning_beam_agent-2_mean: 348.14
    cleaning_beam_agent-2_min: 121
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 32.94
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 590
    cleaning_beam_agent-4_mean: 461.54
    cleaning_beam_agent-4_min: 274
    cleaning_beam_agent-5_max: 335
    cleaning_beam_agent-5_mean: 45.18
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-10-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1085.999999999979
  episode_reward_mean: 876.2199999999843
  episode_reward_min: 352.0000000000035
  episodes_this_iter: 96
  episodes_total: 24768
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12984.021
    learner:
      agent-0:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0379998683929443
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012702324893325567
        model: {}
        policy_loss: -0.0025896821171045303
        total_loss: -0.002300616819411516
        vf_explained_var: 0.03735736012458801
        vf_loss: 21.15947723388672
      agent-1:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.073329210281372
        entropy_coeff: 0.0017600000137463212
        kl: 0.002303625689819455
        model: {}
        policy_loss: -0.004330037161707878
        total_loss: -0.003990879748016596
        vf_explained_var: -0.011306017637252808
        vf_loss: 22.282188415527344
      agent-2:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0700187683105469
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014898533700034022
        model: {}
        policy_loss: -0.0036989478394389153
        total_loss: -0.0034002342727035284
        vf_explained_var: 0.01662832498550415
        vf_loss: 21.819475173950195
      agent-3:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5061110258102417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013570792507380247
        model: {}
        policy_loss: -0.002820042660459876
        total_loss: -0.001849207328632474
        vf_explained_var: 0.15035609900951385
        vf_loss: 18.615922927856445
      agent-4:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9086254835128784
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013178386725485325
        model: {}
        policy_loss: -0.0039483606815338135
        total_loss: -0.003538337303325534
        vf_explained_var: 0.08685076236724854
        vf_loss: 20.092079162597656
      agent-5:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7570164203643799
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012113722041249275
        model: {}
        policy_loss: -0.0033629878889769316
        total_loss: -0.002872281474992633
        vf_explained_var: 0.1726192831993103
        vf_loss: 18.23055648803711
    load_time_ms: 17759.707
    num_steps_sampled: 24768000
    num_steps_trained: 24768000
    sample_time_ms: 129008.103
    update_time_ms: 39.226
  iterations_since_restore: 178
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.619642857142853
    ram_util_percent: 14.749107142857143
  pid: 14340
  policy_reward_max:
    agent-0: 181.00000000000023
    agent-1: 181.00000000000023
    agent-2: 181.00000000000023
    agent-3: 181.00000000000023
    agent-4: 181.00000000000023
    agent-5: 181.00000000000023
  policy_reward_mean:
    agent-0: 146.03666666666663
    agent-1: 146.03666666666663
    agent-2: 146.03666666666663
    agent-3: 146.03666666666663
    agent-4: 146.03666666666663
    agent-5: 146.03666666666663
  policy_reward_min:
    agent-0: 58.66666666666644
    agent-1: 58.66666666666644
    agent-2: 58.66666666666644
    agent-3: 58.66666666666644
    agent-4: 58.66666666666644
    agent-5: 58.66666666666644
  sampler_perf:
    mean_env_wait_ms: 30.741646459501773
    mean_inference_ms: 14.58484181550103
    mean_processing_ms: 65.72530641380446
  time_since_restore: 29405.897087335587
  time_this_iter_s: 157.40593719482422
  time_total_s: 41956.713958740234
  timestamp: 1637064603
  timesteps_since_restore: 17088000
  timesteps_this_iter: 96000
  timesteps_total: 24768000
  training_iteration: 258
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    258 |          41956.7 | 24768000 |   876.22 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 2.77
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 21.67
    apples_agent-1_min: 0
    apples_agent-2_max: 84
    apples_agent-2_mean: 4.65
    apples_agent-2_min: 0
    apples_agent-3_max: 185
    apples_agent-3_mean: 103.88
    apples_agent-3_min: 49
    apples_agent-4_max: 74
    apples_agent-4_mean: 1.79
    apples_agent-4_min: 0
    apples_agent-5_max: 181
    apples_agent-5_mean: 103.28
    apples_agent-5_min: 29
    cleaning_beam_agent-0_max: 605
    cleaning_beam_agent-0_mean: 429.98
    cleaning_beam_agent-0_min: 177
    cleaning_beam_agent-1_max: 536
    cleaning_beam_agent-1_mean: 310.94
    cleaning_beam_agent-1_min: 140
    cleaning_beam_agent-2_max: 569
    cleaning_beam_agent-2_mean: 373.59
    cleaning_beam_agent-2_min: 140
    cleaning_beam_agent-3_max: 154
    cleaning_beam_agent-3_mean: 29.72
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 593
    cleaning_beam_agent-4_mean: 456.64
    cleaning_beam_agent-4_min: 267
    cleaning_beam_agent-5_max: 204
    cleaning_beam_agent-5_mean: 42.96
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-12-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1115.000000000007
  episode_reward_mean: 905.029999999987
  episode_reward_min: 328.00000000000273
  episodes_this_iter: 96
  episodes_total: 24864
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12977.793
    learner:
      agent-0:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.044506549835205
        entropy_coeff: 0.0017600000137463212
        kl: 0.001506476430222392
        model: {}
        policy_loss: -0.003084793919697404
        total_loss: -0.002886321162804961
        vf_explained_var: 0.005692273378372192
        vf_loss: 20.368040084838867
      agent-1:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0827829837799072
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011692835250869393
        model: {}
        policy_loss: -0.0039710430428385735
        total_loss: -0.003694365732371807
        vf_explained_var: -0.03885585069656372
        vf_loss: 21.82372283935547
      agent-2:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0965290069580078
        entropy_coeff: 0.0017600000137463212
        kl: 0.001361452741548419
        model: {}
        policy_loss: -0.0036713355220854282
        total_loss: -0.0035501811653375626
        vf_explained_var: 0.016151651740074158
        vf_loss: 20.51045799255371
      agent-3:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4873379170894623
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009885217295959592
        model: {}
        policy_loss: -0.002512068022042513
        total_loss: -0.001463571097701788
        vf_explained_var: 0.060826078057289124
        vf_loss: 19.062145233154297
      agent-4:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9248321652412415
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028731240890920162
        model: {}
        policy_loss: -0.004379855468869209
        total_loss: -0.004094824194908142
        vf_explained_var: 0.07306452095508575
        vf_loss: 19.12735366821289
      agent-5:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7514553666114807
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011648086365312338
        model: {}
        policy_loss: -0.002949730958789587
        total_loss: -0.0024392963387072086
        vf_explained_var: 0.11153317987918854
        vf_loss: 18.329946517944336
    load_time_ms: 17961.525
    num_steps_sampled: 24864000
    num_steps_trained: 24864000
    sample_time_ms: 129154.098
    update_time_ms: 39.213
  iterations_since_restore: 179
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.753125
    ram_util_percent: 14.733928571428574
  pid: 14340
  policy_reward_max:
    agent-0: 185.83333333333337
    agent-1: 185.83333333333337
    agent-2: 185.83333333333337
    agent-3: 185.83333333333337
    agent-4: 185.83333333333337
    agent-5: 185.83333333333337
  policy_reward_mean:
    agent-0: 150.83833333333337
    agent-1: 150.83833333333337
    agent-2: 150.83833333333337
    agent-3: 150.83833333333337
    agent-4: 150.83833333333337
    agent-5: 150.83833333333337
  policy_reward_min:
    agent-0: 54.66666666666646
    agent-1: 54.66666666666646
    agent-2: 54.66666666666646
    agent-3: 54.66666666666646
    agent-4: 54.66666666666646
    agent-5: 54.66666666666646
  sampler_perf:
    mean_env_wait_ms: 30.749316164122796
    mean_inference_ms: 14.584694047095502
    mean_processing_ms: 65.72490188657413
  time_since_restore: 29563.24388575554
  time_this_iter_s: 157.3467984199524
  time_total_s: 42114.06075716019
  timestamp: 1637064760
  timesteps_since_restore: 17184000
  timesteps_this_iter: 96000
  timesteps_total: 24864000
  training_iteration: 259
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    259 |          42114.1 | 24864000 |   905.03 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 2.51
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 20.86
    apples_agent-1_min: 0
    apples_agent-2_max: 82
    apples_agent-2_mean: 5.3
    apples_agent-2_min: 0
    apples_agent-3_max: 185
    apples_agent-3_mean: 106.8
    apples_agent-3_min: 54
    apples_agent-4_max: 30
    apples_agent-4_mean: 0.87
    apples_agent-4_min: 0
    apples_agent-5_max: 210
    apples_agent-5_mean: 109.41
    apples_agent-5_min: 26
    cleaning_beam_agent-0_max: 583
    cleaning_beam_agent-0_mean: 414.69
    cleaning_beam_agent-0_min: 202
    cleaning_beam_agent-1_max: 489
    cleaning_beam_agent-1_mean: 308.08
    cleaning_beam_agent-1_min: 140
    cleaning_beam_agent-2_max: 548
    cleaning_beam_agent-2_mean: 370.22
    cleaning_beam_agent-2_min: 143
    cleaning_beam_agent-3_max: 77
    cleaning_beam_agent-3_mean: 25.44
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 570
    cleaning_beam_agent-4_mean: 448.6
    cleaning_beam_agent-4_min: 285
    cleaning_beam_agent-5_max: 406
    cleaning_beam_agent-5_mean: 43.65
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-15-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1117.999999999987
  episode_reward_mean: 915.8799999999835
  episode_reward_min: 506.0000000000115
  episodes_this_iter: 96
  episodes_total: 24960
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12967.865
    learner:
      agent-0:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0531895160675049
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014156894758343697
        model: {}
        policy_loss: -0.00282215210609138
        total_loss: -0.0027740339282900095
        vf_explained_var: 0.00885261595249176
        vf_loss: 19.01727294921875
      agent-1:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0887868404388428
        entropy_coeff: 0.0017600000137463212
        kl: 0.001680093235336244
        model: {}
        policy_loss: -0.004052477423101664
        total_loss: -0.003964079078286886
        vf_explained_var: -0.020599573850631714
        vf_loss: 20.0466251373291
      agent-2:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.094679355621338
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014306869124993682
        model: {}
        policy_loss: -0.0035016946494579315
        total_loss: -0.0034952149726450443
        vf_explained_var: 0.014293462038040161
        vf_loss: 19.331157684326172
      agent-3:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46995842456817627
        entropy_coeff: 0.0017600000137463212
        kl: 0.000965240178629756
        model: {}
        policy_loss: -0.0021398277021944523
        total_loss: -0.001135546714067459
        vf_explained_var: 0.03143945336341858
        vf_loss: 18.314054489135742
      agent-4:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9169416427612305
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015798659296706319
        model: {}
        policy_loss: -0.0038514642510563135
        total_loss: -0.003627539612352848
        vf_explained_var: 0.049323782324790955
        vf_loss: 18.377426147460938
      agent-5:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7358224391937256
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019766450859606266
        model: {}
        policy_loss: -0.0032085170969367027
        total_loss: -0.0027260654605925083
        vf_explained_var: 0.07421153783798218
        vf_loss: 17.77499008178711
    load_time_ms: 18069.525
    num_steps_sampled: 24960000
    num_steps_trained: 24960000
    sample_time_ms: 129167.56
    update_time_ms: 41.66
  iterations_since_restore: 180
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.726785714285715
    ram_util_percent: 14.755357142857145
  pid: 14340
  policy_reward_max:
    agent-0: 186.33333333333317
    agent-1: 186.33333333333317
    agent-2: 186.33333333333317
    agent-3: 186.33333333333317
    agent-4: 186.33333333333317
    agent-5: 186.33333333333317
  policy_reward_mean:
    agent-0: 152.6466666666667
    agent-1: 152.6466666666667
    agent-2: 152.6466666666667
    agent-3: 152.6466666666667
    agent-4: 152.6466666666667
    agent-5: 152.6466666666667
  policy_reward_min:
    agent-0: 84.33333333333353
    agent-1: 84.33333333333353
    agent-2: 84.33333333333353
    agent-3: 84.33333333333353
    agent-4: 84.33333333333353
    agent-5: 84.33333333333353
  sampler_perf:
    mean_env_wait_ms: 30.75608682195708
    mean_inference_ms: 14.584440205921835
    mean_processing_ms: 65.72429584867231
  time_since_restore: 29719.992306232452
  time_this_iter_s: 156.74842047691345
  time_total_s: 42270.8091776371
  timestamp: 1637064918
  timesteps_since_restore: 17280000
  timesteps_this_iter: 96000
  timesteps_total: 24960000
  training_iteration: 260
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    260 |          42270.8 | 24960000 |   915.88 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 3.61
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 27.02
    apples_agent-1_min: 0
    apples_agent-2_max: 68
    apples_agent-2_mean: 6.27
    apples_agent-2_min: 0
    apples_agent-3_max: 186
    apples_agent-3_mean: 104.55
    apples_agent-3_min: 26
    apples_agent-4_max: 25
    apples_agent-4_mean: 0.76
    apples_agent-4_min: 0
    apples_agent-5_max: 176
    apples_agent-5_mean: 105.48
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 597
    cleaning_beam_agent-0_mean: 412.87
    cleaning_beam_agent-0_min: 254
    cleaning_beam_agent-1_max: 470
    cleaning_beam_agent-1_mean: 301.03
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 533
    cleaning_beam_agent-2_mean: 345.97
    cleaning_beam_agent-2_min: 168
    cleaning_beam_agent-3_max: 118
    cleaning_beam_agent-3_mean: 31.28
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 613
    cleaning_beam_agent-4_mean: 449.75
    cleaning_beam_agent-4_min: 218
    cleaning_beam_agent-5_max: 361
    cleaning_beam_agent-5_mean: 50.56
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-18-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1096.0000000000018
  episode_reward_mean: 872.2699999999849
  episode_reward_min: 287.99999999999943
  episodes_this_iter: 96
  episodes_total: 25056
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12968.838
    learner:
      agent-0:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0438392162322998
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020981605630367994
        model: {}
        policy_loss: -0.0032083475962281227
        total_loss: -0.0030742567032575607
        vf_explained_var: 0.04984244704246521
        vf_loss: 19.712499618530273
      agent-1:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0811569690704346
        entropy_coeff: 0.0017600000137463212
        kl: 0.001264296704903245
        model: {}
        policy_loss: -0.0036825763527303934
        total_loss: -0.0034366825129836798
        vf_explained_var: -0.031117677688598633
        vf_loss: 21.487321853637695
      agent-2:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0931380987167358
        entropy_coeff: 0.0017600000137463212
        kl: 0.001098833279684186
        model: {}
        policy_loss: -0.0031859041191637516
        total_loss: -0.0031190291047096252
        vf_explained_var: 0.05143466591835022
        vf_loss: 19.907995223999023
      agent-3:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.500693142414093
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007784606423228979
        model: {}
        policy_loss: -0.0024158235173672438
        total_loss: -0.0015257543418556452
        vf_explained_var: 0.14478470385074615
        vf_loss: 17.712881088256836
      agent-4:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9223751425743103
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015839256811887026
        model: {}
        policy_loss: -0.003800896927714348
        total_loss: -0.0034942571073770523
        vf_explained_var: 0.06827425956726074
        vf_loss: 19.300201416015625
      agent-5:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7813186645507812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018025866011157632
        model: {}
        policy_loss: -0.0033437423408031464
        total_loss: -0.002937005367130041
        vf_explained_var: 0.14041338860988617
        vf_loss: 17.818584442138672
    load_time_ms: 19955.231
    num_steps_sampled: 25056000
    num_steps_trained: 25056000
    sample_time_ms: 129096.756
    update_time_ms: 63.897
  iterations_since_restore: 181
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.4644
    ram_util_percent: 14.942000000000002
  pid: 14340
  policy_reward_max:
    agent-0: 182.66666666666637
    agent-1: 182.66666666666637
    agent-2: 182.66666666666637
    agent-3: 182.66666666666637
    agent-4: 182.66666666666637
    agent-5: 182.66666666666637
  policy_reward_mean:
    agent-0: 145.3783333333334
    agent-1: 145.3783333333334
    agent-2: 145.3783333333334
    agent-3: 145.3783333333334
    agent-4: 145.3783333333334
    agent-5: 145.3783333333334
  policy_reward_min:
    agent-0: 47.99999999999989
    agent-1: 47.99999999999989
    agent-2: 47.99999999999989
    agent-3: 47.99999999999989
    agent-4: 47.99999999999989
    agent-5: 47.99999999999989
  sampler_perf:
    mean_env_wait_ms: 30.761943610759573
    mean_inference_ms: 14.583845658936637
    mean_processing_ms: 65.72280661005962
  time_since_restore: 29894.196882724762
  time_this_iter_s: 174.20457649230957
  time_total_s: 42445.01375412941
  timestamp: 1637065093
  timesteps_since_restore: 17376000
  timesteps_this_iter: 96000
  timesteps_total: 25056000
  training_iteration: 261
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    261 |            42445 | 25056000 |   872.27 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 2.17
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 25.37
    apples_agent-1_min: 0
    apples_agent-2_max: 191
    apples_agent-2_mean: 5.55
    apples_agent-2_min: 0
    apples_agent-3_max: 241
    apples_agent-3_mean: 110.51
    apples_agent-3_min: 40
    apples_agent-4_max: 61
    apples_agent-4_mean: 1.87
    apples_agent-4_min: 0
    apples_agent-5_max: 251
    apples_agent-5_mean: 108.45
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 609
    cleaning_beam_agent-0_mean: 426.69
    cleaning_beam_agent-0_min: 179
    cleaning_beam_agent-1_max: 524
    cleaning_beam_agent-1_mean: 301.67
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 603
    cleaning_beam_agent-2_mean: 363.51
    cleaning_beam_agent-2_min: 126
    cleaning_beam_agent-3_max: 112
    cleaning_beam_agent-3_mean: 30.22
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 585
    cleaning_beam_agent-4_mean: 451.1
    cleaning_beam_agent-4_min: 266
    cleaning_beam_agent-5_max: 214
    cleaning_beam_agent-5_mean: 39.8
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-20-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1114.999999999976
  episode_reward_mean: 914.2999999999852
  episode_reward_min: 342.00000000000233
  episodes_this_iter: 96
  episodes_total: 25152
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12986.348
    learner:
      agent-0:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0401684045791626
        entropy_coeff: 0.0017600000137463212
        kl: 0.002307035494595766
        model: {}
        policy_loss: -0.0033888304606080055
        total_loss: -0.0033030384220182896
        vf_explained_var: 0.053614020347595215
        vf_loss: 19.16487693786621
      agent-1:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.081339716911316
        entropy_coeff: 0.0017600000137463212
        kl: 0.001579789794050157
        model: {}
        policy_loss: -0.004257669672369957
        total_loss: -0.00396390724927187
        vf_explained_var: -0.05972951650619507
        vf_loss: 21.969196319580078
      agent-2:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0892882347106934
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018870208878070116
        model: {}
        policy_loss: -0.003489112015813589
        total_loss: -0.0034069358371198177
        vf_explained_var: 0.03267090022563934
        vf_loss: 19.993227005004883
      agent-3:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48038360476493835
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009235579054802656
        model: {}
        policy_loss: -0.0025026104412972927
        total_loss: -0.001568961888551712
        vf_explained_var: 0.11611725389957428
        vf_loss: 17.791263580322266
      agent-4:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9220185875892639
        entropy_coeff: 0.0017600000137463212
        kl: 0.001774006406776607
        model: {}
        policy_loss: -0.004061582963913679
        total_loss: -0.003774788463488221
        vf_explained_var: 0.058588892221450806
        vf_loss: 19.09546661376953
      agent-5:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7432515025138855
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011710310354828835
        model: {}
        policy_loss: -0.0030645288061350584
        total_loss: -0.00254835095256567
        vf_explained_var: 0.10075771808624268
        vf_loss: 18.24297332763672
    load_time_ms: 20113.029
    num_steps_sampled: 25152000
    num_steps_trained: 25152000
    sample_time_ms: 129319.049
    update_time_ms: 67.535
  iterations_since_restore: 182
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.560888888888886
    ram_util_percent: 14.752888888888892
  pid: 14340
  policy_reward_max:
    agent-0: 185.83333333333357
    agent-1: 185.83333333333357
    agent-2: 185.83333333333357
    agent-3: 185.83333333333357
    agent-4: 185.83333333333357
    agent-5: 185.83333333333357
  policy_reward_mean:
    agent-0: 152.38333333333335
    agent-1: 152.38333333333335
    agent-2: 152.38333333333335
    agent-3: 152.38333333333335
    agent-4: 152.38333333333335
    agent-5: 152.38333333333335
  policy_reward_min:
    agent-0: 56.99999999999986
    agent-1: 56.99999999999986
    agent-2: 56.99999999999986
    agent-3: 56.99999999999986
    agent-4: 56.99999999999986
    agent-5: 56.99999999999986
  sampler_perf:
    mean_env_wait_ms: 30.76883072862328
    mean_inference_ms: 14.583345771535008
    mean_processing_ms: 65.72148922577446
  time_since_restore: 30052.36487698555
  time_this_iter_s: 158.16799426078796
  time_total_s: 42603.1817483902
  timestamp: 1637065252
  timesteps_since_restore: 17472000
  timesteps_this_iter: 96000
  timesteps_total: 25152000
  training_iteration: 262
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    262 |          42603.2 | 25152000 |    914.3 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 3.39
    apples_agent-0_min: 0
    apples_agent-1_max: 121
    apples_agent-1_mean: 27.76
    apples_agent-1_min: 0
    apples_agent-2_max: 73
    apples_agent-2_mean: 5.92
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 104.38
    apples_agent-3_min: 20
    apples_agent-4_max: 55
    apples_agent-4_mean: 1.22
    apples_agent-4_min: 0
    apples_agent-5_max: 181
    apples_agent-5_mean: 107.7
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 610
    cleaning_beam_agent-0_mean: 422.23
    cleaning_beam_agent-0_min: 151
    cleaning_beam_agent-1_max: 548
    cleaning_beam_agent-1_mean: 294.71
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 603
    cleaning_beam_agent-2_mean: 369.61
    cleaning_beam_agent-2_min: 143
    cleaning_beam_agent-3_max: 274
    cleaning_beam_agent-3_mean: 33.44
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 602
    cleaning_beam_agent-4_mean: 452.53
    cleaning_beam_agent-4_min: 305
    cleaning_beam_agent-5_max: 135
    cleaning_beam_agent-5_mean: 37.66
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-23-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1075.9999999999989
  episode_reward_mean: 916.1999999999853
  episode_reward_min: 404.00000000001296
  episodes_this_iter: 96
  episodes_total: 25248
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12976.848
    learner:
      agent-0:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0405876636505127
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014279800234362483
        model: {}
        policy_loss: -0.002941626589745283
        total_loss: -0.002941267564892769
        vf_explained_var: 0.07866033911705017
        vf_loss: 18.317968368530273
      agent-1:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.093330979347229
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018572090193629265
        model: {}
        policy_loss: -0.004204099997878075
        total_loss: -0.003972042817622423
        vf_explained_var: -0.05126899480819702
        vf_loss: 21.56322479248047
      agent-2:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0881494283676147
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018071472877636552
        model: {}
        policy_loss: -0.0037641297094523907
        total_loss: -0.0036347354762256145
        vf_explained_var: -0.018811896443367004
        vf_loss: 20.44536781311035
      agent-3:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48188936710357666
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011533501092344522
        model: {}
        policy_loss: -0.0025972763542085886
        total_loss: -0.0016056583262979984
        vf_explained_var: 0.0659385472536087
        vf_loss: 18.39739990234375
      agent-4:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.92154860496521
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019136180635541677
        model: {}
        policy_loss: -0.0038951903115957975
        total_loss: -0.0035838752519339323
        vf_explained_var: 0.027587100863456726
        vf_loss: 19.332426071166992
      agent-5:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7475448250770569
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015358460368588567
        model: {}
        policy_loss: -0.0035049193538725376
        total_loss: -0.003006676211953163
        vf_explained_var: 0.08824457228183746
        vf_loss: 18.139240264892578
    load_time_ms: 21992.28
    num_steps_sampled: 25248000
    num_steps_trained: 25248000
    sample_time_ms: 129262.51
    update_time_ms: 100.19
  iterations_since_restore: 183
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.73132530120482
    ram_util_percent: 14.99116465863454
  pid: 14340
  policy_reward_max:
    agent-0: 179.33333333333334
    agent-1: 179.33333333333334
    agent-2: 179.33333333333334
    agent-3: 179.33333333333334
    agent-4: 179.33333333333334
    agent-5: 179.33333333333334
  policy_reward_mean:
    agent-0: 152.69999999999993
    agent-1: 152.69999999999993
    agent-2: 152.69999999999993
    agent-3: 152.69999999999993
    agent-4: 152.69999999999993
    agent-5: 152.69999999999993
  policy_reward_min:
    agent-0: 67.33333333333299
    agent-1: 67.33333333333299
    agent-2: 67.33333333333299
    agent-3: 67.33333333333299
    agent-4: 67.33333333333299
    agent-5: 67.33333333333299
  sampler_perf:
    mean_env_wait_ms: 30.775322740467747
    mean_inference_ms: 14.583322835929666
    mean_processing_ms: 65.72028636733152
  time_since_restore: 30227.08083128929
  time_this_iter_s: 174.71595430374146
  time_total_s: 42777.89770269394
  timestamp: 1637065427
  timesteps_since_restore: 17568000
  timesteps_this_iter: 96000
  timesteps_total: 25248000
  training_iteration: 263
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    263 |          42777.9 | 25248000 |    916.2 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 86
    apples_agent-0_mean: 4.5
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 25.81
    apples_agent-1_min: 0
    apples_agent-2_max: 212
    apples_agent-2_mean: 12.17
    apples_agent-2_min: 0
    apples_agent-3_max: 265
    apples_agent-3_mean: 104.34
    apples_agent-3_min: 20
    apples_agent-4_max: 55
    apples_agent-4_mean: 1.72
    apples_agent-4_min: 0
    apples_agent-5_max: 256
    apples_agent-5_mean: 105.49
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 610
    cleaning_beam_agent-0_mean: 421.47
    cleaning_beam_agent-0_min: 250
    cleaning_beam_agent-1_max: 513
    cleaning_beam_agent-1_mean: 282.28
    cleaning_beam_agent-1_min: 161
    cleaning_beam_agent-2_max: 580
    cleaning_beam_agent-2_mean: 377.12
    cleaning_beam_agent-2_min: 110
    cleaning_beam_agent-3_max: 179
    cleaning_beam_agent-3_mean: 36.72
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 541
    cleaning_beam_agent-4_mean: 440.65
    cleaning_beam_agent-4_min: 203
    cleaning_beam_agent-5_max: 222
    cleaning_beam_agent-5_mean: 45.61
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-26-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1119.9999999999943
  episode_reward_mean: 890.239999999987
  episode_reward_min: 214.9999999999965
  episodes_this_iter: 96
  episodes_total: 25344
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12972.395
    learner:
      agent-0:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0500887632369995
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010802475735545158
        model: {}
        policy_loss: -0.002905827946960926
        total_loss: -0.0026676952838897705
        vf_explained_var: 0.022111475467681885
        vf_loss: 20.862873077392578
      agent-1:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0968754291534424
        entropy_coeff: 0.0017600000137463212
        kl: 0.002347409725189209
        model: {}
        policy_loss: -0.0040343152359128
        total_loss: -0.003731470089405775
        vf_explained_var: -0.033112525939941406
        vf_loss: 22.333450317382812
      agent-2:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0713891983032227
        entropy_coeff: 0.0017600000137463212
        kl: 0.001764846732839942
        model: {}
        policy_loss: -0.0037636240012943745
        total_loss: -0.00351479509845376
        vf_explained_var: 0.004889696836471558
        vf_loss: 21.344749450683594
      agent-3:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49494075775146484
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008837546920403838
        model: {}
        policy_loss: -0.0026345602236688137
        total_loss: -0.0016843044431880116
        vf_explained_var: 0.1460154950618744
        vf_loss: 18.213539123535156
      agent-4:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9186496138572693
        entropy_coeff: 0.0017600000137463212
        kl: 0.001723518711514771
        model: {}
        policy_loss: -0.00389463035389781
        total_loss: -0.0035392725840210915
        vf_explained_var: 0.07366660237312317
        vf_loss: 19.721765518188477
      agent-5:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7628964185714722
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011951009510084987
        model: {}
        policy_loss: -0.0032665368635207415
        total_loss: -0.0027393349446356297
        vf_explained_var: 0.1286630630493164
        vf_loss: 18.69899559020996
    load_time_ms: 20207.08
    num_steps_sampled: 25344000
    num_steps_trained: 25344000
    sample_time_ms: 129297.372
    update_time_ms: 100.707
  iterations_since_restore: 184
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.49372197309417
    ram_util_percent: 14.760089686098656
  pid: 14340
  policy_reward_max:
    agent-0: 186.6666666666663
    agent-1: 186.6666666666663
    agent-2: 186.6666666666663
    agent-3: 186.6666666666663
    agent-4: 186.6666666666663
    agent-5: 186.6666666666663
  policy_reward_mean:
    agent-0: 148.37333333333333
    agent-1: 148.37333333333333
    agent-2: 148.37333333333333
    agent-3: 148.37333333333333
    agent-4: 148.37333333333333
    agent-5: 148.37333333333333
  policy_reward_min:
    agent-0: 35.833333333333364
    agent-1: 35.833333333333364
    agent-2: 35.833333333333364
    agent-3: 35.833333333333364
    agent-4: 35.833333333333364
    agent-5: 35.833333333333364
  sampler_perf:
    mean_env_wait_ms: 30.78095620755364
    mean_inference_ms: 14.58295363764861
    mean_processing_ms: 65.7180733725545
  time_since_restore: 30383.447033405304
  time_this_iter_s: 156.36620211601257
  time_total_s: 42934.26390480995
  timestamp: 1637065583
  timesteps_since_restore: 17664000
  timesteps_this_iter: 96000
  timesteps_total: 25344000
  training_iteration: 264
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    264 |          42934.3 | 25344000 |   890.24 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 65
    apples_agent-0_mean: 3.35
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 20.7
    apples_agent-1_min: 0
    apples_agent-2_max: 187
    apples_agent-2_mean: 7.95
    apples_agent-2_min: 0
    apples_agent-3_max: 189
    apples_agent-3_mean: 101.7
    apples_agent-3_min: 27
    apples_agent-4_max: 96
    apples_agent-4_mean: 4.52
    apples_agent-4_min: 0
    apples_agent-5_max: 256
    apples_agent-5_mean: 105.34
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 594
    cleaning_beam_agent-0_mean: 402.36
    cleaning_beam_agent-0_min: 204
    cleaning_beam_agent-1_max: 487
    cleaning_beam_agent-1_mean: 306.27
    cleaning_beam_agent-1_min: 192
    cleaning_beam_agent-2_max: 536
    cleaning_beam_agent-2_mean: 363.24
    cleaning_beam_agent-2_min: 185
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 33.57
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 616
    cleaning_beam_agent-4_mean: 432.89
    cleaning_beam_agent-4_min: 192
    cleaning_beam_agent-5_max: 486
    cleaning_beam_agent-5_mean: 41.48
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-29-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1100.999999999997
  episode_reward_mean: 879.789999999987
  episode_reward_min: 293.99999999999955
  episodes_this_iter: 96
  episodes_total: 25440
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12966.559
    learner:
      agent-0:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0469105243682861
        entropy_coeff: 0.0017600000137463212
        kl: 0.001337221940048039
        model: {}
        policy_loss: -0.0029618246480822563
        total_loss: -0.0026727262884378433
        vf_explained_var: 0.043747738003730774
        vf_loss: 21.31662368774414
      agent-1:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0881903171539307
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016052284045144916
        model: {}
        policy_loss: -0.0039881872944533825
        total_loss: -0.0035246401093900204
        vf_explained_var: -0.061544984579086304
        vf_loss: 23.787609100341797
      agent-2:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0855740308761597
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015862154541537166
        model: {}
        policy_loss: -0.0036521833389997482
        total_loss: -0.0033932561054825783
        vf_explained_var: 0.02925574779510498
        vf_loss: 21.69533920288086
      agent-3:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5064233541488647
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011890336172655225
        model: {}
        policy_loss: -0.0027309886645525694
        total_loss: -0.0017737848684191704
        vf_explained_var: 0.17093190550804138
        vf_loss: 18.485057830810547
      agent-4:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9301180839538574
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015857582911849022
        model: {}
        policy_loss: -0.004158266820013523
        total_loss: -0.0037254630587995052
        vf_explained_var: 0.07182121276855469
        vf_loss: 20.698137283325195
      agent-5:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7464689612388611
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021181022748351097
        model: {}
        policy_loss: -0.0037073208950459957
        total_loss: -0.003135373815894127
        vf_explained_var: 0.15480785071849823
        vf_loss: 18.857316970825195
    load_time_ms: 20202.274
    num_steps_sampled: 25440000
    num_steps_trained: 25440000
    sample_time_ms: 129164.449
    update_time_ms: 129.159
  iterations_since_restore: 185
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.687946428571426
    ram_util_percent: 14.696428571428571
  pid: 14340
  policy_reward_max:
    agent-0: 183.49999999999977
    agent-1: 183.49999999999977
    agent-2: 183.49999999999977
    agent-3: 183.49999999999977
    agent-4: 183.49999999999977
    agent-5: 183.49999999999977
  policy_reward_mean:
    agent-0: 146.63166666666672
    agent-1: 146.63166666666672
    agent-2: 146.63166666666672
    agent-3: 146.63166666666672
    agent-4: 146.63166666666672
    agent-5: 146.63166666666672
  policy_reward_min:
    agent-0: 48.99999999999987
    agent-1: 48.99999999999987
    agent-2: 48.99999999999987
    agent-3: 48.99999999999987
    agent-4: 48.99999999999987
    agent-5: 48.99999999999987
  sampler_perf:
    mean_env_wait_ms: 30.786234583081974
    mean_inference_ms: 14.583034346263712
    mean_processing_ms: 65.718235924374
  time_since_restore: 30540.674058675766
  time_this_iter_s: 157.22702527046204
  time_total_s: 43091.490930080414
  timestamp: 1637065740
  timesteps_since_restore: 17760000
  timesteps_this_iter: 96000
  timesteps_total: 25440000
  training_iteration: 265
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    265 |          43091.5 | 25440000 |   879.79 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 3.02
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 24.68
    apples_agent-1_min: 0
    apples_agent-2_max: 190
    apples_agent-2_mean: 8.53
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 99.34
    apples_agent-3_min: 48
    apples_agent-4_max: 78
    apples_agent-4_mean: 3.54
    apples_agent-4_min: 0
    apples_agent-5_max: 211
    apples_agent-5_mean: 108.51
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 646
    cleaning_beam_agent-0_mean: 424.22
    cleaning_beam_agent-0_min: 248
    cleaning_beam_agent-1_max: 481
    cleaning_beam_agent-1_mean: 295.83
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 552
    cleaning_beam_agent-2_mean: 389.12
    cleaning_beam_agent-2_min: 128
    cleaning_beam_agent-3_max: 182
    cleaning_beam_agent-3_mean: 35.55
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 555
    cleaning_beam_agent-4_mean: 421.73
    cleaning_beam_agent-4_min: 229
    cleaning_beam_agent-5_max: 472
    cleaning_beam_agent-5_mean: 48.96
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-31-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1059.9999999999814
  episode_reward_mean: 881.4399999999862
  episode_reward_min: 340.0000000000017
  episodes_this_iter: 96
  episodes_total: 25536
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12911.279
    learner:
      agent-0:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.039839267730713
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013776166597381234
        model: {}
        policy_loss: -0.003058399772271514
        total_loss: -0.0027230498380959034
        vf_explained_var: 0.0216217041015625
        vf_loss: 21.654678344726562
      agent-1:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0902081727981567
        entropy_coeff: 0.0017600000137463212
        kl: 0.001576366601511836
        model: {}
        policy_loss: -0.004225768614560366
        total_loss: -0.003851934801787138
        vf_explained_var: -0.02724429965019226
        vf_loss: 22.926015853881836
      agent-2:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0800144672393799
        entropy_coeff: 0.0017600000137463212
        kl: 0.002068104688078165
        model: {}
        policy_loss: -0.003938315901905298
        total_loss: -0.003605801146477461
        vf_explained_var: -0.0066324323415756226
        vf_loss: 22.333410263061523
      agent-3:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5118435621261597
        entropy_coeff: 0.0017600000137463212
        kl: 0.001697590108960867
        model: {}
        policy_loss: -0.003101808950304985
        total_loss: -0.0020814118906855583
        vf_explained_var: 0.13455869257450104
        vf_loss: 19.21245574951172
      agent-4:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9399178624153137
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014187651686370373
        model: {}
        policy_loss: -0.0037414077669382095
        total_loss: -0.0034307073801755905
        vf_explained_var: 0.11388099193572998
        vf_loss: 19.64960479736328
      agent-5:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7508050799369812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007134738261811435
        model: {}
        policy_loss: -0.0032487399876117706
        total_loss: -0.0027193897403776646
        vf_explained_var: 0.1653144508600235
        vf_loss: 18.507678985595703
    load_time_ms: 20312.358
    num_steps_sampled: 25536000
    num_steps_trained: 25536000
    sample_time_ms: 129129.951
    update_time_ms: 131.464
  iterations_since_restore: 186
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.593805309734517
    ram_util_percent: 14.7783185840708
  pid: 14340
  policy_reward_max:
    agent-0: 176.66666666666652
    agent-1: 176.66666666666652
    agent-2: 176.66666666666652
    agent-3: 176.66666666666652
    agent-4: 176.66666666666652
    agent-5: 176.66666666666652
  policy_reward_mean:
    agent-0: 146.9066666666667
    agent-1: 146.9066666666667
    agent-2: 146.9066666666667
    agent-3: 146.9066666666667
    agent-4: 146.9066666666667
    agent-5: 146.9066666666667
  policy_reward_min:
    agent-0: 56.66666666666651
    agent-1: 56.66666666666651
    agent-2: 56.66666666666651
    agent-3: 56.66666666666651
    agent-4: 56.66666666666651
    agent-5: 56.66666666666651
  sampler_perf:
    mean_env_wait_ms: 30.7918249521194
    mean_inference_ms: 14.582971758943714
    mean_processing_ms: 65.71734799316295
  time_since_restore: 30698.708071231842
  time_this_iter_s: 158.03401255607605
  time_total_s: 43249.52494263649
  timestamp: 1637065899
  timesteps_since_restore: 17856000
  timesteps_this_iter: 96000
  timesteps_total: 25536000
  training_iteration: 266
  trial_id: '00000'
  
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    266 |          43249.5 | 25536000 |   881.44 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 3.0
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 28.34
    apples_agent-1_min: 0
    apples_agent-2_max: 96
    apples_agent-2_mean: 5.35
    apples_agent-2_min: 0
    apples_agent-3_max: 231
    apples_agent-3_mean: 113.57
    apples_agent-3_min: 52
    apples_agent-4_max: 78
    apples_agent-4_mean: 1.69
    apples_agent-4_min: 0
    apples_agent-5_max: 273
    apples_agent-5_mean: 111.98
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 582
    cleaning_beam_agent-0_mean: 432.03
    cleaning_beam_agent-0_min: 260
    cleaning_beam_agent-1_max: 465
    cleaning_beam_agent-1_mean: 287.48
    cleaning_beam_agent-1_min: 184
    cleaning_beam_agent-2_max: 585
    cleaning_beam_agent-2_mean: 384.74
    cleaning_beam_agent-2_min: 155
    cleaning_beam_agent-3_max: 99
    cleaning_beam_agent-3_mean: 29.65
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 535
    cleaning_beam_agent-4_mean: 425.48
    cleaning_beam_agent-4_min: 229
    cleaning_beam_agent-5_max: 196
    cleaning_beam_agent-5_mean: 34.16
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-34-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1066.999999999989
  episode_reward_mean: 907.6099999999849
  episode_reward_min: 443.0000000000059
  episodes_this_iter: 96
  episodes_total: 25632
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12909.631
    learner:
      agent-0:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0234216451644897
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014091653283685446
        model: {}
        policy_loss: -0.0029692952521145344
        total_loss: -0.0028974160086363554
        vf_explained_var: 0.0009429901838302612
        vf_loss: 18.731016159057617
      agent-1:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0894935131072998
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012732223840430379
        model: {}
        policy_loss: -0.0038705458864569664
        total_loss: -0.0036767390556633472
        vf_explained_var: -0.07901287078857422
        vf_loss: 21.113155364990234
      agent-2:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0734175443649292
        entropy_coeff: 0.0017600000137463212
        kl: 0.001360595808364451
        model: {}
        policy_loss: -0.0034860495943576097
        total_loss: -0.003467230824753642
        vf_explained_var: 0.0032396018505096436
        vf_loss: 19.080345153808594
      agent-3:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4711158871650696
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012247285339981318
        model: {}
        policy_loss: -0.0022788424976170063
        total_loss: -0.00131539860740304
        vf_explained_var: 0.04528391361236572
        vf_loss: 17.926088333129883
      agent-4:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.931890070438385
        entropy_coeff: 0.0017600000137463212
        kl: 0.001479198457673192
        model: {}
        policy_loss: -0.003782525658607483
        total_loss: -0.0036578020080924034
        vf_explained_var: 0.06433567404747009
        vf_loss: 17.648488998413086
      agent-5:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7274105548858643
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009708718280307949
        model: {}
        policy_loss: -0.0030569005757570267
        total_loss: -0.002617273014038801
        vf_explained_var: 0.08144713938236237
        vf_loss: 17.19870376586914
    load_time_ms: 20288.973
    num_steps_sampled: 25632000
    num_steps_trained: 25632000
    sample_time_ms: 128951.67
    update_time_ms: 156.357
  iterations_since_restore: 187
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.744758064516127
    ram_util_percent: 14.984677419354842
  pid: 14340
  policy_reward_max:
    agent-0: 177.83333333333292
    agent-1: 177.83333333333292
    agent-2: 177.83333333333292
    agent-3: 177.83333333333292
    agent-4: 177.83333333333292
    agent-5: 177.83333333333292
  policy_reward_mean:
    agent-0: 151.26833333333332
    agent-1: 151.26833333333332
    agent-2: 151.26833333333332
    agent-3: 151.26833333333332
    agent-4: 151.26833333333332
    agent-5: 151.26833333333332
  policy_reward_min:
    agent-0: 73.83333333333336
    agent-1: 73.83333333333336
    agent-2: 73.83333333333336
    agent-3: 73.83333333333336
    agent-4: 73.83333333333336
    agent-5: 73.83333333333336
  sampler_perf:
    mean_env_wait_ms: 30.797619904258067
    mean_inference_ms: 14.582746700813734
    mean_processing_ms: 65.71403486659767
  time_since_restore: 30873.14376592636
  time_this_iter_s: 174.43569469451904
  time_total_s: 43423.96063733101
  timestamp: 1637066074
  timesteps_since_restore: 17952000
  timesteps_this_iter: 96000
  timesteps_total: 25632000
  training_iteration: 267
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    267 |            43424 | 25632000 |   907.61 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 102
    apples_agent-0_mean: 2.66
    apples_agent-0_min: 0
    apples_agent-1_max: 116
    apples_agent-1_mean: 25.9
    apples_agent-1_min: 0
    apples_agent-2_max: 222
    apples_agent-2_mean: 5.89
    apples_agent-2_min: 0
    apples_agent-3_max: 154
    apples_agent-3_mean: 105.69
    apples_agent-3_min: 40
    apples_agent-4_max: 38
    apples_agent-4_mean: 1.69
    apples_agent-4_min: 0
    apples_agent-5_max: 174
    apples_agent-5_mean: 117.51
    apples_agent-5_min: 26
    cleaning_beam_agent-0_max: 586
    cleaning_beam_agent-0_mean: 436.84
    cleaning_beam_agent-0_min: 253
    cleaning_beam_agent-1_max: 502
    cleaning_beam_agent-1_mean: 280.69
    cleaning_beam_agent-1_min: 145
    cleaning_beam_agent-2_max: 526
    cleaning_beam_agent-2_mean: 382.75
    cleaning_beam_agent-2_min: 128
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 30.56
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 568
    cleaning_beam_agent-4_mean: 425.91
    cleaning_beam_agent-4_min: 273
    cleaning_beam_agent-5_max: 273
    cleaning_beam_agent-5_mean: 37.97
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-37-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1103.9999999999964
  episode_reward_mean: 909.1499999999843
  episode_reward_min: 285.0000000000004
  episodes_this_iter: 96
  episodes_total: 25728
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12911.886
    learner:
      agent-0:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0318411588668823
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009998464956879616
        model: {}
        policy_loss: -0.002627011388540268
        total_loss: -0.002508345525711775
        vf_explained_var: 0.07089751958847046
        vf_loss: 19.347091674804688
      agent-1:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1029951572418213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019513022853061557
        model: {}
        policy_loss: -0.004397333599627018
        total_loss: -0.0041517047211527824
        vf_explained_var: -0.01787559688091278
        vf_loss: 21.86903190612793
      agent-2:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0864248275756836
        entropy_coeff: 0.0017600000137463212
        kl: 0.001645921147428453
        model: {}
        policy_loss: -0.003600737312808633
        total_loss: -0.0034699812531471252
        vf_explained_var: 0.030678853392601013
        vf_loss: 20.428627014160156
      agent-3:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48002663254737854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008312498684972525
        model: {}
        policy_loss: -0.0022498550824820995
        total_loss: -0.0012388743925839663
        vf_explained_var: 0.11027605831623077
        vf_loss: 18.558238983154297
      agent-4:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9446059465408325
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019729603081941605
        model: {}
        policy_loss: -0.0038722690660506487
        total_loss: -0.003599368967115879
        vf_explained_var: 0.0809110552072525
        vf_loss: 19.354076385498047
      agent-5:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7476302981376648
        entropy_coeff: 0.0017600000137463212
        kl: 0.001424653921276331
        model: {}
        policy_loss: -0.0031317686662077904
        total_loss: -0.002634141594171524
        vf_explained_var: 0.13606296479701996
        vf_loss: 18.134584426879883
    load_time_ms: 20320.041
    num_steps_sampled: 25728000
    num_steps_trained: 25728000
    sample_time_ms: 128994.742
    update_time_ms: 155.67
  iterations_since_restore: 188
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.66504424778761
    ram_util_percent: 14.755752212389384
  pid: 14340
  policy_reward_max:
    agent-0: 183.99999999999991
    agent-1: 183.99999999999991
    agent-2: 183.99999999999991
    agent-3: 183.99999999999991
    agent-4: 183.99999999999991
    agent-5: 183.99999999999991
  policy_reward_mean:
    agent-0: 151.525
    agent-1: 151.525
    agent-2: 151.525
    agent-3: 151.525
    agent-4: 151.525
    agent-5: 151.525
  policy_reward_min:
    agent-0: 47.49999999999991
    agent-1: 47.49999999999991
    agent-2: 47.49999999999991
    agent-3: 47.49999999999991
    agent-4: 47.49999999999991
    agent-5: 47.49999999999991
  sampler_perf:
    mean_env_wait_ms: 30.803997704705807
    mean_inference_ms: 14.58279417366371
    mean_processing_ms: 65.71472074216393
  time_since_restore: 31031.672342061996
  time_this_iter_s: 158.52857613563538
  time_total_s: 43582.489213466644
  timestamp: 1637066232
  timesteps_since_restore: 18048000
  timesteps_this_iter: 96000
  timesteps_total: 25728000
  training_iteration: 268
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    268 |          43582.5 | 25728000 |   909.15 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 90
    apples_agent-0_mean: 1.74
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 26.46
    apples_agent-1_min: 0
    apples_agent-2_max: 455
    apples_agent-2_mean: 9.04
    apples_agent-2_min: 0
    apples_agent-3_max: 388
    apples_agent-3_mean: 111.39
    apples_agent-3_min: 52
    apples_agent-4_max: 82
    apples_agent-4_mean: 1.59
    apples_agent-4_min: 0
    apples_agent-5_max: 184
    apples_agent-5_mean: 114.13
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 620
    cleaning_beam_agent-0_mean: 433.88
    cleaning_beam_agent-0_min: 255
    cleaning_beam_agent-1_max: 524
    cleaning_beam_agent-1_mean: 282.56
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 588
    cleaning_beam_agent-2_mean: 373.7
    cleaning_beam_agent-2_min: 81
    cleaning_beam_agent-3_max: 248
    cleaning_beam_agent-3_mean: 33.14
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 522
    cleaning_beam_agent-4_mean: 416.01
    cleaning_beam_agent-4_min: 256
    cleaning_beam_agent-5_max: 150
    cleaning_beam_agent-5_mean: 32.01
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-40-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1103.9999999999964
  episode_reward_mean: 913.8899999999858
  episode_reward_min: 424.0000000000056
  episodes_this_iter: 96
  episodes_total: 25824
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12911.711
    learner:
      agent-0:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.024261474609375
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013097724877297878
        model: {}
        policy_loss: -0.0028690523467957973
        total_loss: -0.002798072062432766
        vf_explained_var: 0.052902743220329285
        vf_loss: 18.7368221282959
      agent-1:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0834856033325195
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014457216020673513
        model: {}
        policy_loss: -0.0039575775153934956
        total_loss: -0.0037465952336788177
        vf_explained_var: -0.026315733790397644
        vf_loss: 21.179183959960938
      agent-2:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0693601369857788
        entropy_coeff: 0.0017600000137463212
        kl: 0.002243699738755822
        model: {}
        policy_loss: -0.004110940732061863
        total_loss: -0.0040824837051332
        vf_explained_var: 0.061120226979255676
        vf_loss: 19.10530662536621
      agent-3:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47787073254585266
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011291474802419543
        model: {}
        policy_loss: -0.0024709971621632576
        total_loss: -0.0014855461195111275
        vf_explained_var: 0.08054649829864502
        vf_loss: 18.265012741088867
      agent-4:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9443739652633667
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019407228101044893
        model: {}
        policy_loss: -0.003907374106347561
        total_loss: -0.0036770235747098923
        vf_explained_var: 0.05436049401760101
        vf_loss: 18.924476623535156
      agent-5:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7317632436752319
        entropy_coeff: 0.0017600000137463212
        kl: 0.001484928885474801
        model: {}
        policy_loss: -0.003126705065369606
        total_loss: -0.002644650638103485
        vf_explained_var: 0.11032763123512268
        vf_loss: 17.699562072753906
    load_time_ms: 22107.064
    num_steps_sampled: 25824000
    num_steps_trained: 25824000
    sample_time_ms: 129132.084
    update_time_ms: 181.769
  iterations_since_restore: 189
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.711111111111112
    ram_util_percent: 14.951587301587304
  pid: 14340
  policy_reward_max:
    agent-0: 183.99999999999991
    agent-1: 183.99999999999991
    agent-2: 183.99999999999991
    agent-3: 183.99999999999991
    agent-4: 183.99999999999991
    agent-5: 183.99999999999991
  policy_reward_mean:
    agent-0: 152.31499999999997
    agent-1: 152.31499999999997
    agent-2: 152.31499999999997
    agent-3: 152.31499999999997
    agent-4: 152.31499999999997
    agent-5: 152.31499999999997
  policy_reward_min:
    agent-0: 70.66666666666661
    agent-1: 70.66666666666661
    agent-2: 70.66666666666661
    agent-3: 70.66666666666661
    agent-4: 70.66666666666661
    agent-5: 70.66666666666661
  sampler_perf:
    mean_env_wait_ms: 30.809178125196222
    mean_inference_ms: 14.582737883982587
    mean_processing_ms: 65.71394174627329
  time_since_restore: 31208.47600698471
  time_this_iter_s: 176.80366492271423
  time_total_s: 43759.29287838936
  timestamp: 1637066410
  timesteps_since_restore: 18144000
  timesteps_this_iter: 96000
  timesteps_total: 25824000
  training_iteration: 269
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    269 |          43759.3 | 25824000 |   913.89 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 90
    apples_agent-0_mean: 4.54
    apples_agent-0_min: 0
    apples_agent-1_max: 125
    apples_agent-1_mean: 22.85
    apples_agent-1_min: 0
    apples_agent-2_max: 249
    apples_agent-2_mean: 12.21
    apples_agent-2_min: 0
    apples_agent-3_max: 337
    apples_agent-3_mean: 111.44
    apples_agent-3_min: 41
    apples_agent-4_max: 75
    apples_agent-4_mean: 3.92
    apples_agent-4_min: 0
    apples_agent-5_max: 278
    apples_agent-5_mean: 105.45
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 639
    cleaning_beam_agent-0_mean: 421.45
    cleaning_beam_agent-0_min: 200
    cleaning_beam_agent-1_max: 469
    cleaning_beam_agent-1_mean: 275.62
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 619
    cleaning_beam_agent-2_mean: 373.31
    cleaning_beam_agent-2_min: 123
    cleaning_beam_agent-3_max: 187
    cleaning_beam_agent-3_mean: 33.99
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 505
    cleaning_beam_agent-4_mean: 397.38
    cleaning_beam_agent-4_min: 207
    cleaning_beam_agent-5_max: 191
    cleaning_beam_agent-5_mean: 43.32
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-42-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1105.9999999999927
  episode_reward_mean: 875.0799999999862
  episode_reward_min: 286.99999999999557
  episodes_this_iter: 96
  episodes_total: 25920
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12898.377
    learner:
      agent-0:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.039594054222107
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018086511408910155
        model: {}
        policy_loss: -0.003246805164963007
        total_loss: -0.0030096478294581175
        vf_explained_var: 0.0027132928371429443
        vf_loss: 20.66844367980957
      agent-1:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0975687503814697
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014749548863619566
        model: {}
        policy_loss: -0.004092452581971884
        total_loss: -0.0038239669520407915
        vf_explained_var: -0.053209781646728516
        vf_loss: 22.002059936523438
      agent-2:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.068589210510254
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016024296637624502
        model: {}
        policy_loss: -0.00353427417576313
        total_loss: -0.003237595781683922
        vf_explained_var: -0.042827099561691284
        vf_loss: 21.773969650268555
      agent-3:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5032528638839722
        entropy_coeff: 0.0017600000137463212
        kl: 0.001379149267449975
        model: {}
        policy_loss: -0.002561597153544426
        total_loss: -0.0016033891588449478
        vf_explained_var: 0.11393262445926666
        vf_loss: 18.439334869384766
      agent-4:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9744162559509277
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013472806895151734
        model: {}
        policy_loss: -0.003974675666540861
        total_loss: -0.0038141764234751463
        vf_explained_var: 0.09803418815135956
        vf_loss: 18.754724502563477
      agent-5:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7614274024963379
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011086284648627043
        model: {}
        policy_loss: -0.0034225755371153355
        total_loss: -0.00294140400364995
        vf_explained_var: 0.12274499237537384
        vf_loss: 18.212833404541016
    load_time_ms: 22005.37
    num_steps_sampled: 25920000
    num_steps_trained: 25920000
    sample_time_ms: 129213.769
    update_time_ms: 183.292
  iterations_since_restore: 190
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.73542600896861
    ram_util_percent: 14.650672645739911
  pid: 14340
  policy_reward_max:
    agent-0: 184.33333333333334
    agent-1: 184.33333333333334
    agent-2: 184.33333333333334
    agent-3: 184.33333333333334
    agent-4: 184.33333333333334
    agent-5: 184.33333333333334
  policy_reward_mean:
    agent-0: 145.8466666666667
    agent-1: 145.8466666666667
    agent-2: 145.8466666666667
    agent-3: 145.8466666666667
    agent-4: 145.8466666666667
    agent-5: 145.8466666666667
  policy_reward_min:
    agent-0: 47.833333333333265
    agent-1: 47.833333333333265
    agent-2: 47.833333333333265
    agent-3: 47.833333333333265
    agent-4: 47.833333333333265
    agent-5: 47.833333333333265
  sampler_perf:
    mean_env_wait_ms: 30.813762636819362
    mean_inference_ms: 14.58298808861612
    mean_processing_ms: 65.71517127567449
  time_since_restore: 31364.906299114227
  time_this_iter_s: 156.4302921295166
  time_total_s: 43915.723170518875
  timestamp: 1637066566
  timesteps_since_restore: 18240000
  timesteps_this_iter: 96000
  timesteps_total: 25920000
  training_iteration: 270
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    270 |          43915.7 | 25920000 |   875.08 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 3.51
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 27.19
    apples_agent-1_min: 0
    apples_agent-2_max: 262
    apples_agent-2_mean: 6.12
    apples_agent-2_min: 0
    apples_agent-3_max: 336
    apples_agent-3_mean: 111.17
    apples_agent-3_min: 55
    apples_agent-4_max: 85
    apples_agent-4_mean: 3.1
    apples_agent-4_min: 0
    apples_agent-5_max: 240
    apples_agent-5_mean: 108.61
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 568
    cleaning_beam_agent-0_mean: 421.63
    cleaning_beam_agent-0_min: 220
    cleaning_beam_agent-1_max: 498
    cleaning_beam_agent-1_mean: 257.22
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 592
    cleaning_beam_agent-2_mean: 390.87
    cleaning_beam_agent-2_min: 215
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 33.75
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 518
    cleaning_beam_agent-4_mean: 395.71
    cleaning_beam_agent-4_min: 202
    cleaning_beam_agent-5_max: 444
    cleaning_beam_agent-5_mean: 49.15
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-45-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1070.9999999999688
  episode_reward_mean: 885.1999999999866
  episode_reward_min: 452.00000000000705
  episodes_this_iter: 96
  episodes_total: 26016
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12906.974
    learner:
      agent-0:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0279250144958496
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018203363288193941
        model: {}
        policy_loss: -0.0032001922372728586
        total_loss: -0.0029856576584279537
        vf_explained_var: 0.04018670320510864
        vf_loss: 20.236854553222656
      agent-1:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0966289043426514
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019994645845144987
        model: {}
        policy_loss: -0.004125539213418961
        total_loss: -0.0037864847108721733
        vf_explained_var: -0.068209707736969
        vf_loss: 22.691204071044922
      agent-2:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0801548957824707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015616738237440586
        model: {}
        policy_loss: -0.0037781712599098682
        total_loss: -0.0034837075509130955
        vf_explained_var: -0.03973010182380676
        vf_loss: 21.95532989501953
      agent-3:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5036940574645996
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011129798367619514
        model: {}
        policy_loss: -0.002629172522574663
        total_loss: -0.001653667539358139
        vf_explained_var: 0.11821642518043518
        vf_loss: 18.620059967041016
      agent-4:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9645223021507263
        entropy_coeff: 0.0017600000137463212
        kl: 0.00214935839176178
        model: {}
        policy_loss: -0.004125205799937248
        total_loss: -0.0038293981924653053
        vf_explained_var: 0.058444857597351074
        vf_loss: 19.933704376220703
      agent-5:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7457352876663208
        entropy_coeff: 0.0017600000137463212
        kl: 0.001077121589332819
        model: {}
        policy_loss: -0.003133089281618595
        total_loss: -0.0025980938225984573
        vf_explained_var: 0.1251915991306305
        vf_loss: 18.474885940551758
    load_time_ms: 20148.596
    num_steps_sampled: 26016000
    num_steps_trained: 26016000
    sample_time_ms: 129388.115
    update_time_ms: 161.557
  iterations_since_restore: 191
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.685714285714287
    ram_util_percent: 14.680803571428575
  pid: 14340
  policy_reward_max:
    agent-0: 178.49999999999977
    agent-1: 178.49999999999977
    agent-2: 178.49999999999977
    agent-3: 178.49999999999977
    agent-4: 178.49999999999977
    agent-5: 178.49999999999977
  policy_reward_mean:
    agent-0: 147.5333333333334
    agent-1: 147.5333333333334
    agent-2: 147.5333333333334
    agent-3: 147.5333333333334
    agent-4: 147.5333333333334
    agent-5: 147.5333333333334
  policy_reward_min:
    agent-0: 75.33333333333346
    agent-1: 75.33333333333346
    agent-2: 75.33333333333346
    agent-3: 75.33333333333346
    agent-4: 75.33333333333346
    agent-5: 75.33333333333346
  sampler_perf:
    mean_env_wait_ms: 30.81846142298728
    mean_inference_ms: 14.582917357783359
    mean_processing_ms: 65.71430904318917
  time_since_restore: 31522.230608701706
  time_this_iter_s: 157.32430958747864
  time_total_s: 44073.047480106354
  timestamp: 1637066724
  timesteps_since_restore: 18336000
  timesteps_this_iter: 96000
  timesteps_total: 26016000
  training_iteration: 271
  trial_id: '00000'
  
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    271 |            44073 | 26016000 |    885.2 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 5.07
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 28.23
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 3.22
    apples_agent-2_min: 0
    apples_agent-3_max: 183
    apples_agent-3_mean: 105.26
    apples_agent-3_min: 41
    apples_agent-4_max: 102
    apples_agent-4_mean: 3.48
    apples_agent-4_min: 0
    apples_agent-5_max: 171
    apples_agent-5_mean: 111.83
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 639
    cleaning_beam_agent-0_mean: 429.25
    cleaning_beam_agent-0_min: 172
    cleaning_beam_agent-1_max: 504
    cleaning_beam_agent-1_mean: 254.88
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 600
    cleaning_beam_agent-2_mean: 394.67
    cleaning_beam_agent-2_min: 210
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 33.6
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 501
    cleaning_beam_agent-4_mean: 392.65
    cleaning_beam_agent-4_min: 163
    cleaning_beam_agent-5_max: 197
    cleaning_beam_agent-5_mean: 39.5
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-47-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1090.9999999999789
  episode_reward_mean: 900.0399999999836
  episode_reward_min: 416.0000000000066
  episodes_this_iter: 96
  episodes_total: 26112
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12890.697
    learner:
      agent-0:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9984143376350403
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016590170562267303
        model: {}
        policy_loss: -0.0028635268099606037
        total_loss: -0.002706790342926979
        vf_explained_var: 0.052160829305648804
        vf_loss: 19.139450073242188
      agent-1:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1197103261947632
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019323689630255103
        model: {}
        policy_loss: -0.004342436324805021
        total_loss: -0.004197951406240463
        vf_explained_var: -0.041368067264556885
        vf_loss: 21.151763916015625
      agent-2:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0763680934906006
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019367964705452323
        model: {}
        policy_loss: -0.0037326370365917683
        total_loss: -0.0035393843427300453
        vf_explained_var: -0.0315403938293457
        vf_loss: 20.876632690429688
      agent-3:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4888380765914917
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015339636011049151
        model: {}
        policy_loss: -0.0025039073079824448
        total_loss: -0.0015517591964453459
        vf_explained_var: 0.1013389378786087
        vf_loss: 18.125030517578125
      agent-4:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9583722352981567
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016747615300118923
        model: {}
        policy_loss: -0.00396113283932209
        total_loss: -0.0037400852888822556
        vf_explained_var: 0.05507254600524902
        vf_loss: 19.07782745361328
      agent-5:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.753143846988678
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019261876586824656
        model: {}
        policy_loss: -0.003901686053723097
        total_loss: -0.0035065023694187403
        vf_explained_var: 0.1473284512758255
        vf_loss: 17.207157135009766
    load_time_ms: 19981.631
    num_steps_sampled: 26112000
    num_steps_trained: 26112000
    sample_time_ms: 129252.596
    update_time_ms: 157.816
  iterations_since_restore: 192
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.822624434389137
    ram_util_percent: 14.666968325791858
  pid: 14340
  policy_reward_max:
    agent-0: 181.8333333333329
    agent-1: 181.8333333333329
    agent-2: 181.8333333333329
    agent-3: 181.8333333333329
    agent-4: 181.8333333333329
    agent-5: 181.8333333333329
  policy_reward_mean:
    agent-0: 150.00666666666663
    agent-1: 150.00666666666663
    agent-2: 150.00666666666663
    agent-3: 150.00666666666663
    agent-4: 150.00666666666663
    agent-5: 150.00666666666663
  policy_reward_min:
    agent-0: 69.33333333333314
    agent-1: 69.33333333333314
    agent-2: 69.33333333333314
    agent-3: 69.33333333333314
    agent-4: 69.33333333333314
    agent-5: 69.33333333333314
  sampler_perf:
    mean_env_wait_ms: 30.822941363121934
    mean_inference_ms: 14.582623785818832
    mean_processing_ms: 65.71428089938132
  time_since_restore: 31677.09650158882
  time_this_iter_s: 154.86589288711548
  time_total_s: 44227.91337299347
  timestamp: 1637066879
  timesteps_since_restore: 18432000
  timesteps_this_iter: 96000
  timesteps_total: 26112000
  training_iteration: 272
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    272 |          44227.9 | 26112000 |   900.04 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 2.48
    apples_agent-0_min: 0
    apples_agent-1_max: 126
    apples_agent-1_mean: 28.27
    apples_agent-1_min: 0
    apples_agent-2_max: 146
    apples_agent-2_mean: 4.48
    apples_agent-2_min: 0
    apples_agent-3_max: 197
    apples_agent-3_mean: 105.67
    apples_agent-3_min: 41
    apples_agent-4_max: 98
    apples_agent-4_mean: 1.52
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 118.72
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 624
    cleaning_beam_agent-0_mean: 442.83
    cleaning_beam_agent-0_min: 172
    cleaning_beam_agent-1_max: 530
    cleaning_beam_agent-1_mean: 271.25
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 557
    cleaning_beam_agent-2_mean: 401.66
    cleaning_beam_agent-2_min: 134
    cleaning_beam_agent-3_max: 64
    cleaning_beam_agent-3_mean: 28.24
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 483
    cleaning_beam_agent-4_mean: 396.75
    cleaning_beam_agent-4_min: 240
    cleaning_beam_agent-5_max: 119
    cleaning_beam_agent-5_mean: 32.58
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-50-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1100.9999999999968
  episode_reward_mean: 926.4999999999841
  episode_reward_min: 416.0000000000066
  episodes_this_iter: 96
  episodes_total: 26208
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12885.373
    learner:
      agent-0:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0242289304733276
        entropy_coeff: 0.0017600000137463212
        kl: 0.001379568362608552
        model: {}
        policy_loss: -0.002954591065645218
        total_loss: -0.002827489050105214
        vf_explained_var: -0.010806471109390259
        vf_loss: 19.29749298095703
      agent-1:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.100841999053955
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019301623106002808
        model: {}
        policy_loss: -0.004273775964975357
        total_loss: -0.004138984251767397
        vf_explained_var: -0.05035400390625
        vf_loss: 20.722755432128906
      agent-2:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0674059391021729
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019058363977819681
        model: {}
        policy_loss: -0.003657276974990964
        total_loss: -0.0035465878900140524
        vf_explained_var: -0.021252036094665527
        vf_loss: 19.89322280883789
      agent-3:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4554128646850586
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008935271762311459
        model: {}
        policy_loss: -0.002356120850890875
        total_loss: -0.001392751932144165
        vf_explained_var: 0.07519282400608063
        vf_loss: 17.64897918701172
      agent-4:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9556581974029541
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018779857782647014
        model: {}
        policy_loss: -0.0038948580622673035
        total_loss: -0.0037155216559767723
        vf_explained_var: 0.03741273283958435
        vf_loss: 18.612930297851562
      agent-5:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7272577881813049
        entropy_coeff: 0.0017600000137463212
        kl: 0.001597589929588139
        model: {}
        policy_loss: -0.0032992251217365265
        total_loss: -0.002868049778044224
        vf_explained_var: 0.11580480635166168
        vf_loss: 17.111507415771484
    load_time_ms: 18088.54
    num_steps_sampled: 26208000
    num_steps_trained: 26208000
    sample_time_ms: 129115.08
    update_time_ms: 124.112
  iterations_since_restore: 193
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.060730593607303
    ram_util_percent: 14.647488584474887
  pid: 14340
  policy_reward_max:
    agent-0: 183.49999999999991
    agent-1: 183.49999999999991
    agent-2: 183.49999999999991
    agent-3: 183.49999999999991
    agent-4: 183.49999999999991
    agent-5: 183.49999999999991
  policy_reward_mean:
    agent-0: 154.41666666666669
    agent-1: 154.41666666666669
    agent-2: 154.41666666666669
    agent-3: 154.41666666666669
    agent-4: 154.41666666666669
    agent-5: 154.41666666666669
  policy_reward_min:
    agent-0: 69.33333333333314
    agent-1: 69.33333333333314
    agent-2: 69.33333333333314
    agent-3: 69.33333333333314
    agent-4: 69.33333333333314
    agent-5: 69.33333333333314
  sampler_perf:
    mean_env_wait_ms: 30.82762655108437
    mean_inference_ms: 14.582387979581421
    mean_processing_ms: 65.71351497742342
  time_since_restore: 31831.141114234924
  time_this_iter_s: 154.0446126461029
  time_total_s: 44381.95798563957
  timestamp: 1637067033
  timesteps_since_restore: 18528000
  timesteps_this_iter: 96000
  timesteps_total: 26208000
  training_iteration: 273
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    273 |            44382 | 26208000 |    926.5 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 1.59
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 21.5
    apples_agent-1_min: 0
    apples_agent-2_max: 121
    apples_agent-2_mean: 3.73
    apples_agent-2_min: 0
    apples_agent-3_max: 184
    apples_agent-3_mean: 107.12
    apples_agent-3_min: 40
    apples_agent-4_max: 98
    apples_agent-4_mean: 2.15
    apples_agent-4_min: 0
    apples_agent-5_max: 194
    apples_agent-5_mean: 112.34
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 615
    cleaning_beam_agent-0_mean: 455.77
    cleaning_beam_agent-0_min: 199
    cleaning_beam_agent-1_max: 420
    cleaning_beam_agent-1_mean: 272.7
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 592
    cleaning_beam_agent-2_mean: 415.29
    cleaning_beam_agent-2_min: 244
    cleaning_beam_agent-3_max: 127
    cleaning_beam_agent-3_mean: 29.82
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 523
    cleaning_beam_agent-4_mean: 387.9
    cleaning_beam_agent-4_min: 229
    cleaning_beam_agent-5_max: 231
    cleaning_beam_agent-5_mean: 31.42
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-53-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1071.9999999999945
  episode_reward_mean: 928.0999999999837
  episode_reward_min: 187.99999999999855
  episodes_this_iter: 96
  episodes_total: 26304
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12902.728
    learner:
      agent-0:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.004750370979309
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020067745354026556
        model: {}
        policy_loss: -0.00309640821069479
        total_loss: -0.0028975121676921844
        vf_explained_var: 0.015094667673110962
        vf_loss: 19.67258071899414
      agent-1:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1005010604858398
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013922727666795254
        model: {}
        policy_loss: -0.004144667647778988
        total_loss: -0.003891546744853258
        vf_explained_var: -0.05169945955276489
        vf_loss: 21.90003204345703
      agent-2:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.059735655784607
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016060600755736232
        model: {}
        policy_loss: -0.003774337936192751
        total_loss: -0.0036882536951452494
        vf_explained_var: 0.0452815443277359
        vf_loss: 19.512176513671875
      agent-3:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4624212384223938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010341514134779572
        model: {}
        policy_loss: -0.0025636558420956135
        total_loss: -0.0015405579470098019
        vf_explained_var: 0.09009341895580292
        vf_loss: 18.369619369506836
      agent-4:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9601188898086548
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015274790348485112
        model: {}
        policy_loss: -0.004301727283746004
        total_loss: -0.004027971997857094
        vf_explained_var: 0.03928287327289581
        vf_loss: 19.635637283325195
      agent-5:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6917027831077576
        entropy_coeff: 0.0017600000137463212
        kl: 0.001258944277651608
        model: {}
        policy_loss: -0.0032760929316282272
        total_loss: -0.002713562920689583
        vf_explained_var: 0.1225225031375885
        vf_loss: 17.799291610717773
    load_time_ms: 17965.07
    num_steps_sampled: 26304000
    num_steps_trained: 26304000
    sample_time_ms: 129125.48
    update_time_ms: 158.329
  iterations_since_restore: 194
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.574324324324323
    ram_util_percent: 14.655855855855858
  pid: 14340
  policy_reward_max:
    agent-0: 178.66666666666663
    agent-1: 178.66666666666663
    agent-2: 178.66666666666663
    agent-3: 178.66666666666663
    agent-4: 178.66666666666663
    agent-5: 178.66666666666663
  policy_reward_mean:
    agent-0: 154.68333333333328
    agent-1: 154.68333333333328
    agent-2: 154.68333333333328
    agent-3: 154.68333333333328
    agent-4: 154.68333333333328
    agent-5: 154.68333333333328
  policy_reward_min:
    agent-0: 31.333333333333385
    agent-1: 31.333333333333385
    agent-2: 31.333333333333385
    agent-3: 31.333333333333385
    agent-4: 31.333333333333385
    agent-5: 31.333333333333385
  sampler_perf:
    mean_env_wait_ms: 30.833126157397093
    mean_inference_ms: 14.581911525106621
    mean_processing_ms: 65.71303886405194
  time_since_restore: 31986.907723903656
  time_this_iter_s: 155.7666096687317
  time_total_s: 44537.724595308304
  timestamp: 1637067189
  timesteps_since_restore: 18624000
  timesteps_this_iter: 96000
  timesteps_total: 26304000
  training_iteration: 274
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    274 |          44537.7 | 26304000 |    928.1 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 2.11
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 23.49
    apples_agent-1_min: 0
    apples_agent-2_max: 77
    apples_agent-2_mean: 5.19
    apples_agent-2_min: 0
    apples_agent-3_max: 181
    apples_agent-3_mean: 109.98
    apples_agent-3_min: 28
    apples_agent-4_max: 95
    apples_agent-4_mean: 1.03
    apples_agent-4_min: 0
    apples_agent-5_max: 216
    apples_agent-5_mean: 109.84
    apples_agent-5_min: 46
    cleaning_beam_agent-0_max: 622
    cleaning_beam_agent-0_mean: 448.25
    cleaning_beam_agent-0_min: 189
    cleaning_beam_agent-1_max: 449
    cleaning_beam_agent-1_mean: 279.21
    cleaning_beam_agent-1_min: 150
    cleaning_beam_agent-2_max: 583
    cleaning_beam_agent-2_mean: 385.16
    cleaning_beam_agent-2_min: 130
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 32.2
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 490
    cleaning_beam_agent-4_mean: 381.98
    cleaning_beam_agent-4_min: 290
    cleaning_beam_agent-5_max: 259
    cleaning_beam_agent-5_mean: 37.38
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-55-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1167.0000000000034
  episode_reward_mean: 926.2099999999845
  episode_reward_min: 415.0000000000059
  episodes_this_iter: 96
  episodes_total: 26400
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12897.876
    learner:
      agent-0:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0082743167877197
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014937296509742737
        model: {}
        policy_loss: -0.003328785765916109
        total_loss: -0.003170682117342949
        vf_explained_var: 0.04233464598655701
        vf_loss: 19.326702117919922
      agent-1:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0861353874206543
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013817891012877226
        model: {}
        policy_loss: -0.003879714757204056
        total_loss: -0.003557506948709488
        vf_explained_var: -0.07562527060508728
        vf_loss: 22.338069915771484
      agent-2:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.084839940071106
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014682591427117586
        model: {}
        policy_loss: -0.0037620472721755505
        total_loss: -0.003604346886277199
        vf_explained_var: -0.003821268677711487
        vf_loss: 20.67015838623047
      agent-3:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46822354197502136
        entropy_coeff: 0.0017600000137463212
        kl: 0.00101843208540231
        model: {}
        policy_loss: -0.002500406466424465
        total_loss: -0.001500820741057396
        vf_explained_var: 0.10950881242752075
        vf_loss: 18.236631393432617
      agent-4:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9594292640686035
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014924734132364392
        model: {}
        policy_loss: -0.003820044919848442
        total_loss: -0.0035691738594323397
        vf_explained_var: 0.052752405405044556
        vf_loss: 19.394657135009766
      agent-5:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7121918201446533
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009994979482144117
        model: {}
        policy_loss: -0.0030402736738324165
        total_loss: -0.002498477231711149
        vf_explained_var: 0.11975164711475372
        vf_loss: 17.952545166015625
    load_time_ms: 17808.327
    num_steps_sampled: 26400000
    num_steps_trained: 26400000
    sample_time_ms: 129134.763
    update_time_ms: 130.17
  iterations_since_restore: 195
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.76561085972851
    ram_util_percent: 14.656108597285073
  pid: 14340
  policy_reward_max:
    agent-0: 194.4999999999997
    agent-1: 194.4999999999997
    agent-2: 194.4999999999997
    agent-3: 194.4999999999997
    agent-4: 194.4999999999997
    agent-5: 194.4999999999997
  policy_reward_mean:
    agent-0: 154.36833333333328
    agent-1: 154.36833333333328
    agent-2: 154.36833333333328
    agent-3: 154.36833333333328
    agent-4: 154.36833333333328
    agent-5: 154.36833333333328
  policy_reward_min:
    agent-0: 69.16666666666644
    agent-1: 69.16666666666644
    agent-2: 69.16666666666644
    agent-3: 69.16666666666644
    agent-4: 69.16666666666644
    agent-5: 69.16666666666644
  sampler_perf:
    mean_env_wait_ms: 30.837625359546006
    mean_inference_ms: 14.581365433391358
    mean_processing_ms: 65.71148251922943
  time_since_restore: 32141.944923639297
  time_this_iter_s: 155.03719973564148
  time_total_s: 44692.761795043945
  timestamp: 1637067344
  timesteps_since_restore: 18720000
  timesteps_this_iter: 96000
  timesteps_total: 26400000
  training_iteration: 275
  trial_id: '00000'
  
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    275 |          44692.8 | 26400000 |   926.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 2.64
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 27.7
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 4.36
    apples_agent-2_min: 0
    apples_agent-3_max: 181
    apples_agent-3_mean: 106.87
    apples_agent-3_min: 34
    apples_agent-4_max: 67
    apples_agent-4_mean: 1.93
    apples_agent-4_min: 0
    apples_agent-5_max: 195
    apples_agent-5_mean: 110.52
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 608
    cleaning_beam_agent-0_mean: 438.51
    cleaning_beam_agent-0_min: 214
    cleaning_beam_agent-1_max: 499
    cleaning_beam_agent-1_mean: 278.4
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 541
    cleaning_beam_agent-2_mean: 386.17
    cleaning_beam_agent-2_min: 185
    cleaning_beam_agent-3_max: 125
    cleaning_beam_agent-3_mean: 30.39
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 517
    cleaning_beam_agent-4_mean: 384.62
    cleaning_beam_agent-4_min: 192
    cleaning_beam_agent-5_max: 166
    cleaning_beam_agent-5_mean: 29.16
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-58-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1097.9999999999948
  episode_reward_mean: 930.6599999999846
  episode_reward_min: 309.9999999999996
  episodes_this_iter: 96
  episodes_total: 26496
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12909.173
    learner:
      agent-0:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.005293369293213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014138728147372603
        model: {}
        policy_loss: -0.0026810606941580772
        total_loss: -0.0024246936663985252
        vf_explained_var: 0.01608218252658844
        vf_loss: 20.25681495666504
      agent-1:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0852179527282715
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013995462795719504
        model: {}
        policy_loss: -0.003927380777895451
        total_loss: -0.003586115315556526
        vf_explained_var: -0.049742236733436584
        vf_loss: 22.512500762939453
      agent-2:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0893542766571045
        entropy_coeff: 0.0017600000137463212
        kl: 0.001767219277098775
        model: {}
        policy_loss: -0.003906929865479469
        total_loss: -0.0036613130941987038
        vf_explained_var: -0.022467762231826782
        vf_loss: 21.628803253173828
      agent-3:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47141027450561523
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011609503999352455
        model: {}
        policy_loss: -0.002578295301645994
        total_loss: -0.001520340098068118
        vf_explained_var: 0.1001201719045639
        vf_loss: 18.876361846923828
      agent-4:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9616551399230957
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023177179973572493
        model: {}
        policy_loss: -0.004433133639395237
        total_loss: -0.004172750748693943
        vf_explained_var: 0.07407785952091217
        vf_loss: 19.52896499633789
      agent-5:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.692834198474884
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016465613152831793
        model: {}
        policy_loss: -0.003177463775500655
        total_loss: -0.002586559858173132
        vf_explained_var: 0.13057957589626312
        vf_loss: 18.10292625427246
    load_time_ms: 17594.973
    num_steps_sampled: 26496000
    num_steps_trained: 26496000
    sample_time_ms: 129016.804
    update_time_ms: 114.49
  iterations_since_restore: 196
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.929545454545455
    ram_util_percent: 14.675454545454548
  pid: 14340
  policy_reward_max:
    agent-0: 182.99999999999915
    agent-1: 182.99999999999915
    agent-2: 182.99999999999915
    agent-3: 182.99999999999915
    agent-4: 182.99999999999915
    agent-5: 182.99999999999915
  policy_reward_mean:
    agent-0: 155.10999999999996
    agent-1: 155.10999999999996
    agent-2: 155.10999999999996
    agent-3: 155.10999999999996
    agent-4: 155.10999999999996
    agent-5: 155.10999999999996
  policy_reward_min:
    agent-0: 51.66666666666656
    agent-1: 51.66666666666656
    agent-2: 51.66666666666656
    agent-3: 51.66666666666656
    agent-4: 51.66666666666656
    agent-5: 51.66666666666656
  sampler_perf:
    mean_env_wait_ms: 30.84179132377008
    mean_inference_ms: 14.581110129570074
    mean_processing_ms: 65.71028549100595
  time_since_restore: 32296.39971590042
  time_this_iter_s: 154.45479226112366
  time_total_s: 44847.21658730507
  timestamp: 1637067498
  timesteps_since_restore: 18816000
  timesteps_this_iter: 96000
  timesteps_total: 26496000
  training_iteration: 276
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    276 |          44847.2 | 26496000 |   930.66 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 3.8
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 26.89
    apples_agent-1_min: 0
    apples_agent-2_max: 118
    apples_agent-2_mean: 5.47
    apples_agent-2_min: 0
    apples_agent-3_max: 350
    apples_agent-3_mean: 110.51
    apples_agent-3_min: 23
    apples_agent-4_max: 67
    apples_agent-4_mean: 3.75
    apples_agent-4_min: 0
    apples_agent-5_max: 370
    apples_agent-5_mean: 108.0
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 615
    cleaning_beam_agent-0_mean: 435.78
    cleaning_beam_agent-0_min: 248
    cleaning_beam_agent-1_max: 511
    cleaning_beam_agent-1_mean: 276.38
    cleaning_beam_agent-1_min: 145
    cleaning_beam_agent-2_max: 547
    cleaning_beam_agent-2_mean: 385.62
    cleaning_beam_agent-2_min: 130
    cleaning_beam_agent-3_max: 113
    cleaning_beam_agent-3_mean: 30.69
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 487
    cleaning_beam_agent-4_mean: 364.84
    cleaning_beam_agent-4_min: 192
    cleaning_beam_agent-5_max: 330
    cleaning_beam_agent-5_mean: 38.7
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-01-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1103.9999999999839
  episode_reward_mean: 906.629999999987
  episode_reward_min: 257.9999999999963
  episodes_this_iter: 96
  episodes_total: 26592
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12914.898
    learner:
      agent-0:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.003495454788208
        entropy_coeff: 0.0017600000137463212
        kl: 0.001854183617979288
        model: {}
        policy_loss: -0.0028965678066015244
        total_loss: -0.0024198840837925673
        vf_explained_var: 0.014833390712738037
        vf_loss: 22.428340911865234
      agent-1:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.093222975730896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015362246194854379
        model: {}
        policy_loss: -0.003985616844147444
        total_loss: -0.0035010152496397495
        vf_explained_var: -0.04498210549354553
        vf_loss: 24.086727142333984
      agent-2:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.088977575302124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015766015276312828
        model: {}
        policy_loss: -0.0039052478969097137
        total_loss: -0.0034790216013789177
        vf_explained_var: -0.014494553208351135
        vf_loss: 23.42829132080078
      agent-3:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4763633608818054
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009922680910676718
        model: {}
        policy_loss: -0.0026710652746260166
        total_loss: -0.0015581808984279633
        vf_explained_var: 0.15064871311187744
        vf_loss: 19.512840270996094
      agent-4:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9722474217414856
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013650100445374846
        model: {}
        policy_loss: -0.0034198977518826723
        total_loss: -0.0030586642678827047
        vf_explained_var: 0.10110925137996674
        vf_loss: 20.723867416381836
      agent-5:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7142215967178345
        entropy_coeff: 0.0017600000137463212
        kl: 0.001274174079298973
        model: {}
        policy_loss: -0.002964343875646591
        total_loss: -0.0022413276601582766
        vf_explained_var: 0.13338957726955414
        vf_loss: 19.800445556640625
    load_time_ms: 16855.382
    num_steps_sampled: 26592000
    num_steps_trained: 26592000
    sample_time_ms: 129111.911
    update_time_ms: 84.59
  iterations_since_restore: 197
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.633891213389123
    ram_util_percent: 14.843933054393307
  pid: 14340
  policy_reward_max:
    agent-0: 183.99999999999983
    agent-1: 183.99999999999983
    agent-2: 183.99999999999983
    agent-3: 183.99999999999983
    agent-4: 183.99999999999983
    agent-5: 183.99999999999983
  policy_reward_mean:
    agent-0: 151.10499999999993
    agent-1: 151.10499999999993
    agent-2: 151.10499999999993
    agent-3: 151.10499999999993
    agent-4: 151.10499999999993
    agent-5: 151.10499999999993
  policy_reward_min:
    agent-0: 42.999999999999964
    agent-1: 42.999999999999964
    agent-2: 42.999999999999964
    agent-3: 42.999999999999964
    agent-4: 42.999999999999964
    agent-5: 42.999999999999964
  sampler_perf:
    mean_env_wait_ms: 30.845868586529797
    mean_inference_ms: 14.586081780589558
    mean_processing_ms: 65.71640798048334
  time_since_restore: 32464.11080622673
  time_this_iter_s: 167.7110903263092
  time_total_s: 45014.92767763138
  timestamp: 1637067666
  timesteps_since_restore: 18912000
  timesteps_this_iter: 96000
  timesteps_total: 26592000
  training_iteration: 277
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    277 |          45014.9 | 26592000 |   906.63 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 102
    apples_agent-0_mean: 4.89
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 22.67
    apples_agent-1_min: 0
    apples_agent-2_max: 78
    apples_agent-2_mean: 3.73
    apples_agent-2_min: 0
    apples_agent-3_max: 300
    apples_agent-3_mean: 111.54
    apples_agent-3_min: 48
    apples_agent-4_max: 89
    apples_agent-4_mean: 4.52
    apples_agent-4_min: 0
    apples_agent-5_max: 253
    apples_agent-5_mean: 109.19
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 557
    cleaning_beam_agent-0_mean: 412.84
    cleaning_beam_agent-0_min: 180
    cleaning_beam_agent-1_max: 467
    cleaning_beam_agent-1_mean: 281.7
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 574
    cleaning_beam_agent-2_mean: 386.61
    cleaning_beam_agent-2_min: 191
    cleaning_beam_agent-3_max: 82
    cleaning_beam_agent-3_mean: 29.71
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 474
    cleaning_beam_agent-4_mean: 365.86
    cleaning_beam_agent-4_min: 175
    cleaning_beam_agent-5_max: 299
    cleaning_beam_agent-5_mean: 35.38
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-03-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1091.999999999987
  episode_reward_mean: 895.6799999999871
  episode_reward_min: 384.0000000000019
  episodes_this_iter: 96
  episodes_total: 26688
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12878.923
    learner:
      agent-0:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0053026676177979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011728010140359402
        model: {}
        policy_loss: -0.00285169156268239
        total_loss: -0.002546575851738453
        vf_explained_var: 0.051779016852378845
        vf_loss: 20.744483947753906
      agent-1:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0842821598052979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015639436896890402
        model: {}
        policy_loss: -0.004220566246658564
        total_loss: -0.0038078271318227053
        vf_explained_var: -0.0490725040435791
        vf_loss: 23.21076011657715
      agent-2:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0894361734390259
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014149488415569067
        model: {}
        policy_loss: -0.003461793065071106
        total_loss: -0.003163792658597231
        vf_explained_var: -0.008867591619491577
        vf_loss: 22.154094696044922
      agent-3:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47867757081985474
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008592576486989856
        model: {}
        policy_loss: -0.0023164991289377213
        total_loss: -0.0012504644691944122
        vf_explained_var: 0.12982268631458282
        vf_loss: 19.085073471069336
      agent-4:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9700630307197571
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017410474829375744
        model: {}
        policy_loss: -0.004073625430464745
        total_loss: -0.003822352271527052
        vf_explained_var: 0.10924725234508514
        vf_loss: 19.585844039916992
      agent-5:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7035864591598511
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012315742205828428
        model: {}
        policy_loss: -0.003053297521546483
        total_loss: -0.002441988792270422
        vf_explained_var: 0.15459534525871277
        vf_loss: 18.496219635009766
    load_time_ms: 16859.07
    num_steps_sampled: 26688000
    num_steps_trained: 26688000
    sample_time_ms: 129076.475
    update_time_ms: 84.433
  iterations_since_restore: 198
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.587500000000002
    ram_util_percent: 14.70044642857143
  pid: 14340
  policy_reward_max:
    agent-0: 181.99999999999963
    agent-1: 181.99999999999963
    agent-2: 181.99999999999963
    agent-3: 181.99999999999963
    agent-4: 181.99999999999963
    agent-5: 181.99999999999963
  policy_reward_mean:
    agent-0: 149.28000000000003
    agent-1: 149.28000000000003
    agent-2: 149.28000000000003
    agent-3: 149.28000000000003
    agent-4: 149.28000000000003
    agent-5: 149.28000000000003
  policy_reward_min:
    agent-0: 63.99999999999983
    agent-1: 63.99999999999983
    agent-2: 63.99999999999983
    agent-3: 63.99999999999983
    agent-4: 63.99999999999983
    agent-5: 63.99999999999983
  sampler_perf:
    mean_env_wait_ms: 30.849142616835167
    mean_inference_ms: 14.587341675559099
    mean_processing_ms: 65.71507232995008
  time_since_restore: 32621.964921474457
  time_this_iter_s: 157.85411524772644
  time_total_s: 45172.781792879105
  timestamp: 1637067824
  timesteps_since_restore: 19008000
  timesteps_this_iter: 96000
  timesteps_total: 26688000
  training_iteration: 278
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    278 |          45172.8 | 26688000 |   895.68 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 419
    apples_agent-0_mean: 7.06
    apples_agent-0_min: 0
    apples_agent-1_max: 131
    apples_agent-1_mean: 28.0
    apples_agent-1_min: 0
    apples_agent-2_max: 107
    apples_agent-2_mean: 6.58
    apples_agent-2_min: 0
    apples_agent-3_max: 187
    apples_agent-3_mean: 104.95
    apples_agent-3_min: 35
    apples_agent-4_max: 66
    apples_agent-4_mean: 2.82
    apples_agent-4_min: 0
    apples_agent-5_max: 327
    apples_agent-5_mean: 107.45
    apples_agent-5_min: 38
    cleaning_beam_agent-0_max: 649
    cleaning_beam_agent-0_mean: 421.1
    cleaning_beam_agent-0_min: 142
    cleaning_beam_agent-1_max: 491
    cleaning_beam_agent-1_mean: 287.14
    cleaning_beam_agent-1_min: 148
    cleaning_beam_agent-2_max: 539
    cleaning_beam_agent-2_mean: 372.76
    cleaning_beam_agent-2_min: 192
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 31.4
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 460
    cleaning_beam_agent-4_mean: 361.46
    cleaning_beam_agent-4_min: 198
    cleaning_beam_agent-5_max: 116
    cleaning_beam_agent-5_mean: 32.53
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-06-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1147.9999999999882
  episode_reward_mean: 904.9199999999854
  episode_reward_min: 294.99999999999807
  episodes_this_iter: 96
  episodes_total: 26784
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12879.136
    learner:
      agent-0:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.993131697177887
        entropy_coeff: 0.0017600000137463212
        kl: 0.001008346676826477
        model: {}
        policy_loss: -0.002921151928603649
        total_loss: -0.002518095076084137
        vf_explained_var: 0.07989826798439026
        vf_loss: 21.50969886779785
      agent-1:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0923426151275635
        entropy_coeff: 0.0017600000137463212
        kl: 0.001524597406387329
        model: {}
        policy_loss: -0.003978685010224581
        total_loss: -0.0033855377696454525
        vf_explained_var: -0.06816607713699341
        vf_loss: 25.15668487548828
      agent-2:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.09678316116333
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015563193010166287
        model: {}
        policy_loss: -0.0038890670984983444
        total_loss: -0.0034565357491374016
        vf_explained_var: -0.0015612542629241943
        vf_loss: 23.62867546081543
      agent-3:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4754943251609802
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007578400545753539
        model: {}
        policy_loss: -0.0022426238283514977
        total_loss: -0.0010278797708451748
        vf_explained_var: 0.12559278309345245
        vf_loss: 20.516151428222656
      agent-4:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9711859226226807
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012832072097808123
        model: {}
        policy_loss: -0.003948412835597992
        total_loss: -0.0034696697257459164
        vf_explained_var: 0.07126030325889587
        vf_loss: 21.880319595336914
      agent-5:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7118876576423645
        entropy_coeff: 0.0017600000137463212
        kl: 0.001453388249501586
        model: {}
        policy_loss: -0.003106354735791683
        total_loss: -0.002347419038414955
        vf_explained_var: 0.13922221958637238
        vf_loss: 20.118595123291016
    load_time_ms: 16780.076
    num_steps_sampled: 26784000
    num_steps_trained: 26784000
    sample_time_ms: 128942.815
    update_time_ms: 59.152
  iterations_since_restore: 199
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.83453815261044
    ram_util_percent: 14.992369477911648
  pid: 14340
  policy_reward_max:
    agent-0: 191.33333333333337
    agent-1: 191.33333333333337
    agent-2: 191.33333333333337
    agent-3: 191.33333333333337
    agent-4: 191.33333333333337
    agent-5: 191.33333333333337
  policy_reward_mean:
    agent-0: 150.81999999999994
    agent-1: 150.81999999999994
    agent-2: 150.81999999999994
    agent-3: 150.81999999999994
    agent-4: 150.81999999999994
    agent-5: 150.81999999999994
  policy_reward_min:
    agent-0: 49.16666666666659
    agent-1: 49.16666666666659
    agent-2: 49.16666666666659
    agent-3: 49.16666666666659
    agent-4: 49.16666666666659
    agent-5: 49.16666666666659
  sampler_perf:
    mean_env_wait_ms: 30.851567991685847
    mean_inference_ms: 14.587265365882537
    mean_processing_ms: 65.71502485914952
  time_since_restore: 32796.48232078552
  time_this_iter_s: 174.51739931106567
  time_total_s: 45347.29919219017
  timestamp: 1637067999
  timesteps_since_restore: 19104000
  timesteps_this_iter: 96000
  timesteps_total: 26784000
  training_iteration: 279
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    279 |          45347.3 | 26784000 |   904.92 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 2.61
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 31.47
    apples_agent-1_min: 0
    apples_agent-2_max: 115
    apples_agent-2_mean: 4.68
    apples_agent-2_min: 0
    apples_agent-3_max: 233
    apples_agent-3_mean: 108.4
    apples_agent-3_min: 22
    apples_agent-4_max: 66
    apples_agent-4_mean: 1.21
    apples_agent-4_min: 0
    apples_agent-5_max: 181
    apples_agent-5_mean: 107.55
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 623
    cleaning_beam_agent-0_mean: 428.94
    cleaning_beam_agent-0_min: 211
    cleaning_beam_agent-1_max: 500
    cleaning_beam_agent-1_mean: 287.32
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 570
    cleaning_beam_agent-2_mean: 388.95
    cleaning_beam_agent-2_min: 159
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 29.2
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 456
    cleaning_beam_agent-4_mean: 366.78
    cleaning_beam_agent-4_min: 241
    cleaning_beam_agent-5_max: 217
    cleaning_beam_agent-5_mean: 29.87
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-09-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1111.0000000000025
  episode_reward_mean: 920.8599999999857
  episode_reward_min: 557.0000000000077
  episodes_this_iter: 96
  episodes_total: 26880
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12872.559
    learner:
      agent-0:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0041496753692627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011491107288748026
        model: {}
        policy_loss: -0.0030460632406175137
        total_loss: -0.0028652718756347895
        vf_explained_var: 0.05737939476966858
        vf_loss: 19.480928421020508
      agent-1:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.101813793182373
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016766204498708248
        model: {}
        policy_loss: -0.004168124869465828
        total_loss: -0.0038673097733408213
        vf_explained_var: -0.056825488805770874
        vf_loss: 22.40003204345703
      agent-2:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.075236439704895
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015007185284048319
        model: {}
        policy_loss: -0.0036150221712887287
        total_loss: -0.0035057533532381058
        vf_explained_var: 0.03852388262748718
        vf_loss: 20.016843795776367
      agent-3:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46840983629226685
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009750889148563147
        model: {}
        policy_loss: -0.002337571233510971
        total_loss: -0.0012694476172327995
        vf_explained_var: 0.09468407928943634
        vf_loss: 18.925260543823242
      agent-4:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9718784093856812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013891654089093208
        model: {}
        policy_loss: -0.003692830679938197
        total_loss: -0.0033708172850310802
        vf_explained_var: 0.02012801170349121
        vf_loss: 20.325180053710938
      agent-5:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6843782663345337
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010445727966725826
        model: {}
        policy_loss: -0.0029060847591608763
        total_loss: -0.002311925869435072
        vf_explained_var: 0.12367472052574158
        vf_loss: 17.986621856689453
    load_time_ms: 16779.55
    num_steps_sampled: 26880000
    num_steps_trained: 26880000
    sample_time_ms: 128908.417
    update_time_ms: 55.575
  iterations_since_restore: 200
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.774774774774773
    ram_util_percent: 14.662612612612614
  pid: 14340
  policy_reward_max:
    agent-0: 185.16666666666615
    agent-1: 185.16666666666615
    agent-2: 185.16666666666615
    agent-3: 185.16666666666615
    agent-4: 185.16666666666615
    agent-5: 185.16666666666615
  policy_reward_mean:
    agent-0: 153.47666666666666
    agent-1: 153.47666666666666
    agent-2: 153.47666666666666
    agent-3: 153.47666666666666
    agent-4: 153.47666666666666
    agent-5: 153.47666666666666
  policy_reward_min:
    agent-0: 92.83333333333366
    agent-1: 92.83333333333366
    agent-2: 92.83333333333366
    agent-3: 92.83333333333366
    agent-4: 92.83333333333366
    agent-5: 92.83333333333366
  sampler_perf:
    mean_env_wait_ms: 30.8555603526555
    mean_inference_ms: 14.58687745076685
    mean_processing_ms: 65.7147031645209
  time_since_restore: 32952.46553111076
  time_this_iter_s: 155.9832103252411
  time_total_s: 45503.28240251541
  timestamp: 1637068155
  timesteps_since_restore: 19200000
  timesteps_this_iter: 96000
  timesteps_total: 26880000
  training_iteration: 280
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    280 |          45503.3 | 26880000 |   920.86 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 136
    apples_agent-0_mean: 2.78
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 30.12
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 6.52
    apples_agent-2_min: 0
    apples_agent-3_max: 471
    apples_agent-3_mean: 112.31
    apples_agent-3_min: 51
    apples_agent-4_max: 66
    apples_agent-4_mean: 1.63
    apples_agent-4_min: 0
    apples_agent-5_max: 499
    apples_agent-5_mean: 108.26
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 602
    cleaning_beam_agent-0_mean: 431.34
    cleaning_beam_agent-0_min: 280
    cleaning_beam_agent-1_max: 495
    cleaning_beam_agent-1_mean: 271.91
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 568
    cleaning_beam_agent-2_mean: 378.82
    cleaning_beam_agent-2_min: 145
    cleaning_beam_agent-3_max: 84
    cleaning_beam_agent-3_mean: 27.3
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 478
    cleaning_beam_agent-4_mean: 373.49
    cleaning_beam_agent-4_min: 250
    cleaning_beam_agent-5_max: 505
    cleaning_beam_agent-5_mean: 31.4
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-11-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1083.9999999999868
  episode_reward_mean: 919.8199999999847
  episode_reward_min: 472.99999999999983
  episodes_this_iter: 96
  episodes_total: 26976
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12874.45
    learner:
      agent-0:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0019911527633667
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013274815864861012
        model: {}
        policy_loss: -0.002736881375312805
        total_loss: -0.0024571213871240616
        vf_explained_var: 0.0025394856929779053
        vf_loss: 20.432645797729492
      agent-1:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1169188022613525
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015074771363288164
        model: {}
        policy_loss: -0.004065965302288532
        total_loss: -0.0037703951820731163
        vf_explained_var: -0.08297500014305115
        vf_loss: 22.613466262817383
      agent-2:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0798238515853882
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015718437498435378
        model: {}
        policy_loss: -0.0036710354033857584
        total_loss: -0.003418281441554427
        vf_explained_var: -0.03387634456157684
        vf_loss: 21.532445907592773
      agent-3:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4735310971736908
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008990354253910482
        model: {}
        policy_loss: -0.0023260158486664295
        total_loss: -0.0012889895588159561
        vf_explained_var: 0.09268134832382202
        vf_loss: 18.704410552978516
      agent-4:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9685573577880859
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016063394723460078
        model: {}
        policy_loss: -0.003948181867599487
        total_loss: -0.0037061069160699844
        vf_explained_var: 0.05336536467075348
        vf_loss: 19.46738624572754
      agent-5:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6742268204689026
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019404770573601127
        model: {}
        policy_loss: -0.003129566553980112
        total_loss: -0.00249878061003983
        vf_explained_var: 0.11281043291091919
        vf_loss: 18.17425537109375
    load_time_ms: 16758.915
    num_steps_sampled: 26976000
    num_steps_trained: 26976000
    sample_time_ms: 128609.595
    update_time_ms: 54.735
  iterations_since_restore: 201
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.74660633484163
    ram_util_percent: 14.69819004524887
  pid: 14340
  policy_reward_max:
    agent-0: 180.66666666666677
    agent-1: 180.66666666666677
    agent-2: 180.66666666666677
    agent-3: 180.66666666666677
    agent-4: 180.66666666666677
    agent-5: 180.66666666666677
  policy_reward_mean:
    agent-0: 153.3033333333333
    agent-1: 153.3033333333333
    agent-2: 153.3033333333333
    agent-3: 153.3033333333333
    agent-4: 153.3033333333333
    agent-5: 153.3033333333333
  policy_reward_min:
    agent-0: 78.83333333333339
    agent-1: 78.83333333333339
    agent-2: 78.83333333333339
    agent-3: 78.83333333333339
    agent-4: 78.83333333333339
    agent-5: 78.83333333333339
  sampler_perf:
    mean_env_wait_ms: 30.858437373668792
    mean_inference_ms: 14.58626566078769
    mean_processing_ms: 65.71392397584108
  time_since_restore: 33106.5366768837
  time_this_iter_s: 154.07114577293396
  time_total_s: 45657.353548288345
  timestamp: 1637068310
  timesteps_since_restore: 19296000
  timesteps_this_iter: 96000
  timesteps_total: 26976000
  training_iteration: 281
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    281 |          45657.4 | 26976000 |   919.82 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 2.35
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 25.82
    apples_agent-1_min: 0
    apples_agent-2_max: 149
    apples_agent-2_mean: 10.56
    apples_agent-2_min: 0
    apples_agent-3_max: 201
    apples_agent-3_mean: 108.64
    apples_agent-3_min: 48
    apples_agent-4_max: 55
    apples_agent-4_mean: 2.19
    apples_agent-4_min: 0
    apples_agent-5_max: 158
    apples_agent-5_mean: 100.83
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 577
    cleaning_beam_agent-0_mean: 426.75
    cleaning_beam_agent-0_min: 252
    cleaning_beam_agent-1_max: 545
    cleaning_beam_agent-1_mean: 269.71
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 621
    cleaning_beam_agent-2_mean: 380.16
    cleaning_beam_agent-2_min: 134
    cleaning_beam_agent-3_max: 93
    cleaning_beam_agent-3_mean: 27.77
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 482
    cleaning_beam_agent-4_mean: 372.14
    cleaning_beam_agent-4_min: 204
    cleaning_beam_agent-5_max: 233
    cleaning_beam_agent-5_mean: 32.61
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-14-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1167.0000000000064
  episode_reward_mean: 926.8299999999858
  episode_reward_min: 351.00000000000455
  episodes_this_iter: 96
  episodes_total: 27072
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12878.273
    learner:
      agent-0:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.012673020362854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011139456182718277
        model: {}
        policy_loss: -0.0028906655497848988
        total_loss: -0.0025417269207537174
        vf_explained_var: 0.03177320957183838
        vf_loss: 21.312423706054688
      agent-1:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1137170791625977
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016222400590777397
        model: {}
        policy_loss: -0.003965820651501417
        total_loss: -0.0035810419358313084
        vf_explained_var: -0.05080738663673401
        vf_loss: 23.44920539855957
      agent-2:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0707725286483765
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017733867280185223
        model: {}
        policy_loss: -0.003787229536101222
        total_loss: -0.003499345388263464
        vf_explained_var: 0.03965918719768524
        vf_loss: 21.724443435668945
      agent-3:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4749683141708374
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009683798416517675
        model: {}
        policy_loss: -0.0025353506207466125
        total_loss: -0.0013748742640018463
        vf_explained_var: 0.09267270565032959
        vf_loss: 19.964218139648438
      agent-4:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9649630784988403
        entropy_coeff: 0.0017600000137463212
        kl: 0.001956515247002244
        model: {}
        policy_loss: -0.0037915294524282217
        total_loss: -0.003401166992262006
        vf_explained_var: 0.05017171800136566
        vf_loss: 20.886985778808594
      agent-5:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6798444390296936
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009707252611406147
        model: {}
        policy_loss: -0.00282985414378345
        total_loss: -0.002119295531883836
        vf_explained_var: 0.13207043707370758
        vf_loss: 19.070842742919922
    load_time_ms: 16794.899
    num_steps_sampled: 27072000
    num_steps_trained: 27072000
    sample_time_ms: 128556.127
    update_time_ms: 54.778
  iterations_since_restore: 202
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.88772727272727
    ram_util_percent: 14.72136363636364
  pid: 14340
  policy_reward_max:
    agent-0: 194.49999999999966
    agent-1: 194.49999999999966
    agent-2: 194.49999999999966
    agent-3: 194.49999999999966
    agent-4: 194.49999999999966
    agent-5: 194.49999999999966
  policy_reward_mean:
    agent-0: 154.47166666666658
    agent-1: 154.47166666666658
    agent-2: 154.47166666666658
    agent-3: 154.47166666666658
    agent-4: 154.47166666666658
    agent-5: 154.47166666666658
  policy_reward_min:
    agent-0: 58.499999999999744
    agent-1: 58.499999999999744
    agent-2: 58.499999999999744
    agent-3: 58.499999999999744
    agent-4: 58.499999999999744
    agent-5: 58.499999999999744
  sampler_perf:
    mean_env_wait_ms: 30.862105870346824
    mean_inference_ms: 14.586467337825274
    mean_processing_ms: 65.7119465721961
  time_since_restore: 33261.32400584221
  time_this_iter_s: 154.78732895851135
  time_total_s: 45812.14087724686
  timestamp: 1637068465
  timesteps_since_restore: 19392000
  timesteps_this_iter: 96000
  timesteps_total: 27072000
  training_iteration: 282
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    282 |          45812.1 | 27072000 |   926.83 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 266
    apples_agent-0_mean: 5.64
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 25.61
    apples_agent-1_min: 0
    apples_agent-2_max: 145
    apples_agent-2_mean: 5.55
    apples_agent-2_min: 0
    apples_agent-3_max: 194
    apples_agent-3_mean: 106.53
    apples_agent-3_min: 6
    apples_agent-4_max: 41
    apples_agent-4_mean: 1.89
    apples_agent-4_min: 0
    apples_agent-5_max: 167
    apples_agent-5_mean: 102.65
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 622
    cleaning_beam_agent-0_mean: 414.96
    cleaning_beam_agent-0_min: 182
    cleaning_beam_agent-1_max: 575
    cleaning_beam_agent-1_mean: 297.59
    cleaning_beam_agent-1_min: 147
    cleaning_beam_agent-2_max: 562
    cleaning_beam_agent-2_mean: 376.42
    cleaning_beam_agent-2_min: 119
    cleaning_beam_agent-3_max: 183
    cleaning_beam_agent-3_mean: 29.78
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 491
    cleaning_beam_agent-4_mean: 370.18
    cleaning_beam_agent-4_min: 250
    cleaning_beam_agent-5_max: 162
    cleaning_beam_agent-5_mean: 31.14
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-17-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1167.0000000000064
  episode_reward_mean: 923.3999999999859
  episode_reward_min: 504.0000000000109
  episodes_this_iter: 96
  episodes_total: 27168
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12890.533
    learner:
      agent-0:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0107674598693848
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014808367704972625
        model: {}
        policy_loss: -0.003135761944577098
        total_loss: -0.0028738947585225105
        vf_explained_var: 0.02794615924358368
        vf_loss: 20.408199310302734
      agent-1:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0797743797302246
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017951678019016981
        model: {}
        policy_loss: -0.003700766945257783
        total_loss: -0.0033600188326090574
        vf_explained_var: -0.04904821515083313
        vf_loss: 22.4114990234375
      agent-2:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0835233926773071
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016443200875073671
        model: {}
        policy_loss: -0.0036952639929950237
        total_loss: -0.0034488444216549397
        vf_explained_var: -0.007765293121337891
        vf_loss: 21.53422737121582
      agent-3:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48046135902404785
        entropy_coeff: 0.0017600000137463212
        kl: 0.001029848586767912
        model: {}
        policy_loss: -0.002448471263051033
        total_loss: -0.0013949824497103691
        vf_explained_var: 0.09586423635482788
        vf_loss: 18.991004943847656
      agent-4:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9729103446006775
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015036712866276503
        model: {}
        policy_loss: -0.0038438737392425537
        total_loss: -0.003634076565504074
        vf_explained_var: 0.08381903171539307
        vf_loss: 19.22116470336914
      agent-5:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6516774296760559
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015318247023969889
        model: {}
        policy_loss: -0.0029850988648831844
        total_loss: -0.0024071463849395514
        vf_explained_var: 0.17380458116531372
        vf_loss: 17.249059677124023
    load_time_ms: 16858.707
    num_steps_sampled: 27168000
    num_steps_trained: 27168000
    sample_time_ms: 128656.879
    update_time_ms: 53.838
  iterations_since_restore: 203
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.7990990990991
    ram_util_percent: 14.65810810810811
  pid: 14340
  policy_reward_max:
    agent-0: 194.49999999999966
    agent-1: 194.49999999999966
    agent-2: 194.49999999999966
    agent-3: 194.49999999999966
    agent-4: 194.49999999999966
    agent-5: 194.49999999999966
  policy_reward_mean:
    agent-0: 153.89999999999998
    agent-1: 153.89999999999998
    agent-2: 153.89999999999998
    agent-3: 153.89999999999998
    agent-4: 153.89999999999998
    agent-5: 153.89999999999998
  policy_reward_min:
    agent-0: 84.00000000000014
    agent-1: 84.00000000000014
    agent-2: 84.00000000000014
    agent-3: 84.00000000000014
    agent-4: 84.00000000000014
    agent-5: 84.00000000000014
  sampler_perf:
    mean_env_wait_ms: 30.86528514592083
    mean_inference_ms: 14.586062711921524
    mean_processing_ms: 65.71037661272341
  time_since_restore: 33417.10202097893
  time_this_iter_s: 155.77801513671875
  time_total_s: 45967.918892383575
  timestamp: 1637068621
  timesteps_since_restore: 19488000
  timesteps_this_iter: 96000
  timesteps_total: 27168000
  training_iteration: 283
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    283 |          45967.9 | 27168000 |    923.4 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 2.09
    apples_agent-0_min: 0
    apples_agent-1_max: 131
    apples_agent-1_mean: 26.35
    apples_agent-1_min: 0
    apples_agent-2_max: 70
    apples_agent-2_mean: 6.56
    apples_agent-2_min: 0
    apples_agent-3_max: 186
    apples_agent-3_mean: 111.09
    apples_agent-3_min: 57
    apples_agent-4_max: 42
    apples_agent-4_mean: 1.75
    apples_agent-4_min: 0
    apples_agent-5_max: 162
    apples_agent-5_mean: 97.23
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 551
    cleaning_beam_agent-0_mean: 418.5
    cleaning_beam_agent-0_min: 225
    cleaning_beam_agent-1_max: 512
    cleaning_beam_agent-1_mean: 260.38
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 552
    cleaning_beam_agent-2_mean: 378.45
    cleaning_beam_agent-2_min: 147
    cleaning_beam_agent-3_max: 201
    cleaning_beam_agent-3_mean: 28.19
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 480
    cleaning_beam_agent-4_mean: 381.18
    cleaning_beam_agent-4_min: 244
    cleaning_beam_agent-5_max: 722
    cleaning_beam_agent-5_mean: 46.14
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-19-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1106.9999999999966
  episode_reward_mean: 923.039999999986
  episode_reward_min: 503.0000000000074
  episodes_this_iter: 96
  episodes_total: 27264
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12880.23
    learner:
      agent-0:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0072460174560547
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010950106661766768
        model: {}
        policy_loss: -0.003166397102177143
        total_loss: -0.002860094653442502
        vf_explained_var: 0.014831528067588806
        vf_loss: 20.7905330657959
      agent-1:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1202571392059326
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011319206096231937
        model: {}
        policy_loss: -0.003718361724168062
        total_loss: -0.0034596736077219248
        vf_explained_var: -0.04363200068473816
        vf_loss: 22.303424835205078
      agent-2:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.075204610824585
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018830571789294481
        model: {}
        policy_loss: -0.003759944811463356
        total_loss: -0.003546470310539007
        vf_explained_var: 0.020735234022140503
        vf_loss: 21.058347702026367
      agent-3:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4751702547073364
        entropy_coeff: 0.0017600000137463212
        kl: 0.00078411097638309
        model: {}
        policy_loss: -0.0025116060860455036
        total_loss: -0.001470430986955762
        vf_explained_var: 0.11484375596046448
        vf_loss: 18.774747848510742
      agent-4:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9591702818870544
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017340274062007666
        model: {}
        policy_loss: -0.0036748754791915417
        total_loss: -0.003381151705980301
        vf_explained_var: 0.06097163259983063
        vf_loss: 19.818592071533203
      agent-5:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6795670986175537
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013528717681765556
        model: {}
        policy_loss: -0.002919282764196396
        total_loss: -0.0022910935804247856
        vf_explained_var: 0.13556669652462006
        vf_loss: 18.242267608642578
    load_time_ms: 16979.715
    num_steps_sampled: 27264000
    num_steps_trained: 27264000
    sample_time_ms: 128567.827
    update_time_ms: 18.9
  iterations_since_restore: 204
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.787387387387387
    ram_util_percent: 14.672522522522524
  pid: 14340
  policy_reward_max:
    agent-0: 184.49999999999983
    agent-1: 184.49999999999983
    agent-2: 184.49999999999983
    agent-3: 184.49999999999983
    agent-4: 184.49999999999983
    agent-5: 184.49999999999983
  policy_reward_mean:
    agent-0: 153.83999999999997
    agent-1: 153.83999999999997
    agent-2: 153.83999999999997
    agent-3: 153.83999999999997
    agent-4: 153.83999999999997
    agent-5: 153.83999999999997
  policy_reward_min:
    agent-0: 83.83333333333333
    agent-1: 83.83333333333333
    agent-2: 83.83333333333333
    agent-3: 83.83333333333333
    agent-4: 83.83333333333333
    agent-5: 83.83333333333333
  sampler_perf:
    mean_env_wait_ms: 30.868755251154187
    mean_inference_ms: 14.585708730604717
    mean_processing_ms: 65.70934297916084
  time_since_restore: 33572.73290395737
  time_this_iter_s: 155.63088297843933
  time_total_s: 46123.549775362015
  timestamp: 1637068777
  timesteps_since_restore: 19584000
  timesteps_this_iter: 96000
  timesteps_total: 27264000
  training_iteration: 284
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    284 |          46123.5 | 27264000 |   923.04 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 1.88
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 24.62
    apples_agent-1_min: 0
    apples_agent-2_max: 111
    apples_agent-2_mean: 6.49
    apples_agent-2_min: 0
    apples_agent-3_max: 327
    apples_agent-3_mean: 109.02
    apples_agent-3_min: 56
    apples_agent-4_max: 71
    apples_agent-4_mean: 3.75
    apples_agent-4_min: 0
    apples_agent-5_max: 190
    apples_agent-5_mean: 102.43
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 572
    cleaning_beam_agent-0_mean: 425.82
    cleaning_beam_agent-0_min: 231
    cleaning_beam_agent-1_max: 486
    cleaning_beam_agent-1_mean: 275.45
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 588
    cleaning_beam_agent-2_mean: 378.84
    cleaning_beam_agent-2_min: 147
    cleaning_beam_agent-3_max: 83
    cleaning_beam_agent-3_mean: 25.73
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 482
    cleaning_beam_agent-4_mean: 386.32
    cleaning_beam_agent-4_min: 237
    cleaning_beam_agent-5_max: 156
    cleaning_beam_agent-5_mean: 28.22
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-22-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1144.9999999999998
  episode_reward_mean: 932.5699999999862
  episode_reward_min: 478.0000000000082
  episodes_this_iter: 96
  episodes_total: 27360
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12893.302
    learner:
      agent-0:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0133330821990967
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020078616216778755
        model: {}
        policy_loss: -0.0030873529613018036
        total_loss: -0.0027047432959079742
        vf_explained_var: 0.0033115148544311523
        vf_loss: 21.660747528076172
      agent-1:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0909771919250488
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017622057348489761
        model: {}
        policy_loss: -0.003970605321228504
        total_loss: -0.0035411997232586145
        vf_explained_var: -0.051181137561798096
        vf_loss: 23.495267868041992
      agent-2:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0768024921417236
        entropy_coeff: 0.0017600000137463212
        kl: 0.001367017743177712
        model: {}
        policy_loss: -0.0035633165389299393
        total_loss: -0.003182518295943737
        vf_explained_var: -0.027234911918640137
        vf_loss: 22.75969886779785
      agent-3:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.461428701877594
        entropy_coeff: 0.0017600000137463212
        kl: 0.001112444093450904
        model: {}
        policy_loss: -0.002553282305598259
        total_loss: -0.0014024758711457253
        vf_explained_var: 0.10431480407714844
        vf_loss: 19.62918472290039
      agent-4:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9687591195106506
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015492468373849988
        model: {}
        policy_loss: -0.0037215808406472206
        total_loss: -0.0034508509561419487
        vf_explained_var: 0.09889507293701172
        vf_loss: 19.757448196411133
      agent-5:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6578736901283264
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011156569235026836
        model: {}
        policy_loss: -0.002757508773356676
        total_loss: -0.0020396308973431587
        vf_explained_var: 0.13502468168735504
        vf_loss: 18.75737190246582
    load_time_ms: 17020.877
    num_steps_sampled: 27360000
    num_steps_trained: 27360000
    sample_time_ms: 129002.319
    update_time_ms: 53.488
  iterations_since_restore: 205
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.031441048034935
    ram_util_percent: 14.664192139737997
  pid: 14340
  policy_reward_max:
    agent-0: 190.83333333333312
    agent-1: 190.83333333333312
    agent-2: 190.83333333333312
    agent-3: 190.83333333333312
    agent-4: 190.83333333333312
    agent-5: 190.83333333333312
  policy_reward_mean:
    agent-0: 155.4283333333333
    agent-1: 155.4283333333333
    agent-2: 155.4283333333333
    agent-3: 155.4283333333333
    agent-4: 155.4283333333333
    agent-5: 155.4283333333333
  policy_reward_min:
    agent-0: 79.66666666666657
    agent-1: 79.66666666666657
    agent-2: 79.66666666666657
    agent-3: 79.66666666666657
    agent-4: 79.66666666666657
    agent-5: 79.66666666666657
  sampler_perf:
    mean_env_wait_ms: 30.872195056660466
    mean_inference_ms: 14.585604882664995
    mean_processing_ms: 65.70832881586998
  time_since_restore: 33733.065527915955
  time_this_iter_s: 160.33262395858765
  time_total_s: 46283.8823993206
  timestamp: 1637068937
  timesteps_since_restore: 19680000
  timesteps_this_iter: 96000
  timesteps_total: 27360000
  training_iteration: 285
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    285 |          46283.9 | 27360000 |   932.57 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 2.33
    apples_agent-0_min: 0
    apples_agent-1_max: 111
    apples_agent-1_mean: 28.8
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 4.23
    apples_agent-2_min: 0
    apples_agent-3_max: 274
    apples_agent-3_mean: 110.5
    apples_agent-3_min: 46
    apples_agent-4_max: 56
    apples_agent-4_mean: 1.63
    apples_agent-4_min: 0
    apples_agent-5_max: 226
    apples_agent-5_mean: 105.03
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 571
    cleaning_beam_agent-0_mean: 420.61
    cleaning_beam_agent-0_min: 201
    cleaning_beam_agent-1_max: 456
    cleaning_beam_agent-1_mean: 256.17
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 588
    cleaning_beam_agent-2_mean: 397.5
    cleaning_beam_agent-2_min: 173
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 28.05
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 471
    cleaning_beam_agent-4_mean: 380.55
    cleaning_beam_agent-4_min: 262
    cleaning_beam_agent-5_max: 200
    cleaning_beam_agent-5_mean: 29.35
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-24-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1117.9999999999975
  episode_reward_mean: 928.1899999999872
  episode_reward_min: 533.0000000000061
  episodes_this_iter: 96
  episodes_total: 27456
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12861.688
    learner:
      agent-0:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.014691710472107
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015298076905310154
        model: {}
        policy_loss: -0.0033690514974296093
        total_loss: -0.003174571320414543
        vf_explained_var: 0.05908799171447754
        vf_loss: 19.803377151489258
      agent-1:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1068767309188843
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014369201380759478
        model: {}
        policy_loss: -0.003757644444704056
        total_loss: -0.0034085894003510475
        vf_explained_var: -0.06130331754684448
        vf_loss: 22.97157859802246
      agent-2:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0665993690490723
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014919472159817815
        model: {}
        policy_loss: -0.0036317422054708004
        total_loss: -0.0034027337096631527
        vf_explained_var: 0.003923147916793823
        vf_loss: 21.062227249145508
      agent-3:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4755695164203644
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017418349161744118
        model: {}
        policy_loss: -0.0028259046375751495
        total_loss: -0.0017551209311932325
        vf_explained_var: 0.0937809944152832
        vf_loss: 19.077905654907227
      agent-4:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9676296710968018
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011435120832175016
        model: {}
        policy_loss: -0.0036010975018143654
        total_loss: -0.0033292178995907307
        vf_explained_var: 0.05804617702960968
        vf_loss: 19.74907875061035
      agent-5:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6687933206558228
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008273393614217639
        model: {}
        policy_loss: -0.00291373860090971
        total_loss: -0.0022159223444759846
        vf_explained_var: 0.10090966522693634
        vf_loss: 18.748926162719727
    load_time_ms: 17112.033
    num_steps_sampled: 27456000
    num_steps_trained: 27456000
    sample_time_ms: 129203.577
    update_time_ms: 56.139
  iterations_since_restore: 206
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.74260089686099
    ram_util_percent: 14.668609865470854
  pid: 14340
  policy_reward_max:
    agent-0: 186.33333333333277
    agent-1: 186.33333333333277
    agent-2: 186.33333333333277
    agent-3: 186.33333333333277
    agent-4: 186.33333333333277
    agent-5: 186.33333333333277
  policy_reward_mean:
    agent-0: 154.69833333333327
    agent-1: 154.69833333333327
    agent-2: 154.69833333333327
    agent-3: 154.69833333333327
    agent-4: 154.69833333333327
    agent-5: 154.69833333333327
  policy_reward_min:
    agent-0: 88.83333333333343
    agent-1: 88.83333333333343
    agent-2: 88.83333333333343
    agent-3: 88.83333333333343
    agent-4: 88.83333333333343
    agent-5: 88.83333333333343
  sampler_perf:
    mean_env_wait_ms: 30.87576940663628
    mean_inference_ms: 14.585588346936644
    mean_processing_ms: 65.70845291778699
  time_since_restore: 33890.08135533333
  time_this_iter_s: 157.01582741737366
  time_total_s: 46440.898226737976
  timestamp: 1637069095
  timesteps_since_restore: 19776000
  timesteps_this_iter: 96000
  timesteps_total: 27456000
  training_iteration: 286
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    286 |          46440.9 | 27456000 |   928.19 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 3.87
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 27.01
    apples_agent-1_min: 0
    apples_agent-2_max: 434
    apples_agent-2_mean: 11.45
    apples_agent-2_min: 0
    apples_agent-3_max: 442
    apples_agent-3_mean: 115.27
    apples_agent-3_min: 56
    apples_agent-4_max: 42
    apples_agent-4_mean: 1.69
    apples_agent-4_min: 0
    apples_agent-5_max: 173
    apples_agent-5_mean: 99.38
    apples_agent-5_min: 29
    cleaning_beam_agent-0_max: 555
    cleaning_beam_agent-0_mean: 405.44
    cleaning_beam_agent-0_min: 235
    cleaning_beam_agent-1_max: 461
    cleaning_beam_agent-1_mean: 270.88
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 564
    cleaning_beam_agent-2_mean: 375.63
    cleaning_beam_agent-2_min: 181
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 27.51
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 510
    cleaning_beam_agent-4_mean: 384.14
    cleaning_beam_agent-4_min: 211
    cleaning_beam_agent-5_max: 189
    cleaning_beam_agent-5_mean: 34.66
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-27-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1093.9999999999902
  episode_reward_mean: 914.609999999987
  episode_reward_min: 387.0000000000075
  episodes_this_iter: 96
  episodes_total: 27552
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12857.885
    learner:
      agent-0:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.021943211555481
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013182060793042183
        model: {}
        policy_loss: -0.002850905992090702
        total_loss: -0.002528088167309761
        vf_explained_var: 0.05940447747707367
        vf_loss: 21.21439552307129
      agent-1:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1003509759902954
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016291345236822963
        model: {}
        policy_loss: -0.0039096432738006115
        total_loss: -0.003478052094578743
        vf_explained_var: -0.039814651012420654
        vf_loss: 23.682085037231445
      agent-2:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0637556314468384
        entropy_coeff: 0.0017600000137463212
        kl: 0.002233329229056835
        model: {}
        policy_loss: -0.003904268378391862
        total_loss: -0.003644099459052086
        vf_explained_var: 0.05941374599933624
        vf_loss: 21.323772430419922
      agent-3:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47372180223464966
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011651995591819286
        model: {}
        policy_loss: -0.0024832740891724825
        total_loss: -0.001284666359424591
        vf_explained_var: 0.0959264487028122
        vf_loss: 20.323604583740234
      agent-4:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9625716805458069
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016332294326275587
        model: {}
        policy_loss: -0.003923606127500534
        total_loss: -0.003498232923448086
        vf_explained_var: 0.055985838174819946
        vf_loss: 21.19499969482422
      agent-5:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6891970634460449
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017536646919324994
        model: {}
        policy_loss: -0.002881434978917241
        total_loss: -0.0021373387426137924
        vf_explained_var: 0.12943194806575775
        vf_loss: 19.570819854736328
    load_time_ms: 16924.103
    num_steps_sampled: 27552000
    num_steps_trained: 27552000
    sample_time_ms: 129094.048
    update_time_ms: 65.475
  iterations_since_restore: 207
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.73093220338983
    ram_util_percent: 14.795762711864407
  pid: 14340
  policy_reward_max:
    agent-0: 182.33333333333326
    agent-1: 182.33333333333326
    agent-2: 182.33333333333326
    agent-3: 182.33333333333326
    agent-4: 182.33333333333326
    agent-5: 182.33333333333326
  policy_reward_mean:
    agent-0: 152.43499999999997
    agent-1: 152.43499999999997
    agent-2: 152.43499999999997
    agent-3: 152.43499999999997
    agent-4: 152.43499999999997
    agent-5: 152.43499999999997
  policy_reward_min:
    agent-0: 64.49999999999986
    agent-1: 64.49999999999986
    agent-2: 64.49999999999986
    agent-3: 64.49999999999986
    agent-4: 64.49999999999986
    agent-5: 64.49999999999986
  sampler_perf:
    mean_env_wait_ms: 30.87862976978481
    mean_inference_ms: 14.585159437953207
    mean_processing_ms: 65.70727137415119
  time_since_restore: 34054.909209012985
  time_this_iter_s: 164.82785367965698
  time_total_s: 46605.72608041763
  timestamp: 1637069260
  timesteps_since_restore: 19872000
  timesteps_this_iter: 96000
  timesteps_total: 27552000
  training_iteration: 287
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    287 |          46605.7 | 27552000 |   914.61 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 156
    apples_agent-0_mean: 6.55
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 22.94
    apples_agent-1_min: 0
    apples_agent-2_max: 91
    apples_agent-2_mean: 2.45
    apples_agent-2_min: 0
    apples_agent-3_max: 199
    apples_agent-3_mean: 113.03
    apples_agent-3_min: 44
    apples_agent-4_max: 66
    apples_agent-4_mean: 1.85
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 106.03
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 568
    cleaning_beam_agent-0_mean: 404.49
    cleaning_beam_agent-0_min: 174
    cleaning_beam_agent-1_max: 454
    cleaning_beam_agent-1_mean: 270.85
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 581
    cleaning_beam_agent-2_mean: 406.13
    cleaning_beam_agent-2_min: 209
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 25.62
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 489
    cleaning_beam_agent-4_mean: 387.97
    cleaning_beam_agent-4_min: 178
    cleaning_beam_agent-5_max: 231
    cleaning_beam_agent-5_mean: 32.45
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-30-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1106.99999999998
  episode_reward_mean: 929.1299999999875
  episode_reward_min: 436.0000000000066
  episodes_this_iter: 96
  episodes_total: 27648
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12871.707
    learner:
      agent-0:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0232011079788208
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015861962456256151
        model: {}
        policy_loss: -0.0032797656022012234
        total_loss: -0.003089822828769684
        vf_explained_var: 0.0944107323884964
        vf_loss: 19.907737731933594
      agent-1:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.116154432296753
        entropy_coeff: 0.0017600000137463212
        kl: 0.001982213230803609
        model: {}
        policy_loss: -0.004154638387262821
        total_loss: -0.003778873709961772
        vf_explained_var: -0.057536691427230835
        vf_loss: 23.401962280273438
      agent-2:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0609276294708252
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011410098522901535
        model: {}
        policy_loss: -0.003488980233669281
        total_loss: -0.003222482744604349
        vf_explained_var: 0.019350171089172363
        vf_loss: 21.337303161621094
      agent-3:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4557167887687683
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007704661111347377
        model: {}
        policy_loss: -0.0022051904816180468
        total_loss: -0.0010987133719027042
        vf_explained_var: 0.12393410503864288
        vf_loss: 19.085399627685547
      agent-4:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9560215473175049
        entropy_coeff: 0.0017600000137463212
        kl: 0.002706920262426138
        model: {}
        policy_loss: -0.004098303150385618
        total_loss: -0.0037578768096864223
        vf_explained_var: 0.07220607995986938
        vf_loss: 20.230239868164062
      agent-5:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.686957836151123
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010646362788975239
        model: {}
        policy_loss: -0.0028050532564520836
        total_loss: -0.00218272116035223
        vf_explained_var: 0.16252654790878296
        vf_loss: 18.313739776611328
    load_time_ms: 16812.885
    num_steps_sampled: 27648000
    num_steps_trained: 27648000
    sample_time_ms: 129298.741
    update_time_ms: 65.612
  iterations_since_restore: 208
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.92533333333333
    ram_util_percent: 15.60933333333333
  pid: 14340
  policy_reward_max:
    agent-0: 184.49999999999974
    agent-1: 184.49999999999974
    agent-2: 184.49999999999974
    agent-3: 184.49999999999974
    agent-4: 184.49999999999974
    agent-5: 184.49999999999974
  policy_reward_mean:
    agent-0: 154.855
    agent-1: 154.855
    agent-2: 154.855
    agent-3: 154.855
    agent-4: 154.855
    agent-5: 154.855
  policy_reward_min:
    agent-0: 72.66666666666663
    agent-1: 72.66666666666663
    agent-2: 72.66666666666663
    agent-3: 72.66666666666663
    agent-4: 72.66666666666663
    agent-5: 72.66666666666663
  sampler_perf:
    mean_env_wait_ms: 30.88452255948268
    mean_inference_ms: 14.586614631301456
    mean_processing_ms: 65.71334885076863
  time_since_restore: 34213.51106262207
  time_this_iter_s: 158.60185360908508
  time_total_s: 46764.32793402672
  timestamp: 1637069419
  timesteps_since_restore: 19968000
  timesteps_this_iter: 96000
  timesteps_total: 27648000
  training_iteration: 288
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    288 |          46764.3 | 27648000 |   929.13 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 79
    apples_agent-0_mean: 4.97
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 24.94
    apples_agent-1_min: 0
    apples_agent-2_max: 189
    apples_agent-2_mean: 7.92
    apples_agent-2_min: 0
    apples_agent-3_max: 189
    apples_agent-3_mean: 105.16
    apples_agent-3_min: 25
    apples_agent-4_max: 104
    apples_agent-4_mean: 5.32
    apples_agent-4_min: 0
    apples_agent-5_max: 279
    apples_agent-5_mean: 107.16
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 586
    cleaning_beam_agent-0_mean: 413.48
    cleaning_beam_agent-0_min: 202
    cleaning_beam_agent-1_max: 486
    cleaning_beam_agent-1_mean: 273.72
    cleaning_beam_agent-1_min: 148
    cleaning_beam_agent-2_max: 607
    cleaning_beam_agent-2_mean: 409.29
    cleaning_beam_agent-2_min: 129
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 27.38
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 512
    cleaning_beam_agent-4_mean: 390.67
    cleaning_beam_agent-4_min: 120
    cleaning_beam_agent-5_max: 191
    cleaning_beam_agent-5_mean: 34.98
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-33-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1108.9999999999982
  episode_reward_mean: 896.869999999987
  episode_reward_min: 312.9999999999976
  episodes_this_iter: 96
  episodes_total: 27744
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12875.588
    learner:
      agent-0:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0099495649337769
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018114387057721615
        model: {}
        policy_loss: -0.0028337864205241203
        total_loss: -0.002251120749861002
        vf_explained_var: 0.012164831161499023
        vf_loss: 23.601768493652344
      agent-1:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1112544536590576
        entropy_coeff: 0.0017600000137463212
        kl: 0.001243313425220549
        model: {}
        policy_loss: -0.003746934002265334
        total_loss: -0.0031327547039836645
        vf_explained_var: -0.07622873783111572
        vf_loss: 25.699874877929688
      agent-2:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0427316427230835
        entropy_coeff: 0.0017600000137463212
        kl: 0.002312252065166831
        model: {}
        policy_loss: -0.003300598356872797
        total_loss: -0.002705059479922056
        vf_explained_var: -0.01723293960094452
        vf_loss: 24.30746078491211
      agent-3:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48487910628318787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007756226696074009
        model: {}
        policy_loss: -0.002705301158130169
        total_loss: -0.0015873548109084368
        vf_explained_var: 0.17461971938610077
        vf_loss: 19.713336944580078
      agent-4:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9596483707427979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016620587557554245
        model: {}
        policy_loss: -0.0038830875419080257
        total_loss: -0.003421616740524769
        vf_explained_var: 0.09905123710632324
        vf_loss: 21.504512786865234
      agent-5:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6999119520187378
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016354405088350177
        model: {}
        policy_loss: -0.0032593486830592155
        total_loss: -0.0024680844508111477
        vf_explained_var: 0.15257850289344788
        vf_loss: 20.231098175048828
    load_time_ms: 16179.885
    num_steps_sampled: 27744000
    num_steps_trained: 27744000
    sample_time_ms: 129375.673
    update_time_ms: 67.632
  iterations_since_restore: 209
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.443983402489625
    ram_util_percent: 16.483817427385894
  pid: 14340
  policy_reward_max:
    agent-0: 184.83333333333297
    agent-1: 184.83333333333297
    agent-2: 184.83333333333297
    agent-3: 184.83333333333297
    agent-4: 184.83333333333297
    agent-5: 184.83333333333297
  policy_reward_mean:
    agent-0: 149.47833333333332
    agent-1: 149.47833333333332
    agent-2: 149.47833333333332
    agent-3: 149.47833333333332
    agent-4: 149.47833333333332
    agent-5: 149.47833333333332
  policy_reward_min:
    agent-0: 52.16666666666665
    agent-1: 52.16666666666665
    agent-2: 52.16666666666665
    agent-3: 52.16666666666665
    agent-4: 52.16666666666665
    agent-5: 52.16666666666665
  sampler_perf:
    mean_env_wait_ms: 30.889505838258785
    mean_inference_ms: 14.58691725336824
    mean_processing_ms: 65.73734350503622
  time_since_restore: 34382.43702149391
  time_this_iter_s: 168.92595887184143
  time_total_s: 46933.25389289856
  timestamp: 1637069588
  timesteps_since_restore: 20064000
  timesteps_this_iter: 96000
  timesteps_total: 27744000
  training_iteration: 289
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    289 |          46933.3 | 27744000 |   896.87 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 65
    apples_agent-0_mean: 2.87
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 26.95
    apples_agent-1_min: 0
    apples_agent-2_max: 65
    apples_agent-2_mean: 5.36
    apples_agent-2_min: 0
    apples_agent-3_max: 166
    apples_agent-3_mean: 110.18
    apples_agent-3_min: 39
    apples_agent-4_max: 67
    apples_agent-4_mean: 2.6
    apples_agent-4_min: 0
    apples_agent-5_max: 363
    apples_agent-5_mean: 105.02
    apples_agent-5_min: 19
    cleaning_beam_agent-0_max: 597
    cleaning_beam_agent-0_mean: 409.25
    cleaning_beam_agent-0_min: 222
    cleaning_beam_agent-1_max: 470
    cleaning_beam_agent-1_mean: 272.43
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 600
    cleaning_beam_agent-2_mean: 407.15
    cleaning_beam_agent-2_min: 151
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 25.57
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 544
    cleaning_beam_agent-4_mean: 409.37
    cleaning_beam_agent-4_min: 120
    cleaning_beam_agent-5_max: 468
    cleaning_beam_agent-5_mean: 35.85
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-35-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1117.0000000000064
  episode_reward_mean: 928.6699999999877
  episode_reward_min: 312.9999999999976
  episodes_this_iter: 96
  episodes_total: 27840
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12932.298
    learner:
      agent-0:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.031794786453247
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015326940920203924
        model: {}
        policy_loss: -0.0031484386418014765
        total_loss: -0.00283108651638031
        vf_explained_var: 0.07629190385341644
        vf_loss: 21.3331241607666
      agent-1:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.095731258392334
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019337309058755636
        model: {}
        policy_loss: -0.004115546587854624
        total_loss: -0.0035715769045054913
        vf_explained_var: -0.04943525791168213
        vf_loss: 24.724559783935547
      agent-2:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0615746974945068
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017025232082232833
        model: {}
        policy_loss: -0.003978540189564228
        total_loss: -0.0036433106288313866
        vf_explained_var: 0.05712957680225372
        vf_loss: 22.035999298095703
      agent-3:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4492266774177551
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009800251573324203
        model: {}
        policy_loss: -0.0023080315440893173
        total_loss: -0.0010559218935668468
        vf_explained_var: 0.1168108731508255
        vf_loss: 20.427467346191406
      agent-4:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.950496256351471
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014215278206393123
        model: {}
        policy_loss: -0.003683330724015832
        total_loss: -0.0032388654071837664
        vf_explained_var: 0.08004288375377655
        vf_loss: 21.173376083374023
      agent-5:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6812032461166382
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012372630881145597
        model: {}
        policy_loss: -0.003202741965651512
        total_loss: -0.0024095484986901283
        vf_explained_var: 0.13833868503570557
        vf_loss: 19.921133041381836
    load_time_ms: 16285.812
    num_steps_sampled: 27840000
    num_steps_trained: 27840000
    sample_time_ms: 129517.75
    update_time_ms: 68.584
  iterations_since_restore: 210
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.459734513274338
    ram_util_percent: 16.295575221238938
  pid: 14340
  policy_reward_max:
    agent-0: 186.16666666666632
    agent-1: 186.16666666666632
    agent-2: 186.16666666666632
    agent-3: 186.16666666666632
    agent-4: 186.16666666666632
    agent-5: 186.16666666666632
  policy_reward_mean:
    agent-0: 154.77833333333322
    agent-1: 154.77833333333322
    agent-2: 154.77833333333322
    agent-3: 154.77833333333322
    agent-4: 154.77833333333322
    agent-5: 154.77833333333322
  policy_reward_min:
    agent-0: 52.16666666666665
    agent-1: 52.16666666666665
    agent-2: 52.16666666666665
    agent-3: 52.16666666666665
    agent-4: 52.16666666666665
    agent-5: 52.16666666666665
  sampler_perf:
    mean_env_wait_ms: 30.896852990541397
    mean_inference_ms: 14.588418262466567
    mean_processing_ms: 65.74682490134235
  time_since_restore: 34541.511989831924
  time_this_iter_s: 159.0749683380127
  time_total_s: 47092.32886123657
  timestamp: 1637069747
  timesteps_since_restore: 20160000
  timesteps_this_iter: 96000
  timesteps_total: 27840000
  training_iteration: 290
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    290 |          47092.3 | 27840000 |   928.67 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 73
    apples_agent-0_mean: 2.52
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 22.08
    apples_agent-1_min: 0
    apples_agent-2_max: 254
    apples_agent-2_mean: 13.04
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 115.61
    apples_agent-3_min: 60
    apples_agent-4_max: 39
    apples_agent-4_mean: 1.65
    apples_agent-4_min: 0
    apples_agent-5_max: 189
    apples_agent-5_mean: 109.78
    apples_agent-5_min: 69
    cleaning_beam_agent-0_max: 545
    cleaning_beam_agent-0_mean: 385.15
    cleaning_beam_agent-0_min: 205
    cleaning_beam_agent-1_max: 424
    cleaning_beam_agent-1_mean: 277.38
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 621
    cleaning_beam_agent-2_mean: 403.26
    cleaning_beam_agent-2_min: 111
    cleaning_beam_agent-3_max: 88
    cleaning_beam_agent-3_mean: 24.05
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 484
    cleaning_beam_agent-4_mean: 408.3
    cleaning_beam_agent-4_min: 278
    cleaning_beam_agent-5_max: 175
    cleaning_beam_agent-5_mean: 31.31
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-38-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1131.0000000000018
  episode_reward_mean: 942.3499999999863
  episode_reward_min: 454.0000000000083
  episodes_this_iter: 96
  episodes_total: 27936
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12943.87
    learner:
      agent-0:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0306737422943115
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026401786599308252
        model: {}
        policy_loss: -0.0037162944208830595
        total_loss: -0.003391508013010025
        vf_explained_var: 0.03794470429420471
        vf_loss: 21.387733459472656
      agent-1:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.091170072555542
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019711193162947893
        model: {}
        policy_loss: -0.004085634835064411
        total_loss: -0.0036646323278546333
        vf_explained_var: -0.04341670870780945
        vf_loss: 23.41461944580078
      agent-2:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0481053590774536
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019045330118387938
        model: {}
        policy_loss: -0.003542352467775345
        total_loss: -0.0030899459961801767
        vf_explained_var: -0.03073638677597046
        vf_loss: 22.970741271972656
      agent-3:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45187506079673767
        entropy_coeff: 0.0017600000137463212
        kl: 0.000952184374909848
        model: {}
        policy_loss: -0.0023746765218675137
        total_loss: -0.0012742907274514437
        vf_explained_var: 0.13348832726478577
        vf_loss: 18.95684242248535
      agent-4:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9539950489997864
        entropy_coeff: 0.0017600000137463212
        kl: 0.002083756495267153
        model: {}
        policy_loss: -0.004170413129031658
        total_loss: -0.003869897685945034
        vf_explained_var: 0.10060073435306549
        vf_loss: 19.79546546936035
      agent-5:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6815438866615295
        entropy_coeff: 0.0017600000137463212
        kl: 0.001310978434048593
        model: {}
        policy_loss: -0.002964976243674755
        total_loss: -0.0022763172164559364
        vf_explained_var: 0.13958072662353516
        vf_loss: 18.88177490234375
    load_time_ms: 16425.697
    num_steps_sampled: 27936000
    num_steps_trained: 27936000
    sample_time_ms: 129867.355
    update_time_ms: 82.135
  iterations_since_restore: 211
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.044493392070486
    ram_util_percent: 16.388546255506608
  pid: 14340
  policy_reward_max:
    agent-0: 188.49999999999966
    agent-1: 188.49999999999966
    agent-2: 188.49999999999966
    agent-3: 188.49999999999966
    agent-4: 188.49999999999966
    agent-5: 188.49999999999966
  policy_reward_mean:
    agent-0: 157.05833333333328
    agent-1: 157.05833333333328
    agent-2: 157.05833333333328
    agent-3: 157.05833333333328
    agent-4: 157.05833333333328
    agent-5: 157.05833333333328
  policy_reward_min:
    agent-0: 75.66666666666669
    agent-1: 75.66666666666669
    agent-2: 75.66666666666669
    agent-3: 75.66666666666669
    agent-4: 75.66666666666669
    agent-5: 75.66666666666669
  sampler_perf:
    mean_env_wait_ms: 30.903494512159362
    mean_inference_ms: 14.589572314157094
    mean_processing_ms: 65.75227072682213
  time_since_restore: 34700.760921001434
  time_this_iter_s: 159.2489311695099
  time_total_s: 47251.57779240608
  timestamp: 1637069907
  timesteps_since_restore: 20256000
  timesteps_this_iter: 96000
  timesteps_total: 27936000
  training_iteration: 291
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    291 |          47251.6 | 27936000 |   942.35 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 5.62
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 23.58
    apples_agent-1_min: 0
    apples_agent-2_max: 88
    apples_agent-2_mean: 4.96
    apples_agent-2_min: 0
    apples_agent-3_max: 274
    apples_agent-3_mean: 111.36
    apples_agent-3_min: 50
    apples_agent-4_max: 67
    apples_agent-4_mean: 2.63
    apples_agent-4_min: 0
    apples_agent-5_max: 291
    apples_agent-5_mean: 110.78
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 489
    cleaning_beam_agent-0_mean: 372.9
    cleaning_beam_agent-0_min: 210
    cleaning_beam_agent-1_max: 527
    cleaning_beam_agent-1_mean: 267.9
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 609
    cleaning_beam_agent-2_mean: 376.9
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 105
    cleaning_beam_agent-3_mean: 23.96
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 528
    cleaning_beam_agent-4_mean: 404.31
    cleaning_beam_agent-4_min: 164
    cleaning_beam_agent-5_max: 136
    cleaning_beam_agent-5_mean: 28.31
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-41-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1108.999999999983
  episode_reward_mean: 922.9299999999863
  episode_reward_min: 270.999999999998
  episodes_this_iter: 96
  episodes_total: 28032
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12964.522
    learner:
      agent-0:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0204832553863525
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010747958440333605
        model: {}
        policy_loss: -0.0031325779855251312
        total_loss: -0.002693619579076767
        vf_explained_var: 0.05576823651790619
        vf_loss: 22.350101470947266
      agent-1:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.099707007408142
        entropy_coeff: 0.0017600000137463212
        kl: 0.001326612662523985
        model: {}
        policy_loss: -0.0038700394798070192
        total_loss: -0.003227775916457176
        vf_explained_var: -0.08908739686012268
        vf_loss: 25.777469635009766
      agent-2:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.082960605621338
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011172605445608497
        model: {}
        policy_loss: -0.003482556203380227
        total_loss: -0.0030745400581508875
        vf_explained_var: 0.01771405339241028
        vf_loss: 23.1402587890625
      agent-3:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4619857370853424
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007895558374002576
        model: {}
        policy_loss: -0.0023683826439082623
        total_loss: -0.0011232455726712942
        vf_explained_var: 0.1240929365158081
        vf_loss: 20.582313537597656
      agent-4:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9587686061859131
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016758153215050697
        model: {}
        policy_loss: -0.0037594682071357965
        total_loss: -0.0032624683808535337
        vf_explained_var: 0.07163752615451813
        vf_loss: 21.844348907470703
      agent-5:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6979337930679321
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011865893611684442
        model: {}
        policy_loss: -0.003134234109893441
        total_loss: -0.0023288035299628973
        vf_explained_var: 0.13454298675060272
        vf_loss: 20.337953567504883
    load_time_ms: 18291.008
    num_steps_sampled: 28032000
    num_steps_trained: 28032000
    sample_time_ms: 130028.129
    update_time_ms: 84.555
  iterations_since_restore: 212
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.558799999999998
    ram_util_percent: 16.337999999999997
  pid: 14340
  policy_reward_max:
    agent-0: 184.83333333333294
    agent-1: 184.83333333333294
    agent-2: 184.83333333333294
    agent-3: 184.83333333333294
    agent-4: 184.83333333333294
    agent-5: 184.83333333333294
  policy_reward_mean:
    agent-0: 153.82166666666663
    agent-1: 153.82166666666663
    agent-2: 153.82166666666663
    agent-3: 153.82166666666663
    agent-4: 153.82166666666663
    agent-5: 153.82166666666663
  policy_reward_min:
    agent-0: 45.166666666666615
    agent-1: 45.166666666666615
    agent-2: 45.166666666666615
    agent-3: 45.166666666666615
    agent-4: 45.166666666666615
    agent-5: 45.166666666666615
  sampler_perf:
    mean_env_wait_ms: 30.907232408491392
    mean_inference_ms: 14.590339336145428
    mean_processing_ms: 65.75668653417753
  time_since_restore: 34875.99524521828
  time_this_iter_s: 175.23432421684265
  time_total_s: 47426.812116622925
  timestamp: 1637070082
  timesteps_since_restore: 20352000
  timesteps_this_iter: 96000
  timesteps_total: 28032000
  training_iteration: 292
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    292 |          47426.8 | 28032000 |   922.93 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 77
    apples_agent-0_mean: 4.57
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 22.69
    apples_agent-1_min: 0
    apples_agent-2_max: 84
    apples_agent-2_mean: 3.45
    apples_agent-2_min: 0
    apples_agent-3_max: 303
    apples_agent-3_mean: 118.93
    apples_agent-3_min: 42
    apples_agent-4_max: 80
    apples_agent-4_mean: 3.81
    apples_agent-4_min: 0
    apples_agent-5_max: 260
    apples_agent-5_mean: 111.18
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 510
    cleaning_beam_agent-0_mean: 379.49
    cleaning_beam_agent-0_min: 206
    cleaning_beam_agent-1_max: 483
    cleaning_beam_agent-1_mean: 278.25
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 542
    cleaning_beam_agent-2_mean: 390.78
    cleaning_beam_agent-2_min: 188
    cleaning_beam_agent-3_max: 72
    cleaning_beam_agent-3_mean: 22.89
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 501
    cleaning_beam_agent-4_mean: 395.83
    cleaning_beam_agent-4_min: 218
    cleaning_beam_agent-5_max: 186
    cleaning_beam_agent-5_mean: 31.27
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-43-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1123.9999999999964
  episode_reward_mean: 932.1999999999867
  episode_reward_min: 566.0000000000042
  episodes_this_iter: 96
  episodes_total: 28128
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12969.525
    learner:
      agent-0:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0295578241348267
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019599576480686665
        model: {}
        policy_loss: -0.0036073392257094383
        total_loss: -0.003378190565854311
        vf_explained_var: 0.06490062177181244
        vf_loss: 20.411718368530273
      agent-1:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0977636575698853
        entropy_coeff: 0.0017600000137463212
        kl: 0.001492800423875451
        model: {}
        policy_loss: -0.004061633720993996
        total_loss: -0.003705634269863367
        vf_explained_var: -0.048901140689849854
        vf_loss: 22.88063621520996
      agent-2:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.072816252708435
        entropy_coeff: 0.0017600000137463212
        kl: 0.001424386166036129
        model: {}
        policy_loss: -0.0031286515295505524
        total_loss: -0.002958145923912525
        vf_explained_var: 0.046147748827934265
        vf_loss: 20.586652755737305
      agent-3:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4447588622570038
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009620845667086542
        model: {}
        policy_loss: -0.0023239129222929478
        total_loss: -0.0011822240194305778
        vf_explained_var: 0.10507239401340485
        vf_loss: 19.24464988708496
      agent-4:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9611207842826843
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015696481568738818
        model: {}
        policy_loss: -0.0037324619479477406
        total_loss: -0.0034151049330830574
        vf_explained_var: 0.07311002910137177
        vf_loss: 20.08931541442871
      agent-5:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7090462446212769
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015888474881649017
        model: {}
        policy_loss: -0.003021331736817956
        total_loss: -0.002399202436208725
        vf_explained_var: 0.13361015915870667
        vf_loss: 18.70052719116211
    load_time_ms: 18307.976
    num_steps_sampled: 28128000
    num_steps_trained: 28128000
    sample_time_ms: 130057.963
    update_time_ms: 87.703
  iterations_since_restore: 213
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.48744394618834
    ram_util_percent: 16.349327354260087
  pid: 14340
  policy_reward_max:
    agent-0: 187.33333333333303
    agent-1: 187.33333333333303
    agent-2: 187.33333333333303
    agent-3: 187.33333333333303
    agent-4: 187.33333333333303
    agent-5: 187.33333333333303
  policy_reward_mean:
    agent-0: 155.36666666666665
    agent-1: 155.36666666666665
    agent-2: 155.36666666666665
    agent-3: 155.36666666666665
    agent-4: 155.36666666666665
    agent-5: 155.36666666666665
  policy_reward_min:
    agent-0: 94.33333333333375
    agent-1: 94.33333333333375
    agent-2: 94.33333333333375
    agent-3: 94.33333333333375
    agent-4: 94.33333333333375
    agent-5: 94.33333333333375
  sampler_perf:
    mean_env_wait_ms: 30.911470458373405
    mean_inference_ms: 14.590680020491206
    mean_processing_ms: 65.75932207468695
  time_since_restore: 35032.339136600494
  time_this_iter_s: 156.3438913822174
  time_total_s: 47583.15600800514
  timestamp: 1637070239
  timesteps_since_restore: 20448000
  timesteps_this_iter: 96000
  timesteps_total: 28128000
  training_iteration: 293
  trial_id: '00000'
  
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
== Status ==
Memory usage on this node: 30.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    293 |          47583.2 | 28128000 |    932.2 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 3.52
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 24.65
    apples_agent-1_min: 0
    apples_agent-2_max: 273
    apples_agent-2_mean: 8.52
    apples_agent-2_min: 0
    apples_agent-3_max: 178
    apples_agent-3_mean: 112.77
    apples_agent-3_min: 32
    apples_agent-4_max: 112
    apples_agent-4_mean: 3.46
    apples_agent-4_min: 0
    apples_agent-5_max: 263
    apples_agent-5_mean: 109.63
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 595
    cleaning_beam_agent-0_mean: 380.16
    cleaning_beam_agent-0_min: 155
    cleaning_beam_agent-1_max: 510
    cleaning_beam_agent-1_mean: 265.02
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 594
    cleaning_beam_agent-2_mean: 373.03
    cleaning_beam_agent-2_min: 121
    cleaning_beam_agent-3_max: 136
    cleaning_beam_agent-3_mean: 25.33
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 531
    cleaning_beam_agent-4_mean: 409.01
    cleaning_beam_agent-4_min: 268
    cleaning_beam_agent-5_max: 300
    cleaning_beam_agent-5_mean: 37.44
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-46-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1105.9999999999934
  episode_reward_mean: 911.2299999999857
  episode_reward_min: 364.00000000000534
  episodes_this_iter: 96
  episodes_total: 28224
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13016.96
    learner:
      agent-0:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0162384510040283
        entropy_coeff: 0.0017600000137463212
        kl: 0.002109730150550604
        model: {}
        policy_loss: -0.0034024505876004696
        total_loss: -0.0028901705518364906
        vf_explained_var: 0.029132187366485596
        vf_loss: 23.008617401123047
      agent-1:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1069074869155884
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019676813390105963
        model: {}
        policy_loss: -0.003812649054452777
        total_loss: -0.003261934733018279
        vf_explained_var: -0.05230805277824402
        vf_loss: 24.988718032836914
      agent-2:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0768744945526123
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013726192992180586
        model: {}
        policy_loss: -0.0033608300145715475
        total_loss: -0.0030781810637563467
        vf_explained_var: 0.07927924394607544
        vf_loss: 21.779518127441406
      agent-3:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4658621549606323
        entropy_coeff: 0.0017600000137463212
        kl: 0.00119186716619879
        model: {}
        policy_loss: -0.0026585529558360577
        total_loss: -0.0014755597803741693
        vf_explained_var: 0.15348029136657715
        vf_loss: 20.029115676879883
      agent-4:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9545867443084717
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014096889644861221
        model: {}
        policy_loss: -0.0037369844503700733
        total_loss: -0.0032376348972320557
        vf_explained_var: 0.07936610281467438
        vf_loss: 21.794233322143555
      agent-5:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7125340104103088
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015145246870815754
        model: {}
        policy_loss: -0.0032531684264540672
        total_loss: -0.002504019532352686
        vf_explained_var: 0.1537344604730606
        vf_loss: 20.032123565673828
    load_time_ms: 18176.456
    num_steps_sampled: 28224000
    num_steps_trained: 28224000
    sample_time_ms: 130251.628
    update_time_ms: 88.406
  iterations_since_restore: 214
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.339013452914795
    ram_util_percent: 16.359641255605382
  pid: 14340
  policy_reward_max:
    agent-0: 184.3333333333327
    agent-1: 184.3333333333327
    agent-2: 184.3333333333327
    agent-3: 184.3333333333327
    agent-4: 184.3333333333327
    agent-5: 184.3333333333327
  policy_reward_mean:
    agent-0: 151.87166666666667
    agent-1: 151.87166666666667
    agent-2: 151.87166666666667
    agent-3: 151.87166666666667
    agent-4: 151.87166666666667
    agent-5: 151.87166666666667
  policy_reward_min:
    agent-0: 60.6666666666664
    agent-1: 60.6666666666664
    agent-2: 60.6666666666664
    agent-3: 60.6666666666664
    agent-4: 60.6666666666664
    agent-5: 60.6666666666664
  sampler_perf:
    mean_env_wait_ms: 30.915719191821303
    mean_inference_ms: 14.591408443073881
    mean_processing_ms: 65.76282228925092
  time_since_restore: 35189.06200647354
  time_this_iter_s: 156.72286987304688
  time_total_s: 47739.87887787819
  timestamp: 1637070395
  timesteps_since_restore: 20544000
  timesteps_this_iter: 96000
  timesteps_total: 28224000
  training_iteration: 294
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    294 |          47739.9 | 28224000 |   911.23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 3.21
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 23.45
    apples_agent-1_min: 0
    apples_agent-2_max: 174
    apples_agent-2_mean: 6.91
    apples_agent-2_min: 0
    apples_agent-3_max: 283
    apples_agent-3_mean: 113.81
    apples_agent-3_min: 46
    apples_agent-4_max: 68
    apples_agent-4_mean: 1.99
    apples_agent-4_min: 0
    apples_agent-5_max: 215
    apples_agent-5_mean: 113.86
    apples_agent-5_min: 53
    cleaning_beam_agent-0_max: 536
    cleaning_beam_agent-0_mean: 396.82
    cleaning_beam_agent-0_min: 163
    cleaning_beam_agent-1_max: 490
    cleaning_beam_agent-1_mean: 265.1
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 598
    cleaning_beam_agent-2_mean: 377.63
    cleaning_beam_agent-2_min: 173
    cleaning_beam_agent-3_max: 73
    cleaning_beam_agent-3_mean: 21.29
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 507
    cleaning_beam_agent-4_mean: 400.52
    cleaning_beam_agent-4_min: 225
    cleaning_beam_agent-5_max: 180
    cleaning_beam_agent-5_mean: 25.59
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-49-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1105.9999999999936
  episode_reward_mean: 941.2799999999869
  episode_reward_min: 360.0000000000011
  episodes_this_iter: 96
  episodes_total: 28320
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13018.451
    learner:
      agent-0:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0082943439483643
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016400720924139023
        model: {}
        policy_loss: -0.0035843169316649437
        total_loss: -0.003247862681746483
        vf_explained_var: 0.05507895350456238
        vf_loss: 21.110485076904297
      agent-1:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0982197523117065
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014521066332235932
        model: {}
        policy_loss: -0.003760435152798891
        total_loss: -0.003402921836823225
        vf_explained_var: -0.011983901262283325
        vf_loss: 22.90378189086914
      agent-2:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0666009187698364
        entropy_coeff: 0.0017600000137463212
        kl: 0.00143300904892385
        model: {}
        policy_loss: -0.0034351772628724575
        total_loss: -0.0030651804991066456
        vf_explained_var: -0.005486965179443359
        vf_loss: 22.47215461730957
      agent-3:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46021825075149536
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006579583860002458
        model: {}
        policy_loss: -0.0023137927055358887
        total_loss: -0.001170414499938488
        vf_explained_var: 0.12091562151908875
        vf_loss: 19.53362274169922
      agent-4:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9583524465560913
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014745426597073674
        model: {}
        policy_loss: -0.0035093463957309723
        total_loss: -0.003150824923068285
        vf_explained_var: 0.08648239076137543
        vf_loss: 20.45220947265625
      agent-5:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6929106712341309
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012789091560989618
        model: {}
        policy_loss: -0.0032357219606637955
        total_loss: -0.0025767190381884575
        vf_explained_var: 0.1495097279548645
        vf_loss: 18.785219192504883
    load_time_ms: 18129.833
    num_steps_sampled: 28320000
    num_steps_trained: 28320000
    sample_time_ms: 129872.08
    update_time_ms: 79.238
  iterations_since_restore: 215
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.375675675675673
    ram_util_percent: 15.805855855855858
  pid: 14340
  policy_reward_max:
    agent-0: 184.33333333333297
    agent-1: 184.33333333333297
    agent-2: 184.33333333333297
    agent-3: 184.33333333333297
    agent-4: 184.33333333333297
    agent-5: 184.33333333333297
  policy_reward_mean:
    agent-0: 156.87999999999997
    agent-1: 156.87999999999997
    agent-2: 156.87999999999997
    agent-3: 156.87999999999997
    agent-4: 156.87999999999997
    agent-5: 156.87999999999997
  policy_reward_min:
    agent-0: 59.99999999999989
    agent-1: 59.99999999999989
    agent-2: 59.99999999999989
    agent-3: 59.99999999999989
    agent-4: 59.99999999999989
    agent-5: 59.99999999999989
  sampler_perf:
    mean_env_wait_ms: 30.91996462811348
    mean_inference_ms: 14.59230376547017
    mean_processing_ms: 65.76607730725071
  time_since_restore: 35345.01341700554
  time_this_iter_s: 155.95141053199768
  time_total_s: 47895.83028841019
  timestamp: 1637070552
  timesteps_since_restore: 20640000
  timesteps_this_iter: 96000
  timesteps_total: 28320000
  training_iteration: 295
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    295 |          47895.8 | 28320000 |   941.28 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 99
    apples_agent-0_mean: 5.03
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 24.98
    apples_agent-1_min: 0
    apples_agent-2_max: 84
    apples_agent-2_mean: 7.19
    apples_agent-2_min: 0
    apples_agent-3_max: 187
    apples_agent-3_mean: 113.91
    apples_agent-3_min: 25
    apples_agent-4_max: 194
    apples_agent-4_mean: 4.39
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 110.37
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 560
    cleaning_beam_agent-0_mean: 398.58
    cleaning_beam_agent-0_min: 249
    cleaning_beam_agent-1_max: 463
    cleaning_beam_agent-1_mean: 248.07
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 572
    cleaning_beam_agent-2_mean: 374.87
    cleaning_beam_agent-2_min: 137
    cleaning_beam_agent-3_max: 78
    cleaning_beam_agent-3_mean: 22.55
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 525
    cleaning_beam_agent-4_mean: 402.06
    cleaning_beam_agent-4_min: 244
    cleaning_beam_agent-5_max: 72
    cleaning_beam_agent-5_mean: 20.84
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-51-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1137.9999999999834
  episode_reward_mean: 929.9199999999863
  episode_reward_min: 346.00000000000585
  episodes_this_iter: 96
  episodes_total: 28416
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13032.265
    learner:
      agent-0:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0170623064041138
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017008727882057428
        model: {}
        policy_loss: -0.0031036946456879377
        total_loss: -0.002810379955917597
        vf_explained_var: 0.025351792573928833
        vf_loss: 20.833446502685547
      agent-1:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1178956031799316
        entropy_coeff: 0.0017600000137463212
        kl: 0.001653251121751964
        model: {}
        policy_loss: -0.004085269756615162
        total_loss: -0.003806048072874546
        vf_explained_var: -0.03772038221359253
        vf_loss: 22.467174530029297
      agent-2:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0832628011703491
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013288477202877402
        model: {}
        policy_loss: -0.0034621150698512793
        total_loss: -0.00326994014903903
        vf_explained_var: 0.021989375352859497
        vf_loss: 20.987144470214844
      agent-3:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4681406617164612
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009374830988235772
        model: {}
        policy_loss: -0.0024192086420953274
        total_loss: -0.001327036414295435
        vf_explained_var: 0.1018858551979065
        vf_loss: 19.161022186279297
      agent-4:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9675017595291138
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011172824306413531
        model: {}
        policy_loss: -0.0034114113077521324
        total_loss: -0.0031577826011925936
        vf_explained_var: 0.08417151868343353
        vf_loss: 19.564300537109375
      agent-5:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6826092004776001
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012958610896021128
        model: {}
        policy_loss: -0.0029776189476251602
        total_loss: -0.002340150997042656
        vf_explained_var: 0.13369935750961304
        vf_loss: 18.388593673706055
    load_time_ms: 18020.499
    num_steps_sampled: 28416000
    num_steps_trained: 28416000
    sample_time_ms: 129837.771
    update_time_ms: 76.639
  iterations_since_restore: 216
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.226126126126125
    ram_util_percent: 16.354054054054053
  pid: 14340
  policy_reward_max:
    agent-0: 189.6666666666663
    agent-1: 189.6666666666663
    agent-2: 189.6666666666663
    agent-3: 189.6666666666663
    agent-4: 189.6666666666663
    agent-5: 189.6666666666663
  policy_reward_mean:
    agent-0: 154.98666666666662
    agent-1: 154.98666666666662
    agent-2: 154.98666666666662
    agent-3: 154.98666666666662
    agent-4: 154.98666666666662
    agent-5: 154.98666666666662
  policy_reward_min:
    agent-0: 57.66666666666638
    agent-1: 57.66666666666638
    agent-2: 57.66666666666638
    agent-3: 57.66666666666638
    agent-4: 57.66666666666638
    agent-5: 57.66666666666638
  sampler_perf:
    mean_env_wait_ms: 30.924202078719656
    mean_inference_ms: 14.593132351566998
    mean_processing_ms: 65.77108695387032
  time_since_restore: 35500.70594286919
  time_this_iter_s: 155.69252586364746
  time_total_s: 48051.522814273834
  timestamp: 1637070707
  timesteps_since_restore: 20736000
  timesteps_this_iter: 96000
  timesteps_total: 28416000
  training_iteration: 296
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    296 |          48051.5 | 28416000 |   929.92 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 99
    apples_agent-0_mean: 4.18
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 24.32
    apples_agent-1_min: 0
    apples_agent-2_max: 149
    apples_agent-2_mean: 5.71
    apples_agent-2_min: 0
    apples_agent-3_max: 198
    apples_agent-3_mean: 115.89
    apples_agent-3_min: 58
    apples_agent-4_max: 37
    apples_agent-4_mean: 0.65
    apples_agent-4_min: 0
    apples_agent-5_max: 188
    apples_agent-5_mean: 115.04
    apples_agent-5_min: 63
    cleaning_beam_agent-0_max: 529
    cleaning_beam_agent-0_mean: 390.5
    cleaning_beam_agent-0_min: 239
    cleaning_beam_agent-1_max: 470
    cleaning_beam_agent-1_mean: 253.77
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 535
    cleaning_beam_agent-2_mean: 379.46
    cleaning_beam_agent-2_min: 193
    cleaning_beam_agent-3_max: 89
    cleaning_beam_agent-3_mean: 19.01
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 489
    cleaning_beam_agent-4_mean: 403.5
    cleaning_beam_agent-4_min: 294
    cleaning_beam_agent-5_max: 141
    cleaning_beam_agent-5_mean: 21.21
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-54-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1118.9999999999866
  episode_reward_mean: 960.1099999999853
  episode_reward_min: 770.999999999981
  episodes_this_iter: 96
  episodes_total: 28512
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13045.439
    learner:
      agent-0:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0122090578079224
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015162334311753511
        model: {}
        policy_loss: -0.0030392573680728674
        total_loss: -0.0028575281612575054
        vf_explained_var: 0.02114565670490265
        vf_loss: 19.6321964263916
      agent-1:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1022858619689941
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016458877362310886
        model: {}
        policy_loss: -0.0039480337873101234
        total_loss: -0.0036991601809859276
        vf_explained_var: -0.05718931555747986
        vf_loss: 21.888967514038086
      agent-2:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0805050134658813
        entropy_coeff: 0.0017600000137463212
        kl: 0.001863268087618053
        model: {}
        policy_loss: -0.003537384793162346
        total_loss: -0.0033865461591631174
        vf_explained_var: -0.007167980074882507
        vf_loss: 20.5252685546875
      agent-3:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4480249881744385
        entropy_coeff: 0.0017600000137463212
        kl: 0.000947125896345824
        model: {}
        policy_loss: -0.002219817601144314
        total_loss: -0.001153365010395646
        vf_explained_var: 0.06365557014942169
        vf_loss: 18.549766540527344
      agent-4:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.959977924823761
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011774664744734764
        model: {}
        policy_loss: -0.0036630036775022745
        total_loss: -0.003398985369130969
        vf_explained_var: 0.020630881190299988
        vf_loss: 19.535810470581055
      agent-5:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6700820922851562
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018142365152016282
        model: {}
        policy_loss: -0.002941901097074151
        total_loss: -0.002363154198974371
        vf_explained_var: 0.11382018029689789
        vf_loss: 17.580909729003906
    load_time_ms: 17128.624
    num_steps_sampled: 28512000
    num_steps_trained: 28512000
    sample_time_ms: 129940.421
    update_time_ms: 66.629
  iterations_since_restore: 217
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.397321428571427
    ram_util_percent: 16.544642857142854
  pid: 14340
  policy_reward_max:
    agent-0: 186.5000000000001
    agent-1: 186.5000000000001
    agent-2: 186.5000000000001
    agent-3: 186.5000000000001
    agent-4: 186.5000000000001
    agent-5: 186.5000000000001
  policy_reward_mean:
    agent-0: 160.01833333333326
    agent-1: 160.01833333333326
    agent-2: 160.01833333333326
    agent-3: 160.01833333333326
    agent-4: 160.01833333333326
    agent-5: 160.01833333333326
  policy_reward_min:
    agent-0: 128.50000000000057
    agent-1: 128.50000000000057
    agent-2: 128.50000000000057
    agent-3: 128.50000000000057
    agent-4: 128.50000000000057
    agent-5: 128.50000000000057
  sampler_perf:
    mean_env_wait_ms: 30.927811902515
    mean_inference_ms: 14.593488085364061
    mean_processing_ms: 65.77494369293626
  time_since_restore: 35657.673144340515
  time_this_iter_s: 156.96720147132874
  time_total_s: 48208.49001574516
  timestamp: 1637070864
  timesteps_since_restore: 20832000
  timesteps_this_iter: 96000
  timesteps_total: 28512000
  training_iteration: 297
  trial_id: '00000'
  
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
== Status ==
Memory usage on this node: 30.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    297 |          48208.5 | 28512000 |   960.11 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 4.94
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 26.53
    apples_agent-1_min: 0
    apples_agent-2_max: 141
    apples_agent-2_mean: 3.77
    apples_agent-2_min: 0
    apples_agent-3_max: 198
    apples_agent-3_mean: 115.16
    apples_agent-3_min: 44
    apples_agent-4_max: 62
    apples_agent-4_mean: 2.33
    apples_agent-4_min: 0
    apples_agent-5_max: 226
    apples_agent-5_mean: 110.5
    apples_agent-5_min: 56
    cleaning_beam_agent-0_max: 538
    cleaning_beam_agent-0_mean: 386.39
    cleaning_beam_agent-0_min: 168
    cleaning_beam_agent-1_max: 470
    cleaning_beam_agent-1_mean: 245.68
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 542
    cleaning_beam_agent-2_mean: 389.79
    cleaning_beam_agent-2_min: 112
    cleaning_beam_agent-3_max: 89
    cleaning_beam_agent-3_mean: 21.04
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 507
    cleaning_beam_agent-4_mean: 398.27
    cleaning_beam_agent-4_min: 287
    cleaning_beam_agent-5_max: 127
    cleaning_beam_agent-5_mean: 24.75
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-57-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1183.0000000000025
  episode_reward_mean: 932.7099999999855
  episode_reward_min: 492.00000000001137
  episodes_this_iter: 96
  episodes_total: 28608
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13035.297
    learner:
      agent-0:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0258889198303223
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014707788359373808
        model: {}
        policy_loss: -0.0031933116260915995
        total_loss: -0.0030131209641695023
        vf_explained_var: 0.07268339395523071
        vf_loss: 19.857528686523438
      agent-1:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1045701503753662
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017146860482171178
        model: {}
        policy_loss: -0.003950599581003189
        total_loss: -0.003669493831694126
        vf_explained_var: -0.02675396203994751
        vf_loss: 22.251544952392578
      agent-2:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0840054750442505
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016448921523988247
        model: {}
        policy_loss: -0.0033481717109680176
        total_loss: -0.0030961884185671806
        vf_explained_var: -0.007993534207344055
        vf_loss: 21.598337173461914
      agent-3:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46805962920188904
        entropy_coeff: 0.0017600000137463212
        kl: 0.001083242241293192
        model: {}
        policy_loss: -0.0023064420092850924
        total_loss: -0.0012032019440084696
        vf_explained_var: 0.09673362970352173
        vf_loss: 19.270248413085938
      agent-4:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9683567881584167
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015707637649029493
        model: {}
        policy_loss: -0.0036370293237268925
        total_loss: -0.003331222105771303
        vf_explained_var: 0.056428611278533936
        vf_loss: 20.101163864135742
      agent-5:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6903992891311646
        entropy_coeff: 0.0017600000137463212
        kl: 0.001482920371927321
        model: {}
        policy_loss: -0.0031435517594218254
        total_loss: -0.002496459288522601
        vf_explained_var: 0.12377357482910156
        vf_loss: 18.62198257446289
    load_time_ms: 17095.341
    num_steps_sampled: 28608000
    num_steps_trained: 28608000
    sample_time_ms: 129710.736
    update_time_ms: 66.741
  iterations_since_restore: 218
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.20135746606335
    ram_util_percent: 16.528959276018096
  pid: 14340
  policy_reward_max:
    agent-0: 197.16666666666654
    agent-1: 197.16666666666654
    agent-2: 197.16666666666654
    agent-3: 197.16666666666654
    agent-4: 197.16666666666654
    agent-5: 197.16666666666654
  policy_reward_mean:
    agent-0: 155.4516666666666
    agent-1: 155.4516666666666
    agent-2: 155.4516666666666
    agent-3: 155.4516666666666
    agent-4: 155.4516666666666
    agent-5: 155.4516666666666
  policy_reward_min:
    agent-0: 82.00000000000006
    agent-1: 82.00000000000006
    agent-2: 82.00000000000006
    agent-3: 82.00000000000006
    agent-4: 82.00000000000006
    agent-5: 82.00000000000006
  sampler_perf:
    mean_env_wait_ms: 30.930950630158105
    mean_inference_ms: 14.593584938654425
    mean_processing_ms: 65.7777675468363
  time_since_restore: 35813.49662208557
  time_this_iter_s: 155.82347774505615
  time_total_s: 48364.31349349022
  timestamp: 1637071020
  timesteps_since_restore: 20928000
  timesteps_this_iter: 96000
  timesteps_total: 28608000
  training_iteration: 298
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    298 |          48364.3 | 28608000 |   932.71 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 6.07
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 23.69
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 6.27
    apples_agent-2_min: 0
    apples_agent-3_max: 161
    apples_agent-3_mean: 112.76
    apples_agent-3_min: 42
    apples_agent-4_max: 102
    apples_agent-4_mean: 2.87
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 102.04
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 487
    cleaning_beam_agent-0_mean: 350.26
    cleaning_beam_agent-0_min: 82
    cleaning_beam_agent-1_max: 407
    cleaning_beam_agent-1_mean: 250.21
    cleaning_beam_agent-1_min: 134
    cleaning_beam_agent-2_max: 513
    cleaning_beam_agent-2_mean: 370.84
    cleaning_beam_agent-2_min: 147
    cleaning_beam_agent-3_max: 192
    cleaning_beam_agent-3_mean: 23.71
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 583
    cleaning_beam_agent-4_mean: 394.48
    cleaning_beam_agent-4_min: 195
    cleaning_beam_agent-5_max: 317
    cleaning_beam_agent-5_mean: 30.51
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_08-59-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1161.0000000000023
  episode_reward_mean: 908.8499999999871
  episode_reward_min: 351.00000000000074
  episodes_this_iter: 96
  episodes_total: 28704
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13039.954
    learner:
      agent-0:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.02084219455719
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013224199647083879
        model: {}
        policy_loss: -0.002943628001958132
        total_loss: -0.0027689156122505665
        vf_explained_var: 0.11116200685501099
        vf_loss: 19.713985443115234
      agent-1:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1145176887512207
        entropy_coeff: 0.0017600000137463212
        kl: 0.002708165207877755
        model: {}
        policy_loss: -0.00420679897069931
        total_loss: -0.003862516488879919
        vf_explained_var: -0.03802010416984558
        vf_loss: 23.058340072631836
      agent-2:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.082869529724121
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016421389300376177
        model: {}
        policy_loss: -0.0035599181428551674
        total_loss: -0.0032133206259459257
        vf_explained_var: -0.015108436346054077
        vf_loss: 22.524465560913086
      agent-3:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49951061606407166
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014044428244233131
        model: {}
        policy_loss: -0.0028384029865264893
        total_loss: -0.0018375460058450699
        vf_explained_var: 0.15038417279720306
        vf_loss: 18.800003051757812
      agent-4:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9720898866653442
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016202249098569155
        model: {}
        policy_loss: -0.003815028117969632
        total_loss: -0.003473254619166255
        vf_explained_var: 0.07384920120239258
        vf_loss: 20.52652359008789
      agent-5:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7014685273170471
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013629175955429673
        model: {}
        policy_loss: -0.0033753702882677317
        total_loss: -0.0027146735228598118
        vf_explained_var: 0.14670465886592865
        vf_loss: 18.952831268310547
    load_time_ms: 16621.053
    num_steps_sampled: 28704000
    num_steps_trained: 28704000
    sample_time_ms: 129513.504
    update_time_ms: 63.871
  iterations_since_restore: 219
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.214718614718617
    ram_util_percent: 16.68311688311688
  pid: 14340
  policy_reward_max:
    agent-0: 193.49999999999957
    agent-1: 193.49999999999957
    agent-2: 193.49999999999957
    agent-3: 193.49999999999957
    agent-4: 193.49999999999957
    agent-5: 193.49999999999957
  policy_reward_mean:
    agent-0: 151.47500000000002
    agent-1: 151.47500000000002
    agent-2: 151.47500000000002
    agent-3: 151.47500000000002
    agent-4: 151.47500000000002
    agent-5: 151.47500000000002
  policy_reward_min:
    agent-0: 58.499999999999886
    agent-1: 58.499999999999886
    agent-2: 58.499999999999886
    agent-3: 58.499999999999886
    agent-4: 58.499999999999886
    agent-5: 58.499999999999886
  sampler_perf:
    mean_env_wait_ms: 30.933182080337897
    mean_inference_ms: 14.593859146749274
    mean_processing_ms: 65.78083880373627
  time_since_restore: 35975.73221564293
  time_this_iter_s: 162.2355935573578
  time_total_s: 48526.54908704758
  timestamp: 1637071183
  timesteps_since_restore: 21024000
  timesteps_this_iter: 96000
  timesteps_total: 28704000
  training_iteration: 299
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    299 |          48526.5 | 28704000 |   908.85 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 72
    apples_agent-0_mean: 3.56
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 25.55
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 4.11
    apples_agent-2_min: 0
    apples_agent-3_max: 166
    apples_agent-3_mean: 115.86
    apples_agent-3_min: 69
    apples_agent-4_max: 165
    apples_agent-4_mean: 2.49
    apples_agent-4_min: 0
    apples_agent-5_max: 178
    apples_agent-5_mean: 103.06
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 514
    cleaning_beam_agent-0_mean: 365.25
    cleaning_beam_agent-0_min: 82
    cleaning_beam_agent-1_max: 394
    cleaning_beam_agent-1_mean: 232.89
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 583
    cleaning_beam_agent-2_mean: 373.81
    cleaning_beam_agent-2_min: 214
    cleaning_beam_agent-3_max: 68
    cleaning_beam_agent-3_mean: 19.98
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 508
    cleaning_beam_agent-4_mean: 397.33
    cleaning_beam_agent-4_min: 257
    cleaning_beam_agent-5_max: 256
    cleaning_beam_agent-5_mean: 24.69
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-02-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1133.9999999999907
  episode_reward_mean: 950.3199999999863
  episode_reward_min: 591.0000000000005
  episodes_this_iter: 96
  episodes_total: 28800
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13013.129
    learner:
      agent-0:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0116453170776367
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018221234204247594
        model: {}
        policy_loss: -0.002898481208831072
        total_loss: -0.0026039951480925083
        vf_explained_var: 0.017330393195152283
        vf_loss: 20.749801635742188
      agent-1:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1201280355453491
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019013667479157448
        model: {}
        policy_loss: -0.004207033198326826
        total_loss: -0.003894046414643526
        vf_explained_var: -0.05370098352432251
        vf_loss: 22.844158172607422
      agent-2:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0963335037231445
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018200090853497386
        model: {}
        policy_loss: -0.0033641995396465063
        total_loss: -0.0032025007531046867
        vf_explained_var: 0.026214271783828735
        vf_loss: 20.912437438964844
      agent-3:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45815351605415344
        entropy_coeff: 0.0017600000137463212
        kl: 0.001277407631278038
        model: {}
        policy_loss: -0.002670799382030964
        total_loss: -0.0014852797612547874
        vf_explained_var: 0.05798569321632385
        vf_loss: 19.918704986572266
      agent-4:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9659727215766907
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016058176988735795
        model: {}
        policy_loss: -0.003518129466101527
        total_loss: -0.0031888249795883894
        vf_explained_var: 0.04135788977146149
        vf_loss: 20.294164657592773
      agent-5:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6500508785247803
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013089101994410157
        model: {}
        policy_loss: -0.002753754146397114
        total_loss: -0.001998899970203638
        vf_explained_var: 0.09460684657096863
        vf_loss: 18.989484786987305
    load_time_ms: 16533.358
    num_steps_sampled: 28800000
    num_steps_trained: 28800000
    sample_time_ms: 129320.559
    update_time_ms: 62.69
  iterations_since_restore: 220
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 29.085135135135133
    ram_util_percent: 16.61846846846847
  pid: 14340
  policy_reward_max:
    agent-0: 188.99999999999966
    agent-1: 188.99999999999966
    agent-2: 188.99999999999966
    agent-3: 188.99999999999966
    agent-4: 188.99999999999966
    agent-5: 188.99999999999966
  policy_reward_mean:
    agent-0: 158.38666666666657
    agent-1: 158.38666666666657
    agent-2: 158.38666666666657
    agent-3: 158.38666666666657
    agent-4: 158.38666666666657
    agent-5: 158.38666666666657
  policy_reward_min:
    agent-0: 98.50000000000011
    agent-1: 98.50000000000011
    agent-2: 98.50000000000011
    agent-3: 98.50000000000011
    agent-4: 98.50000000000011
    agent-5: 98.50000000000011
  sampler_perf:
    mean_env_wait_ms: 30.935147647115038
    mean_inference_ms: 14.594123079394796
    mean_processing_ms: 65.78462909347309
  time_since_restore: 36131.72667789459
  time_this_iter_s: 155.9944622516632
  time_total_s: 48682.54354929924
  timestamp: 1637071339
  timesteps_since_restore: 21120000
  timesteps_this_iter: 96000
  timesteps_total: 28800000
  training_iteration: 300
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    300 |          48682.5 | 28800000 |   950.32 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 93
    apples_agent-0_mean: 3.35
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 28.0
    apples_agent-1_min: 0
    apples_agent-2_max: 157
    apples_agent-2_mean: 4.65
    apples_agent-2_min: 0
    apples_agent-3_max: 174
    apples_agent-3_mean: 109.34
    apples_agent-3_min: 43
    apples_agent-4_max: 88
    apples_agent-4_mean: 3.8
    apples_agent-4_min: 0
    apples_agent-5_max: 154
    apples_agent-5_mean: 102.97
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 564
    cleaning_beam_agent-0_mean: 382.26
    cleaning_beam_agent-0_min: 190
    cleaning_beam_agent-1_max: 410
    cleaning_beam_agent-1_mean: 214.71
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 483
    cleaning_beam_agent-2_mean: 357.9
    cleaning_beam_agent-2_min: 129
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 22.86
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 480
    cleaning_beam_agent-4_mean: 397.42
    cleaning_beam_agent-4_min: 229
    cleaning_beam_agent-5_max: 178
    cleaning_beam_agent-5_mean: 27.81
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-04-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1106.000000000002
  episode_reward_mean: 939.8399999999867
  episode_reward_min: 296.99999999999903
  episodes_this_iter: 96
  episodes_total: 28896
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13026.946
    learner:
      agent-0:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0156363248825073
        entropy_coeff: 0.0017600000137463212
        kl: 0.00139248080085963
        model: {}
        policy_loss: -0.002874256344512105
        total_loss: -0.0026035418268293142
        vf_explained_var: 0.019572749733924866
        vf_loss: 20.582351684570312
      agent-1:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1358309984207153
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012721250532194972
        model: {}
        policy_loss: -0.0037264442071318626
        total_loss: -0.003526470623910427
        vf_explained_var: -0.03270122408866882
        vf_loss: 21.990327835083008
      agent-2:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0861949920654297
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018885699100792408
        model: {}
        policy_loss: -0.003704104572534561
        total_loss: -0.003440466709434986
        vf_explained_var: -0.022471904754638672
        vf_loss: 21.753433227539062
      agent-3:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46461430191993713
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016595234628766775
        model: {}
        policy_loss: -0.002799880923703313
        total_loss: -0.001704951748251915
        vf_explained_var: 0.0907631516456604
        vf_loss: 19.126497268676758
      agent-4:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9723082780838013
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016415755962952971
        model: {}
        policy_loss: -0.0038082506507635117
        total_loss: -0.0035596834495663643
        vf_explained_var: 0.06648223102092743
        vf_loss: 19.59829330444336
      agent-5:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6654320955276489
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018616608576849103
        model: {}
        policy_loss: -0.0032422607764601707
        total_loss: -0.0025393469259142876
        vf_explained_var: 0.10540194809436798
        vf_loss: 18.740705490112305
    load_time_ms: 16558.911
    num_steps_sampled: 28896000
    num_steps_trained: 28896000
    sample_time_ms: 129144.634
    update_time_ms: 49.505
  iterations_since_restore: 221
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.860352422907486
    ram_util_percent: 15.767841409691632
  pid: 14340
  policy_reward_max:
    agent-0: 184.33333333333348
    agent-1: 184.33333333333348
    agent-2: 184.33333333333348
    agent-3: 184.33333333333348
    agent-4: 184.33333333333348
    agent-5: 184.33333333333348
  policy_reward_mean:
    agent-0: 156.63999999999996
    agent-1: 156.63999999999996
    agent-2: 156.63999999999996
    agent-3: 156.63999999999996
    agent-4: 156.63999999999996
    agent-5: 156.63999999999996
  policy_reward_min:
    agent-0: 49.499999999999915
    agent-1: 49.499999999999915
    agent-2: 49.499999999999915
    agent-3: 49.499999999999915
    agent-4: 49.499999999999915
    agent-5: 49.499999999999915
  sampler_perf:
    mean_env_wait_ms: 30.936925645240034
    mean_inference_ms: 14.59454134061626
    mean_processing_ms: 65.79240026849271
  time_since_restore: 36289.413007974625
  time_this_iter_s: 157.68633008003235
  time_total_s: 48840.22987937927
  timestamp: 1637071498
  timesteps_since_restore: 21216000
  timesteps_this_iter: 96000
  timesteps_total: 28896000
  training_iteration: 301
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    301 |          48840.2 | 28896000 |   939.84 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 91
    apples_agent-0_mean: 3.59
    apples_agent-0_min: 0
    apples_agent-1_max: 115
    apples_agent-1_mean: 26.3
    apples_agent-1_min: 0
    apples_agent-2_max: 71
    apples_agent-2_mean: 5.26
    apples_agent-2_min: 0
    apples_agent-3_max: 173
    apples_agent-3_mean: 111.4
    apples_agent-3_min: 56
    apples_agent-4_max: 124
    apples_agent-4_mean: 4.0
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 101.75
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 560
    cleaning_beam_agent-0_mean: 377.45
    cleaning_beam_agent-0_min: 148
    cleaning_beam_agent-1_max: 389
    cleaning_beam_agent-1_mean: 216.2
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 598
    cleaning_beam_agent-2_mean: 374.84
    cleaning_beam_agent-2_min: 203
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 18.73
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 486
    cleaning_beam_agent-4_mean: 388.6
    cleaning_beam_agent-4_min: 117
    cleaning_beam_agent-5_max: 155
    cleaning_beam_agent-5_mean: 27.72
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-07-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1108.9999999999939
  episode_reward_mean: 931.749999999986
  episode_reward_min: 351.0000000000065
  episodes_this_iter: 96
  episodes_total: 28992
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13002.653
    learner:
      agent-0:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0150971412658691
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016673268983140588
        model: {}
        policy_loss: -0.0031912382692098618
        total_loss: -0.002772131934762001
        vf_explained_var: 0.07815736532211304
        vf_loss: 22.056772232055664
      agent-1:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.122565507888794
        entropy_coeff: 0.0017600000137463212
        kl: 0.001580598996952176
        model: {}
        policy_loss: -0.004001223016530275
        total_loss: -0.003442479530349374
        vf_explained_var: -0.04305136203765869
        vf_loss: 25.344562530517578
      agent-2:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.075836420059204
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016327614430338144
        model: {}
        policy_loss: -0.003677558619529009
        total_loss: -0.0031935032457113266
        vf_explained_var: 0.014211907982826233
        vf_loss: 23.77529525756836
      agent-3:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4655366539955139
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009294464252889156
        model: {}
        policy_loss: -0.0023917946964502335
        total_loss: -0.00118667958304286
        vf_explained_var: 0.15576143562793732
        vf_loss: 20.244611740112305
      agent-4:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9793761968612671
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020008173305541277
        model: {}
        policy_loss: -0.003860768862068653
        total_loss: -0.0034482404589653015
        vf_explained_var: 0.11311334371566772
        vf_loss: 21.362279891967773
      agent-5:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6800714731216431
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010958313941955566
        model: {}
        policy_loss: -0.003241718513891101
        total_loss: -0.0024857379030436277
        vf_explained_var: 0.18393270671367645
        vf_loss: 19.529048919677734
    load_time_ms: 14805.685
    num_steps_sampled: 28992000
    num_steps_trained: 28992000
    sample_time_ms: 129021.151
    update_time_ms: 47.087
  iterations_since_restore: 222
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.64260089686099
    ram_util_percent: 14.701345291479823
  pid: 14340
  policy_reward_max:
    agent-0: 184.8333333333331
    agent-1: 184.8333333333331
    agent-2: 184.8333333333331
    agent-3: 184.8333333333331
    agent-4: 184.8333333333331
    agent-5: 184.8333333333331
  policy_reward_mean:
    agent-0: 155.29166666666663
    agent-1: 155.29166666666663
    agent-2: 155.29166666666663
    agent-3: 155.29166666666663
    agent-4: 155.29166666666663
    agent-5: 155.29166666666663
  policy_reward_min:
    agent-0: 58.49999999999984
    agent-1: 58.49999999999984
    agent-2: 58.49999999999984
    agent-3: 58.49999999999984
    agent-4: 58.49999999999984
    agent-5: 58.49999999999984
  sampler_perf:
    mean_env_wait_ms: 30.937286408318176
    mean_inference_ms: 14.59426457567782
    mean_processing_ms: 65.79111977604128
  time_since_restore: 36445.63097381592
  time_this_iter_s: 156.21796584129333
  time_total_s: 48996.447845220566
  timestamp: 1637071655
  timesteps_since_restore: 21312000
  timesteps_this_iter: 96000
  timesteps_total: 28992000
  training_iteration: 302
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    302 |          48996.4 | 28992000 |   931.75 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 2.64
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 24.59
    apples_agent-1_min: 0
    apples_agent-2_max: 145
    apples_agent-2_mean: 7.15
    apples_agent-2_min: 0
    apples_agent-3_max: 178
    apples_agent-3_mean: 109.59
    apples_agent-3_min: 35
    apples_agent-4_max: 60
    apples_agent-4_mean: 2.3
    apples_agent-4_min: 0
    apples_agent-5_max: 167
    apples_agent-5_mean: 100.64
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 504
    cleaning_beam_agent-0_mean: 364.61
    cleaning_beam_agent-0_min: 148
    cleaning_beam_agent-1_max: 344
    cleaning_beam_agent-1_mean: 195.91
    cleaning_beam_agent-1_min: 78
    cleaning_beam_agent-2_max: 537
    cleaning_beam_agent-2_mean: 375.88
    cleaning_beam_agent-2_min: 166
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 23.52
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 501
    cleaning_beam_agent-4_mean: 389.4
    cleaning_beam_agent-4_min: 151
    cleaning_beam_agent-5_max: 188
    cleaning_beam_agent-5_mean: 25.02
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-10-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1125.999999999987
  episode_reward_mean: 927.5999999999873
  episode_reward_min: 327.999999999999
  episodes_this_iter: 96
  episodes_total: 29088
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13002.914
    learner:
      agent-0:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0261151790618896
        entropy_coeff: 0.0017600000137463212
        kl: 0.001173228258267045
        model: {}
        policy_loss: -0.0031375165563076735
        total_loss: -0.0027261381037533283
        vf_explained_var: 0.06856098771095276
        vf_loss: 22.173419952392578
      agent-1:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1361008882522583
        entropy_coeff: 0.0017600000137463212
        kl: 0.001513963332399726
        model: {}
        policy_loss: -0.0037849792279303074
        total_loss: -0.003348573110997677
        vf_explained_var: -0.011870801448822021
        vf_loss: 24.35942268371582
      agent-2:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.080617904663086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013613022165372968
        model: {}
        policy_loss: -0.003284694626927376
        total_loss: -0.0028003263287246227
        vf_explained_var: -2.7194619178771973e-05
        vf_loss: 23.86254119873047
      agent-3:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46087396144866943
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015379912219941616
        model: {}
        policy_loss: -0.0027335677295923233
        total_loss: -0.0015735295601189137
        vf_explained_var: 0.17263810336589813
        vf_loss: 19.711780548095703
      agent-4:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9728216528892517
        entropy_coeff: 0.0017600000137463212
        kl: 0.001936664804816246
        model: {}
        policy_loss: -0.003992027603089809
        total_loss: -0.003540009493008256
        vf_explained_var: 0.09304985404014587
        vf_loss: 21.6418514251709
      agent-5:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6606555581092834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012896552216261625
        model: {}
        policy_loss: -0.0031869644299149513
        total_loss: -0.0024003800936043262
        vf_explained_var: 0.1785907745361328
        vf_loss: 19.493379592895508
    load_time_ms: 14838.557
    num_steps_sampled: 29088000
    num_steps_trained: 29088000
    sample_time_ms: 128932.01
    update_time_ms: 44.441
  iterations_since_restore: 223
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.735585585585586
    ram_util_percent: 14.753153153153157
  pid: 14340
  policy_reward_max:
    agent-0: 187.66666666666723
    agent-1: 187.66666666666723
    agent-2: 187.66666666666723
    agent-3: 187.66666666666723
    agent-4: 187.66666666666723
    agent-5: 187.66666666666723
  policy_reward_mean:
    agent-0: 154.59999999999994
    agent-1: 154.59999999999994
    agent-2: 154.59999999999994
    agent-3: 154.59999999999994
    agent-4: 154.59999999999994
    agent-5: 154.59999999999994
  policy_reward_min:
    agent-0: 54.66666666666665
    agent-1: 54.66666666666665
    agent-2: 54.66666666666665
    agent-3: 54.66666666666665
    agent-4: 54.66666666666665
    agent-5: 54.66666666666665
  sampler_perf:
    mean_env_wait_ms: 30.937434349310962
    mean_inference_ms: 14.593896028236673
    mean_processing_ms: 65.79034645428801
  time_since_restore: 36601.44407463074
  time_this_iter_s: 155.81310081481934
  time_total_s: 49152.260946035385
  timestamp: 1637071811
  timesteps_since_restore: 21408000
  timesteps_this_iter: 96000
  timesteps_total: 29088000
  training_iteration: 303
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    303 |          49152.3 | 29088000 |    927.6 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 2.9
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 26.93
    apples_agent-1_min: 0
    apples_agent-2_max: 173
    apples_agent-2_mean: 8.09
    apples_agent-2_min: 0
    apples_agent-3_max: 178
    apples_agent-3_mean: 110.97
    apples_agent-3_min: 25
    apples_agent-4_max: 58
    apples_agent-4_mean: 2.41
    apples_agent-4_min: 0
    apples_agent-5_max: 183
    apples_agent-5_mean: 106.16
    apples_agent-5_min: 47
    cleaning_beam_agent-0_max: 483
    cleaning_beam_agent-0_mean: 371.97
    cleaning_beam_agent-0_min: 127
    cleaning_beam_agent-1_max: 356
    cleaning_beam_agent-1_mean: 204.9
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 519
    cleaning_beam_agent-2_mean: 363.51
    cleaning_beam_agent-2_min: 144
    cleaning_beam_agent-3_max: 84
    cleaning_beam_agent-3_mean: 26.3
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 473
    cleaning_beam_agent-4_mean: 396.77
    cleaning_beam_agent-4_min: 293
    cleaning_beam_agent-5_max: 166
    cleaning_beam_agent-5_mean: 29.02
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 4
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-13-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1143.000000000008
  episode_reward_mean: 926.0199999999872
  episode_reward_min: 303.99999999999704
  episodes_this_iter: 96
  episodes_total: 29184
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12946.352
    learner:
      agent-0:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.016700029373169
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012186961248517036
        model: {}
        policy_loss: -0.003111278871074319
        total_loss: -0.0026960796676576138
        vf_explained_var: 0.05880554020404816
        vf_loss: 22.045894622802734
      agent-1:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1372838020324707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018352835904806852
        model: {}
        policy_loss: -0.004244545940309763
        total_loss: -0.003787026973441243
        vf_explained_var: -0.03780326247215271
        vf_loss: 24.591407775878906
      agent-2:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0794644355773926
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016072795260697603
        model: {}
        policy_loss: -0.003586541162803769
        total_loss: -0.0031928441021591425
        vf_explained_var: 0.03156961500644684
        vf_loss: 22.935556411743164
      agent-3:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46805518865585327
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007562198443338275
        model: {}
        policy_loss: -0.0024851523339748383
        total_loss: -0.0013434598222374916
        vf_explained_var: 0.16021142899990082
        vf_loss: 19.654682159423828
      agent-4:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.973638653755188
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018327998695895076
        model: {}
        policy_loss: -0.0035340788308531046
        total_loss: -0.0030324598774313927
        vf_explained_var: 0.05549663305282593
        vf_loss: 22.1522216796875
      agent-5:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6775519847869873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010674763470888138
        model: {}
        policy_loss: -0.0031714201904833317
        total_loss: -0.0023361416533589363
        vf_explained_var: 0.13276205956935883
        vf_loss: 20.277746200561523
    load_time_ms: 16733.186
    num_steps_sampled: 29184000
    num_steps_trained: 29184000
    sample_time_ms: 128800.952
    update_time_ms: 45.275
  iterations_since_restore: 224
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.61774193548387
    ram_util_percent: 14.958870967741936
  pid: 14340
  policy_reward_max:
    agent-0: 190.49999999999946
    agent-1: 190.49999999999946
    agent-2: 190.49999999999946
    agent-3: 190.49999999999946
    agent-4: 190.49999999999946
    agent-5: 190.49999999999946
  policy_reward_mean:
    agent-0: 154.33666666666662
    agent-1: 154.33666666666662
    agent-2: 154.33666666666662
    agent-3: 154.33666666666662
    agent-4: 154.33666666666662
    agent-5: 154.33666666666662
  policy_reward_min:
    agent-0: 50.666666666666586
    agent-1: 50.666666666666586
    agent-2: 50.666666666666586
    agent-3: 50.666666666666586
    agent-4: 50.666666666666586
    agent-5: 50.666666666666586
  sampler_perf:
    mean_env_wait_ms: 30.937578146531674
    mean_inference_ms: 14.593639151145073
    mean_processing_ms: 65.78948535472983
  time_since_restore: 36775.20746564865
  time_this_iter_s: 173.76339101791382
  time_total_s: 49326.0243370533
  timestamp: 1637071985
  timesteps_since_restore: 21504000
  timesteps_this_iter: 96000
  timesteps_total: 29184000
  training_iteration: 304
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    304 |            49326 | 29184000 |   926.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 65
    apples_agent-0_mean: 3.72
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 25.87
    apples_agent-1_min: 0
    apples_agent-2_max: 141
    apples_agent-2_mean: 4.67
    apples_agent-2_min: 0
    apples_agent-3_max: 183
    apples_agent-3_mean: 114.85
    apples_agent-3_min: 41
    apples_agent-4_max: 75
    apples_agent-4_mean: 3.23
    apples_agent-4_min: 0
    apples_agent-5_max: 217
    apples_agent-5_mean: 104.89
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 492
    cleaning_beam_agent-0_mean: 362.86
    cleaning_beam_agent-0_min: 182
    cleaning_beam_agent-1_max: 339
    cleaning_beam_agent-1_mean: 186.23
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 500
    cleaning_beam_agent-2_mean: 382.14
    cleaning_beam_agent-2_min: 190
    cleaning_beam_agent-3_max: 75
    cleaning_beam_agent-3_mean: 25.43
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 505
    cleaning_beam_agent-4_mean: 393.9
    cleaning_beam_agent-4_min: 228
    cleaning_beam_agent-5_max: 348
    cleaning_beam_agent-5_mean: 27.83
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-15-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1138.9999999999934
  episode_reward_mean: 926.0499999999859
  episode_reward_min: 438.0000000000116
  episodes_this_iter: 96
  episodes_total: 29280
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12951.171
    learner:
      agent-0:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0226483345031738
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024377666413784027
        model: {}
        policy_loss: -0.003567090258002281
        total_loss: -0.003287191968411207
        vf_explained_var: 0.03696645796298981
        vf_loss: 20.797622680664062
      agent-1:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.124856948852539
        entropy_coeff: 0.0017600000137463212
        kl: 0.002291426295414567
        model: {}
        policy_loss: -0.003957497887313366
        total_loss: -0.003591005690395832
        vf_explained_var: -0.07048922777175903
        vf_loss: 23.462390899658203
      agent-2:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0753618478775024
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015705159166827798
        model: {}
        policy_loss: -0.0037204327527433634
        total_loss: -0.0033965660259127617
        vf_explained_var: -0.02265210449695587
        vf_loss: 22.1650390625
      agent-3:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4585157334804535
        entropy_coeff: 0.0017600000137463212
        kl: 0.001569697167724371
        model: {}
        policy_loss: -0.0024523399770259857
        total_loss: -0.0013612834736704826
        vf_explained_var: 0.11984333395957947
        vf_loss: 18.98045539855957
      agent-4:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.989445149898529
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024813252966850996
        model: {}
        policy_loss: -0.0037322414573282003
        total_loss: -0.0034971414133906364
        vf_explained_var: 0.08594347536563873
        vf_loss: 19.765262603759766
      agent-5:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6792231798171997
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011047518346458673
        model: {}
        policy_loss: -0.003118145279586315
        total_loss: -0.0023906435817480087
        vf_explained_var: 0.10749711096286774
        vf_loss: 19.229360580444336
    load_time_ms: 16858.035
    num_steps_sampled: 29280000
    num_steps_trained: 29280000
    sample_time_ms: 128800.107
    update_time_ms: 19.492
  iterations_since_restore: 225
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.413392857142856
    ram_util_percent: 14.758482142857146
  pid: 14340
  policy_reward_max:
    agent-0: 189.833333333333
    agent-1: 189.833333333333
    agent-2: 189.833333333333
    agent-3: 189.833333333333
    agent-4: 189.833333333333
    agent-5: 189.833333333333
  policy_reward_mean:
    agent-0: 154.3416666666666
    agent-1: 154.3416666666666
    agent-2: 154.3416666666666
    agent-3: 154.3416666666666
    agent-4: 154.3416666666666
    agent-5: 154.3416666666666
  policy_reward_min:
    agent-0: 72.99999999999999
    agent-1: 72.99999999999999
    agent-2: 72.99999999999999
    agent-3: 72.99999999999999
    agent-4: 72.99999999999999
    agent-5: 72.99999999999999
  sampler_perf:
    mean_env_wait_ms: 30.938050329737507
    mean_inference_ms: 14.593543682136842
    mean_processing_ms: 65.78899129208787
  time_since_restore: 36932.176102638245
  time_this_iter_s: 156.9686369895935
  time_total_s: 49482.99297404289
  timestamp: 1637072142
  timesteps_since_restore: 21600000
  timesteps_this_iter: 96000
  timesteps_total: 29280000
  training_iteration: 305
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    305 |            49483 | 29280000 |   926.05 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 3.76
    apples_agent-0_min: 0
    apples_agent-1_max: 111
    apples_agent-1_mean: 26.54
    apples_agent-1_min: 0
    apples_agent-2_max: 211
    apples_agent-2_mean: 7.06
    apples_agent-2_min: 0
    apples_agent-3_max: 164
    apples_agent-3_mean: 108.14
    apples_agent-3_min: 40
    apples_agent-4_max: 70
    apples_agent-4_mean: 4.0
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 99.26
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 512
    cleaning_beam_agent-0_mean: 358.77
    cleaning_beam_agent-0_min: 115
    cleaning_beam_agent-1_max: 420
    cleaning_beam_agent-1_mean: 196.82
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 538
    cleaning_beam_agent-2_mean: 371.8
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 26.31
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 525
    cleaning_beam_agent-4_mean: 387.19
    cleaning_beam_agent-4_min: 192
    cleaning_beam_agent-5_max: 233
    cleaning_beam_agent-5_mean: 32.62
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-18-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1139.9999999999807
  episode_reward_mean: 919.6599999999872
  episode_reward_min: 338.0000000000025
  episodes_this_iter: 96
  episodes_total: 29376
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12966.962
    learner:
      agent-0:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0311704874038696
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013275237288326025
        model: {}
        policy_loss: -0.0029684859327971935
        total_loss: -0.0024442945141345263
        vf_explained_var: 0.05845580995082855
        vf_loss: 23.390520095825195
      agent-1:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1318691968917847
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011689316015690565
        model: {}
        policy_loss: -0.0036665829829871655
        total_loss: -0.0030414853245019913
        vf_explained_var: -0.042966604232788086
        vf_loss: 26.171878814697266
      agent-2:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0649527311325073
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015482684830203652
        model: {}
        policy_loss: -0.0034420867450535297
        total_loss: -0.0027716895565390587
        vf_explained_var: -0.024187803268432617
        vf_loss: 25.447120666503906
      agent-3:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46727174520492554
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012440105201676488
        model: {}
        policy_loss: -0.002704045968130231
        total_loss: -0.0015499137807637453
        vf_explained_var: 0.2031918317079544
        vf_loss: 19.76527976989746
      agent-4:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9796968102455139
        entropy_coeff: 0.0017600000137463212
        kl: 0.001384790986776352
        model: {}
        policy_loss: -0.003706797491759062
        total_loss: -0.003258171956986189
        vf_explained_var: 0.12410730123519897
        vf_loss: 21.728946685791016
      agent-5:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6695391535758972
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018239603377878666
        model: {}
        policy_loss: -0.0032993480563163757
        total_loss: -0.0024209516122937202
        vf_explained_var: 0.17068322002887726
        vf_loss: 20.567832946777344
    load_time_ms: 16985.233
    num_steps_sampled: 29376000
    num_steps_trained: 29376000
    sample_time_ms: 128681.96
    update_time_ms: 36.542
  iterations_since_restore: 226
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.71216216216216
    ram_util_percent: 14.716216216216218
  pid: 14340
  policy_reward_max:
    agent-0: 189.99999999999991
    agent-1: 189.99999999999991
    agent-2: 189.99999999999991
    agent-3: 189.99999999999991
    agent-4: 189.99999999999991
    agent-5: 189.99999999999991
  policy_reward_mean:
    agent-0: 153.2766666666666
    agent-1: 153.2766666666666
    agent-2: 153.2766666666666
    agent-3: 153.2766666666666
    agent-4: 153.2766666666666
    agent-5: 153.2766666666666
  policy_reward_min:
    agent-0: 56.33333333333316
    agent-1: 56.33333333333316
    agent-2: 56.33333333333316
    agent-3: 56.33333333333316
    agent-4: 56.33333333333316
    agent-5: 56.33333333333316
  sampler_perf:
    mean_env_wait_ms: 30.9378244216018
    mean_inference_ms: 14.593251004138995
    mean_processing_ms: 65.78704860464845
  time_since_restore: 37088.359537124634
  time_this_iter_s: 156.18343448638916
  time_total_s: 49639.17640852928
  timestamp: 1637072299
  timesteps_since_restore: 21696000
  timesteps_this_iter: 96000
  timesteps_total: 29376000
  training_iteration: 306
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    306 |          49639.2 | 29376000 |   919.66 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 208
    apples_agent-0_mean: 5.91
    apples_agent-0_min: 0
    apples_agent-1_max: 125
    apples_agent-1_mean: 27.03
    apples_agent-1_min: 0
    apples_agent-2_max: 46
    apples_agent-2_mean: 3.12
    apples_agent-2_min: 0
    apples_agent-3_max: 183
    apples_agent-3_mean: 113.31
    apples_agent-3_min: 52
    apples_agent-4_max: 108
    apples_agent-4_mean: 4.13
    apples_agent-4_min: 0
    apples_agent-5_max: 329
    apples_agent-5_mean: 97.88
    apples_agent-5_min: 14
    cleaning_beam_agent-0_max: 488
    cleaning_beam_agent-0_mean: 356.71
    cleaning_beam_agent-0_min: 153
    cleaning_beam_agent-1_max: 364
    cleaning_beam_agent-1_mean: 202.45
    cleaning_beam_agent-1_min: 101
    cleaning_beam_agent-2_max: 531
    cleaning_beam_agent-2_mean: 384.46
    cleaning_beam_agent-2_min: 200
    cleaning_beam_agent-3_max: 90
    cleaning_beam_agent-3_mean: 23.01
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 504
    cleaning_beam_agent-4_mean: 390.52
    cleaning_beam_agent-4_min: 161
    cleaning_beam_agent-5_max: 375
    cleaning_beam_agent-5_mean: 31.54
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-20-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1149.9999999999993
  episode_reward_mean: 911.1899999999877
  episode_reward_min: 483.00000000001046
  episodes_this_iter: 96
  episodes_total: 29472
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12962.135
    learner:
      agent-0:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0269852876663208
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018365212017670274
        model: {}
        policy_loss: -0.0032387867104262114
        total_loss: -0.002892895368859172
        vf_explained_var: 0.06032705307006836
        vf_loss: 21.53384780883789
      agent-1:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1270275115966797
        entropy_coeff: 0.0017600000137463212
        kl: 0.001292067812755704
        model: {}
        policy_loss: -0.004025107715278864
        total_loss: -0.0035442975349724293
        vf_explained_var: -0.0725393295288086
        vf_loss: 24.64380645751953
      agent-2:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0733813047409058
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016948741395026445
        model: {}
        policy_loss: -0.003187216119840741
        total_loss: -0.0028740514535456896
        vf_explained_var: 0.037814125418663025
        vf_loss: 22.023193359375
      agent-3:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4804672300815582
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013201490510255098
        model: {}
        policy_loss: -0.0027247511316090822
        total_loss: -0.001636242726817727
        vf_explained_var: 0.155616894364357
        vf_loss: 19.341304779052734
      agent-4:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9800677299499512
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015394907677546144
        model: {}
        policy_loss: -0.0038141030818223953
        total_loss: -0.0034293141216039658
        vf_explained_var: 0.07894839346408844
        vf_loss: 21.097082138061523
      agent-5:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6707847714424133
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009800295811146498
        model: {}
        policy_loss: -0.0029601161368191242
        total_loss: -0.0022169891744852066
        vf_explained_var: 0.16196219623088837
        vf_loss: 19.23710823059082
    load_time_ms: 16844.014
    num_steps_sampled: 29472000
    num_steps_trained: 29472000
    sample_time_ms: 128534.114
    update_time_ms: 39.843
  iterations_since_restore: 227
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.554545454545455
    ram_util_percent: 14.656818181818181
  pid: 14340
  policy_reward_max:
    agent-0: 191.66666666666646
    agent-1: 191.66666666666646
    agent-2: 191.66666666666646
    agent-3: 191.66666666666646
    agent-4: 191.66666666666646
    agent-5: 191.66666666666646
  policy_reward_mean:
    agent-0: 151.86499999999998
    agent-1: 151.86499999999998
    agent-2: 151.86499999999998
    agent-3: 151.86499999999998
    agent-4: 151.86499999999998
    agent-5: 151.86499999999998
  policy_reward_min:
    agent-0: 80.50000000000001
    agent-1: 80.50000000000001
    agent-2: 80.50000000000001
    agent-3: 80.50000000000001
    agent-4: 80.50000000000001
    agent-5: 80.50000000000001
  sampler_perf:
    mean_env_wait_ms: 30.93804146152369
    mean_inference_ms: 14.592970072707374
    mean_processing_ms: 65.78576025152063
  time_since_restore: 37242.38564515114
  time_this_iter_s: 154.02610802650452
  time_total_s: 49793.202516555786
  timestamp: 1637072453
  timesteps_since_restore: 21792000
  timesteps_this_iter: 96000
  timesteps_total: 29472000
  training_iteration: 307
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    307 |          49793.2 | 29472000 |   911.19 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 4.61
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 26.46
    apples_agent-1_min: 0
    apples_agent-2_max: 253
    apples_agent-2_mean: 5.52
    apples_agent-2_min: 0
    apples_agent-3_max: 322
    apples_agent-3_mean: 114.24
    apples_agent-3_min: 37
    apples_agent-4_max: 61
    apples_agent-4_mean: 3.14
    apples_agent-4_min: 0
    apples_agent-5_max: 166
    apples_agent-5_mean: 100.6
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 373.5
    cleaning_beam_agent-0_min: 204
    cleaning_beam_agent-1_max: 338
    cleaning_beam_agent-1_mean: 190.24
    cleaning_beam_agent-1_min: 89
    cleaning_beam_agent-2_max: 528
    cleaning_beam_agent-2_mean: 391.61
    cleaning_beam_agent-2_min: 197
    cleaning_beam_agent-3_max: 84
    cleaning_beam_agent-3_mean: 26.61
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 508
    cleaning_beam_agent-4_mean: 395.97
    cleaning_beam_agent-4_min: 192
    cleaning_beam_agent-5_max: 141
    cleaning_beam_agent-5_mean: 22.81
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-23-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1113.999999999989
  episode_reward_mean: 931.6199999999872
  episode_reward_min: 558.9999999999978
  episodes_this_iter: 96
  episodes_total: 29568
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12946.498
    learner:
      agent-0:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0446600914001465
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011518399696797132
        model: {}
        policy_loss: -0.0030574218835681677
        total_loss: -0.0026583168655633926
        vf_explained_var: 0.0018694698810577393
        vf_loss: 22.377071380615234
      agent-1:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1376581192016602
        entropy_coeff: 0.0017600000137463212
        kl: 0.002110258676111698
        model: {}
        policy_loss: -0.003994280938059092
        total_loss: -0.003633451648056507
        vf_explained_var: -0.04690560698509216
        vf_loss: 23.631057739257812
      agent-2:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0552794933319092
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015172046842053533
        model: {}
        policy_loss: -0.0034227361902594566
        total_loss: -0.003113269340246916
        vf_explained_var: 0.03856396675109863
        vf_loss: 21.667572021484375
      agent-3:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46371060609817505
        entropy_coeff: 0.0017600000137463212
        kl: 0.00111609254963696
        model: {}
        policy_loss: -0.00222288747318089
        total_loss: -0.0010560164228081703
        vf_explained_var: 0.1148790568113327
        vf_loss: 19.830013275146484
      agent-4:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9809458255767822
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012046340852975845
        model: {}
        policy_loss: -0.003280419623479247
        total_loss: -0.002908478258177638
        vf_explained_var: 0.061588436365127563
        vf_loss: 20.984073638916016
      agent-5:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6403011679649353
        entropy_coeff: 0.0017600000137463212
        kl: 0.001105994451791048
        model: {}
        policy_loss: -0.003079441376030445
        total_loss: -0.0022533833980560303
        vf_explained_var: 0.12508592009544373
        vf_loss: 19.529890060424805
    load_time_ms: 18297.594
    num_steps_sampled: 29568000
    num_steps_trained: 29568000
    sample_time_ms: 128300.075
    update_time_ms: 39.593
  iterations_since_restore: 228
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.203750000000003
    ram_util_percent: 14.879583333333334
  pid: 14340
  policy_reward_max:
    agent-0: 185.66666666666637
    agent-1: 185.66666666666637
    agent-2: 185.66666666666637
    agent-3: 185.66666666666637
    agent-4: 185.66666666666637
    agent-5: 185.66666666666637
  policy_reward_mean:
    agent-0: 155.26999999999995
    agent-1: 155.26999999999995
    agent-2: 155.26999999999995
    agent-3: 155.26999999999995
    agent-4: 155.26999999999995
    agent-5: 155.26999999999995
  policy_reward_min:
    agent-0: 93.16666666666669
    agent-1: 93.16666666666669
    agent-2: 93.16666666666669
    agent-3: 93.16666666666669
    agent-4: 93.16666666666669
    agent-5: 93.16666666666669
  sampler_perf:
    mean_env_wait_ms: 30.938862555546038
    mean_inference_ms: 14.592636651354832
    mean_processing_ms: 65.78480160522423
  time_since_restore: 37410.27870607376
  time_this_iter_s: 167.89306092262268
  time_total_s: 49961.09557747841
  timestamp: 1637072622
  timesteps_since_restore: 21888000
  timesteps_this_iter: 96000
  timesteps_total: 29568000
  training_iteration: 308
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    308 |          49961.1 | 29568000 |   931.62 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 68
    apples_agent-0_mean: 5.12
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 25.86
    apples_agent-1_min: 0
    apples_agent-2_max: 100
    apples_agent-2_mean: 4.55
    apples_agent-2_min: 0
    apples_agent-3_max: 182
    apples_agent-3_mean: 109.99
    apples_agent-3_min: 47
    apples_agent-4_max: 96
    apples_agent-4_mean: 3.86
    apples_agent-4_min: 0
    apples_agent-5_max: 154
    apples_agent-5_mean: 97.87
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 478
    cleaning_beam_agent-0_mean: 361.66
    cleaning_beam_agent-0_min: 236
    cleaning_beam_agent-1_max: 423
    cleaning_beam_agent-1_mean: 197.43
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 551
    cleaning_beam_agent-2_mean: 402.02
    cleaning_beam_agent-2_min: 152
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 26.84
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 482
    cleaning_beam_agent-4_mean: 387.07
    cleaning_beam_agent-4_min: 196
    cleaning_beam_agent-5_max: 169
    cleaning_beam_agent-5_mean: 28.76
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-26-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1115.999999999988
  episode_reward_mean: 928.7799999999868
  episode_reward_min: 440.9999999999997
  episodes_this_iter: 96
  episodes_total: 29664
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12950.642
    learner:
      agent-0:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0480302572250366
        entropy_coeff: 0.0017600000137463212
        kl: 0.002123821759596467
        model: {}
        policy_loss: -0.003577015595510602
        total_loss: -0.003388518001884222
        vf_explained_var: 0.09397576749324799
        vf_loss: 20.330322265625
      agent-1:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.132947564125061
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016804409679025412
        model: {}
        policy_loss: -0.003965528681874275
        total_loss: -0.003606641199439764
        vf_explained_var: -0.043583840131759644
        vf_loss: 23.528779983520508
      agent-2:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0537526607513428
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015227122930809855
        model: {}
        policy_loss: -0.003243681276217103
        total_loss: -0.0028828687500208616
        vf_explained_var: 0.011204823851585388
        vf_loss: 22.154150009155273
      agent-3:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4720323085784912
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012997136218473315
        model: {}
        policy_loss: -0.002415616996586323
        total_loss: -0.001306528807617724
        vf_explained_var: 0.13265615701675415
        vf_loss: 19.398693084716797
      agent-4:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9930326342582703
        entropy_coeff: 0.0017600000137463212
        kl: 0.001966201700270176
        model: {}
        policy_loss: -0.0042099729180336
        total_loss: -0.003801139537245035
        vf_explained_var: 0.03737550973892212
        vf_loss: 21.56570816040039
      agent-5:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6588000655174255
        entropy_coeff: 0.0017600000137463212
        kl: 0.001433471916243434
        model: {}
        policy_loss: -0.0030121710151433945
        total_loss: -0.0022639285307377577
        vf_explained_var: 0.14626608788967133
        vf_loss: 19.077314376831055
    load_time_ms: 17700.54
    num_steps_sampled: 29664000
    num_steps_trained: 29664000
    sample_time_ms: 128302.508
    update_time_ms: 39.326
  iterations_since_restore: 229
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.617937219730937
    ram_util_percent: 14.716143497757852
  pid: 14340
  policy_reward_max:
    agent-0: 185.9999999999998
    agent-1: 185.9999999999998
    agent-2: 185.9999999999998
    agent-3: 185.9999999999998
    agent-4: 185.9999999999998
    agent-5: 185.9999999999998
  policy_reward_mean:
    agent-0: 154.79666666666662
    agent-1: 154.79666666666662
    agent-2: 154.79666666666662
    agent-3: 154.79666666666662
    agent-4: 154.79666666666662
    agent-5: 154.79666666666662
  policy_reward_min:
    agent-0: 73.49999999999987
    agent-1: 73.49999999999987
    agent-2: 73.49999999999987
    agent-3: 73.49999999999987
    agent-4: 73.49999999999987
    agent-5: 73.49999999999987
  sampler_perf:
    mean_env_wait_ms: 30.939245242382757
    mean_inference_ms: 14.592361833373484
    mean_processing_ms: 65.78508223020611
  time_since_restore: 37566.636571884155
  time_this_iter_s: 156.3578658103943
  time_total_s: 50117.4534432888
  timestamp: 1637072779
  timesteps_since_restore: 21984000
  timesteps_this_iter: 96000
  timesteps_total: 29664000
  training_iteration: 309
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    309 |          50117.5 | 29664000 |   928.78 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 119
    apples_agent-0_mean: 7.22
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 24.87
    apples_agent-1_min: 0
    apples_agent-2_max: 32
    apples_agent-2_mean: 1.47
    apples_agent-2_min: 0
    apples_agent-3_max: 182
    apples_agent-3_mean: 115.95
    apples_agent-3_min: 41
    apples_agent-4_max: 92
    apples_agent-4_mean: 3.35
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 107.56
    apples_agent-5_min: 53
    cleaning_beam_agent-0_max: 511
    cleaning_beam_agent-0_mean: 364.77
    cleaning_beam_agent-0_min: 207
    cleaning_beam_agent-1_max: 427
    cleaning_beam_agent-1_mean: 213.26
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 509
    cleaning_beam_agent-2_mean: 413.41
    cleaning_beam_agent-2_min: 257
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 26.04
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 478
    cleaning_beam_agent-4_mean: 380.49
    cleaning_beam_agent-4_min: 192
    cleaning_beam_agent-5_max: 89
    cleaning_beam_agent-5_mean: 23.72
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-28-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1147.0000000000023
  episode_reward_mean: 942.1299999999868
  episode_reward_min: 440.0000000000102
  episodes_this_iter: 96
  episodes_total: 29760
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12959.993
    learner:
      agent-0:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0310444831848145
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016735803801566362
        model: {}
        policy_loss: -0.0032785837538540363
        total_loss: -0.003055168315768242
        vf_explained_var: 0.0808238536119461
        vf_loss: 20.380537033081055
      agent-1:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1322121620178223
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018612864660099149
        model: {}
        policy_loss: -0.004064826760441065
        total_loss: -0.0036437741946429014
        vf_explained_var: -0.07691764831542969
        vf_loss: 24.137447357177734
      agent-2:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0412847995758057
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024002795107662678
        model: {}
        policy_loss: -0.0035767406225204468
        total_loss: -0.003209946444258094
        vf_explained_var: 0.003367498517036438
        vf_loss: 21.994550704956055
      agent-3:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46934399008750916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006496896967291832
        model: {}
        policy_loss: -0.002293534576892853
        total_loss: -0.0012248922139406204
        vf_explained_var: 0.1421848088502884
        vf_loss: 18.946929931640625
      agent-4:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9799613952636719
        entropy_coeff: 0.0017600000137463212
        kl: 0.001661666901782155
        model: {}
        policy_loss: -0.003801794024184346
        total_loss: -0.003427553456276655
        vf_explained_var: 0.04844126105308533
        vf_loss: 20.989713668823242
      agent-5:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6556261777877808
        entropy_coeff: 0.0017600000137463212
        kl: 0.001553613692522049
        model: {}
        policy_loss: -0.003030662424862385
        total_loss: -0.002350006951019168
        vf_explained_var: 0.16549503803253174
        vf_loss: 18.345565795898438
    load_time_ms: 17783.271
    num_steps_sampled: 29760000
    num_steps_trained: 29760000
    sample_time_ms: 128193.175
    update_time_ms: 49.403
  iterations_since_restore: 230
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.497309417040356
    ram_util_percent: 14.755156950672648
  pid: 14340
  policy_reward_max:
    agent-0: 191.1666666666664
    agent-1: 191.1666666666664
    agent-2: 191.1666666666664
    agent-3: 191.1666666666664
    agent-4: 191.1666666666664
    agent-5: 191.1666666666664
  policy_reward_mean:
    agent-0: 157.02166666666662
    agent-1: 157.02166666666662
    agent-2: 157.02166666666662
    agent-3: 157.02166666666662
    agent-4: 157.02166666666662
    agent-5: 157.02166666666662
  policy_reward_min:
    agent-0: 73.33333333333324
    agent-1: 73.33333333333324
    agent-2: 73.33333333333324
    agent-3: 73.33333333333324
    agent-4: 73.33333333333324
    agent-5: 73.33333333333324
  sampler_perf:
    mean_env_wait_ms: 30.940085394165894
    mean_inference_ms: 14.592610952279943
    mean_processing_ms: 65.78506806899568
  time_since_restore: 37722.51632285118
  time_this_iter_s: 155.87975096702576
  time_total_s: 50273.33319425583
  timestamp: 1637072935
  timesteps_since_restore: 22080000
  timesteps_this_iter: 96000
  timesteps_total: 29760000
  training_iteration: 310
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    310 |          50273.3 | 29760000 |   942.13 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 116
    apples_agent-0_mean: 4.71
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 23.31
    apples_agent-1_min: 0
    apples_agent-2_max: 240
    apples_agent-2_mean: 5.68
    apples_agent-2_min: 0
    apples_agent-3_max: 254
    apples_agent-3_mean: 117.24
    apples_agent-3_min: 61
    apples_agent-4_max: 64
    apples_agent-4_mean: 1.78
    apples_agent-4_min: 0
    apples_agent-5_max: 283
    apples_agent-5_mean: 105.6
    apples_agent-5_min: 56
    cleaning_beam_agent-0_max: 496
    cleaning_beam_agent-0_mean: 380.58
    cleaning_beam_agent-0_min: 196
    cleaning_beam_agent-1_max: 355
    cleaning_beam_agent-1_mean: 211.93
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 553
    cleaning_beam_agent-2_mean: 421.59
    cleaning_beam_agent-2_min: 208
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 24.04
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 488
    cleaning_beam_agent-4_mean: 394.56
    cleaning_beam_agent-4_min: 282
    cleaning_beam_agent-5_max: 129
    cleaning_beam_agent-5_mean: 20.92
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-31-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1151.0000000000107
  episode_reward_mean: 963.5799999999866
  episode_reward_min: 565.0000000000053
  episodes_this_iter: 96
  episodes_total: 29856
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12951.919
    learner:
      agent-0:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0283044576644897
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018733160104602575
        model: {}
        policy_loss: -0.00354643352329731
        total_loss: -0.003208870068192482
        vf_explained_var: 0.019917219877243042
        vf_loss: 21.473804473876953
      agent-1:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1356074810028076
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014605229953303933
        model: {}
        policy_loss: -0.0038205168675631285
        total_loss: -0.0034675553906708956
        vf_explained_var: -0.056373417377471924
        vf_loss: 23.516292572021484
      agent-2:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0431151390075684
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018564291531220078
        model: {}
        policy_loss: -0.0034913294948637486
        total_loss: -0.0031830607913434505
        vf_explained_var: 0.028074920177459717
        vf_loss: 21.441495895385742
      agent-3:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4519250690937042
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012781055411323905
        model: {}
        policy_loss: -0.002224828116595745
        total_loss: -0.0010139495134353638
        vf_explained_var: 0.08496743440628052
        vf_loss: 20.062702178955078
      agent-4:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.991337239742279
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015491547528654337
        model: {}
        policy_loss: -0.003459033789113164
        total_loss: -0.0030792993493378162
        vf_explained_var: 0.03200191259384155
        vf_loss: 21.244869232177734
      agent-5:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6156814098358154
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010065605165436864
        model: {}
        policy_loss: -0.0026068836450576782
        total_loss: -0.0017638588324189186
        vf_explained_var: 0.11065424978733063
        vf_loss: 19.26620101928711
    load_time_ms: 17761.203
    num_steps_sampled: 29856000
    num_steps_trained: 29856000
    sample_time_ms: 128136.24
    update_time_ms: 91.312
  iterations_since_restore: 231
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.794642857142858
    ram_util_percent: 14.714285714285717
  pid: 14340
  policy_reward_max:
    agent-0: 191.83333333333334
    agent-1: 191.83333333333334
    agent-2: 191.83333333333334
    agent-3: 191.83333333333334
    agent-4: 191.83333333333334
    agent-5: 191.83333333333334
  policy_reward_mean:
    agent-0: 160.5966666666666
    agent-1: 160.5966666666666
    agent-2: 160.5966666666666
    agent-3: 160.5966666666666
    agent-4: 160.5966666666666
    agent-5: 160.5966666666666
  policy_reward_min:
    agent-0: 94.16666666666674
    agent-1: 94.16666666666674
    agent-2: 94.16666666666674
    agent-3: 94.16666666666674
    agent-4: 94.16666666666674
    agent-5: 94.16666666666674
  sampler_perf:
    mean_env_wait_ms: 30.942387195435607
    mean_inference_ms: 14.592621993436758
    mean_processing_ms: 65.78540247708735
  time_since_restore: 37880.10879611969
  time_this_iter_s: 157.5924732685089
  time_total_s: 50430.92566752434
  timestamp: 1637073093
  timesteps_since_restore: 22176000
  timesteps_this_iter: 96000
  timesteps_total: 29856000
  training_iteration: 311
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    311 |          50430.9 | 29856000 |   963.58 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 177
    apples_agent-0_mean: 7.36
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 27.03
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 3.65
    apples_agent-2_min: 0
    apples_agent-3_max: 222
    apples_agent-3_mean: 117.61
    apples_agent-3_min: 48
    apples_agent-4_max: 73
    apples_agent-4_mean: 1.45
    apples_agent-4_min: 0
    apples_agent-5_max: 164
    apples_agent-5_mean: 99.6
    apples_agent-5_min: 61
    cleaning_beam_agent-0_max: 509
    cleaning_beam_agent-0_mean: 378.68
    cleaning_beam_agent-0_min: 143
    cleaning_beam_agent-1_max: 393
    cleaning_beam_agent-1_mean: 189.76
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 609
    cleaning_beam_agent-2_mean: 406.34
    cleaning_beam_agent-2_min: 237
    cleaning_beam_agent-3_max: 155
    cleaning_beam_agent-3_mean: 30.2
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 472
    cleaning_beam_agent-4_mean: 388.94
    cleaning_beam_agent-4_min: 245
    cleaning_beam_agent-5_max: 218
    cleaning_beam_agent-5_mean: 22.58
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-34-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1128.0000000000073
  episode_reward_mean: 939.6299999999871
  episode_reward_min: 529.0000000000127
  episodes_this_iter: 96
  episodes_total: 29952
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12945.9
    learner:
      agent-0:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0160536766052246
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013405964709818363
        model: {}
        policy_loss: -0.003011177759617567
        total_loss: -0.002642220351845026
        vf_explained_var: 0.06483013927936554
        vf_loss: 21.57209014892578
      agent-1:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.138129711151123
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020796533208340406
        model: {}
        policy_loss: -0.003958356566727161
        total_loss: -0.0035888655111193657
        vf_explained_var: -0.02863481640815735
        vf_loss: 23.72597885131836
      agent-2:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0439826250076294
        entropy_coeff: 0.0017600000137463212
        kl: 0.001867801183834672
        model: {}
        policy_loss: -0.0036255745217204094
        total_loss: -0.0032603004947304726
        vf_explained_var: 0.042716771364212036
        vf_loss: 22.02683448791504
      agent-3:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4788329601287842
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014622430317103863
        model: {}
        policy_loss: -0.002617497928440571
        total_loss: -0.0014227164210751653
        vf_explained_var: 0.11358809471130371
        vf_loss: 20.375267028808594
      agent-4:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9843650460243225
        entropy_coeff: 0.0017600000137463212
        kl: 0.001298714429140091
        model: {}
        policy_loss: -0.0033428678289055824
        total_loss: -0.002888314425945282
        vf_explained_var: 0.0447814017534256
        vf_loss: 21.870365142822266
      agent-5:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6522471904754639
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011637348216027021
        model: {}
        policy_loss: -0.0030556898564100266
        total_loss: -0.0022536935284733772
        vf_explained_var: 0.14619454741477966
        vf_loss: 19.499479293823242
    load_time_ms: 18810.231
    num_steps_sampled: 29952000
    num_steps_trained: 29952000
    sample_time_ms: 128097.625
    update_time_ms: 91.941
  iterations_since_restore: 232
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.34135021097046
    ram_util_percent: 14.859915611814348
  pid: 14340
  policy_reward_max:
    agent-0: 187.99999999999955
    agent-1: 187.99999999999955
    agent-2: 187.99999999999955
    agent-3: 187.99999999999955
    agent-4: 187.99999999999955
    agent-5: 187.99999999999955
  policy_reward_mean:
    agent-0: 156.60499999999993
    agent-1: 156.60499999999993
    agent-2: 156.60499999999993
    agent-3: 156.60499999999993
    agent-4: 156.60499999999993
    agent-5: 156.60499999999993
  policy_reward_min:
    agent-0: 88.16666666666681
    agent-1: 88.16666666666681
    agent-2: 88.16666666666681
    agent-3: 88.16666666666681
    agent-4: 88.16666666666681
    agent-5: 88.16666666666681
  sampler_perf:
    mean_env_wait_ms: 30.943077130691606
    mean_inference_ms: 14.59229900903545
    mean_processing_ms: 65.79498579217298
  time_since_restore: 38046.36123466492
  time_this_iter_s: 166.25243854522705
  time_total_s: 50597.178106069565
  timestamp: 1637073259
  timesteps_since_restore: 22272000
  timesteps_this_iter: 96000
  timesteps_total: 29952000
  training_iteration: 312
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    312 |          50597.2 | 29952000 |   939.63 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 3.54
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 24.35
    apples_agent-1_min: 0
    apples_agent-2_max: 104
    apples_agent-2_mean: 6.86
    apples_agent-2_min: 0
    apples_agent-3_max: 443
    apples_agent-3_mean: 119.39
    apples_agent-3_min: 58
    apples_agent-4_max: 50
    apples_agent-4_mean: 1.23
    apples_agent-4_min: 0
    apples_agent-5_max: 400
    apples_agent-5_mean: 104.15
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 572
    cleaning_beam_agent-0_mean: 394.87
    cleaning_beam_agent-0_min: 239
    cleaning_beam_agent-1_max: 386
    cleaning_beam_agent-1_mean: 196.53
    cleaning_beam_agent-1_min: 100
    cleaning_beam_agent-2_max: 579
    cleaning_beam_agent-2_mean: 394.39
    cleaning_beam_agent-2_min: 196
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 27.69
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 477
    cleaning_beam_agent-4_mean: 390.71
    cleaning_beam_agent-4_min: 262
    cleaning_beam_agent-5_max: 92
    cleaning_beam_agent-5_mean: 20.15
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-36-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1147.0000000000057
  episode_reward_mean: 934.7899999999886
  episode_reward_min: 406.0000000000068
  episodes_this_iter: 96
  episodes_total: 30048
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12940.661
    learner:
      agent-0:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0285953283309937
        entropy_coeff: 0.0017600000137463212
        kl: 0.001281430828385055
        model: {}
        policy_loss: -0.0031215206254273653
        total_loss: -0.0026136154774576426
        vf_explained_var: 0.0683458000421524
        vf_loss: 23.182344436645508
      agent-1:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1430119276046753
        entropy_coeff: 0.0017600000137463212
        kl: 0.001964571885764599
        model: {}
        policy_loss: -0.004115580581128597
        total_loss: -0.003513291012495756
        vf_explained_var: -0.047991931438446045
        vf_loss: 26.139930725097656
      agent-2:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0571688413619995
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012221288634464145
        model: {}
        policy_loss: -0.0036077797412872314
        total_loss: -0.0030681095086038113
        vf_explained_var: 0.04839108884334564
        vf_loss: 24.002872467041016
      agent-3:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4810986816883087
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012167789973318577
        model: {}
        policy_loss: -0.002522798255085945
        total_loss: -0.0013070255517959595
        vf_explained_var: 0.17305053770542145
        vf_loss: 20.625091552734375
      agent-4:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.989946722984314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017830258002504706
        model: {}
        policy_loss: -0.003796099917963147
        total_loss: -0.003166310489177704
        vf_explained_var: 0.044405847787857056
        vf_loss: 23.72096824645996
      agent-5:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.64111328125
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010839089518412948
        model: {}
        policy_loss: -0.0029120957478880882
        total_loss: -0.001909223385155201
        vf_explained_var: 0.14274165034294128
        vf_loss: 21.312345504760742
    load_time_ms: 18832.016
    num_steps_sampled: 30048000
    num_steps_trained: 30048000
    sample_time_ms: 128094.482
    update_time_ms: 94.44
  iterations_since_restore: 233
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.726576576576573
    ram_util_percent: 14.681531531531533
  pid: 14340
  policy_reward_max:
    agent-0: 191.16666666666617
    agent-1: 191.16666666666617
    agent-2: 191.16666666666617
    agent-3: 191.16666666666617
    agent-4: 191.16666666666617
    agent-5: 191.16666666666617
  policy_reward_mean:
    agent-0: 155.79833333333326
    agent-1: 155.79833333333326
    agent-2: 155.79833333333326
    agent-3: 155.79833333333326
    agent-4: 155.79833333333326
    agent-5: 155.79833333333326
  policy_reward_min:
    agent-0: 67.66666666666647
    agent-1: 67.66666666666647
    agent-2: 67.66666666666647
    agent-3: 67.66666666666647
    agent-4: 67.66666666666647
    agent-5: 67.66666666666647
  sampler_perf:
    mean_env_wait_ms: 30.943835378560586
    mean_inference_ms: 14.592317412676783
    mean_processing_ms: 65.79595484479196
  time_since_restore: 38202.26772761345
  time_this_iter_s: 155.9064929485321
  time_total_s: 50753.0845990181
  timestamp: 1637073416
  timesteps_since_restore: 22368000
  timesteps_this_iter: 96000
  timesteps_total: 30048000
  training_iteration: 313
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    313 |          50753.1 | 30048000 |   934.79 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 75
    apples_agent-0_mean: 3.94
    apples_agent-0_min: 0
    apples_agent-1_max: 111
    apples_agent-1_mean: 27.25
    apples_agent-1_min: 0
    apples_agent-2_max: 77
    apples_agent-2_mean: 2.84
    apples_agent-2_min: 0
    apples_agent-3_max: 179
    apples_agent-3_mean: 117.78
    apples_agent-3_min: 40
    apples_agent-4_max: 53
    apples_agent-4_mean: 2.08
    apples_agent-4_min: 0
    apples_agent-5_max: 188
    apples_agent-5_mean: 100.99
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 576
    cleaning_beam_agent-0_mean: 397.52
    cleaning_beam_agent-0_min: 241
    cleaning_beam_agent-1_max: 379
    cleaning_beam_agent-1_mean: 207.19
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 553
    cleaning_beam_agent-2_mean: 410.17
    cleaning_beam_agent-2_min: 196
    cleaning_beam_agent-3_max: 99
    cleaning_beam_agent-3_mean: 25.1
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 495
    cleaning_beam_agent-4_mean: 395.69
    cleaning_beam_agent-4_min: 246
    cleaning_beam_agent-5_max: 419
    cleaning_beam_agent-5_mean: 21.82
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-39-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1137.9999999999975
  episode_reward_mean: 958.1999999999867
  episode_reward_min: 452.00000000000534
  episodes_this_iter: 96
  episodes_total: 30144
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12960.286
    learner:
      agent-0:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0143709182739258
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026081381365656853
        model: {}
        policy_loss: -0.0031738504767417908
        total_loss: -0.002858636900782585
        vf_explained_var: -0.009862631559371948
        vf_loss: 21.005006790161133
      agent-1:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1461243629455566
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018920127768069506
        model: {}
        policy_loss: -0.003972014412283897
        total_loss: -0.003825856139883399
        vf_explained_var: -0.026554852724075317
        vf_loss: 21.633377075195312
      agent-2:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0452921390533447
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018962642643600702
        model: {}
        policy_loss: -0.003873453475534916
        total_loss: -0.0035637489054352045
        vf_explained_var: -0.016278550028800964
        vf_loss: 21.49416732788086
      agent-3:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44282662868499756
        entropy_coeff: 0.0017600000137463212
        kl: 0.001139630563557148
        model: {}
        policy_loss: -0.0024983487091958523
        total_loss: -0.0013431154657155275
        vf_explained_var: 0.06558217108249664
        vf_loss: 19.346065521240234
      agent-4:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9744894504547119
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020825210958719254
        model: {}
        policy_loss: -0.004093823954463005
        total_loss: -0.003819406498223543
        vf_explained_var: 0.039153531193733215
        vf_loss: 19.89522361755371
      agent-5:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6118007898330688
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010828421218320727
        model: {}
        policy_loss: -0.0025708600878715515
        total_loss: -0.0017710840329527855
        vf_explained_var: 0.09167042374610901
        vf_loss: 18.76544761657715
    load_time_ms: 16933.703
    num_steps_sampled: 30144000
    num_steps_trained: 30144000
    sample_time_ms: 128075.698
    update_time_ms: 93.017
  iterations_since_restore: 234
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.816742081447963
    ram_util_percent: 14.724886877828057
  pid: 14340
  policy_reward_max:
    agent-0: 189.6666666666665
    agent-1: 189.6666666666665
    agent-2: 189.6666666666665
    agent-3: 189.6666666666665
    agent-4: 189.6666666666665
    agent-5: 189.6666666666665
  policy_reward_mean:
    agent-0: 159.69999999999993
    agent-1: 159.69999999999993
    agent-2: 159.69999999999993
    agent-3: 159.69999999999993
    agent-4: 159.69999999999993
    agent-5: 159.69999999999993
  policy_reward_min:
    agent-0: 75.3333333333333
    agent-1: 75.3333333333333
    agent-2: 75.3333333333333
    agent-3: 75.3333333333333
    agent-4: 75.3333333333333
    agent-5: 75.3333333333333
  sampler_perf:
    mean_env_wait_ms: 30.945649958675414
    mean_inference_ms: 14.591983696572852
    mean_processing_ms: 65.79529632382199
  time_since_restore: 38357.08042693138
  time_this_iter_s: 154.81269931793213
  time_total_s: 50907.89729833603
  timestamp: 1637073571
  timesteps_since_restore: 22464000
  timesteps_this_iter: 96000
  timesteps_total: 30144000
  training_iteration: 314
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    314 |          50907.9 | 30144000 |    958.2 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 2.15
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 23.11
    apples_agent-1_min: 0
    apples_agent-2_max: 83
    apples_agent-2_mean: 7.84
    apples_agent-2_min: 0
    apples_agent-3_max: 180
    apples_agent-3_mean: 119.47
    apples_agent-3_min: 50
    apples_agent-4_max: 46
    apples_agent-4_mean: 0.75
    apples_agent-4_min: 0
    apples_agent-5_max: 226
    apples_agent-5_mean: 98.96
    apples_agent-5_min: 66
    cleaning_beam_agent-0_max: 564
    cleaning_beam_agent-0_mean: 417.81
    cleaning_beam_agent-0_min: 242
    cleaning_beam_agent-1_max: 330
    cleaning_beam_agent-1_mean: 212.67
    cleaning_beam_agent-1_min: 71
    cleaning_beam_agent-2_max: 585
    cleaning_beam_agent-2_mean: 384.84
    cleaning_beam_agent-2_min: 161
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 23.4
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 507
    cleaning_beam_agent-4_mean: 404.75
    cleaning_beam_agent-4_min: 245
    cleaning_beam_agent-5_max: 78
    cleaning_beam_agent-5_mean: 16.58
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-42-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1180.000000000016
  episode_reward_mean: 964.9899999999881
  episode_reward_min: 409.00000000000284
  episodes_this_iter: 96
  episodes_total: 30240
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12947.233
    learner:
      agent-0:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0199085474014282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012034905375912786
        model: {}
        policy_loss: -0.002900572493672371
        total_loss: -0.002508792094886303
        vf_explained_var: 0.013550251722335815
        vf_loss: 21.86822509765625
      agent-1:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1413654088974
        entropy_coeff: 0.0017600000137463212
        kl: 0.002206412609666586
        model: {}
        policy_loss: -0.004080930724740028
        total_loss: -0.003725019283592701
        vf_explained_var: -0.05621209740638733
        vf_loss: 23.647109985351562
      agent-2:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0486187934875488
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013020188780501485
        model: {}
        policy_loss: -0.002980622462928295
        total_loss: -0.0025992856826633215
        vf_explained_var: 0.02792234718799591
        vf_loss: 22.269027709960938
      agent-3:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45375946164131165
        entropy_coeff: 0.0017600000137463212
        kl: 0.00126146269030869
        model: {}
        policy_loss: -0.002334033604711294
        total_loss: -0.0011688453378155828
        vf_explained_var: 0.11032237112522125
        vf_loss: 19.638019561767578
      agent-4:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9762476086616516
        entropy_coeff: 0.0017600000137463212
        kl: 0.00146431103348732
        model: {}
        policy_loss: -0.0036048106849193573
        total_loss: -0.0031467657536268234
        vf_explained_var: 0.01492282748222351
        vf_loss: 21.762420654296875
      agent-5:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5968925952911377
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011549049522727728
        model: {}
        policy_loss: -0.0025623738765716553
        total_loss: -0.0016547120176255703
        vf_explained_var: 0.10406742990016937
        vf_loss: 19.58194351196289
    load_time_ms: 16974.822
    num_steps_sampled: 30240000
    num_steps_trained: 30240000
    sample_time_ms: 128024.91
    update_time_ms: 94.305
  iterations_since_restore: 235
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.748430493273545
    ram_util_percent: 14.691031390134532
  pid: 14340
  policy_reward_max:
    agent-0: 196.66666666666632
    agent-1: 196.66666666666632
    agent-2: 196.66666666666632
    agent-3: 196.66666666666632
    agent-4: 196.66666666666632
    agent-5: 196.66666666666632
  policy_reward_mean:
    agent-0: 160.83166666666656
    agent-1: 160.83166666666656
    agent-2: 160.83166666666656
    agent-3: 160.83166666666656
    agent-4: 160.83166666666656
    agent-5: 160.83166666666656
  policy_reward_min:
    agent-0: 68.1666666666667
    agent-1: 68.1666666666667
    agent-2: 68.1666666666667
    agent-3: 68.1666666666667
    agent-4: 68.1666666666667
    agent-5: 68.1666666666667
  sampler_perf:
    mean_env_wait_ms: 30.946806911323307
    mean_inference_ms: 14.59187988852545
    mean_processing_ms: 65.79459500500099
  time_since_restore: 38513.82974028587
  time_this_iter_s: 156.7493133544922
  time_total_s: 51064.64661169052
  timestamp: 1637073728
  timesteps_since_restore: 22560000
  timesteps_this_iter: 96000
  timesteps_total: 30240000
  training_iteration: 315
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    315 |          51064.6 | 30240000 |   964.99 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 99
    apples_agent-0_mean: 4.01
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 24.59
    apples_agent-1_min: 0
    apples_agent-2_max: 89
    apples_agent-2_mean: 5.35
    apples_agent-2_min: 0
    apples_agent-3_max: 202
    apples_agent-3_mean: 121.07
    apples_agent-3_min: 36
    apples_agent-4_max: 71
    apples_agent-4_mean: 1.9
    apples_agent-4_min: 0
    apples_agent-5_max: 186
    apples_agent-5_mean: 95.34
    apples_agent-5_min: 52
    cleaning_beam_agent-0_max: 539
    cleaning_beam_agent-0_mean: 411.22
    cleaning_beam_agent-0_min: 196
    cleaning_beam_agent-1_max: 346
    cleaning_beam_agent-1_mean: 208.29
    cleaning_beam_agent-1_min: 71
    cleaning_beam_agent-2_max: 533
    cleaning_beam_agent-2_mean: 385.88
    cleaning_beam_agent-2_min: 231
    cleaning_beam_agent-3_max: 104
    cleaning_beam_agent-3_mean: 26.44
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 507
    cleaning_beam_agent-4_mean: 401.97
    cleaning_beam_agent-4_min: 165
    cleaning_beam_agent-5_max: 169
    cleaning_beam_agent-5_mean: 18.04
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-44-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1150.9999999999875
  episode_reward_mean: 962.3599999999881
  episode_reward_min: 320.0000000000016
  episodes_this_iter: 96
  episodes_total: 30336
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12929.966
    learner:
      agent-0:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0107814073562622
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010666915914043784
        model: {}
        policy_loss: -0.0029754231218248606
        total_loss: -0.00259764539077878
        vf_explained_var: 0.0923527330160141
        vf_loss: 21.567541122436523
      agent-1:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1389942169189453
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012819068506360054
        model: {}
        policy_loss: -0.003732034470885992
        total_loss: -0.0032673003152012825
        vf_explained_var: -0.030474096536636353
        vf_loss: 24.693649291992188
      agent-2:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.065528392791748
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013731600483879447
        model: {}
        policy_loss: -0.0035212868824601173
        total_loss: -0.0030718185007572174
        vf_explained_var: 0.029283955693244934
        vf_loss: 23.247953414916992
      agent-3:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44589418172836304
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010691528441384435
        model: {}
        policy_loss: -0.002603224478662014
        total_loss: -0.0013475548475980759
        vf_explained_var: 0.13885252177715302
        vf_loss: 20.404468536376953
      agent-4:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9769606590270996
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016407149378210306
        model: {}
        policy_loss: -0.0038657025434076786
        total_loss: -0.003394906409084797
        vf_explained_var: 0.08057822287082672
        vf_loss: 21.90245819091797
      agent-5:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6080606579780579
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007031661225482821
        model: {}
        policy_loss: -0.0026615792885422707
        total_loss: -0.00181041588075459
        vf_explained_var: 0.18042124807834625
        vf_loss: 19.213478088378906
    load_time_ms: 16998.657
    num_steps_sampled: 30336000
    num_steps_trained: 30336000
    sample_time_ms: 128041.193
    update_time_ms: 83.084
  iterations_since_restore: 236
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.275545851528385
    ram_util_percent: 14.74585152838428
  pid: 14340
  policy_reward_max:
    agent-0: 191.83333333333312
    agent-1: 191.83333333333312
    agent-2: 191.83333333333312
    agent-3: 191.83333333333312
    agent-4: 191.83333333333312
    agent-5: 191.83333333333312
  policy_reward_mean:
    agent-0: 160.3933333333332
    agent-1: 160.3933333333332
    agent-2: 160.3933333333332
    agent-3: 160.3933333333332
    agent-4: 160.3933333333332
    agent-5: 160.3933333333332
  policy_reward_min:
    agent-0: 53.333333333333236
    agent-1: 53.333333333333236
    agent-2: 53.333333333333236
    agent-3: 53.333333333333236
    agent-4: 53.333333333333236
    agent-5: 53.333333333333236
  sampler_perf:
    mean_env_wait_ms: 30.948612137548768
    mean_inference_ms: 14.591954653282164
    mean_processing_ms: 65.79447607332882
  time_since_restore: 38670.05401468277
  time_this_iter_s: 156.22427439689636
  time_total_s: 51220.87088608742
  timestamp: 1637073889
  timesteps_since_restore: 22656000
  timesteps_this_iter: 96000
  timesteps_total: 30336000
  training_iteration: 316
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    316 |          51220.9 | 30336000 |   962.36 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 72
    apples_agent-0_mean: 2.9
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 25.93
    apples_agent-1_min: 0
    apples_agent-2_max: 189
    apples_agent-2_mean: 5.52
    apples_agent-2_min: 0
    apples_agent-3_max: 219
    apples_agent-3_mean: 122.25
    apples_agent-3_min: 48
    apples_agent-4_max: 62
    apples_agent-4_mean: 1.64
    apples_agent-4_min: 0
    apples_agent-5_max: 278
    apples_agent-5_mean: 98.16
    apples_agent-5_min: 58
    cleaning_beam_agent-0_max: 593
    cleaning_beam_agent-0_mean: 416.43
    cleaning_beam_agent-0_min: 196
    cleaning_beam_agent-1_max: 435
    cleaning_beam_agent-1_mean: 224.71
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 537
    cleaning_beam_agent-2_mean: 390.66
    cleaning_beam_agent-2_min: 161
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 26.6
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 537
    cleaning_beam_agent-4_mean: 407.5
    cleaning_beam_agent-4_min: 225
    cleaning_beam_agent-5_max: 61
    cleaning_beam_agent-5_mean: 13.25
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-47-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1155.999999999993
  episode_reward_mean: 981.5699999999885
  episode_reward_min: 490.00000000001614
  episodes_this_iter: 96
  episodes_total: 30432
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12928.066
    learner:
      agent-0:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0367132425308228
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020466966088861227
        model: {}
        policy_loss: -0.003567313775420189
        total_loss: -0.003149600699543953
        vf_explained_var: 0.0014297962188720703
        vf_loss: 22.42325782775879
      agent-1:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1274535655975342
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019491100683808327
        model: {}
        policy_loss: -0.003741414751857519
        total_loss: -0.003304621670395136
        vf_explained_var: -0.05611073970794678
        vf_loss: 24.211090087890625
      agent-2:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0578389167785645
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018448804039508104
        model: {}
        policy_loss: -0.0033129164949059486
        total_loss: -0.00287033524364233
        vf_explained_var: 0.012145847082138062
        vf_loss: 23.043781280517578
      agent-3:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4237796664237976
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008829042781144381
        model: {}
        policy_loss: -0.002116565592586994
        total_loss: -0.0008204784244298935
        vf_explained_var: 0.0904376208782196
        vf_loss: 20.419422149658203
      agent-4:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9774956107139587
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013692438369616866
        model: {}
        policy_loss: -0.0036808722652494907
        total_loss: -0.0032313349656760693
        vf_explained_var: 0.0374336838722229
        vf_loss: 21.699295043945312
      agent-5:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5850843191146851
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014728103997185826
        model: {}
        policy_loss: -0.0027297241613268852
        total_loss: -0.0017682160250842571
        vf_explained_var: 0.10171167552471161
        vf_loss: 19.912601470947266
    load_time_ms: 18969.668
    num_steps_sampled: 30432000
    num_steps_trained: 30432000
    sample_time_ms: 128230.498
    update_time_ms: 80.619
  iterations_since_restore: 237
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.666135458167332
    ram_util_percent: 14.92788844621514
  pid: 14340
  policy_reward_max:
    agent-0: 192.6666666666666
    agent-1: 192.6666666666666
    agent-2: 192.6666666666666
    agent-3: 192.6666666666666
    agent-4: 192.6666666666666
    agent-5: 192.6666666666666
  policy_reward_mean:
    agent-0: 163.59499999999989
    agent-1: 163.59499999999989
    agent-2: 163.59499999999989
    agent-3: 163.59499999999989
    agent-4: 163.59499999999989
    agent-5: 163.59499999999989
  policy_reward_min:
    agent-0: 81.66666666666667
    agent-1: 81.66666666666667
    agent-2: 81.66666666666667
    agent-3: 81.66666666666667
    agent-4: 81.66666666666667
    agent-5: 81.66666666666667
  sampler_perf:
    mean_env_wait_ms: 30.950498737581285
    mean_inference_ms: 14.591589408038733
    mean_processing_ms: 65.79359568033092
  time_since_restore: 38845.700807094574
  time_this_iter_s: 175.6467924118042
  time_total_s: 51396.51767849922
  timestamp: 1637074065
  timesteps_since_restore: 22752000
  timesteps_this_iter: 96000
  timesteps_total: 30432000
  training_iteration: 317
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    317 |          51396.5 | 30432000 |   981.57 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 77
    apples_agent-0_mean: 2.86
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 29.17
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 3.9
    apples_agent-2_min: 0
    apples_agent-3_max: 172
    apples_agent-3_mean: 117.94
    apples_agent-3_min: 45
    apples_agent-4_max: 69
    apples_agent-4_mean: 2.5
    apples_agent-4_min: 0
    apples_agent-5_max: 166
    apples_agent-5_mean: 99.5
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 541
    cleaning_beam_agent-0_mean: 406.77
    cleaning_beam_agent-0_min: 211
    cleaning_beam_agent-1_max: 426
    cleaning_beam_agent-1_mean: 213.23
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 584
    cleaning_beam_agent-2_mean: 389.05
    cleaning_beam_agent-2_min: 147
    cleaning_beam_agent-3_max: 104
    cleaning_beam_agent-3_mean: 26.09
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 483
    cleaning_beam_agent-4_mean: 406.28
    cleaning_beam_agent-4_min: 266
    cleaning_beam_agent-5_max: 113
    cleaning_beam_agent-5_mean: 19.83
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 3
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-50-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1165.99999999999
  episode_reward_mean: 953.6999999999873
  episode_reward_min: 285.9999999999989
  episodes_this_iter: 96
  episodes_total: 30528
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12940.713
    learner:
      agent-0:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0286751985549927
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012229010462760925
        model: {}
        policy_loss: -0.003071427345275879
        total_loss: -0.0025840592570602894
        vf_explained_var: 0.009034708142280579
        vf_loss: 22.97832489013672
      agent-1:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1352086067199707
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019368641078472137
        model: {}
        policy_loss: -0.003998949192464352
        total_loss: -0.003526393324136734
        vf_explained_var: -0.0639960765838623
        vf_loss: 24.705245971679688
      agent-2:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0696347951889038
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016698334366083145
        model: {}
        policy_loss: -0.0034987209364771843
        total_loss: -0.003068143967539072
        vf_explained_var: 0.011621609330177307
        vf_loss: 23.131391525268555
      agent-3:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45874840021133423
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008122390136122704
        model: {}
        policy_loss: -0.0022981446236371994
        total_loss: -0.0010636060032993555
        vf_explained_var: 0.1201837956905365
        vf_loss: 20.419355392456055
      agent-4:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9843701124191284
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017739466857165098
        model: {}
        policy_loss: -0.003791193477809429
        total_loss: -0.0033954130485653877
        vf_explained_var: 0.08222797513008118
        vf_loss: 21.282669067382812
      agent-5:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6220795512199402
        entropy_coeff: 0.0017600000137463212
        kl: 0.001409290125593543
        model: {}
        policy_loss: -0.0029741607140749693
        total_loss: -0.002096147509291768
        vf_explained_var: 0.1455363631248474
        vf_loss: 19.728742599487305
    load_time_ms: 17648.497
    num_steps_sampled: 30528000
    num_steps_trained: 30528000
    sample_time_ms: 128536.733
    update_time_ms: 85.632
  iterations_since_restore: 238
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.29377777777778
    ram_util_percent: 14.751555555555557
  pid: 14340
  policy_reward_max:
    agent-0: 194.33333333333303
    agent-1: 194.33333333333303
    agent-2: 194.33333333333303
    agent-3: 194.33333333333303
    agent-4: 194.33333333333303
    agent-5: 194.33333333333303
  policy_reward_mean:
    agent-0: 158.94999999999993
    agent-1: 158.94999999999993
    agent-2: 158.94999999999993
    agent-3: 158.94999999999993
    agent-4: 158.94999999999993
    agent-5: 158.94999999999993
  policy_reward_min:
    agent-0: 47.666666666666536
    agent-1: 47.666666666666536
    agent-2: 47.666666666666536
    agent-3: 47.666666666666536
    agent-4: 47.666666666666536
    agent-5: 47.666666666666536
  sampler_perf:
    mean_env_wait_ms: 30.952742889511324
    mean_inference_ms: 14.591661719453057
    mean_processing_ms: 65.79318590800882
  time_since_restore: 39003.59938120842
  time_this_iter_s: 157.89857411384583
  time_total_s: 51554.41625261307
  timestamp: 1637074223
  timesteps_since_restore: 22848000
  timesteps_this_iter: 96000
  timesteps_total: 30528000
  training_iteration: 318
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    318 |          51554.4 | 30528000 |    953.7 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 77
    apples_agent-0_mean: 2.26
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 26.23
    apples_agent-1_min: 0
    apples_agent-2_max: 368
    apples_agent-2_mean: 12.06
    apples_agent-2_min: 0
    apples_agent-3_max: 273
    apples_agent-3_mean: 122.73
    apples_agent-3_min: 57
    apples_agent-4_max: 73
    apples_agent-4_mean: 2.9
    apples_agent-4_min: 0
    apples_agent-5_max: 257
    apples_agent-5_mean: 105.27
    apples_agent-5_min: 56
    cleaning_beam_agent-0_max: 521
    cleaning_beam_agent-0_mean: 400.69
    cleaning_beam_agent-0_min: 211
    cleaning_beam_agent-1_max: 381
    cleaning_beam_agent-1_mean: 221.79
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 575
    cleaning_beam_agent-2_mean: 378.07
    cleaning_beam_agent-2_min: 104
    cleaning_beam_agent-3_max: 79
    cleaning_beam_agent-3_mean: 22.68
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 499
    cleaning_beam_agent-4_mean: 397.92
    cleaning_beam_agent-4_min: 181
    cleaning_beam_agent-5_max: 150
    cleaning_beam_agent-5_mean: 16.79
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-52-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1122.0000000000016
  episode_reward_mean: 951.2399999999882
  episode_reward_min: 600.0000000000014
  episodes_this_iter: 96
  episodes_total: 30624
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12929.597
    learner:
      agent-0:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0262117385864258
        entropy_coeff: 0.0017600000137463212
        kl: 0.000920108228456229
        model: {}
        policy_loss: -0.00289584556594491
        total_loss: -0.0024350155144929886
        vf_explained_var: 0.0009745210409164429
        vf_loss: 22.66958999633789
      agent-1:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1514815092086792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017540595727041364
        model: {}
        policy_loss: -0.003981512505561113
        total_loss: -0.0035866396501660347
        vf_explained_var: -0.07202112674713135
        vf_loss: 24.21479034423828
      agent-2:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.061505675315857
        entropy_coeff: 0.0017600000137463212
        kl: 0.001387214520946145
        model: {}
        policy_loss: -0.0035943840630352497
        total_loss: -0.003154436592012644
        vf_explained_var: -0.004901304841041565
        vf_loss: 23.08197784423828
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43423694372177124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012471560621634126
        model: {}
        policy_loss: -0.0022475337609648705
        total_loss: -0.0009792651981115341
        vf_explained_var: 0.09775400161743164
        vf_loss: 20.325244903564453
      agent-4:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.985335111618042
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019036278827115893
        model: {}
        policy_loss: -0.003955435939133167
        total_loss: -0.0035742938052862883
        vf_explained_var: 0.06347626447677612
        vf_loss: 21.153310775756836
      agent-5:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6144163608551025
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011089564068242908
        model: {}
        policy_loss: -0.0028321542777121067
        total_loss: -0.0019276509992778301
        vf_explained_var: 0.11486703157424927
        vf_loss: 19.858760833740234
    load_time_ms: 17453.984
    num_steps_sampled: 30624000
    num_steps_trained: 30624000
    sample_time_ms: 128507.672
    update_time_ms: 90.413
  iterations_since_restore: 239
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.888584474885842
    ram_util_percent: 14.7027397260274
  pid: 14340
  policy_reward_max:
    agent-0: 186.99999999999946
    agent-1: 186.99999999999946
    agent-2: 186.99999999999946
    agent-3: 186.99999999999946
    agent-4: 186.99999999999946
    agent-5: 186.99999999999946
  policy_reward_mean:
    agent-0: 158.53999999999988
    agent-1: 158.53999999999988
    agent-2: 158.53999999999988
    agent-3: 158.53999999999988
    agent-4: 158.53999999999988
    agent-5: 158.53999999999988
  policy_reward_min:
    agent-0: 100.00000000000047
    agent-1: 100.00000000000047
    agent-2: 100.00000000000047
    agent-3: 100.00000000000047
    agent-4: 100.00000000000047
    agent-5: 100.00000000000047
  sampler_perf:
    mean_env_wait_ms: 30.954121074600277
    mean_inference_ms: 14.591545976737038
    mean_processing_ms: 65.792794589543
  time_since_restore: 39157.675884485245
  time_this_iter_s: 154.07650327682495
  time_total_s: 51708.49275588989
  timestamp: 1637074377
  timesteps_since_restore: 22944000
  timesteps_this_iter: 96000
  timesteps_total: 30624000
  training_iteration: 319
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    319 |          51708.5 | 30624000 |   951.24 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 331
    apples_agent-0_mean: 7.7
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 27.21
    apples_agent-1_min: 0
    apples_agent-2_max: 126
    apples_agent-2_mean: 5.63
    apples_agent-2_min: 0
    apples_agent-3_max: 185
    apples_agent-3_mean: 122.06
    apples_agent-3_min: 56
    apples_agent-4_max: 73
    apples_agent-4_mean: 2.17
    apples_agent-4_min: 0
    apples_agent-5_max: 366
    apples_agent-5_mean: 101.77
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 504
    cleaning_beam_agent-0_mean: 388.33
    cleaning_beam_agent-0_min: 150
    cleaning_beam_agent-1_max: 394
    cleaning_beam_agent-1_mean: 220.9
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 576
    cleaning_beam_agent-2_mean: 384.99
    cleaning_beam_agent-2_min: 217
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 21.28
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 487
    cleaning_beam_agent-4_mean: 398.4
    cleaning_beam_agent-4_min: 181
    cleaning_beam_agent-5_max: 390
    cleaning_beam_agent-5_mean: 26.1
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-55-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1152.9999999999893
  episode_reward_mean: 960.849999999989
  episode_reward_min: 377.00000000001154
  episodes_this_iter: 96
  episodes_total: 30720
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12924.339
    learner:
      agent-0:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0251660346984863
        entropy_coeff: 0.0017600000137463212
        kl: 0.000982123427093029
        model: {}
        policy_loss: -0.002873573452234268
        total_loss: -0.002430792897939682
        vf_explained_var: 0.013973861932754517
        vf_loss: 22.470714569091797
      agent-1:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.147264003753662
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023289755918085575
        model: {}
        policy_loss: -0.003965242300182581
        total_loss: -0.0036338528152555227
        vf_explained_var: -0.036408960819244385
        vf_loss: 23.505754470825195
      agent-2:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0678786039352417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017287100199609995
        model: {}
        policy_loss: -0.003411697456613183
        total_loss: -0.00299014407210052
        vf_explained_var: -0.004258885979652405
        vf_loss: 23.01018524169922
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43860912322998047
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008014605264179409
        model: {}
        policy_loss: -0.0022275042720139027
        total_loss: -0.0009684041142463684
        vf_explained_var: 0.09547202289104462
        vf_loss: 20.310548782348633
      agent-4:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9811443090438843
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013609675224870443
        model: {}
        policy_loss: -0.00362744415178895
        total_loss: -0.0031878761947155
        vf_explained_var: 0.04128843545913696
        vf_loss: 21.663822174072266
      agent-5:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6212254166603088
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007141437963582575
        model: {}
        policy_loss: -0.002273610094562173
        total_loss: -0.0013579067308455706
        vf_explained_var: 0.10325591266155243
        vf_loss: 20.090593338012695
    load_time_ms: 17513.635
    num_steps_sampled: 30720000
    num_steps_trained: 30720000
    sample_time_ms: 128659.344
    update_time_ms: 79.704
  iterations_since_restore: 240
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.63911111111111
    ram_util_percent: 14.75066666666667
  pid: 14340
  policy_reward_max:
    agent-0: 192.16666666666686
    agent-1: 192.16666666666686
    agent-2: 192.16666666666686
    agent-3: 192.16666666666686
    agent-4: 192.16666666666686
    agent-5: 192.16666666666686
  policy_reward_mean:
    agent-0: 160.1416666666666
    agent-1: 160.1416666666666
    agent-2: 160.1416666666666
    agent-3: 160.1416666666666
    agent-4: 160.1416666666666
    agent-5: 160.1416666666666
  policy_reward_min:
    agent-0: 62.833333333333066
    agent-1: 62.833333333333066
    agent-2: 62.833333333333066
    agent-3: 62.833333333333066
    agent-4: 62.833333333333066
    agent-5: 62.833333333333066
  sampler_perf:
    mean_env_wait_ms: 30.95605244858348
    mean_inference_ms: 14.591558114106727
    mean_processing_ms: 65.79220740711095
  time_since_restore: 39315.556257009506
  time_this_iter_s: 157.88037252426147
  time_total_s: 51866.373128414154
  timestamp: 1637074535
  timesteps_since_restore: 23040000
  timesteps_this_iter: 96000
  timesteps_total: 30720000
  training_iteration: 320
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    320 |          51866.4 | 30720000 |   960.85 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 3.39
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 24.73
    apples_agent-1_min: 0
    apples_agent-2_max: 103
    apples_agent-2_mean: 5.63
    apples_agent-2_min: 0
    apples_agent-3_max: 244
    apples_agent-3_mean: 128.47
    apples_agent-3_min: 51
    apples_agent-4_max: 34
    apples_agent-4_mean: 0.79
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 95.63
    apples_agent-5_min: 57
    cleaning_beam_agent-0_max: 520
    cleaning_beam_agent-0_mean: 401.61
    cleaning_beam_agent-0_min: 278
    cleaning_beam_agent-1_max: 442
    cleaning_beam_agent-1_mean: 235.23
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 567
    cleaning_beam_agent-2_mean: 371.63
    cleaning_beam_agent-2_min: 194
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 22.01
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 491
    cleaning_beam_agent-4_mean: 396.31
    cleaning_beam_agent-4_min: 262
    cleaning_beam_agent-5_max: 85
    cleaning_beam_agent-5_mean: 17.18
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_09-58-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1145.9999999999898
  episode_reward_mean: 985.9899999999866
  episode_reward_min: 502.00000000000273
  episodes_this_iter: 96
  episodes_total: 30816
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12898.951
    learner:
      agent-0:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0249781608581543
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026756874285638332
        model: {}
        policy_loss: -0.0034568249247968197
        total_loss: -0.003051654202863574
        vf_explained_var: 0.025817498564720154
        vf_loss: 22.091320037841797
      agent-1:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1412098407745361
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014268987579271197
        model: {}
        policy_loss: -0.0038051940500736237
        total_loss: -0.0034836800768971443
        vf_explained_var: -0.03725004196166992
        vf_loss: 23.300430297851562
      agent-2:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.079704999923706
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013729296624660492
        model: {}
        policy_loss: -0.0033980216830968857
        total_loss: -0.0030053548980504274
        vf_explained_var: 0.001505434513092041
        vf_loss: 22.929494857788086
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43946412205696106
        entropy_coeff: 0.0017600000137463212
        kl: 0.001401649322360754
        model: {}
        policy_loss: -0.0025533256120979786
        total_loss: -0.001338134054094553
        vf_explained_var: 0.1104651540517807
        vf_loss: 19.886451721191406
      agent-4:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9889914393424988
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018200393533334136
        model: {}
        policy_loss: -0.004056563135236502
        total_loss: -0.0036279517225921154
        vf_explained_var: 0.03840303421020508
        vf_loss: 21.692344665527344
      agent-5:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5716822147369385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015026769833639264
        model: {}
        policy_loss: -0.002670849673449993
        total_loss: -0.0017326520755887032
        vf_explained_var: 0.11982353031635284
        vf_loss: 19.443588256835938
    load_time_ms: 17370.44
    num_steps_sampled: 30816000
    num_steps_trained: 30816000
    sample_time_ms: 128598.652
    update_time_ms: 65.191
  iterations_since_restore: 241
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.399999999999995
    ram_util_percent: 14.643049327354262
  pid: 14340
  policy_reward_max:
    agent-0: 191.00000000000009
    agent-1: 191.00000000000009
    agent-2: 191.00000000000009
    agent-3: 191.00000000000009
    agent-4: 191.00000000000009
    agent-5: 191.00000000000009
  policy_reward_mean:
    agent-0: 164.33166666666654
    agent-1: 164.33166666666654
    agent-2: 164.33166666666654
    agent-3: 164.33166666666654
    agent-4: 164.33166666666654
    agent-5: 164.33166666666654
  policy_reward_min:
    agent-0: 83.66666666666669
    agent-1: 83.66666666666669
    agent-2: 83.66666666666669
    agent-3: 83.66666666666669
    agent-4: 83.66666666666669
    agent-5: 83.66666666666669
  sampler_perf:
    mean_env_wait_ms: 30.956654752783017
    mean_inference_ms: 14.591290664723047
    mean_processing_ms: 65.79151785466534
  time_since_restore: 39470.353248119354
  time_this_iter_s: 154.79699110984802
  time_total_s: 52021.170119524
  timestamp: 1637074692
  timesteps_since_restore: 23136000
  timesteps_this_iter: 96000
  timesteps_total: 30816000
  training_iteration: 321
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    321 |          52021.2 | 30816000 |   985.99 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 114
    apples_agent-0_mean: 3.36
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 23.98
    apples_agent-1_min: 0
    apples_agent-2_max: 113
    apples_agent-2_mean: 6.94
    apples_agent-2_min: 0
    apples_agent-3_max: 184
    apples_agent-3_mean: 123.83
    apples_agent-3_min: 62
    apples_agent-4_max: 40
    apples_agent-4_mean: 1.35
    apples_agent-4_min: 0
    apples_agent-5_max: 164
    apples_agent-5_mean: 99.84
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 508
    cleaning_beam_agent-0_mean: 391.95
    cleaning_beam_agent-0_min: 207
    cleaning_beam_agent-1_max: 435
    cleaning_beam_agent-1_mean: 226.16
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 519
    cleaning_beam_agent-2_mean: 363.27
    cleaning_beam_agent-2_min: 172
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 23.86
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 488
    cleaning_beam_agent-4_mean: 405.62
    cleaning_beam_agent-4_min: 159
    cleaning_beam_agent-5_max: 92
    cleaning_beam_agent-5_mean: 17.15
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-01-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1191.9999999999964
  episode_reward_mean: 975.3199999999872
  episode_reward_min: 314.000000000001
  episodes_this_iter: 96
  episodes_total: 30912
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12921.704
    learner:
      agent-0:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0270440578460693
        entropy_coeff: 0.0017600000137463212
        kl: 0.002071747090667486
        model: {}
        policy_loss: -0.0031641910318285227
        total_loss: -0.0027431691996753216
        vf_explained_var: 0.021803125739097595
        vf_loss: 22.286216735839844
      agent-1:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1517606973648071
        entropy_coeff: 0.0017600000137463212
        kl: 0.002073870040476322
        model: {}
        policy_loss: -0.0039146095514297485
        total_loss: -0.0035742847248911858
        vf_explained_var: -0.04405909776687622
        vf_loss: 23.674224853515625
      agent-2:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0906586647033691
        entropy_coeff: 0.0017600000137463212
        kl: 0.001431540586054325
        model: {}
        policy_loss: -0.003644780721515417
        total_loss: -0.0033177570439875126
        vf_explained_var: 0.0298614501953125
        vf_loss: 22.465805053710938
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43321728706359863
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014515327056869864
        model: {}
        policy_loss: -0.0025897668674588203
        total_loss: -0.0013976830523461103
        vf_explained_var: 0.1371045857667923
        vf_loss: 19.54546546936035
      agent-4:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9819281101226807
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019196136854588985
        model: {}
        policy_loss: -0.004057792015373707
        total_loss: -0.003585603553801775
        vf_explained_var: 0.03650142252445221
        vf_loss: 22.003822326660156
      agent-5:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5937181115150452
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013413013657554984
        model: {}
        policy_loss: -0.0030091339722275734
        total_loss: -0.0021378188394010067
        vf_explained_var: 0.1450928896665573
        vf_loss: 19.16259765625
    load_time_ms: 18163.914
    num_steps_sampled: 30912000
    num_steps_trained: 30912000
    sample_time_ms: 128598.218
    update_time_ms: 64.621
  iterations_since_restore: 242
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.775100401606426
    ram_util_percent: 14.959437751004018
  pid: 14340
  policy_reward_max:
    agent-0: 198.66666666666615
    agent-1: 198.66666666666615
    agent-2: 198.66666666666615
    agent-3: 198.66666666666615
    agent-4: 198.66666666666615
    agent-5: 198.66666666666615
  policy_reward_mean:
    agent-0: 162.5533333333332
    agent-1: 162.5533333333332
    agent-2: 162.5533333333332
    agent-3: 162.5533333333332
    agent-4: 162.5533333333332
    agent-5: 162.5533333333332
  policy_reward_min:
    agent-0: 52.33333333333319
    agent-1: 52.33333333333319
    agent-2: 52.33333333333319
    agent-3: 52.33333333333319
    agent-4: 52.33333333333319
    agent-5: 52.33333333333319
  sampler_perf:
    mean_env_wait_ms: 30.957807234891526
    mean_inference_ms: 14.59124539272469
    mean_processing_ms: 65.79127723391446
  time_since_restore: 39644.77089166641
  time_this_iter_s: 174.4176435470581
  time_total_s: 52195.58776307106
  timestamp: 1637074867
  timesteps_since_restore: 23232000
  timesteps_this_iter: 96000
  timesteps_total: 30912000
  training_iteration: 322
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    322 |          52195.6 | 30912000 |   975.32 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 65
    apples_agent-0_mean: 4.18
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 28.13
    apples_agent-1_min: 0
    apples_agent-2_max: 98
    apples_agent-2_mean: 2.69
    apples_agent-2_min: 0
    apples_agent-3_max: 191
    apples_agent-3_mean: 118.94
    apples_agent-3_min: 42
    apples_agent-4_max: 70
    apples_agent-4_mean: 4.47
    apples_agent-4_min: 0
    apples_agent-5_max: 171
    apples_agent-5_mean: 92.59
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 521
    cleaning_beam_agent-0_mean: 390.31
    cleaning_beam_agent-0_min: 227
    cleaning_beam_agent-1_max: 470
    cleaning_beam_agent-1_mean: 227.85
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 486
    cleaning_beam_agent-2_mean: 371.54
    cleaning_beam_agent-2_min: 134
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 23.32
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 478
    cleaning_beam_agent-4_mean: 387.2
    cleaning_beam_agent-4_min: 206
    cleaning_beam_agent-5_max: 266
    cleaning_beam_agent-5_mean: 22.93
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-03-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1204.000000000004
  episode_reward_mean: 948.48999999999
  episode_reward_min: 261.9999999999962
  episodes_this_iter: 96
  episodes_total: 31008
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12919.218
    learner:
      agent-0:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0228337049484253
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012888459023088217
        model: {}
        policy_loss: -0.0028818482533097267
        total_loss: -0.0021420414559543133
        vf_explained_var: 0.050129592418670654
        vf_loss: 25.399921417236328
      agent-1:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1467952728271484
        entropy_coeff: 0.0017600000137463212
        kl: 0.001498062745667994
        model: {}
        policy_loss: -0.003856032621115446
        total_loss: -0.0030454727821052074
        vf_explained_var: -0.05971696972846985
        vf_loss: 28.289194107055664
      agent-2:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0845848321914673
        entropy_coeff: 0.0017600000137463212
        kl: 0.002075297525152564
        model: {}
        policy_loss: -0.003838579636067152
        total_loss: -0.0030905213207006454
        vf_explained_var: 0.00814768671989441
        vf_loss: 26.569278717041016
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44952625036239624
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010450296103954315
        model: {}
        policy_loss: -0.002475721761584282
        total_loss: -0.0010765886399894953
        vf_explained_var: 0.18032975494861603
        vf_loss: 21.902973175048828
      agent-4:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9968374967575073
        entropy_coeff: 0.0017600000137463212
        kl: 0.002013362478464842
        model: {}
        policy_loss: -0.004412754904478788
        total_loss: -0.0037929508835077286
        vf_explained_var: 0.11717592179775238
        vf_loss: 23.742340087890625
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6149100065231323
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010245740413665771
        model: {}
        policy_loss: -0.002857261337339878
        total_loss: -0.0017686029896140099
        vf_explained_var: 0.18779626488685608
        vf_loss: 21.70901870727539
    load_time_ms: 18141.654
    num_steps_sampled: 31008000
    num_steps_trained: 31008000
    sample_time_ms: 128590.91
    update_time_ms: 62.051
  iterations_since_restore: 243
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.79009009009009
    ram_util_percent: 14.640990990990991
  pid: 14340
  policy_reward_max:
    agent-0: 200.6666666666666
    agent-1: 200.6666666666666
    agent-2: 200.6666666666666
    agent-3: 200.6666666666666
    agent-4: 200.6666666666666
    agent-5: 200.6666666666666
  policy_reward_mean:
    agent-0: 158.08166666666656
    agent-1: 158.08166666666656
    agent-2: 158.08166666666656
    agent-3: 158.08166666666656
    agent-4: 158.08166666666656
    agent-5: 158.08166666666656
  policy_reward_min:
    agent-0: 43.66666666666656
    agent-1: 43.66666666666656
    agent-2: 43.66666666666656
    agent-3: 43.66666666666656
    agent-4: 43.66666666666656
    agent-5: 43.66666666666656
  sampler_perf:
    mean_env_wait_ms: 30.958609201272697
    mean_inference_ms: 14.591235676874051
    mean_processing_ms: 65.79077101229048
  time_since_restore: 39800.650032281876
  time_this_iter_s: 155.87914061546326
  time_total_s: 52351.46690368652
  timestamp: 1637075023
  timesteps_since_restore: 23328000
  timesteps_this_iter: 96000
  timesteps_total: 31008000
  training_iteration: 323
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    323 |          52351.5 | 31008000 |   948.49 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 89
    apples_agent-0_mean: 2.56
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 27.71
    apples_agent-1_min: 0
    apples_agent-2_max: 35
    apples_agent-2_mean: 2.01
    apples_agent-2_min: 0
    apples_agent-3_max: 386
    apples_agent-3_mean: 126.4
    apples_agent-3_min: 61
    apples_agent-4_max: 81
    apples_agent-4_mean: 3.42
    apples_agent-4_min: 0
    apples_agent-5_max: 422
    apples_agent-5_mean: 99.49
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 497
    cleaning_beam_agent-0_mean: 397.11
    cleaning_beam_agent-0_min: 142
    cleaning_beam_agent-1_max: 391
    cleaning_beam_agent-1_mean: 236.36
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 495
    cleaning_beam_agent-2_mean: 383.54
    cleaning_beam_agent-2_min: 220
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 23.66
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 466
    cleaning_beam_agent-4_mean: 391.0
    cleaning_beam_agent-4_min: 220
    cleaning_beam_agent-5_max: 266
    cleaning_beam_agent-5_mean: 20.33
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-06-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1163.0000000000055
  episode_reward_mean: 958.1899999999899
  episode_reward_min: 452.0000000000109
  episodes_this_iter: 96
  episodes_total: 31104
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12900.21
    learner:
      agent-0:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.028529167175293
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011871502501890063
        model: {}
        policy_loss: -0.003057715017348528
        total_loss: -0.002505956217646599
        vf_explained_var: 0.058298707008361816
        vf_loss: 23.619699478149414
      agent-1:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1301825046539307
        entropy_coeff: 0.0017600000137463212
        kl: 0.001419584034010768
        model: {}
        policy_loss: -0.0036693038418889046
        total_loss: -0.0030102459713816643
        vf_explained_var: -0.060623615980148315
        vf_loss: 26.481782913208008
      agent-2:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0837950706481934
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013340761652216315
        model: {}
        policy_loss: -0.0035982306580990553
        total_loss: -0.0030365977436304092
        vf_explained_var: 0.014243632555007935
        vf_loss: 24.691097259521484
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42704230546951294
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012568323872983456
        model: {}
        policy_loss: -0.002454339526593685
        total_loss: -0.001080840826034546
        vf_explained_var: 0.1478738933801651
        vf_loss: 21.250951766967773
      agent-4:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9872332811355591
        entropy_coeff: 0.0017600000137463212
        kl: 0.001896028988994658
        model: {}
        policy_loss: -0.0037405584007501602
        total_loss: -0.003235657699406147
        vf_explained_var: 0.10771134495735168
        vf_loss: 22.424285888671875
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5820364952087402
        entropy_coeff: 0.0017600000137463212
        kl: 0.002012797864153981
        model: {}
        policy_loss: -0.0027058455161750317
        total_loss: -0.0015628671972081065
        vf_explained_var: 0.12765568494796753
        vf_loss: 21.67364501953125
    load_time_ms: 19390.255
    num_steps_sampled: 31104000
    num_steps_trained: 31104000
    sample_time_ms: 128632.32
    update_time_ms: 61.96
  iterations_since_restore: 244
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.127615062761507
    ram_util_percent: 14.906276150627617
  pid: 14340
  policy_reward_max:
    agent-0: 193.8333333333332
    agent-1: 193.8333333333332
    agent-2: 193.8333333333332
    agent-3: 193.8333333333332
    agent-4: 193.8333333333332
    agent-5: 193.8333333333332
  policy_reward_mean:
    agent-0: 159.69833333333324
    agent-1: 159.69833333333324
    agent-2: 159.69833333333324
    agent-3: 159.69833333333324
    agent-4: 159.69833333333324
    agent-5: 159.69833333333324
  policy_reward_min:
    agent-0: 75.33333333333336
    agent-1: 75.33333333333336
    agent-2: 75.33333333333336
    agent-3: 75.33333333333336
    agent-4: 75.33333333333336
    agent-5: 75.33333333333336
  sampler_perf:
    mean_env_wait_ms: 30.95969847559044
    mean_inference_ms: 14.596819790815946
    mean_processing_ms: 65.79050098034861
  time_since_restore: 39968.168627500534
  time_this_iter_s: 167.51859521865845
  time_total_s: 52518.98549890518
  timestamp: 1637075191
  timesteps_since_restore: 23424000
  timesteps_this_iter: 96000
  timesteps_total: 31104000
  training_iteration: 324
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 26.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    324 |            52519 | 31104000 |   958.19 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 5.05
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 26.55
    apples_agent-1_min: 0
    apples_agent-2_max: 97
    apples_agent-2_mean: 3.57
    apples_agent-2_min: 0
    apples_agent-3_max: 224
    apples_agent-3_mean: 122.46
    apples_agent-3_min: 68
    apples_agent-4_max: 55
    apples_agent-4_mean: 1.67
    apples_agent-4_min: 0
    apples_agent-5_max: 196
    apples_agent-5_mean: 97.61
    apples_agent-5_min: 38
    cleaning_beam_agent-0_max: 509
    cleaning_beam_agent-0_mean: 395.06
    cleaning_beam_agent-0_min: 286
    cleaning_beam_agent-1_max: 423
    cleaning_beam_agent-1_mean: 237.51
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 530
    cleaning_beam_agent-2_mean: 382.92
    cleaning_beam_agent-2_min: 205
    cleaning_beam_agent-3_max: 97
    cleaning_beam_agent-3_mean: 24.36
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 486
    cleaning_beam_agent-4_mean: 398.41
    cleaning_beam_agent-4_min: 279
    cleaning_beam_agent-5_max: 185
    cleaning_beam_agent-5_mean: 18.35
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-09-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1182.0000000000018
  episode_reward_mean: 979.1399999999885
  episode_reward_min: 566.999999999999
  episodes_this_iter: 96
  episodes_total: 31200
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12924.249
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.029710292816162
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016785733168944716
        model: {}
        policy_loss: -0.002960296580567956
        total_loss: -0.002483227988705039
        vf_explained_var: 0.036350831389427185
        vf_loss: 22.893625259399414
      agent-1:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.137324571609497
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013252601493149996
        model: {}
        policy_loss: -0.0036776307970285416
        total_loss: -0.0032166007440537214
        vf_explained_var: -0.03314277529716492
        vf_loss: 24.627208709716797
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0849144458770752
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015444684540852904
        model: {}
        policy_loss: -0.003470065537840128
        total_loss: -0.0030206344090402126
        vf_explained_var: 0.031488239765167236
        vf_loss: 23.58877944946289
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4195214807987213
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012991047697141767
        model: {}
        policy_loss: -0.0022998200729489326
        total_loss: -0.0009723763214424253
        vf_explained_var: 0.12485595047473907
        vf_loss: 20.658023834228516
      agent-4:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9917722344398499
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017856198828667402
        model: {}
        policy_loss: -0.0038194898515939713
        total_loss: -0.003283439204096794
        vf_explained_var: 0.05001474916934967
        vf_loss: 22.815696716308594
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5912271738052368
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017540769185870886
        model: {}
        policy_loss: -0.0027546174824237823
        total_loss: -0.001773785101249814
        vf_explained_var: 0.1407846361398697
        vf_loss: 20.21391487121582
    load_time_ms: 19373.838
    num_steps_sampled: 31200000
    num_steps_trained: 31200000
    sample_time_ms: 128673.742
    update_time_ms: 60.87
  iterations_since_restore: 245
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.657589285714284
    ram_util_percent: 14.695089285714287
  pid: 14340
  policy_reward_max:
    agent-0: 196.9999999999997
    agent-1: 196.9999999999997
    agent-2: 196.9999999999997
    agent-3: 196.9999999999997
    agent-4: 196.9999999999997
    agent-5: 196.9999999999997
  policy_reward_mean:
    agent-0: 163.1899999999999
    agent-1: 163.1899999999999
    agent-2: 163.1899999999999
    agent-3: 163.1899999999999
    agent-4: 163.1899999999999
    agent-5: 163.1899999999999
  policy_reward_min:
    agent-0: 94.50000000000003
    agent-1: 94.50000000000003
    agent-2: 94.50000000000003
    agent-3: 94.50000000000003
    agent-4: 94.50000000000003
    agent-5: 94.50000000000003
  sampler_perf:
    mean_env_wait_ms: 30.961066894885697
    mean_inference_ms: 14.59736185148871
    mean_processing_ms: 65.79032328866553
  time_since_restore: 40125.40010714531
  time_this_iter_s: 157.2314796447754
  time_total_s: 52676.21697854996
  timestamp: 1637075348
  timesteps_since_restore: 23520000
  timesteps_this_iter: 96000
  timesteps_total: 31200000
  training_iteration: 325
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    325 |          52676.2 | 31200000 |   979.14 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 3.13
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 28.91
    apples_agent-1_min: 0
    apples_agent-2_max: 252
    apples_agent-2_mean: 14.9
    apples_agent-2_min: 0
    apples_agent-3_max: 323
    apples_agent-3_mean: 122.62
    apples_agent-3_min: 55
    apples_agent-4_max: 53
    apples_agent-4_mean: 0.57
    apples_agent-4_min: 0
    apples_agent-5_max: 293
    apples_agent-5_mean: 100.71
    apples_agent-5_min: 47
    cleaning_beam_agent-0_max: 491
    cleaning_beam_agent-0_mean: 387.3
    cleaning_beam_agent-0_min: 278
    cleaning_beam_agent-1_max: 412
    cleaning_beam_agent-1_mean: 224.36
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 542
    cleaning_beam_agent-2_mean: 372.05
    cleaning_beam_agent-2_min: 160
    cleaning_beam_agent-3_max: 62
    cleaning_beam_agent-3_mean: 22.92
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 486
    cleaning_beam_agent-4_mean: 399.16
    cleaning_beam_agent-4_min: 268
    cleaning_beam_agent-5_max: 172
    cleaning_beam_agent-5_mean: 22.2
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-11-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1166.9999999999893
  episode_reward_mean: 976.609999999988
  episode_reward_min: 546.0000000000094
  episodes_this_iter: 96
  episodes_total: 31296
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12924.925
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0368353128433228
        entropy_coeff: 0.0017600000137463212
        kl: 0.002021675929427147
        model: {}
        policy_loss: -0.0032186596654355526
        total_loss: -0.002802422735840082
        vf_explained_var: 0.05244822800159454
        vf_loss: 22.410690307617188
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1497488021850586
        entropy_coeff: 0.0017600000137463212
        kl: 0.001484467415139079
        model: {}
        policy_loss: -0.0037264679558575153
        total_loss: -0.0032652472145855427
        vf_explained_var: -0.050981104373931885
        vf_loss: 24.847789764404297
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0747379064559937
        entropy_coeff: 0.0017600000137463212
        kl: 0.001717630890198052
        model: {}
        policy_loss: -0.003401754656806588
        total_loss: -0.003001570235937834
        vf_explained_var: 0.05835901200771332
        vf_loss: 22.91722297668457
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4075791835784912
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009755799546837807
        model: {}
        policy_loss: -0.002187103498727083
        total_loss: -0.0008306332165375352
        vf_explained_var: 0.11291186511516571
        vf_loss: 20.738086700439453
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9903115630149841
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020491890609264374
        model: {}
        policy_loss: -0.0034928256645798683
        total_loss: -0.0029314043931663036
        vf_explained_var: 0.024838507175445557
        vf_loss: 23.043685913085938
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5845066905021667
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012704358668997884
        model: {}
        policy_loss: -0.002421341836452484
        total_loss: -0.0013071116991341114
        vf_explained_var: 0.08305056393146515
        vf_loss: 21.42963409423828
    load_time_ms: 19367.567
    num_steps_sampled: 31296000
    num_steps_trained: 31296000
    sample_time_ms: 128750.274
    update_time_ms: 65.078
  iterations_since_restore: 246
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.66875
    ram_util_percent: 14.67544642857143
  pid: 14340
  policy_reward_max:
    agent-0: 194.50000000000009
    agent-1: 194.50000000000009
    agent-2: 194.50000000000009
    agent-3: 194.50000000000009
    agent-4: 194.50000000000009
    agent-5: 194.50000000000009
  policy_reward_mean:
    agent-0: 162.7683333333332
    agent-1: 162.7683333333332
    agent-2: 162.7683333333332
    agent-3: 162.7683333333332
    agent-4: 162.7683333333332
    agent-5: 162.7683333333332
  policy_reward_min:
    agent-0: 91.00000000000001
    agent-1: 91.00000000000001
    agent-2: 91.00000000000001
    agent-3: 91.00000000000001
    agent-4: 91.00000000000001
    agent-5: 91.00000000000001
  sampler_perf:
    mean_env_wait_ms: 30.962522802857663
    mean_inference_ms: 14.597419830712388
    mean_processing_ms: 65.79129190232665
  time_since_restore: 40282.399017333984
  time_this_iter_s: 156.99891018867493
  time_total_s: 52833.21588873863
  timestamp: 1637075506
  timesteps_since_restore: 23616000
  timesteps_this_iter: 96000
  timesteps_total: 31296000
  training_iteration: 326
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    326 |          52833.2 | 31296000 |   976.61 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 76
    apples_agent-0_mean: 3.49
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 33.3
    apples_agent-1_min: 0
    apples_agent-2_max: 543
    apples_agent-2_mean: 9.49
    apples_agent-2_min: 0
    apples_agent-3_max: 195
    apples_agent-3_mean: 125.41
    apples_agent-3_min: 74
    apples_agent-4_max: 85
    apples_agent-4_mean: 2.47
    apples_agent-4_min: 0
    apples_agent-5_max: 183
    apples_agent-5_mean: 100.7
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 542
    cleaning_beam_agent-0_mean: 417.38
    cleaning_beam_agent-0_min: 195
    cleaning_beam_agent-1_max: 413
    cleaning_beam_agent-1_mean: 228.04
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 533
    cleaning_beam_agent-2_mean: 376.4
    cleaning_beam_agent-2_min: 45
    cleaning_beam_agent-3_max: 80
    cleaning_beam_agent-3_mean: 22.27
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 491
    cleaning_beam_agent-4_mean: 399.96
    cleaning_beam_agent-4_min: 216
    cleaning_beam_agent-5_max: 191
    cleaning_beam_agent-5_mean: 16.34
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-14-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1169.9999999999952
  episode_reward_mean: 978.8199999999877
  episode_reward_min: 580.0000000000093
  episodes_this_iter: 96
  episodes_total: 31392
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12915.826
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.015810251235962
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018402107525616884
        model: {}
        policy_loss: -0.0031859716400504112
        total_loss: -0.0027845031581819057
        vf_explained_var: 0.02239769697189331
        vf_loss: 21.89291000366211
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1321361064910889
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016375610139220953
        model: {}
        policy_loss: -0.0038868822157382965
        total_loss: -0.00350724789313972
        vf_explained_var: -0.060975730419158936
        vf_loss: 23.72195053100586
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.072965383529663
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014016563072800636
        model: {}
        policy_loss: -0.003405389841645956
        total_loss: -0.0030051833018660545
        vf_explained_var: 0.0007155388593673706
        vf_loss: 22.886245727539062
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4239518940448761
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010748065542429686
        model: {}
        policy_loss: -0.002207346260547638
        total_loss: -0.0009434595704078674
        vf_explained_var: 0.09518000483512878
        vf_loss: 20.100433349609375
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9879486560821533
        entropy_coeff: 0.0017600000137463212
        kl: 0.001901482348330319
        model: {}
        policy_loss: -0.0038682795129716396
        total_loss: -0.0034707956947386265
        vf_explained_var: 0.049034520983695984
        vf_loss: 21.36273765563965
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.573919415473938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013993587344884872
        model: {}
        policy_loss: -0.0027790723834186792
        total_loss: -0.0017957771196961403
        vf_explained_var: 0.09900403022766113
        vf_loss: 19.93391990661621
    load_time_ms: 17564.053
    num_steps_sampled: 31392000
    num_steps_trained: 31392000
    sample_time_ms: 128800.655
    update_time_ms: 64.401
  iterations_since_restore: 247
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.560888888888886
    ram_util_percent: 14.748000000000003
  pid: 14340
  policy_reward_max:
    agent-0: 195.0000000000002
    agent-1: 195.0000000000002
    agent-2: 195.0000000000002
    agent-3: 195.0000000000002
    agent-4: 195.0000000000002
    agent-5: 195.0000000000002
  policy_reward_mean:
    agent-0: 163.13666666666657
    agent-1: 163.13666666666657
    agent-2: 163.13666666666657
    agent-3: 163.13666666666657
    agent-4: 163.13666666666657
    agent-5: 163.13666666666657
  policy_reward_min:
    agent-0: 96.66666666666661
    agent-1: 96.66666666666661
    agent-2: 96.66666666666661
    agent-3: 96.66666666666661
    agent-4: 96.66666666666661
    agent-5: 96.66666666666661
  sampler_perf:
    mean_env_wait_ms: 30.964085494564312
    mean_inference_ms: 14.59714942665095
    mean_processing_ms: 65.78968586408189
  time_since_restore: 40440.356736660004
  time_this_iter_s: 157.9577193260193
  time_total_s: 52991.17360806465
  timestamp: 1637075664
  timesteps_since_restore: 23712000
  timesteps_this_iter: 96000
  timesteps_total: 31392000
  training_iteration: 327
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    327 |          52991.2 | 31392000 |   978.82 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 84
    apples_agent-0_mean: 1.92
    apples_agent-0_min: 0
    apples_agent-1_max: 110
    apples_agent-1_mean: 26.49
    apples_agent-1_min: 0
    apples_agent-2_max: 543
    apples_agent-2_mean: 10.68
    apples_agent-2_min: 0
    apples_agent-3_max: 214
    apples_agent-3_mean: 125.28
    apples_agent-3_min: 43
    apples_agent-4_max: 59
    apples_agent-4_mean: 1.74
    apples_agent-4_min: 0
    apples_agent-5_max: 150
    apples_agent-5_mean: 95.29
    apples_agent-5_min: 47
    cleaning_beam_agent-0_max: 538
    cleaning_beam_agent-0_mean: 419.21
    cleaning_beam_agent-0_min: 245
    cleaning_beam_agent-1_max: 439
    cleaning_beam_agent-1_mean: 230.75
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 522
    cleaning_beam_agent-2_mean: 376.48
    cleaning_beam_agent-2_min: 45
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 22.51
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 481
    cleaning_beam_agent-4_mean: 395.8
    cleaning_beam_agent-4_min: 213
    cleaning_beam_agent-5_max: 60
    cleaning_beam_agent-5_mean: 14.01
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-17-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1162.000000000001
  episode_reward_mean: 980.0399999999885
  episode_reward_min: 514.0000000000148
  episodes_this_iter: 96
  episodes_total: 31488
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12904.206
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0154657363891602
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011441458482295275
        model: {}
        policy_loss: -0.002852133708074689
        total_loss: -0.0023559429682791233
        vf_explained_var: -0.0006829947233200073
        vf_loss: 22.834129333496094
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.149814486503601
        entropy_coeff: 0.0017600000137463212
        kl: 0.001395057188346982
        model: {}
        policy_loss: -0.003818912198767066
        total_loss: -0.0034374059177935123
        vf_explained_var: -0.044672876596450806
        vf_loss: 24.05181121826172
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0785748958587646
        entropy_coeff: 0.0017600000137463212
        kl: 0.001671990379691124
        model: {}
        policy_loss: -0.003540034405887127
        total_loss: -0.003116251900792122
        vf_explained_var: 0.02082294225692749
        vf_loss: 23.220752716064453
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4196661114692688
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011096542002633214
        model: {}
        policy_loss: -0.0020588887855410576
        total_loss: -0.0008120692800730467
        vf_explained_var: 0.12748193740844727
        vf_loss: 19.854324340820312
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9993177652359009
        entropy_coeff: 0.0017600000137463212
        kl: 0.002267188858240843
        model: {}
        policy_loss: -0.003909253980964422
        total_loss: -0.0035639686975628138
        vf_explained_var: 0.0912294089794159
        vf_loss: 21.040855407714844
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5584652423858643
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009850712958723307
        model: {}
        policy_loss: -0.0024495339021086693
        total_loss: -0.0013857078738510609
        vf_explained_var: 0.09991143643856049
        vf_loss: 20.46722412109375
    load_time_ms: 19420.879
    num_steps_sampled: 31488000
    num_steps_trained: 31488000
    sample_time_ms: 128602.152
    update_time_ms: 70.514
  iterations_since_restore: 248
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.86265060240964
    ram_util_percent: 14.983132530120484
  pid: 14340
  policy_reward_max:
    agent-0: 193.66666666666643
    agent-1: 193.66666666666643
    agent-2: 193.66666666666643
    agent-3: 193.66666666666643
    agent-4: 193.66666666666643
    agent-5: 193.66666666666643
  policy_reward_mean:
    agent-0: 163.3399999999999
    agent-1: 163.3399999999999
    agent-2: 163.3399999999999
    agent-3: 163.3399999999999
    agent-4: 163.3399999999999
    agent-5: 163.3399999999999
  policy_reward_min:
    agent-0: 85.66666666666686
    agent-1: 85.66666666666686
    agent-2: 85.66666666666686
    agent-3: 85.66666666666686
    agent-4: 85.66666666666686
    agent-5: 85.66666666666686
  sampler_perf:
    mean_env_wait_ms: 30.966414886089815
    mean_inference_ms: 14.596953851398975
    mean_processing_ms: 65.78811301669026
  time_since_restore: 40614.808650016785
  time_this_iter_s: 174.451913356781
  time_total_s: 53165.62552142143
  timestamp: 1637075839
  timesteps_since_restore: 23808000
  timesteps_this_iter: 96000
  timesteps_total: 31488000
  training_iteration: 328
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    328 |          53165.6 | 31488000 |   980.04 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 2.57
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 25.07
    apples_agent-1_min: 0
    apples_agent-2_max: 135
    apples_agent-2_mean: 7.6
    apples_agent-2_min: 0
    apples_agent-3_max: 316
    apples_agent-3_mean: 126.41
    apples_agent-3_min: 53
    apples_agent-4_max: 37
    apples_agent-4_mean: 1.33
    apples_agent-4_min: 0
    apples_agent-5_max: 216
    apples_agent-5_mean: 92.69
    apples_agent-5_min: 47
    cleaning_beam_agent-0_max: 566
    cleaning_beam_agent-0_mean: 432.36
    cleaning_beam_agent-0_min: 154
    cleaning_beam_agent-1_max: 460
    cleaning_beam_agent-1_mean: 235.81
    cleaning_beam_agent-1_min: 75
    cleaning_beam_agent-2_max: 575
    cleaning_beam_agent-2_mean: 391.49
    cleaning_beam_agent-2_min: 181
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 22.24
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 534
    cleaning_beam_agent-4_mean: 382.93
    cleaning_beam_agent-4_min: 204
    cleaning_beam_agent-5_max: 176
    cleaning_beam_agent-5_mean: 20.09
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-19-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1175.9999999999873
  episode_reward_mean: 976.8399999999896
  episode_reward_min: 395.0000000000022
  episodes_this_iter: 96
  episodes_total: 31584
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12910.694
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0090436935424805
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020397002808749676
        model: {}
        policy_loss: -0.0033913375809788704
        total_loss: -0.0029013976454734802
        vf_explained_var: 0.03439527750015259
        vf_loss: 22.658557891845703
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1440768241882324
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014518466778099537
        model: {}
        policy_loss: -0.0037313627544790506
        total_loss: -0.003329782048240304
        vf_explained_var: -0.022155940532684326
        vf_loss: 24.151565551757812
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0651326179504395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015818245010450482
        model: {}
        policy_loss: -0.0036412812769412994
        total_loss: -0.0031178486533463
        vf_explained_var: 0.005455434322357178
        vf_loss: 23.980663299560547
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42509448528289795
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009701216476969421
        model: {}
        policy_loss: -0.00230283010751009
        total_loss: -0.0010276641696691513
        vf_explained_var: 0.13788346946239471
        vf_loss: 20.23333168029785
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0095425844192505
        entropy_coeff: 0.0017600000137463212
        kl: 0.001519349287264049
        model: {}
        policy_loss: -0.0039770761504769325
        total_loss: -0.003488770918920636
        vf_explained_var: 0.046830639243125916
        vf_loss: 22.65100860595703
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5497217178344727
        entropy_coeff: 0.0017600000137463212
        kl: 0.001225971500389278
        model: {}
        policy_loss: -0.0027907707262784243
        total_loss: -0.0016760507132858038
        vf_explained_var: 0.10861457884311676
        vf_loss: 20.822317123413086
    load_time_ms: 19479.164
    num_steps_sampled: 31584000
    num_steps_trained: 31584000
    sample_time_ms: 128731.03
    update_time_ms: 67.984
  iterations_since_restore: 249
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.676681614349775
    ram_util_percent: 14.732286995515699
  pid: 14340
  policy_reward_max:
    agent-0: 195.99999999999943
    agent-1: 195.99999999999943
    agent-2: 195.99999999999943
    agent-3: 195.99999999999943
    agent-4: 195.99999999999943
    agent-5: 195.99999999999943
  policy_reward_mean:
    agent-0: 162.80666666666656
    agent-1: 162.80666666666656
    agent-2: 162.80666666666656
    agent-3: 162.80666666666656
    agent-4: 162.80666666666656
    agent-5: 162.80666666666656
  policy_reward_min:
    agent-0: 65.83333333333323
    agent-1: 65.83333333333323
    agent-2: 65.83333333333323
    agent-3: 65.83333333333323
    agent-4: 65.83333333333323
    agent-5: 65.83333333333323
  sampler_perf:
    mean_env_wait_ms: 30.96887807989911
    mean_inference_ms: 14.596969923526238
    mean_processing_ms: 65.7873176063132
  time_since_restore: 40771.12917661667
  time_this_iter_s: 156.32052659988403
  time_total_s: 53321.94604802132
  timestamp: 1637075995
  timesteps_since_restore: 23904000
  timesteps_this_iter: 96000
  timesteps_total: 31584000
  training_iteration: 329
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    329 |          53321.9 | 31584000 |   976.84 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 221
    apples_agent-0_mean: 4.06
    apples_agent-0_min: 0
    apples_agent-1_max: 169
    apples_agent-1_mean: 28.8
    apples_agent-1_min: 0
    apples_agent-2_max: 507
    apples_agent-2_mean: 14.69
    apples_agent-2_min: 0
    apples_agent-3_max: 303
    apples_agent-3_mean: 136.4
    apples_agent-3_min: 65
    apples_agent-4_max: 36
    apples_agent-4_mean: 1.39
    apples_agent-4_min: 0
    apples_agent-5_max: 307
    apples_agent-5_mean: 101.49
    apples_agent-5_min: 55
    cleaning_beam_agent-0_max: 607
    cleaning_beam_agent-0_mean: 442.35
    cleaning_beam_agent-0_min: 211
    cleaning_beam_agent-1_max: 418
    cleaning_beam_agent-1_mean: 223.56
    cleaning_beam_agent-1_min: 75
    cleaning_beam_agent-2_max: 561
    cleaning_beam_agent-2_mean: 393.85
    cleaning_beam_agent-2_min: 59
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 22.36
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 506
    cleaning_beam_agent-4_mean: 387.42
    cleaning_beam_agent-4_min: 270
    cleaning_beam_agent-5_max: 114
    cleaning_beam_agent-5_mean: 15.11
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-22-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1135.000000000002
  episode_reward_mean: 988.749999999989
  episode_reward_min: 648.9999999999943
  episodes_this_iter: 96
  episodes_total: 31680
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12885.84
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.000870943069458
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013456029118970037
        model: {}
        policy_loss: -0.003269630717113614
        total_loss: -0.002841314533725381
        vf_explained_var: 0.03031882643699646
        vf_loss: 21.89850616455078
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1533173322677612
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016652275808155537
        model: {}
        policy_loss: -0.003954736515879631
        total_loss: -0.0034939534962177277
        vf_explained_var: -0.09457209706306458
        vf_loss: 24.90619468688965
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0700592994689941
        entropy_coeff: 0.0017600000137463212
        kl: 0.001965399133041501
        model: {}
        policy_loss: -0.0033791987225413322
        total_loss: -0.0030252281576395035
        vf_explained_var: 0.03485362231731415
        vf_loss: 22.37273406982422
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4295758306980133
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012804220896214247
        model: {}
        policy_loss: -0.0020669130608439445
        total_loss: -0.0007060961797833443
        vf_explained_var: 0.0628110021352768
        vf_loss: 21.168716430664062
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9913090467453003
        entropy_coeff: 0.0017600000137463212
        kl: 0.002295517362654209
        model: {}
        policy_loss: -0.004139118827879429
        total_loss: -0.0037207696586847305
        vf_explained_var: 0.04963637888431549
        vf_loss: 21.630502700805664
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5536741018295288
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013088143896311522
        model: {}
        policy_loss: -0.0024882822763174772
        total_loss: -0.0013723818119615316
        vf_explained_var: 0.06647263467311859
        vf_loss: 20.90363311767578
    load_time_ms: 19413.8
    num_steps_sampled: 31680000
    num_steps_trained: 31680000
    sample_time_ms: 128694.499
    update_time_ms: 73.857
  iterations_since_restore: 250
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.75381165919283
    ram_util_percent: 14.69596412556054
  pid: 14340
  policy_reward_max:
    agent-0: 189.16666666666666
    agent-1: 189.16666666666666
    agent-2: 189.16666666666666
    agent-3: 189.16666666666666
    agent-4: 189.16666666666666
    agent-5: 189.16666666666666
  policy_reward_mean:
    agent-0: 164.79166666666657
    agent-1: 164.79166666666657
    agent-2: 164.79166666666657
    agent-3: 164.79166666666657
    agent-4: 164.79166666666657
    agent-5: 164.79166666666657
  policy_reward_min:
    agent-0: 108.16666666666696
    agent-1: 108.16666666666696
    agent-2: 108.16666666666696
    agent-3: 108.16666666666696
    agent-4: 108.16666666666696
    agent-5: 108.16666666666696
  sampler_perf:
    mean_env_wait_ms: 30.9716883358369
    mean_inference_ms: 14.59723608065564
    mean_processing_ms: 65.78751197031347
  time_since_restore: 40927.75252175331
  time_this_iter_s: 156.62334513664246
  time_total_s: 53478.56939315796
  timestamp: 1637076152
  timesteps_since_restore: 24000000
  timesteps_this_iter: 96000
  timesteps_total: 31680000
  training_iteration: 330
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    330 |          53478.6 | 31680000 |   988.75 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 2.33
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 23.41
    apples_agent-1_min: 0
    apples_agent-2_max: 310
    apples_agent-2_mean: 10.28
    apples_agent-2_min: 0
    apples_agent-3_max: 402
    apples_agent-3_mean: 135.74
    apples_agent-3_min: 29
    apples_agent-4_max: 63
    apples_agent-4_mean: 2.71
    apples_agent-4_min: 0
    apples_agent-5_max: 391
    apples_agent-5_mean: 97.0
    apples_agent-5_min: 28
    cleaning_beam_agent-0_max: 543
    cleaning_beam_agent-0_mean: 442.4
    cleaning_beam_agent-0_min: 305
    cleaning_beam_agent-1_max: 395
    cleaning_beam_agent-1_mean: 224.57
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 563
    cleaning_beam_agent-2_mean: 372.66
    cleaning_beam_agent-2_min: 186
    cleaning_beam_agent-3_max: 141
    cleaning_beam_agent-3_mean: 27.95
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 469
    cleaning_beam_agent-4_mean: 389.42
    cleaning_beam_agent-4_min: 224
    cleaning_beam_agent-5_max: 227
    cleaning_beam_agent-5_mean: 18.5
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-25-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1141.0000000000164
  episode_reward_mean: 962.8099999999899
  episode_reward_min: 481.00000000001023
  episodes_this_iter: 96
  episodes_total: 31776
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12889.748
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9960986971855164
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013642923440784216
        model: {}
        policy_loss: -0.0029676537960767746
        total_loss: -0.0025030868127942085
        vf_explained_var: 0.03589893877506256
        vf_loss: 22.176956176757812
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1505451202392578
        entropy_coeff: 0.0017600000137463212
        kl: 0.001225135987624526
        model: {}
        policy_loss: -0.003607311053201556
        total_loss: -0.0031763380393385887
        vf_explained_var: -0.06567895412445068
        vf_loss: 24.55931282043457
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0739665031433105
        entropy_coeff: 0.0017600000137463212
        kl: 0.001023735385388136
        model: {}
        policy_loss: -0.0032267854548990726
        total_loss: -0.0027652117423713207
        vf_explained_var: -0.008783012628555298
        vf_loss: 23.51753044128418
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4435197710990906
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011635362170636654
        model: {}
        policy_loss: -0.0023813487496227026
        total_loss: -0.0010475123999640346
        vf_explained_var: 0.08057238161563873
        vf_loss: 21.144315719604492
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9935897588729858
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019519105553627014
        model: {}
        policy_loss: -0.0038082627579569817
        total_loss: -0.0033786725252866745
        vf_explained_var: 0.05459342896938324
        vf_loss: 21.783103942871094
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5683214664459229
        entropy_coeff: 0.0017600000137463212
        kl: 0.001802315004169941
        model: {}
        policy_loss: -0.0024799667298793793
        total_loss: -0.0013871360570192337
        vf_explained_var: 0.08924196660518646
        vf_loss: 20.93073081970215
    load_time_ms: 19546.928
    num_steps_sampled: 31776000
    num_steps_trained: 31776000
    sample_time_ms: 128796.077
    update_time_ms: 74.405
  iterations_since_restore: 251
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.361403508771932
    ram_util_percent: 14.739473684210529
  pid: 14340
  policy_reward_max:
    agent-0: 190.16666666666637
    agent-1: 190.16666666666637
    agent-2: 190.16666666666637
    agent-3: 190.16666666666637
    agent-4: 190.16666666666637
    agent-5: 190.16666666666637
  policy_reward_mean:
    agent-0: 160.46833333333328
    agent-1: 160.46833333333328
    agent-2: 160.46833333333328
    agent-3: 160.46833333333328
    agent-4: 160.46833333333328
    agent-5: 160.46833333333328
  policy_reward_min:
    agent-0: 80.16666666666679
    agent-1: 80.16666666666679
    agent-2: 80.16666666666679
    agent-3: 80.16666666666679
    agent-4: 80.16666666666679
    agent-5: 80.16666666666679
  sampler_perf:
    mean_env_wait_ms: 30.973813488819463
    mean_inference_ms: 14.597001856024812
    mean_processing_ms: 65.78697532319838
  time_since_restore: 41084.97435474396
  time_this_iter_s: 157.22183299064636
  time_total_s: 53635.791226148605
  timestamp: 1637076313
  timesteps_since_restore: 24096000
  timesteps_this_iter: 96000
  timesteps_total: 31776000
  training_iteration: 331
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    331 |          53635.8 | 31776000 |   962.81 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 95
    apples_agent-0_mean: 4.07
    apples_agent-0_min: 0
    apples_agent-1_max: 114
    apples_agent-1_mean: 25.27
    apples_agent-1_min: 0
    apples_agent-2_max: 77
    apples_agent-2_mean: 3.38
    apples_agent-2_min: 0
    apples_agent-3_max: 423
    apples_agent-3_mean: 137.47
    apples_agent-3_min: 52
    apples_agent-4_max: 78
    apples_agent-4_mean: 3.48
    apples_agent-4_min: 0
    apples_agent-5_max: 414
    apples_agent-5_mean: 94.23
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 591
    cleaning_beam_agent-0_mean: 441.81
    cleaning_beam_agent-0_min: 295
    cleaning_beam_agent-1_max: 404
    cleaning_beam_agent-1_mean: 227.59
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 518
    cleaning_beam_agent-2_mean: 383.65
    cleaning_beam_agent-2_min: 91
    cleaning_beam_agent-3_max: 141
    cleaning_beam_agent-3_mean: 25.6
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 473
    cleaning_beam_agent-4_mean: 382.81
    cleaning_beam_agent-4_min: 198
    cleaning_beam_agent-5_max: 166
    cleaning_beam_agent-5_mean: 25.09
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-28-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1143.9999999999914
  episode_reward_mean: 947.729999999988
  episode_reward_min: 274.99999999999835
  episodes_this_iter: 96
  episodes_total: 31872
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12894.657
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9989540576934814
        entropy_coeff: 0.0017600000137463212
        kl: 0.001948273740708828
        model: {}
        policy_loss: -0.0032862569205462933
        total_loss: -0.0026702634058892727
        vf_explained_var: 0.02389499545097351
        vf_loss: 23.74150848388672
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.142105221748352
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021547405049204826
        model: {}
        policy_loss: -0.0037141903303563595
        total_loss: -0.0031711915507912636
        vf_explained_var: -0.0498223602771759
        vf_loss: 25.53101348876953
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0827715396881104
        entropy_coeff: 0.0017600000137463212
        kl: 0.002262649592012167
        model: {}
        policy_loss: -0.0037976147141307592
        total_loss: -0.0033550523221492767
        vf_explained_var: 0.03879609704017639
        vf_loss: 23.482398986816406
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4623362421989441
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007105448748916388
        model: {}
        policy_loss: -0.002498180139809847
        total_loss: -0.0011348207481205463
        vf_explained_var: 0.1045922338962555
        vf_loss: 21.770671844482422
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9901110529899597
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015270947478711605
        model: {}
        policy_loss: -0.0035434188321232796
        total_loss: -0.0030030838679522276
        vf_explained_var: 0.0626397430896759
        vf_loss: 22.829303741455078
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5893098711967468
        entropy_coeff: 0.0017600000137463212
        kl: 0.001089771161787212
        model: {}
        policy_loss: -0.0025845770724117756
        total_loss: -0.001505956519395113
        vf_explained_var: 0.13113903999328613
        vf_loss: 21.158031463623047
    load_time_ms: 19507.645
    num_steps_sampled: 31872000
    num_steps_trained: 31872000
    sample_time_ms: 128941.838
    update_time_ms: 74.785
  iterations_since_restore: 252
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.6632
    ram_util_percent: 15.7208
  pid: 14340
  policy_reward_max:
    agent-0: 190.66666666666637
    agent-1: 190.66666666666637
    agent-2: 190.66666666666637
    agent-3: 190.66666666666637
    agent-4: 190.66666666666637
    agent-5: 190.66666666666637
  policy_reward_mean:
    agent-0: 157.95499999999996
    agent-1: 157.95499999999996
    agent-2: 157.95499999999996
    agent-3: 157.95499999999996
    agent-4: 157.95499999999996
    agent-5: 157.95499999999996
  policy_reward_min:
    agent-0: 45.833333333333336
    agent-1: 45.833333333333336
    agent-2: 45.833333333333336
    agent-3: 45.833333333333336
    agent-4: 45.833333333333336
    agent-5: 45.833333333333336
  sampler_perf:
    mean_env_wait_ms: 30.975916647616767
    mean_inference_ms: 14.596996742395762
    mean_processing_ms: 65.78704658531227
  time_since_restore: 41260.52574443817
  time_this_iter_s: 175.55138969421387
  time_total_s: 53811.34261584282
  timestamp: 1637076488
  timesteps_since_restore: 24192000
  timesteps_this_iter: 96000
  timesteps_total: 31872000
  training_iteration: 332
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    332 |          53811.3 | 31872000 |   947.73 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 2.48
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 24.3
    apples_agent-1_min: 0
    apples_agent-2_max: 148
    apples_agent-2_mean: 4.9
    apples_agent-2_min: 0
    apples_agent-3_max: 252
    apples_agent-3_mean: 138.82
    apples_agent-3_min: 59
    apples_agent-4_max: 60
    apples_agent-4_mean: 1.75
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 97.26
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 563
    cleaning_beam_agent-0_mean: 435.19
    cleaning_beam_agent-0_min: 193
    cleaning_beam_agent-1_max: 511
    cleaning_beam_agent-1_mean: 241.99
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 501
    cleaning_beam_agent-2_mean: 373.53
    cleaning_beam_agent-2_min: 111
    cleaning_beam_agent-3_max: 136
    cleaning_beam_agent-3_mean: 24.85
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 510
    cleaning_beam_agent-4_mean: 402.19
    cleaning_beam_agent-4_min: 244
    cleaning_beam_agent-5_max: 163
    cleaning_beam_agent-5_mean: 17.91
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-30-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1150.9999999999861
  episode_reward_mean: 979.7499999999894
  episode_reward_min: 439.0000000000104
  episodes_this_iter: 96
  episodes_total: 31968
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12896.585
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0056160688400269
        entropy_coeff: 0.0017600000137463212
        kl: 0.000981308869086206
        model: {}
        policy_loss: -0.002933452371507883
        total_loss: -0.002366861095651984
        vf_explained_var: 0.012529775500297546
        vf_loss: 23.364757537841797
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.149008870124817
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017560319975018501
        model: {}
        policy_loss: -0.003909509629011154
        total_loss: -0.0034210544545203447
        vf_explained_var: -0.05766040086746216
        vf_loss: 25.10711097717285
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0921710729599
        entropy_coeff: 0.0017600000137463212
        kl: 0.001750818220898509
        model: {}
        policy_loss: -0.0033651383128017187
        total_loss: -0.0028773811645805836
        vf_explained_var: 0.0015393048524856567
        vf_loss: 24.0997371673584
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44308990240097046
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007404889329336584
        model: {}
        policy_loss: -0.0022315122187137604
        total_loss: -0.0009346343576908112
        vf_explained_var: 0.11928237974643707
        vf_loss: 20.767166137695312
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9864895939826965
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016402137698605657
        model: {}
        policy_loss: -0.0036648456007242203
        total_loss: -0.0031177066266536713
        vf_explained_var: 0.03649795055389404
        vf_loss: 22.8336124420166
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.576554000377655
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012342955451458693
        model: {}
        policy_loss: -0.0026886058039963245
        total_loss: -0.0016838929150253534
        vf_explained_var: 0.14339186251163483
        vf_loss: 20.194496154785156
    load_time_ms: 19527.475
    num_steps_sampled: 31968000
    num_steps_trained: 31968000
    sample_time_ms: 129116.879
    update_time_ms: 78.066
  iterations_since_restore: 253
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.802232142857143
    ram_util_percent: 15.848214285714283
  pid: 14340
  policy_reward_max:
    agent-0: 191.83333333333343
    agent-1: 191.83333333333343
    agent-2: 191.83333333333343
    agent-3: 191.83333333333343
    agent-4: 191.83333333333343
    agent-5: 191.83333333333343
  policy_reward_mean:
    agent-0: 163.2916666666666
    agent-1: 163.2916666666666
    agent-2: 163.2916666666666
    agent-3: 163.2916666666666
    agent-4: 163.2916666666666
    agent-5: 163.2916666666666
  policy_reward_min:
    agent-0: 73.1666666666667
    agent-1: 73.1666666666667
    agent-2: 73.1666666666667
    agent-3: 73.1666666666667
    agent-4: 73.1666666666667
    agent-5: 73.1666666666667
  sampler_perf:
    mean_env_wait_ms: 30.97814200120937
    mean_inference_ms: 14.596874352082517
    mean_processing_ms: 65.78839743626371
  time_since_restore: 41418.07952666283
  time_this_iter_s: 157.55378222465515
  time_total_s: 53968.896398067474
  timestamp: 1637076646
  timesteps_since_restore: 24288000
  timesteps_this_iter: 96000
  timesteps_total: 31968000
  training_iteration: 333
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    333 |          53968.9 | 31968000 |   979.75 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 279
    apples_agent-0_mean: 6.41
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 26.82
    apples_agent-1_min: 0
    apples_agent-2_max: 66
    apples_agent-2_mean: 4.0
    apples_agent-2_min: 0
    apples_agent-3_max: 258
    apples_agent-3_mean: 136.72
    apples_agent-3_min: 46
    apples_agent-4_max: 73
    apples_agent-4_mean: 1.73
    apples_agent-4_min: 0
    apples_agent-5_max: 455
    apples_agent-5_mean: 99.49
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 582
    cleaning_beam_agent-0_mean: 428.32
    cleaning_beam_agent-0_min: 164
    cleaning_beam_agent-1_max: 417
    cleaning_beam_agent-1_mean: 253.69
    cleaning_beam_agent-1_min: 89
    cleaning_beam_agent-2_max: 537
    cleaning_beam_agent-2_mean: 373.12
    cleaning_beam_agent-2_min: 172
    cleaning_beam_agent-3_max: 90
    cleaning_beam_agent-3_mean: 24.49
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 474
    cleaning_beam_agent-4_mean: 396.71
    cleaning_beam_agent-4_min: 230
    cleaning_beam_agent-5_max: 121
    cleaning_beam_agent-5_mean: 20.17
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-33-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1150.9999999999727
  episode_reward_mean: 965.2299999999876
  episode_reward_min: 524.0000000000118
  episodes_this_iter: 96
  episodes_total: 32064
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12917.458
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0026211738586426
        entropy_coeff: 0.0017600000137463212
        kl: 0.001243044389411807
        model: {}
        policy_loss: -0.0028431671671569347
        total_loss: -0.002544943243265152
        vf_explained_var: 0.05573771893978119
        vf_loss: 20.62842559814453
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1324467658996582
        entropy_coeff: 0.0017600000137463212
        kl: 0.002055754419416189
        model: {}
        policy_loss: -0.004173856228590012
        total_loss: -0.0038521941751241684
        vf_explained_var: -0.06260958313941956
        vf_loss: 23.147693634033203
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1006475687026978
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019274745136499405
        model: {}
        policy_loss: -0.00345158064737916
        total_loss: -0.003101751673966646
        vf_explained_var: -0.044857680797576904
        vf_loss: 22.869678497314453
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4511200189590454
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013253878569230437
        model: {}
        policy_loss: -0.002332680858671665
        total_loss: -0.0011464916169643402
        vf_explained_var: 0.0877857357263565
        vf_loss: 19.801589965820312
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9803425669670105
        entropy_coeff: 0.0017600000137463212
        kl: 0.002953483723104
        model: {}
        policy_loss: -0.004030841402709484
        total_loss: -0.0036385105922818184
        vf_explained_var: 0.02362753450870514
        vf_loss: 21.17733383178711
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5574831366539001
        entropy_coeff: 0.0017600000137463212
        kl: 0.001198861631564796
        model: {}
        policy_loss: -0.002389943227171898
        total_loss: -0.0013653882779181004
        vf_explained_var: 0.07420401275157928
        vf_loss: 20.057281494140625
    load_time_ms: 18431.475
    num_steps_sampled: 32064000
    num_steps_trained: 32064000
    sample_time_ms: 129198.428
    update_time_ms: 84.458
  iterations_since_restore: 254
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.75155555555555
    ram_util_percent: 15.819111111111113
  pid: 14340
  policy_reward_max:
    agent-0: 191.8333333333331
    agent-1: 191.8333333333331
    agent-2: 191.8333333333331
    agent-3: 191.8333333333331
    agent-4: 191.8333333333331
    agent-5: 191.8333333333331
  policy_reward_mean:
    agent-0: 160.8716666666666
    agent-1: 160.8716666666666
    agent-2: 160.8716666666666
    agent-3: 160.8716666666666
    agent-4: 160.8716666666666
    agent-5: 160.8716666666666
  policy_reward_min:
    agent-0: 87.33333333333354
    agent-1: 87.33333333333354
    agent-2: 87.33333333333354
    agent-3: 87.33333333333354
    agent-4: 87.33333333333354
    agent-5: 87.33333333333354
  sampler_perf:
    mean_env_wait_ms: 30.980752738812253
    mean_inference_ms: 14.597008840750448
    mean_processing_ms: 65.78880982418414
  time_since_restore: 41575.71414875984
  time_this_iter_s: 157.63462209701538
  time_total_s: 54126.53102016449
  timestamp: 1637076804
  timesteps_since_restore: 24384000
  timesteps_this_iter: 96000
  timesteps_total: 32064000
  training_iteration: 334
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    334 |          54126.5 | 32064000 |   965.23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 3.96
    apples_agent-0_min: 0
    apples_agent-1_max: 111
    apples_agent-1_mean: 30.22
    apples_agent-1_min: 0
    apples_agent-2_max: 86
    apples_agent-2_mean: 3.89
    apples_agent-2_min: 0
    apples_agent-3_max: 220
    apples_agent-3_mean: 136.72
    apples_agent-3_min: 46
    apples_agent-4_max: 12
    apples_agent-4_mean: 0.23
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 95.04
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 583
    cleaning_beam_agent-0_mean: 427.21
    cleaning_beam_agent-0_min: 256
    cleaning_beam_agent-1_max: 453
    cleaning_beam_agent-1_mean: 258.39
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 549
    cleaning_beam_agent-2_mean: 364.63
    cleaning_beam_agent-2_min: 153
    cleaning_beam_agent-3_max: 123
    cleaning_beam_agent-3_mean: 23.54
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 479
    cleaning_beam_agent-4_mean: 407.46
    cleaning_beam_agent-4_min: 280
    cleaning_beam_agent-5_max: 197
    cleaning_beam_agent-5_mean: 20.77
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-36-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1141.9999999999989
  episode_reward_mean: 992.7499999999885
  episode_reward_min: 458.0000000000061
  episodes_this_iter: 96
  episodes_total: 32160
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12889.688
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0143766403198242
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017660383600741625
        model: {}
        policy_loss: -0.0032146614976227283
        total_loss: -0.0028437520377337933
        vf_explained_var: 0.0479811429977417
        vf_loss: 21.56214141845703
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1424468755722046
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018012507352977991
        model: {}
        policy_loss: -0.003963529597967863
        total_loss: -0.003631583182141185
        vf_explained_var: -0.030207008123397827
        vf_loss: 23.426542282104492
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1136798858642578
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015435790410265326
        model: {}
        policy_loss: -0.0034559546038508415
        total_loss: -0.00306343799456954
        vf_explained_var: -0.027644038200378418
        vf_loss: 23.525924682617188
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4223902225494385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011598628479987383
        model: {}
        policy_loss: -0.0023027723655104637
        total_loss: -0.0010649124160408974
        vf_explained_var: 0.11457403004169464
        vf_loss: 19.812639236450195
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9885909557342529
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020189431961625814
        model: {}
        policy_loss: -0.003912841901183128
        total_loss: -0.0034572193399071693
        vf_explained_var: 0.028164297342300415
        vf_loss: 21.95543098449707
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5613794326782227
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010354407131671906
        model: {}
        policy_loss: -0.0026385029777884483
        total_loss: -0.0016216537915170193
        vf_explained_var: 0.10369251668453217
        vf_loss: 20.04877471923828
    load_time_ms: 18473.302
    num_steps_sampled: 32160000
    num_steps_trained: 32160000
    sample_time_ms: 129217.029
    update_time_ms: 84.825
  iterations_since_restore: 255
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.819111111111113
    ram_util_percent: 15.814666666666668
  pid: 14340
  policy_reward_max:
    agent-0: 190.33333333333286
    agent-1: 190.33333333333286
    agent-2: 190.33333333333286
    agent-3: 190.33333333333286
    agent-4: 190.33333333333286
    agent-5: 190.33333333333286
  policy_reward_mean:
    agent-0: 165.45833333333317
    agent-1: 165.45833333333317
    agent-2: 165.45833333333317
    agent-3: 165.45833333333317
    agent-4: 165.45833333333317
    agent-5: 165.45833333333317
  policy_reward_min:
    agent-0: 76.33333333333319
    agent-1: 76.33333333333319
    agent-2: 76.33333333333319
    agent-3: 76.33333333333319
    agent-4: 76.33333333333319
    agent-5: 76.33333333333319
  sampler_perf:
    mean_env_wait_ms: 30.98356726206134
    mean_inference_ms: 14.596952468717541
    mean_processing_ms: 65.79030710065094
  time_since_restore: 41733.321118831635
  time_this_iter_s: 157.6069700717926
  time_total_s: 54284.13799023628
  timestamp: 1637076962
  timesteps_since_restore: 24480000
  timesteps_this_iter: 96000
  timesteps_total: 32160000
  training_iteration: 335
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    335 |          54284.1 | 32160000 |   992.75 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 3.28
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 29.52
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 2.53
    apples_agent-2_min: 0
    apples_agent-3_max: 207
    apples_agent-3_mean: 133.62
    apples_agent-3_min: 71
    apples_agent-4_max: 82
    apples_agent-4_mean: 2.28
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 95.87
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 558
    cleaning_beam_agent-0_mean: 415.46
    cleaning_beam_agent-0_min: 267
    cleaning_beam_agent-1_max: 449
    cleaning_beam_agent-1_mean: 257.54
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 570
    cleaning_beam_agent-2_mean: 370.94
    cleaning_beam_agent-2_min: 213
    cleaning_beam_agent-3_max: 79
    cleaning_beam_agent-3_mean: 22.96
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 480
    cleaning_beam_agent-4_mean: 401.23
    cleaning_beam_agent-4_min: 243
    cleaning_beam_agent-5_max: 92
    cleaning_beam_agent-5_mean: 18.88
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-38-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1188.999999999994
  episode_reward_mean: 984.5299999999894
  episode_reward_min: 397.00000000000784
  episodes_this_iter: 96
  episodes_total: 32256
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12902.698
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0208680629730225
        entropy_coeff: 0.0017600000137463212
        kl: 0.001444146502763033
        model: {}
        policy_loss: -0.0029662468004971743
        total_loss: -0.0024739643558859825
        vf_explained_var: 0.0015597939491271973
        vf_loss: 22.89011573791504
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1422367095947266
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018598887836560607
        model: {}
        policy_loss: -0.003991219215095043
        total_loss: -0.0035929414443671703
        vf_explained_var: -0.050479233264923096
        vf_loss: 24.08612823486328
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1086708307266235
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017480116803199053
        model: {}
        policy_loss: -0.0036452391650527716
        total_loss: -0.0032384535297751427
        vf_explained_var: -0.01689741015434265
        vf_loss: 23.580480575561523
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42660436034202576
        entropy_coeff: 0.0017600000137463212
        kl: 0.000894061173312366
        model: {}
        policy_loss: -0.0022319499403238297
        total_loss: -0.001008298248052597
        vf_explained_var: 0.13259592652320862
        vf_loss: 19.7447509765625
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9832078218460083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018008921761065722
        model: {}
        policy_loss: -0.003913221415132284
        total_loss: -0.003478734754025936
        vf_explained_var: 0.0643831342458725
        vf_loss: 21.649309158325195
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.558020293712616
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008960016421042383
        model: {}
        policy_loss: -0.0027114858385175467
        total_loss: -0.001643888303078711
        vf_explained_var: 0.09972932934761047
        vf_loss: 20.497135162353516
    load_time_ms: 18470.661
    num_steps_sampled: 32256000
    num_steps_trained: 32256000
    sample_time_ms: 129268.936
    update_time_ms: 75.822
  iterations_since_restore: 256
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.28839285714286
    ram_util_percent: 15.839285714285714
  pid: 14340
  policy_reward_max:
    agent-0: 198.1666666666668
    agent-1: 198.1666666666668
    agent-2: 198.1666666666668
    agent-3: 198.1666666666668
    agent-4: 198.1666666666668
    agent-5: 198.1666666666668
  policy_reward_mean:
    agent-0: 164.08833333333325
    agent-1: 164.08833333333325
    agent-2: 164.08833333333325
    agent-3: 164.08833333333325
    agent-4: 164.08833333333325
    agent-5: 164.08833333333325
  policy_reward_min:
    agent-0: 66.16666666666644
    agent-1: 66.16666666666644
    agent-2: 66.16666666666644
    agent-3: 66.16666666666644
    agent-4: 66.16666666666644
    agent-5: 66.16666666666644
  sampler_perf:
    mean_env_wait_ms: 30.985847763058924
    mean_inference_ms: 14.596992395718651
    mean_processing_ms: 65.7916758814436
  time_since_restore: 41890.827916145325
  time_this_iter_s: 157.50679731369019
  time_total_s: 54441.64478754997
  timestamp: 1637077120
  timesteps_since_restore: 24576000
  timesteps_this_iter: 96000
  timesteps_total: 32256000
  training_iteration: 336
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    336 |          54441.6 | 32256000 |   984.53 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 75
    apples_agent-0_mean: 3.97
    apples_agent-0_min: 0
    apples_agent-1_max: 116
    apples_agent-1_mean: 25.85
    apples_agent-1_min: 0
    apples_agent-2_max: 67
    apples_agent-2_mean: 5.16
    apples_agent-2_min: 0
    apples_agent-3_max: 210
    apples_agent-3_mean: 128.54
    apples_agent-3_min: 52
    apples_agent-4_max: 54
    apples_agent-4_mean: 1.18
    apples_agent-4_min: 0
    apples_agent-5_max: 158
    apples_agent-5_mean: 92.36
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 594
    cleaning_beam_agent-0_mean: 400.77
    cleaning_beam_agent-0_min: 231
    cleaning_beam_agent-1_max: 472
    cleaning_beam_agent-1_mean: 259.57
    cleaning_beam_agent-1_min: 128
    cleaning_beam_agent-2_max: 462
    cleaning_beam_agent-2_mean: 346.53
    cleaning_beam_agent-2_min: 141
    cleaning_beam_agent-3_max: 208
    cleaning_beam_agent-3_mean: 26.5
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 501
    cleaning_beam_agent-4_mean: 404.13
    cleaning_beam_agent-4_min: 253
    cleaning_beam_agent-5_max: 248
    cleaning_beam_agent-5_mean: 18.69
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-41-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1172.9999999999948
  episode_reward_mean: 983.8999999999872
  episode_reward_min: 460.00000000000693
  episodes_this_iter: 96
  episodes_total: 32352
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12917.14
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0203801393508911
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011864126427099109
        model: {}
        policy_loss: -0.0030940109863877296
        total_loss: -0.0027112364768981934
        vf_explained_var: 0.04066282510757446
        vf_loss: 21.786457061767578
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.146581768989563
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017494092462584376
        model: {}
        policy_loss: -0.0037522437050938606
        total_loss: -0.003373493440449238
        vf_explained_var: -0.0513574481010437
        vf_loss: 23.967317581176758
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1200730800628662
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019463873468339443
        model: {}
        policy_loss: -0.0036623235791921616
        total_loss: -0.0034036170691251755
        vf_explained_var: 0.041822344064712524
        vf_loss: 22.300382614135742
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4360509514808655
        entropy_coeff: 0.0017600000137463212
        kl: 0.001091010170057416
        model: {}
        policy_loss: -0.002489004284143448
        total_loss: -0.0012701054802164435
        vf_explained_var: 0.12150172889232635
        vf_loss: 19.863473892211914
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9777030944824219
        entropy_coeff: 0.0017600000137463212
        kl: 0.002481085481122136
        model: {}
        policy_loss: -0.003893032670021057
        total_loss: -0.003499680897220969
        vf_explained_var: 0.07271873950958252
        vf_loss: 21.141122817993164
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5478187203407288
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009090867242775857
        model: {}
        policy_loss: -0.0024433289654552937
        total_loss: -0.0014189104549586773
        vf_explained_var: 0.11910322308540344
        vf_loss: 19.885778427124023
    load_time_ms: 18497.344
    num_steps_sampled: 32352000
    num_steps_trained: 32352000
    sample_time_ms: 129075.776
    update_time_ms: 76.207
  iterations_since_restore: 257
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.502232142857142
    ram_util_percent: 15.823660714285714
  pid: 14340
  policy_reward_max:
    agent-0: 195.50000000000043
    agent-1: 195.50000000000043
    agent-2: 195.50000000000043
    agent-3: 195.50000000000043
    agent-4: 195.50000000000043
    agent-5: 195.50000000000043
  policy_reward_mean:
    agent-0: 163.98333333333318
    agent-1: 163.98333333333318
    agent-2: 163.98333333333318
    agent-3: 163.98333333333318
    agent-4: 163.98333333333318
    agent-5: 163.98333333333318
  policy_reward_min:
    agent-0: 76.66666666666681
    agent-1: 76.66666666666681
    agent-2: 76.66666666666681
    agent-3: 76.66666666666681
    agent-4: 76.66666666666681
    agent-5: 76.66666666666681
  sampler_perf:
    mean_env_wait_ms: 30.98693652295961
    mean_inference_ms: 14.596811535121581
    mean_processing_ms: 65.79127239137482
  time_since_restore: 42047.31907868385
  time_this_iter_s: 156.49116253852844
  time_total_s: 54598.1359500885
  timestamp: 1637077277
  timesteps_since_restore: 24672000
  timesteps_this_iter: 96000
  timesteps_total: 32352000
  training_iteration: 337
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    337 |          54598.1 | 32352000 |    983.9 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 3.02
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 25.35
    apples_agent-1_min: 0
    apples_agent-2_max: 86
    apples_agent-2_mean: 6.09
    apples_agent-2_min: 0
    apples_agent-3_max: 224
    apples_agent-3_mean: 129.85
    apples_agent-3_min: 54
    apples_agent-4_max: 88
    apples_agent-4_mean: 3.24
    apples_agent-4_min: 0
    apples_agent-5_max: 143
    apples_agent-5_mean: 87.99
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 518
    cleaning_beam_agent-0_mean: 409.37
    cleaning_beam_agent-0_min: 184
    cleaning_beam_agent-1_max: 418
    cleaning_beam_agent-1_mean: 268.25
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 498
    cleaning_beam_agent-2_mean: 352.81
    cleaning_beam_agent-2_min: 204
    cleaning_beam_agent-3_max: 100
    cleaning_beam_agent-3_mean: 25.0
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 499
    cleaning_beam_agent-4_mean: 413.92
    cleaning_beam_agent-4_min: 288
    cleaning_beam_agent-5_max: 339
    cleaning_beam_agent-5_mean: 22.7
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-43-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1145.0000000000055
  episode_reward_mean: 989.149999999988
  episode_reward_min: 537.0000000000126
  episodes_this_iter: 96
  episodes_total: 32448
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12941.239
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0175702571868896
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017115769442170858
        model: {}
        policy_loss: -0.003118488471955061
        total_loss: -0.002679217839613557
        vf_explained_var: 0.01350751519203186
        vf_loss: 22.30195426940918
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1558232307434082
        entropy_coeff: 0.0017600000137463212
        kl: 0.002363773761317134
        model: {}
        policy_loss: -0.004017670173197985
        total_loss: -0.0036859516985714436
        vf_explained_var: -0.05151888728141785
        vf_loss: 23.659685134887695
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1065237522125244
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013161248061805964
        model: {}
        policy_loss: -0.0034643025137484074
        total_loss: -0.0031492384150624275
        vf_explained_var: 0.0038094669580459595
        vf_loss: 22.62544822692871
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43496623635292053
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007341423188336194
        model: {}
        policy_loss: -0.0020876536145806313
        total_loss: -0.000857633538544178
        vf_explained_var: 0.10807427763938904
        vf_loss: 19.955595016479492
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9794659614562988
        entropy_coeff: 0.0017600000137463212
        kl: 0.002071910537779331
        model: {}
        policy_loss: -0.004015814512968063
        total_loss: -0.0035743219777941704
        vf_explained_var: 0.046834319829940796
        vf_loss: 21.653514862060547
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5519724488258362
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009473920799791813
        model: {}
        policy_loss: -0.002643710933625698
        total_loss: -0.0016591362655162811
        vf_explained_var: 0.12719058990478516
        vf_loss: 19.560474395751953
    load_time_ms: 16530.931
    num_steps_sampled: 32448000
    num_steps_trained: 32448000
    sample_time_ms: 129154.996
    update_time_ms: 72.468
  iterations_since_restore: 258
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.94774774774775
    ram_util_percent: 15.769369369369368
  pid: 14340
  policy_reward_max:
    agent-0: 190.833333333333
    agent-1: 190.833333333333
    agent-2: 190.833333333333
    agent-3: 190.833333333333
    agent-4: 190.833333333333
    agent-5: 190.833333333333
  policy_reward_mean:
    agent-0: 164.8583333333332
    agent-1: 164.8583333333332
    agent-2: 164.8583333333332
    agent-3: 164.8583333333332
    agent-4: 164.8583333333332
    agent-5: 164.8583333333332
  policy_reward_min:
    agent-0: 89.5000000000002
    agent-1: 89.5000000000002
    agent-2: 89.5000000000002
    agent-3: 89.5000000000002
    agent-4: 89.5000000000002
    agent-5: 89.5000000000002
  sampler_perf:
    mean_env_wait_ms: 30.989793554581595
    mean_inference_ms: 14.596632255586128
    mean_processing_ms: 65.79243388484569
  time_since_restore: 42203.101460933685
  time_this_iter_s: 155.78238224983215
  time_total_s: 54753.91833233833
  timestamp: 1637077433
  timesteps_since_restore: 24768000
  timesteps_this_iter: 96000
  timesteps_total: 32448000
  training_iteration: 338
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    338 |          54753.9 | 32448000 |   989.15 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 562
    apples_agent-0_mean: 8.46
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 26.88
    apples_agent-1_min: 0
    apples_agent-2_max: 114
    apples_agent-2_mean: 7.11
    apples_agent-2_min: 0
    apples_agent-3_max: 449
    apples_agent-3_mean: 133.9
    apples_agent-3_min: 61
    apples_agent-4_max: 37
    apples_agent-4_mean: 0.82
    apples_agent-4_min: 0
    apples_agent-5_max: 189
    apples_agent-5_mean: 93.78
    apples_agent-5_min: 27
    cleaning_beam_agent-0_max: 513
    cleaning_beam_agent-0_mean: 392.59
    cleaning_beam_agent-0_min: 69
    cleaning_beam_agent-1_max: 464
    cleaning_beam_agent-1_mean: 258.04
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 435
    cleaning_beam_agent-2_mean: 338.9
    cleaning_beam_agent-2_min: 145
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 25.36
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 515
    cleaning_beam_agent-4_mean: 403.46
    cleaning_beam_agent-4_min: 264
    cleaning_beam_agent-5_max: 195
    cleaning_beam_agent-5_mean: 25.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-46-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1176.0000000000016
  episode_reward_mean: 986.9599999999904
  episode_reward_min: 486.00000000001717
  episodes_this_iter: 96
  episodes_total: 32544
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12941.692
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0193158388137817
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012205372331663966
        model: {}
        policy_loss: -0.0029466068372130394
        total_loss: -0.002604624256491661
        vf_explained_var: 0.05268087983131409
        vf_loss: 21.359773635864258
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.147878646850586
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015146476216614246
        model: {}
        policy_loss: -0.0038915625773370266
        total_loss: -0.003545238170772791
        vf_explained_var: -0.05801445245742798
        vf_loss: 23.665878295898438
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1139428615570068
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010781774763017893
        model: {}
        policy_loss: -0.003089356468990445
        total_loss: -0.0027073973324149847
        vf_explained_var: -0.031066805124282837
        vf_loss: 23.42499351501465
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4221852719783783
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007109940052032471
        model: {}
        policy_loss: -0.0020914056804031134
        total_loss: -0.0007545873522758484
        vf_explained_var: 0.0602535605430603
        vf_loss: 20.798635482788086
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.98973548412323
        entropy_coeff: 0.0017600000137463212
        kl: 0.001813274109736085
        model: {}
        policy_loss: -0.004117513075470924
        total_loss: -0.0036508000921458006
        vf_explained_var: 0.012394100427627563
        vf_loss: 22.08648109436035
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5479952096939087
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014857297064736485
        model: {}
        policy_loss: -0.002482939511537552
        total_loss: -0.001395955216139555
        vf_explained_var: 0.07529681921005249
        vf_loss: 20.51456642150879
    load_time_ms: 16597.236
    num_steps_sampled: 32544000
    num_steps_trained: 32544000
    sample_time_ms: 129194.46
    update_time_ms: 70.558
  iterations_since_restore: 259
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.651785714285715
    ram_util_percent: 15.783928571428573
  pid: 14340
  policy_reward_max:
    agent-0: 196.0000000000002
    agent-1: 196.0000000000002
    agent-2: 196.0000000000002
    agent-3: 196.0000000000002
    agent-4: 196.0000000000002
    agent-5: 196.0000000000002
  policy_reward_mean:
    agent-0: 164.49333333333325
    agent-1: 164.49333333333325
    agent-2: 164.49333333333325
    agent-3: 164.49333333333325
    agent-4: 164.49333333333325
    agent-5: 164.49333333333325
  policy_reward_min:
    agent-0: 81.00000000000027
    agent-1: 81.00000000000027
    agent-2: 81.00000000000027
    agent-3: 81.00000000000027
    agent-4: 81.00000000000027
    agent-5: 81.00000000000027
  sampler_perf:
    mean_env_wait_ms: 30.99124691881432
    mean_inference_ms: 14.596621091665718
    mean_processing_ms: 65.79293020798764
  time_since_restore: 42360.07625865936
  time_this_iter_s: 156.9747977256775
  time_total_s: 54910.89313006401
  timestamp: 1637077590
  timesteps_since_restore: 24864000
  timesteps_this_iter: 96000
  timesteps_total: 32544000
  training_iteration: 339
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    339 |          54910.9 | 32544000 |   986.96 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 299
    apples_agent-0_mean: 6.09
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 22.5
    apples_agent-1_min: 0
    apples_agent-2_max: 93
    apples_agent-2_mean: 5.8
    apples_agent-2_min: 0
    apples_agent-3_max: 269
    apples_agent-3_mean: 132.52
    apples_agent-3_min: 56
    apples_agent-4_max: 56
    apples_agent-4_mean: 0.82
    apples_agent-4_min: 0
    apples_agent-5_max: 222
    apples_agent-5_mean: 91.81
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 515
    cleaning_beam_agent-0_mean: 387.9
    cleaning_beam_agent-0_min: 133
    cleaning_beam_agent-1_max: 462
    cleaning_beam_agent-1_mean: 254.29
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 484
    cleaning_beam_agent-2_mean: 352.69
    cleaning_beam_agent-2_min: 198
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 24.64
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 526
    cleaning_beam_agent-4_mean: 406.73
    cleaning_beam_agent-4_min: 248
    cleaning_beam_agent-5_max: 230
    cleaning_beam_agent-5_mean: 21.69
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-49-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1154.9999999999893
  episode_reward_mean: 971.4299999999878
  episode_reward_min: 470.0000000000076
  episodes_this_iter: 96
  episodes_total: 32640
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12952.274
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0269725322723389
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013015393633395433
        model: {}
        policy_loss: -0.003115200437605381
        total_loss: -0.0027898806147277355
        vf_explained_var: 0.03879973292350769
        vf_loss: 21.327953338623047
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.156891942024231
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012730015441775322
        model: {}
        policy_loss: -0.0036258860491216183
        total_loss: -0.003332630032673478
        vf_explained_var: -0.05682578682899475
        vf_loss: 23.293907165527344
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1119141578674316
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019872798584401608
        model: {}
        policy_loss: -0.0034068659879267216
        total_loss: -0.0031259702518582344
        vf_explained_var: -0.002292424440383911
        vf_loss: 22.378646850585938
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43192702531814575
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008956186939030886
        model: {}
        policy_loss: -0.002078065648674965
        total_loss: -0.0008104629814624786
        vf_explained_var: 0.0767274796962738
        vf_loss: 20.27793312072754
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9840871095657349
        entropy_coeff: 0.0017600000137463212
        kl: 0.00190748181194067
        model: {}
        policy_loss: -0.004137205891311169
        total_loss: -0.003715905826538801
        vf_explained_var: 0.02496117353439331
        vf_loss: 21.532928466796875
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.551018238067627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006028875941410661
        model: {}
        policy_loss: -0.002467614598572254
        total_loss: -0.0014043273404240608
        vf_explained_var: 0.07559189200401306
        vf_loss: 20.330810546875
    load_time_ms: 16497.939
    num_steps_sampled: 32640000
    num_steps_trained: 32640000
    sample_time_ms: 129014.285
    update_time_ms: 92.836
  iterations_since_restore: 260
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.79681818181818
    ram_util_percent: 15.823636363636362
  pid: 14340
  policy_reward_max:
    agent-0: 192.49999999999957
    agent-1: 192.49999999999957
    agent-2: 192.49999999999957
    agent-3: 192.49999999999957
    agent-4: 192.49999999999957
    agent-5: 192.49999999999957
  policy_reward_mean:
    agent-0: 161.9049999999999
    agent-1: 161.9049999999999
    agent-2: 161.9049999999999
    agent-3: 161.9049999999999
    agent-4: 161.9049999999999
    agent-5: 161.9049999999999
  policy_reward_min:
    agent-0: 78.33333333333339
    agent-1: 78.33333333333339
    agent-2: 78.33333333333339
    agent-3: 78.33333333333339
    agent-4: 78.33333333333339
    agent-5: 78.33333333333339
  sampler_perf:
    mean_env_wait_ms: 30.992751708963652
    mean_inference_ms: 14.59665365464644
    mean_processing_ms: 65.79245965192966
  time_since_restore: 42514.27237677574
  time_this_iter_s: 154.19611811637878
  time_total_s: 55065.08924818039
  timestamp: 1637077745
  timesteps_since_restore: 24960000
  timesteps_this_iter: 96000
  timesteps_total: 32640000
  training_iteration: 340
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    340 |          55065.1 | 32640000 |   971.43 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 246
    apples_agent-0_mean: 6.64
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 22.96
    apples_agent-1_min: 0
    apples_agent-2_max: 167
    apples_agent-2_mean: 4.73
    apples_agent-2_min: 0
    apples_agent-3_max: 354
    apples_agent-3_mean: 132.53
    apples_agent-3_min: 51
    apples_agent-4_max: 87
    apples_agent-4_mean: 2.87
    apples_agent-4_min: 0
    apples_agent-5_max: 331
    apples_agent-5_mean: 98.71
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 523
    cleaning_beam_agent-0_mean: 392.56
    cleaning_beam_agent-0_min: 258
    cleaning_beam_agent-1_max: 441
    cleaning_beam_agent-1_mean: 244.54
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 507
    cleaning_beam_agent-2_mean: 365.97
    cleaning_beam_agent-2_min: 154
    cleaning_beam_agent-3_max: 127
    cleaning_beam_agent-3_mean: 24.57
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 513
    cleaning_beam_agent-4_mean: 409.28
    cleaning_beam_agent-4_min: 165
    cleaning_beam_agent-5_max: 312
    cleaning_beam_agent-5_mean: 22.32
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-51-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1197.0000000000018
  episode_reward_mean: 969.7999999999903
  episode_reward_min: 334.99999999999903
  episodes_this_iter: 96
  episodes_total: 32736
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12960.504
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0167078971862793
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020286249928176403
        model: {}
        policy_loss: -0.002965534571558237
        total_loss: -0.0022799125872552395
        vf_explained_var: 0.026125475764274597
        vf_loss: 24.750286102294922
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1536312103271484
        entropy_coeff: 0.0017600000137463212
        kl: 0.001749892602674663
        model: {}
        policy_loss: -0.0037813913077116013
        total_loss: -0.0032039983198046684
        vf_explained_var: -0.02622288465499878
        vf_loss: 26.07785415649414
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1032888889312744
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016511823050677776
        model: {}
        policy_loss: -0.003400698071345687
        total_loss: -0.002873459365218878
        vf_explained_var: 0.033928677439689636
        vf_loss: 24.690288543701172
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4253537654876709
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012386183952912688
        model: {}
        policy_loss: -0.0022902903147041798
        total_loss: -0.000858125276863575
        vf_explained_var: 0.14108465611934662
        vf_loss: 21.80788803100586
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9831103682518005
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016633897321298718
        model: {}
        policy_loss: -0.0040216767229139805
        total_loss: -0.003412852995097637
        vf_explained_var: 0.08062832057476044
        vf_loss: 23.390979766845703
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5434203147888184
        entropy_coeff: 0.0017600000137463212
        kl: 0.00110341003164649
        model: {}
        policy_loss: -0.0026648947969079018
        total_loss: -0.0013800142332911491
        vf_explained_var: 0.11750300228595734
        vf_loss: 22.412981033325195
    load_time_ms: 16419.758
    num_steps_sampled: 32736000
    num_steps_trained: 32736000
    sample_time_ms: 128972.436
    update_time_ms: 64.753
  iterations_since_restore: 261
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.808071748878927
    ram_util_percent: 15.761883408071748
  pid: 14340
  policy_reward_max:
    agent-0: 199.49999999999991
    agent-1: 199.49999999999991
    agent-2: 199.49999999999991
    agent-3: 199.49999999999991
    agent-4: 199.49999999999991
    agent-5: 199.49999999999991
  policy_reward_mean:
    agent-0: 161.63333333333324
    agent-1: 161.63333333333324
    agent-2: 161.63333333333324
    agent-3: 161.63333333333324
    agent-4: 161.63333333333324
    agent-5: 161.63333333333324
  policy_reward_min:
    agent-0: 55.83333333333316
    agent-1: 55.83333333333316
    agent-2: 55.83333333333316
    agent-3: 55.83333333333316
    agent-4: 55.83333333333316
    agent-5: 55.83333333333316
  sampler_perf:
    mean_env_wait_ms: 30.99450835665203
    mean_inference_ms: 14.596590983148099
    mean_processing_ms: 65.79233747992376
  time_since_restore: 42670.112777233124
  time_this_iter_s: 155.8404004573822
  time_total_s: 55220.92964863777
  timestamp: 1637077902
  timesteps_since_restore: 25056000
  timesteps_this_iter: 96000
  timesteps_total: 32736000
  training_iteration: 341
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    341 |          55220.9 | 32736000 |    969.8 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 424
    apples_agent-0_mean: 7.62
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 23.79
    apples_agent-1_min: 0
    apples_agent-2_max: 171
    apples_agent-2_mean: 7.74
    apples_agent-2_min: 0
    apples_agent-3_max: 231
    apples_agent-3_mean: 130.58
    apples_agent-3_min: 52
    apples_agent-4_max: 71
    apples_agent-4_mean: 3.45
    apples_agent-4_min: 0
    apples_agent-5_max: 491
    apples_agent-5_mean: 99.02
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 523
    cleaning_beam_agent-0_mean: 404.65
    cleaning_beam_agent-0_min: 249
    cleaning_beam_agent-1_max: 524
    cleaning_beam_agent-1_mean: 241.5
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 512
    cleaning_beam_agent-2_mean: 354.8
    cleaning_beam_agent-2_min: 160
    cleaning_beam_agent-3_max: 89
    cleaning_beam_agent-3_mean: 21.49
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 484
    cleaning_beam_agent-4_mean: 403.3
    cleaning_beam_agent-4_min: 137
    cleaning_beam_agent-5_max: 89
    cleaning_beam_agent-5_mean: 18.86
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-54-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1197.0000000000018
  episode_reward_mean: 979.4699999999888
  episode_reward_min: 447.0000000000108
  episodes_this_iter: 96
  episodes_total: 32832
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12949.35
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0078274011611938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018245649989694357
        model: {}
        policy_loss: -0.002956284210085869
        total_loss: -0.002237795852124691
        vf_explained_var: 0.03882971405982971
        vf_loss: 24.922643661499023
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1628611087799072
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018114536069333553
        model: {}
        policy_loss: -0.0035764556378126144
        total_loss: -0.0029289075173437595
        vf_explained_var: -0.03765794634819031
        vf_loss: 26.941864013671875
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1039897203445435
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013817992294207215
        model: {}
        policy_loss: -0.003499723505228758
        total_loss: -0.002768933307379484
        vf_explained_var: -0.01917165517807007
        vf_loss: 26.738086700439453
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4282407760620117
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012321167159825563
        model: {}
        policy_loss: -0.002416843781247735
        total_loss: -0.0009797869715839624
        vf_explained_var: 0.15478572249412537
        vf_loss: 21.9075927734375
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.992189884185791
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015331689501181245
        model: {}
        policy_loss: -0.0037910621613264084
        total_loss: -0.003208171809092164
        vf_explained_var: 0.10762487351894379
        vf_loss: 23.29143524169922
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.552656352519989
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005695901345461607
        model: {}
        policy_loss: -0.0023407181724905968
        total_loss: -0.0010997967328876257
        vf_explained_var: 0.14561153948307037
        vf_loss: 22.13595199584961
    load_time_ms: 14579.001
    num_steps_sampled: 32832000
    num_steps_trained: 32832000
    sample_time_ms: 128942.827
    update_time_ms: 64.496
  iterations_since_restore: 262
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.886547085201787
    ram_util_percent: 15.771300448430491
  pid: 14340
  policy_reward_max:
    agent-0: 199.49999999999991
    agent-1: 199.49999999999991
    agent-2: 199.49999999999991
    agent-3: 199.49999999999991
    agent-4: 199.49999999999991
    agent-5: 199.49999999999991
  policy_reward_mean:
    agent-0: 163.24499999999986
    agent-1: 163.24499999999986
    agent-2: 163.24499999999986
    agent-3: 163.24499999999986
    agent-4: 163.24499999999986
    agent-5: 163.24499999999986
  policy_reward_min:
    agent-0: 74.5000000000001
    agent-1: 74.5000000000001
    agent-2: 74.5000000000001
    agent-3: 74.5000000000001
    agent-4: 74.5000000000001
    agent-5: 74.5000000000001
  sampler_perf:
    mean_env_wait_ms: 30.9960627605222
    mean_inference_ms: 14.59660520806225
    mean_processing_ms: 65.79355670460656
  time_since_restore: 42826.79993367195
  time_this_iter_s: 156.68715643882751
  time_total_s: 55377.6168050766
  timestamp: 1637078059
  timesteps_since_restore: 25152000
  timesteps_this_iter: 96000
  timesteps_total: 32832000
  training_iteration: 342
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    342 |          55377.6 | 32832000 |   979.47 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 335
    apples_agent-0_mean: 5.19
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 24.03
    apples_agent-1_min: 0
    apples_agent-2_max: 116
    apples_agent-2_mean: 5.68
    apples_agent-2_min: 0
    apples_agent-3_max: 197
    apples_agent-3_mean: 133.14
    apples_agent-3_min: 73
    apples_agent-4_max: 106
    apples_agent-4_mean: 1.72
    apples_agent-4_min: 0
    apples_agent-5_max: 453
    apples_agent-5_mean: 97.54
    apples_agent-5_min: 46
    cleaning_beam_agent-0_max: 560
    cleaning_beam_agent-0_mean: 412.17
    cleaning_beam_agent-0_min: 215
    cleaning_beam_agent-1_max: 412
    cleaning_beam_agent-1_mean: 243.64
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 506
    cleaning_beam_agent-2_mean: 364.59
    cleaning_beam_agent-2_min: 164
    cleaning_beam_agent-3_max: 108
    cleaning_beam_agent-3_mean: 21.88
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 518
    cleaning_beam_agent-4_mean: 405.68
    cleaning_beam_agent-4_min: 194
    cleaning_beam_agent-5_max: 118
    cleaning_beam_agent-5_mean: 15.22
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-56-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1211.0000000000095
  episode_reward_mean: 1000.4799999999883
  episode_reward_min: 370.0000000000008
  episodes_this_iter: 96
  episodes_total: 32928
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12939.807
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0048222541809082
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017783527728170156
        model: {}
        policy_loss: -0.003124216804280877
        total_loss: -0.002772828098386526
        vf_explained_var: 0.0653076022863388
        vf_loss: 21.198762893676758
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.153321623802185
        entropy_coeff: 0.0017600000137463212
        kl: 0.001527308952063322
        model: {}
        policy_loss: -0.003798439633101225
        total_loss: -0.003444259287789464
        vf_explained_var: -0.03900271654129028
        vf_loss: 23.840251922607422
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.092301845550537
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020500966347754
        model: {}
        policy_loss: -0.0034409156069159508
        total_loss: -0.0030244356021285057
        vf_explained_var: -0.01092328131198883
        vf_loss: 23.389331817626953
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42021644115448
        entropy_coeff: 0.0017600000137463212
        kl: 0.001335209235548973
        model: {}
        policy_loss: -0.0021957196295261383
        total_loss: -0.0008679996244609356
        vf_explained_var: 0.08946584165096283
        vf_loss: 20.673030853271484
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9904512763023376
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015096422284841537
        model: {}
        policy_loss: -0.003926422912627459
        total_loss: -0.003520562779158354
        vf_explained_var: 0.06413303315639496
        vf_loss: 21.49053955078125
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.520602822303772
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008362961816601455
        model: {}
        policy_loss: -0.002489960752427578
        total_loss: -0.0013591945171356201
        vf_explained_var: 0.09837992489337921
        vf_loss: 20.47028923034668
    load_time_ms: 14441.797
    num_steps_sampled: 32928000
    num_steps_trained: 32928000
    sample_time_ms: 128796.835
    update_time_ms: 76.396
  iterations_since_restore: 263
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.65630630630631
    ram_util_percent: 15.741441441441443
  pid: 14340
  policy_reward_max:
    agent-0: 201.83333333333312
    agent-1: 201.83333333333312
    agent-2: 201.83333333333312
    agent-3: 201.83333333333312
    agent-4: 201.83333333333312
    agent-5: 201.83333333333312
  policy_reward_mean:
    agent-0: 166.7466666666665
    agent-1: 166.7466666666665
    agent-2: 166.7466666666665
    agent-3: 166.7466666666665
    agent-4: 166.7466666666665
    agent-5: 166.7466666666665
  policy_reward_min:
    agent-0: 61.66666666666663
    agent-1: 61.66666666666663
    agent-2: 61.66666666666663
    agent-3: 61.66666666666663
    agent-4: 61.66666666666663
    agent-5: 61.66666666666663
  sampler_perf:
    mean_env_wait_ms: 30.99813164048383
    mean_inference_ms: 14.596763914999404
    mean_processing_ms: 65.79365760564819
  time_since_restore: 42981.563637018204
  time_this_iter_s: 154.76370334625244
  time_total_s: 55532.38050842285
  timestamp: 1637078214
  timesteps_since_restore: 25248000
  timesteps_this_iter: 96000
  timesteps_total: 32928000
  training_iteration: 343
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    343 |          55532.4 | 32928000 |  1000.48 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 2.77
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 23.56
    apples_agent-1_min: 0
    apples_agent-2_max: 84
    apples_agent-2_mean: 5.58
    apples_agent-2_min: 0
    apples_agent-3_max: 243
    apples_agent-3_mean: 134.72
    apples_agent-3_min: 49
    apples_agent-4_max: 45
    apples_agent-4_mean: 2.3
    apples_agent-4_min: 0
    apples_agent-5_max: 225
    apples_agent-5_mean: 90.97
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 534
    cleaning_beam_agent-0_mean: 403.99
    cleaning_beam_agent-0_min: 197
    cleaning_beam_agent-1_max: 385
    cleaning_beam_agent-1_mean: 229.86
    cleaning_beam_agent-1_min: 89
    cleaning_beam_agent-2_max: 526
    cleaning_beam_agent-2_mean: 364.46
    cleaning_beam_agent-2_min: 161
    cleaning_beam_agent-3_max: 83
    cleaning_beam_agent-3_mean: 20.49
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 527
    cleaning_beam_agent-4_mean: 395.85
    cleaning_beam_agent-4_min: 239
    cleaning_beam_agent-5_max: 131
    cleaning_beam_agent-5_mean: 22.03
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_10-59-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1198.000000000014
  episode_reward_mean: 992.5199999999905
  episode_reward_min: 291.00000000000097
  episodes_this_iter: 96
  episodes_total: 33024
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12929.21
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0193971395492554
        entropy_coeff: 0.0017600000137463212
        kl: 0.001690019154921174
        model: {}
        policy_loss: -0.0032011568546295166
        total_loss: -0.0026847212575376034
        vf_explained_var: 0.02560575306415558
        vf_loss: 23.10574722290039
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.164973258972168
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015535835409536958
        model: {}
        policy_loss: -0.0033563878387212753
        total_loss: -0.0029540788382291794
        vf_explained_var: -0.03144478797912598
        vf_loss: 24.52665901184082
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0887236595153809
        entropy_coeff: 0.0017600000137463212
        kl: 0.001431137789040804
        model: {}
        policy_loss: -0.003315065987408161
        total_loss: -0.0027718069031834602
        vf_explained_var: -0.023428544402122498
        vf_loss: 24.594120025634766
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43921875953674316
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011243614135310054
        model: {}
        policy_loss: -0.0023734169080853462
        total_loss: -0.0010769437067210674
        vf_explained_var: 0.12651847302913666
        vf_loss: 20.694990158081055
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9900639057159424
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015376281226053834
        model: {}
        policy_loss: -0.003770751878619194
        total_loss: -0.003239489160478115
        vf_explained_var: 0.04803137481212616
        vf_loss: 22.73774528503418
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5436934232711792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010517493356019258
        model: {}
        policy_loss: -0.0028168088756501675
        total_loss: -0.0016983323730528355
        vf_explained_var: 0.1235666424036026
        vf_loss: 20.753759384155273
    load_time_ms: 14285.769
    num_steps_sampled: 33024000
    num_steps_trained: 33024000
    sample_time_ms: 128892.908
    update_time_ms: 70.224
  iterations_since_restore: 264
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.881614349775788
    ram_util_percent: 15.74439461883408
  pid: 14340
  policy_reward_max:
    agent-0: 199.66666666666617
    agent-1: 199.66666666666617
    agent-2: 199.66666666666617
    agent-3: 199.66666666666617
    agent-4: 199.66666666666617
    agent-5: 199.66666666666617
  policy_reward_mean:
    agent-0: 165.41999999999985
    agent-1: 165.41999999999985
    agent-2: 165.41999999999985
    agent-3: 165.41999999999985
    agent-4: 165.41999999999985
    agent-5: 165.41999999999985
  policy_reward_min:
    agent-0: 48.49999999999994
    agent-1: 48.49999999999994
    agent-2: 48.49999999999994
    agent-3: 48.49999999999994
    agent-4: 48.49999999999994
    agent-5: 48.49999999999994
  sampler_perf:
    mean_env_wait_ms: 30.999941645395086
    mean_inference_ms: 14.597084739668276
    mean_processing_ms: 65.79445204861662
  time_since_restore: 43138.43197178841
  time_this_iter_s: 156.86833477020264
  time_total_s: 55689.248843193054
  timestamp: 1637078371
  timesteps_since_restore: 25344000
  timesteps_this_iter: 96000
  timesteps_total: 33024000
  training_iteration: 344
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    344 |          55689.2 | 33024000 |   992.52 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 2.78
    apples_agent-0_min: 0
    apples_agent-1_max: 173
    apples_agent-1_mean: 28.07
    apples_agent-1_min: 0
    apples_agent-2_max: 472
    apples_agent-2_mean: 15.12
    apples_agent-2_min: 0
    apples_agent-3_max: 269
    apples_agent-3_mean: 138.19
    apples_agent-3_min: 50
    apples_agent-4_max: 79
    apples_agent-4_mean: 1.77
    apples_agent-4_min: 0
    apples_agent-5_max: 334
    apples_agent-5_mean: 94.91
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 564
    cleaning_beam_agent-0_mean: 383.97
    cleaning_beam_agent-0_min: 154
    cleaning_beam_agent-1_max: 523
    cleaning_beam_agent-1_mean: 226.35
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 604
    cleaning_beam_agent-2_mean: 355.13
    cleaning_beam_agent-2_min: 70
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 19.96
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 487
    cleaning_beam_agent-4_mean: 399.74
    cleaning_beam_agent-4_min: 288
    cleaning_beam_agent-5_max: 119
    cleaning_beam_agent-5_mean: 19.64
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-02-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1204.0000000000018
  episode_reward_mean: 976.5599999999881
  episode_reward_min: 401.000000000006
  episodes_this_iter: 96
  episodes_total: 33120
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12927.027
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.016956090927124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016390667296946049
        model: {}
        policy_loss: -0.003068402875214815
        total_loss: -0.002558378968387842
        vf_explained_var: 0.04852239787578583
        vf_loss: 22.998687744140625
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1739683151245117
        entropy_coeff: 0.0017600000137463212
        kl: 0.001813631970435381
        model: {}
        policy_loss: -0.0038630980998277664
        total_loss: -0.0034262919798493385
        vf_explained_var: -0.03544154763221741
        vf_loss: 25.029926300048828
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0708339214324951
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016499058110639453
        model: {}
        policy_loss: -0.0036057867109775543
        total_loss: -0.003146248869597912
        vf_explained_var: 0.03617897629737854
        vf_loss: 23.44205665588379
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.440289169549942
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012102227192372084
        model: {}
        policy_loss: -0.002412856090813875
        total_loss: -0.0010835783323273063
        vf_explained_var: 0.1278124451637268
        vf_loss: 21.041854858398438
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.980445146560669
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013698132243007421
        model: {}
        policy_loss: -0.003507685847580433
        total_loss: -0.0029144701547920704
        vf_explained_var: 0.0403067022562027
        vf_loss: 23.188003540039062
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5552359819412231
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011607767082750797
        model: {}
        policy_loss: -0.0024666250683367252
        total_loss: -0.0013182139955461025
        vf_explained_var: 0.11814525723457336
        vf_loss: 21.25626564025879
    load_time_ms: 14095.653
    num_steps_sampled: 33120000
    num_steps_trained: 33120000
    sample_time_ms: 128828.0
    update_time_ms: 69.898
  iterations_since_restore: 265
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.80497737556561
    ram_util_percent: 14.978280542986429
  pid: 14340
  policy_reward_max:
    agent-0: 200.66666666666654
    agent-1: 200.66666666666654
    agent-2: 200.66666666666654
    agent-3: 200.66666666666654
    agent-4: 200.66666666666654
    agent-5: 200.66666666666654
  policy_reward_mean:
    agent-0: 162.7599999999999
    agent-1: 162.7599999999999
    agent-2: 162.7599999999999
    agent-3: 162.7599999999999
    agent-4: 162.7599999999999
    agent-5: 162.7599999999999
  policy_reward_min:
    agent-0: 66.83333333333316
    agent-1: 66.83333333333316
    agent-2: 66.83333333333316
    agent-3: 66.83333333333316
    agent-4: 66.83333333333316
    agent-5: 66.83333333333316
  sampler_perf:
    mean_env_wait_ms: 31.00049916385343
    mean_inference_ms: 14.596906037021077
    mean_processing_ms: 65.79413429431384
  time_since_restore: 43293.41526436806
  time_this_iter_s: 154.98329257965088
  time_total_s: 55844.232135772705
  timestamp: 1637078526
  timesteps_since_restore: 25440000
  timesteps_this_iter: 96000
  timesteps_total: 33120000
  training_iteration: 345
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    345 |          55844.2 | 33120000 |   976.56 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 473
    apples_agent-0_mean: 9.39
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 28.92
    apples_agent-1_min: 0
    apples_agent-2_max: 42
    apples_agent-2_mean: 3.97
    apples_agent-2_min: 0
    apples_agent-3_max: 186
    apples_agent-3_mean: 131.55
    apples_agent-3_min: 32
    apples_agent-4_max: 72
    apples_agent-4_mean: 1.69
    apples_agent-4_min: 0
    apples_agent-5_max: 554
    apples_agent-5_mean: 98.72
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 644
    cleaning_beam_agent-0_mean: 400.53
    cleaning_beam_agent-0_min: 104
    cleaning_beam_agent-1_max: 494
    cleaning_beam_agent-1_mean: 223.15
    cleaning_beam_agent-1_min: 79
    cleaning_beam_agent-2_max: 536
    cleaning_beam_agent-2_mean: 358.25
    cleaning_beam_agent-2_min: 174
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 19.14
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 507
    cleaning_beam_agent-4_mean: 403.36
    cleaning_beam_agent-4_min: 232
    cleaning_beam_agent-5_max: 195
    cleaning_beam_agent-5_mean: 19.76
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-04-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1224.9999999999914
  episode_reward_mean: 977.5399999999886
  episode_reward_min: 266.9999999999976
  episodes_this_iter: 96
  episodes_total: 33216
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12910.911
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.974358320236206
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019770788494497538
        model: {}
        policy_loss: -0.003027742262929678
        total_loss: -0.002158178947865963
        vf_explained_var: 0.04062272608280182
        vf_loss: 25.84431266784668
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1645394563674927
        entropy_coeff: 0.0017600000137463212
        kl: 0.001321269664913416
        model: {}
        policy_loss: -0.003624497912824154
        total_loss: -0.002979506738483906
        vf_explained_var: 0.0007894635200500488
        vf_loss: 26.945817947387695
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0879812240600586
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016484226798638701
        model: {}
        policy_loss: -0.003163950052112341
        total_loss: -0.0023347768001258373
        vf_explained_var: -0.019494622945785522
        vf_loss: 27.440189361572266
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4302513003349304
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007670157356187701
        model: {}
        policy_loss: -0.002295156940817833
        total_loss: -0.0007447432726621628
        vf_explained_var: 0.141041100025177
        vf_loss: 23.076595306396484
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9713718295097351
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013324269093573093
        model: {}
        policy_loss: -0.004065421409904957
        total_loss: -0.0033579636365175247
        vf_explained_var: 0.10240978002548218
        vf_loss: 24.17072296142578
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5582135319709778
        entropy_coeff: 0.0017600000137463212
        kl: 0.001192343421280384
        model: {}
        policy_loss: -0.002703022677451372
        total_loss: -0.0012915320694446564
        vf_explained_var: 0.11034618318080902
        vf_loss: 23.939451217651367
    load_time_ms: 13947.738
    num_steps_sampled: 33216000
    num_steps_trained: 33216000
    sample_time_ms: 128593.422
    update_time_ms: 69.04
  iterations_since_restore: 266
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.98767123287671
    ram_util_percent: 14.646118721461189
  pid: 14340
  policy_reward_max:
    agent-0: 204.16666666666654
    agent-1: 204.16666666666654
    agent-2: 204.16666666666654
    agent-3: 204.16666666666654
    agent-4: 204.16666666666654
    agent-5: 204.16666666666654
  policy_reward_mean:
    agent-0: 162.9233333333332
    agent-1: 162.9233333333332
    agent-2: 162.9233333333332
    agent-3: 162.9233333333332
    agent-4: 162.9233333333332
    agent-5: 162.9233333333332
  policy_reward_min:
    agent-0: 44.499999999999986
    agent-1: 44.499999999999986
    agent-2: 44.499999999999986
    agent-3: 44.499999999999986
    agent-4: 44.499999999999986
    agent-5: 44.499999999999986
  sampler_perf:
    mean_env_wait_ms: 31.001615165676082
    mean_inference_ms: 14.596697398293964
    mean_processing_ms: 65.7937034518864
  time_since_restore: 43446.97768449783
  time_this_iter_s: 153.562420129776
  time_total_s: 55997.79455590248
  timestamp: 1637078680
  timesteps_since_restore: 25536000
  timesteps_this_iter: 96000
  timesteps_total: 33216000
  training_iteration: 346
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    346 |          55997.8 | 33216000 |   977.54 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 181
    apples_agent-0_mean: 8.19
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 27.08
    apples_agent-1_min: 0
    apples_agent-2_max: 61
    apples_agent-2_mean: 5.47
    apples_agent-2_min: 0
    apples_agent-3_max: 204
    apples_agent-3_mean: 130.22
    apples_agent-3_min: 57
    apples_agent-4_max: 62
    apples_agent-4_mean: 2.16
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 92.19
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 568
    cleaning_beam_agent-0_mean: 408.94
    cleaning_beam_agent-0_min: 233
    cleaning_beam_agent-1_max: 381
    cleaning_beam_agent-1_mean: 218.8
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 607
    cleaning_beam_agent-2_mean: 355.62
    cleaning_beam_agent-2_min: 167
    cleaning_beam_agent-3_max: 63
    cleaning_beam_agent-3_mean: 21.06
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 493
    cleaning_beam_agent-4_mean: 404.59
    cleaning_beam_agent-4_min: 256
    cleaning_beam_agent-5_max: 354
    cleaning_beam_agent-5_mean: 24.12
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-07-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1163.0000000000036
  episode_reward_mean: 964.0899999999881
  episode_reward_min: 449.0000000000084
  episodes_this_iter: 96
  episodes_total: 33312
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12920.463
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9909974336624146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014779218472540379
        model: {}
        policy_loss: -0.0031345614697784185
        total_loss: -0.0027158975135535
        vf_explained_var: 0.07469239830970764
        vf_loss: 21.628196716308594
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1636385917663574
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018800441175699234
        model: {}
        policy_loss: -0.0036124703474342823
        total_loss: -0.003252936527132988
        vf_explained_var: -0.030354321002960205
        vf_loss: 24.07535171508789
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0823209285736084
        entropy_coeff: 0.0017600000137463212
        kl: 0.00164689589291811
        model: {}
        policy_loss: -0.003452499397099018
        total_loss: -0.0029827447142452
        vf_explained_var: -0.014592230319976807
        vf_loss: 23.746400833129883
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45086219906806946
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010531586594879627
        model: {}
        policy_loss: -0.002368734683841467
        total_loss: -0.0011320579797029495
        vf_explained_var: 0.13090430200099945
        vf_loss: 20.301959991455078
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.976958692073822
        entropy_coeff: 0.0017600000137463212
        kl: 0.002058158628642559
        model: {}
        policy_loss: -0.0039011933840811253
        total_loss: -0.003323161043226719
        vf_explained_var: 0.016299709677696228
        vf_loss: 22.974807739257812
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5713459849357605
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009532751282677054
        model: {}
        policy_loss: -0.0027810614556074142
        total_loss: -0.0017648120410740376
        vf_explained_var: 0.1340639442205429
        vf_loss: 20.21820068359375
    load_time_ms: 13879.681
    num_steps_sampled: 33312000
    num_steps_trained: 33312000
    sample_time_ms: 128551.076
    update_time_ms: 69.024
  iterations_since_restore: 267
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.957918552036197
    ram_util_percent: 14.690497737556562
  pid: 14340
  policy_reward_max:
    agent-0: 193.83333333333297
    agent-1: 193.83333333333297
    agent-2: 193.83333333333297
    agent-3: 193.83333333333297
    agent-4: 193.83333333333297
    agent-5: 193.83333333333297
  policy_reward_mean:
    agent-0: 160.6816666666666
    agent-1: 160.6816666666666
    agent-2: 160.6816666666666
    agent-3: 160.6816666666666
    agent-4: 160.6816666666666
    agent-5: 160.6816666666666
  policy_reward_min:
    agent-0: 74.83333333333329
    agent-1: 74.83333333333329
    agent-2: 74.83333333333329
    agent-3: 74.83333333333329
    agent-4: 74.83333333333329
    agent-5: 74.83333333333329
  sampler_perf:
    mean_env_wait_ms: 31.0025284854079
    mean_inference_ms: 14.596873458285952
    mean_processing_ms: 65.79619877659871
  time_since_restore: 43602.45767760277
  time_this_iter_s: 155.4799931049347
  time_total_s: 56153.274549007416
  timestamp: 1637078836
  timesteps_since_restore: 25632000
  timesteps_this_iter: 96000
  timesteps_total: 33312000
  training_iteration: 347
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    347 |          56153.3 | 33312000 |   964.09 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 84
    apples_agent-0_mean: 3.37
    apples_agent-0_min: 0
    apples_agent-1_max: 144
    apples_agent-1_mean: 29.01
    apples_agent-1_min: 0
    apples_agent-2_max: 622
    apples_agent-2_mean: 12.96
    apples_agent-2_min: 0
    apples_agent-3_max: 454
    apples_agent-3_mean: 134.05
    apples_agent-3_min: 43
    apples_agent-4_max: 50
    apples_agent-4_mean: 1.58
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 92.79
    apples_agent-5_min: 59
    cleaning_beam_agent-0_max: 564
    cleaning_beam_agent-0_mean: 416.74
    cleaning_beam_agent-0_min: 238
    cleaning_beam_agent-1_max: 344
    cleaning_beam_agent-1_mean: 203.83
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 560
    cleaning_beam_agent-2_mean: 361.78
    cleaning_beam_agent-2_min: 44
    cleaning_beam_agent-3_max: 79
    cleaning_beam_agent-3_mean: 19.76
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 486
    cleaning_beam_agent-4_mean: 394.43
    cleaning_beam_agent-4_min: 250
    cleaning_beam_agent-5_max: 121
    cleaning_beam_agent-5_mean: 16.2
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-09-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1174.9999999999989
  episode_reward_mean: 995.89999999999
  episode_reward_min: 381.0000000000069
  episodes_this_iter: 96
  episodes_total: 33408
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12910.057
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9968240261077881
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010413703275844455
        model: {}
        policy_loss: -0.002805545926094055
        total_loss: -0.002255641855299473
        vf_explained_var: 0.02645564079284668
        vf_loss: 23.043127059936523
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1598601341247559
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017172661609947681
        model: {}
        policy_loss: -0.003955415915697813
        total_loss: -0.003563622012734413
        vf_explained_var: -0.01953303813934326
        vf_loss: 24.33148956298828
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0631450414657593
        entropy_coeff: 0.0017600000137463212
        kl: 0.001256164745427668
        model: {}
        policy_loss: -0.0034337504766881466
        total_loss: -0.0029162135906517506
        vf_explained_var: 0.0005766749382019043
        vf_loss: 23.88672637939453
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4374888837337494
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012241386575624347
        model: {}
        policy_loss: -0.002404350321739912
        total_loss: -0.001103616552427411
        vf_explained_var: 0.12233735620975494
        vf_loss: 20.70713996887207
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9818795919418335
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019164161058142781
        model: {}
        policy_loss: -0.003999356180429459
        total_loss: -0.0035212726797908545
        vf_explained_var: 0.07309101521968842
        vf_loss: 22.06192398071289
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5493324995040894
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010040171910077333
        model: {}
        policy_loss: -0.0025432314723730087
        total_loss: -0.0014075897634029388
        vf_explained_var: 0.10978430509567261
        vf_loss: 21.0246639251709
    load_time_ms: 14015.021
    num_steps_sampled: 33408000
    num_steps_trained: 33408000
    sample_time_ms: 128618.62
    update_time_ms: 67.683
  iterations_since_restore: 268
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.519111111111112
    ram_util_percent: 14.688000000000002
  pid: 14340
  policy_reward_max:
    agent-0: 195.83333333333346
    agent-1: 195.83333333333346
    agent-2: 195.83333333333346
    agent-3: 195.83333333333346
    agent-4: 195.83333333333346
    agent-5: 195.83333333333346
  policy_reward_mean:
    agent-0: 165.9833333333332
    agent-1: 165.9833333333332
    agent-2: 165.9833333333332
    agent-3: 165.9833333333332
    agent-4: 165.9833333333332
    agent-5: 165.9833333333332
  policy_reward_min:
    agent-0: 63.499999999999766
    agent-1: 63.499999999999766
    agent-2: 63.499999999999766
    agent-3: 63.499999999999766
    agent-4: 63.499999999999766
    agent-5: 63.499999999999766
  sampler_perf:
    mean_env_wait_ms: 31.003324089239516
    mean_inference_ms: 14.596964645062313
    mean_processing_ms: 65.7957656153857
  time_since_restore: 43760.115099191666
  time_this_iter_s: 157.6574215888977
  time_total_s: 56310.93197059631
  timestamp: 1637078994
  timesteps_since_restore: 25728000
  timesteps_this_iter: 96000
  timesteps_total: 33408000
  training_iteration: 348
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    348 |          56310.9 | 33408000 |    995.9 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 1.67
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 29.32
    apples_agent-1_min: 0
    apples_agent-2_max: 252
    apples_agent-2_mean: 7.66
    apples_agent-2_min: 0
    apples_agent-3_max: 296
    apples_agent-3_mean: 140.35
    apples_agent-3_min: 45
    apples_agent-4_max: 60
    apples_agent-4_mean: 1.88
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 89.69
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 547
    cleaning_beam_agent-0_mean: 431.56
    cleaning_beam_agent-0_min: 289
    cleaning_beam_agent-1_max: 404
    cleaning_beam_agent-1_mean: 213.73
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 524
    cleaning_beam_agent-2_mean: 355.91
    cleaning_beam_agent-2_min: 137
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 18.42
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 489
    cleaning_beam_agent-4_mean: 385.36
    cleaning_beam_agent-4_min: 193
    cleaning_beam_agent-5_max: 100
    cleaning_beam_agent-5_mean: 14.35
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-12-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1163.9999999999923
  episode_reward_mean: 1014.18999999999
  episode_reward_min: 329.9999999999997
  episodes_this_iter: 96
  episodes_total: 33504
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12922.63
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.987075924873352
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018239205237478018
        model: {}
        policy_loss: -0.003025570884346962
        total_loss: -0.002484463155269623
        vf_explained_var: -0.008904248476028442
        vf_loss: 22.783620834350586
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1617964506149292
        entropy_coeff: 0.0017600000137463212
        kl: 0.001673782942816615
        model: {}
        policy_loss: -0.004036732483655214
        total_loss: -0.003667703364044428
        vf_explained_var: -0.054626256227493286
        vf_loss: 24.137924194335938
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0830259323120117
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019423647318035364
        model: {}
        policy_loss: -0.00354154035449028
        total_loss: -0.0031186186242848635
        vf_explained_var: -0.012015461921691895
        vf_loss: 23.29045867919922
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4379993975162506
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009856068063527346
        model: {}
        policy_loss: -0.002271766308695078
        total_loss: -0.0009921271121129394
        vf_explained_var: 0.09108397364616394
        vf_loss: 20.505155563354492
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9828732013702393
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018555173883214593
        model: {}
        policy_loss: -0.0040935734286904335
        total_loss: -0.003693685866892338
        vf_explained_var: 0.07332512736320496
        vf_loss: 21.297470092773438
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5382834672927856
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014431570889428258
        model: {}
        policy_loss: -0.0027947649359703064
        total_loss: -0.0017146309837698936
        vf_explained_var: 0.10028514266014099
        vf_loss: 20.275146484375
    load_time_ms: 13897.387
    num_steps_sampled: 33504000
    num_steps_trained: 33504000
    sample_time_ms: 128606.213
    update_time_ms: 67.313
  iterations_since_restore: 269
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.65381165919283
    ram_util_percent: 14.723318385650227
  pid: 14340
  policy_reward_max:
    agent-0: 193.99999999999955
    agent-1: 193.99999999999955
    agent-2: 193.99999999999955
    agent-3: 193.99999999999955
    agent-4: 193.99999999999955
    agent-5: 193.99999999999955
  policy_reward_mean:
    agent-0: 169.03166666666652
    agent-1: 169.03166666666652
    agent-2: 169.03166666666652
    agent-3: 169.03166666666652
    agent-4: 169.03166666666652
    agent-5: 169.03166666666652
  policy_reward_min:
    agent-0: 54.999999999999865
    agent-1: 54.999999999999865
    agent-2: 54.999999999999865
    agent-3: 54.999999999999865
    agent-4: 54.999999999999865
    agent-5: 54.999999999999865
  sampler_perf:
    mean_env_wait_ms: 31.00480699103681
    mean_inference_ms: 14.597042743518397
    mean_processing_ms: 65.79582448177862
  time_since_restore: 43915.95728850365
  time_this_iter_s: 155.8421893119812
  time_total_s: 56466.774159908295
  timestamp: 1637079151
  timesteps_since_restore: 25824000
  timesteps_this_iter: 96000
  timesteps_total: 33504000
  training_iteration: 349
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    349 |          56466.8 | 33504000 |  1014.19 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 320
    apples_agent-0_mean: 6.2
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 26.86
    apples_agent-1_min: 0
    apples_agent-2_max: 158
    apples_agent-2_mean: 4.69
    apples_agent-2_min: 0
    apples_agent-3_max: 182
    apples_agent-3_mean: 124.89
    apples_agent-3_min: 22
    apples_agent-4_max: 85
    apples_agent-4_mean: 5.96
    apples_agent-4_min: 0
    apples_agent-5_max: 191
    apples_agent-5_mean: 88.74
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 593
    cleaning_beam_agent-0_mean: 430.24
    cleaning_beam_agent-0_min: 208
    cleaning_beam_agent-1_max: 424
    cleaning_beam_agent-1_mean: 218.32
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 680
    cleaning_beam_agent-2_mean: 370.91
    cleaning_beam_agent-2_min: 83
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 21.97
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 491
    cleaning_beam_agent-4_mean: 384.67
    cleaning_beam_agent-4_min: 213
    cleaning_beam_agent-5_max: 115
    cleaning_beam_agent-5_mean: 15.11
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-15-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1206.0000000000125
  episode_reward_mean: 965.3599999999923
  episode_reward_min: 286.99999999999847
  episodes_this_iter: 96
  episodes_total: 33600
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12986.952
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.990209698677063
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010659907711669803
        model: {}
        policy_loss: -0.0026142988353967667
        total_loss: -0.0017664460465312004
        vf_explained_var: 0.048317521810531616
        vf_loss: 25.906230926513672
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1529403924942017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024028043262660503
        model: {}
        policy_loss: -0.003898943541571498
        total_loss: -0.003108154982328415
        vf_explained_var: -0.03543287515640259
        vf_loss: 28.199649810791016
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0739221572875977
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010619227541610599
        model: {}
        policy_loss: -0.003148188814520836
        total_loss: -0.0024170256219804287
        vf_explained_var: 0.03706997632980347
        vf_loss: 26.212661743164062
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4688940644264221
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013275148812681437
        model: {}
        policy_loss: -0.002792749088257551
        total_loss: -0.0014310660772025585
        vf_explained_var: 0.19636669754981995
        vf_loss: 21.869335174560547
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9853134155273438
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015430750790983438
        model: {}
        policy_loss: -0.0037991823628544807
        total_loss: -0.0030491501092910767
        vf_explained_var: 0.08850342035293579
        vf_loss: 24.841842651367188
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5724697113037109
        entropy_coeff: 0.0017600000137463212
        kl: 0.001611865358427167
        model: {}
        policy_loss: -0.002702119294553995
        total_loss: -0.001474906224757433
        vf_explained_var: 0.18079043924808502
        vf_loss: 22.347599029541016
    load_time_ms: 14034.285
    num_steps_sampled: 33600000
    num_steps_trained: 33600000
    sample_time_ms: 128768.71
    update_time_ms: 39.5
  iterations_since_restore: 270
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.45580357142857
    ram_util_percent: 14.711160714285715
  pid: 14340
  policy_reward_max:
    agent-0: 200.99999999999963
    agent-1: 200.99999999999963
    agent-2: 200.99999999999963
    agent-3: 200.99999999999963
    agent-4: 200.99999999999963
    agent-5: 200.99999999999963
  policy_reward_mean:
    agent-0: 160.89333333333326
    agent-1: 160.89333333333326
    agent-2: 160.89333333333326
    agent-3: 160.89333333333326
    agent-4: 160.89333333333326
    agent-5: 160.89333333333326
  policy_reward_min:
    agent-0: 47.83333333333326
    agent-1: 47.83333333333326
    agent-2: 47.83333333333326
    agent-3: 47.83333333333326
    agent-4: 47.83333333333326
    agent-5: 47.83333333333326
  sampler_perf:
    mean_env_wait_ms: 31.006357787596244
    mean_inference_ms: 14.596882570876842
    mean_processing_ms: 65.7960650992305
  time_since_restore: 44073.476026535034
  time_this_iter_s: 157.51873803138733
  time_total_s: 56624.29289793968
  timestamp: 1637079308
  timesteps_since_restore: 25920000
  timesteps_this_iter: 96000
  timesteps_total: 33600000
  training_iteration: 350
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    350 |          56624.3 | 33600000 |   965.36 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 176
    apples_agent-0_mean: 5.39
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 29.71
    apples_agent-1_min: 0
    apples_agent-2_max: 129
    apples_agent-2_mean: 6.37
    apples_agent-2_min: 0
    apples_agent-3_max: 272
    apples_agent-3_mean: 132.85
    apples_agent-3_min: 53
    apples_agent-4_max: 53
    apples_agent-4_mean: 1.72
    apples_agent-4_min: 0
    apples_agent-5_max: 129
    apples_agent-5_mean: 85.78
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 584
    cleaning_beam_agent-0_mean: 419.3
    cleaning_beam_agent-0_min: 230
    cleaning_beam_agent-1_max: 386
    cleaning_beam_agent-1_mean: 220.69
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 547
    cleaning_beam_agent-2_mean: 373.96
    cleaning_beam_agent-2_min: 154
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 20.52
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 487
    cleaning_beam_agent-4_mean: 394.01
    cleaning_beam_agent-4_min: 203
    cleaning_beam_agent-5_max: 89
    cleaning_beam_agent-5_mean: 15.71
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-17-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1226.0000000000032
  episode_reward_mean: 995.74999999999
  episode_reward_min: 377.0000000000067
  episodes_this_iter: 96
  episodes_total: 33696
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12968.221
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.989912211894989
        entropy_coeff: 0.0017600000137463212
        kl: 0.002320328261703253
        model: {}
        policy_loss: -0.003288751933723688
        total_loss: -0.0027043474838137627
        vf_explained_var: 0.05331714451313019
        vf_loss: 23.26650047302246
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1612492799758911
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012575590517371893
        model: {}
        policy_loss: -0.0036576930433511734
        total_loss: -0.003246001899242401
        vf_explained_var: 0.00031904876232147217
        vf_loss: 24.554885864257812
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0744562149047852
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017406446859240532
        model: {}
        policy_loss: -0.00346877658739686
        total_loss: -0.002873276360332966
        vf_explained_var: -0.00686250627040863
        vf_loss: 24.865446090698242
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4437362551689148
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010635103099048138
        model: {}
        policy_loss: -0.0025098142214119434
        total_loss: -0.001167819369584322
        vf_explained_var: 0.1337597817182541
        vf_loss: 21.229707717895508
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9807657599449158
        entropy_coeff: 0.0017600000137463212
        kl: 0.001947353477589786
        model: {}
        policy_loss: -0.003866333281621337
        total_loss: -0.00336556532420218
        vf_explained_var: 0.09426803886890411
        vf_loss: 22.269184112548828
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5520210266113281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010929587297141552
        model: {}
        policy_loss: -0.0025702621787786484
        total_loss: -0.0014310735277831554
        vf_explained_var: 0.1370253711938858
        vf_loss: 21.10746192932129
    load_time_ms: 14130.039
    num_steps_sampled: 33696000
    num_steps_trained: 33696000
    sample_time_ms: 128746.643
    update_time_ms: 40.518
  iterations_since_restore: 271
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.599999999999998
    ram_util_percent: 14.75156950672646
  pid: 14340
  policy_reward_max:
    agent-0: 204.33333333333314
    agent-1: 204.33333333333314
    agent-2: 204.33333333333314
    agent-3: 204.33333333333314
    agent-4: 204.33333333333314
    agent-5: 204.33333333333314
  policy_reward_mean:
    agent-0: 165.95833333333314
    agent-1: 165.95833333333314
    agent-2: 165.95833333333314
    agent-3: 165.95833333333314
    agent-4: 165.95833333333314
    agent-5: 165.95833333333314
  policy_reward_min:
    agent-0: 62.83333333333305
    agent-1: 62.83333333333305
    agent-2: 62.83333333333305
    agent-3: 62.83333333333305
    agent-4: 62.83333333333305
    agent-5: 62.83333333333305
  sampler_perf:
    mean_env_wait_ms: 31.007820990659255
    mean_inference_ms: 14.596594877447092
    mean_processing_ms: 65.7966691680739
  time_since_restore: 44229.83753991127
  time_this_iter_s: 156.36151337623596
  time_total_s: 56780.65441131592
  timestamp: 1637079465
  timesteps_since_restore: 26016000
  timesteps_this_iter: 96000
  timesteps_total: 33696000
  training_iteration: 351
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    351 |          56780.7 | 33696000 |   995.75 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 2.31
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 29.63
    apples_agent-1_min: 0
    apples_agent-2_max: 191
    apples_agent-2_mean: 6.45
    apples_agent-2_min: 0
    apples_agent-3_max: 190
    apples_agent-3_mean: 130.24
    apples_agent-3_min: 61
    apples_agent-4_max: 91
    apples_agent-4_mean: 2.62
    apples_agent-4_min: 0
    apples_agent-5_max: 225
    apples_agent-5_mean: 89.01
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 545
    cleaning_beam_agent-0_mean: 420.69
    cleaning_beam_agent-0_min: 290
    cleaning_beam_agent-1_max: 462
    cleaning_beam_agent-1_mean: 218.15
    cleaning_beam_agent-1_min: 69
    cleaning_beam_agent-2_max: 575
    cleaning_beam_agent-2_mean: 378.44
    cleaning_beam_agent-2_min: 154
    cleaning_beam_agent-3_max: 93
    cleaning_beam_agent-3_mean: 19.02
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 467
    cleaning_beam_agent-4_mean: 395.46
    cleaning_beam_agent-4_min: 283
    cleaning_beam_agent-5_max: 369
    cleaning_beam_agent-5_mean: 16.74
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-20-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1178.9999999999995
  episode_reward_mean: 1003.0999999999912
  episode_reward_min: 528.000000000013
  episodes_this_iter: 96
  episodes_total: 33792
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12984.307
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9982016086578369
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010353295365348458
        model: {}
        policy_loss: -0.003097174223512411
        total_loss: -0.0025013862177729607
        vf_explained_var: 0.01914672553539276
        vf_loss: 23.526220321655273
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1566084623336792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013109310530126095
        model: {}
        policy_loss: -0.003490016795694828
        total_loss: -0.003084100317209959
        vf_explained_var: -0.014046803116798401
        vf_loss: 24.415470123291016
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.069190502166748
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022649457678198814
        model: {}
        policy_loss: -0.0034714764915406704
        total_loss: -0.002966436557471752
        vf_explained_var: 0.016127467155456543
        vf_loss: 23.868148803710938
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44545310735702515
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014001592062413692
        model: {}
        policy_loss: -0.0026953183114528656
        total_loss: -0.0013240324333310127
        vf_explained_var: 0.10466448962688446
        vf_loss: 21.552852630615234
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9753451943397522
        entropy_coeff: 0.0017600000137463212
        kl: 0.002218045759946108
        model: {}
        policy_loss: -0.0042085167951881886
        total_loss: -0.0036265328526496887
        vf_explained_var: 0.04736645519733429
        vf_loss: 22.985919952392578
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5169911980628967
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007648515747860074
        model: {}
        policy_loss: -0.002573818201199174
        total_loss: -0.0013372255489230156
        vf_explained_var: 0.1058850884437561
        vf_loss: 21.46500015258789
    load_time_ms: 15992.893
    num_steps_sampled: 33792000
    num_steps_trained: 33792000
    sample_time_ms: 128708.906
    update_time_ms: 40.199
  iterations_since_restore: 272
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.3104
    ram_util_percent: 15.148800000000003
  pid: 14340
  policy_reward_max:
    agent-0: 196.49999999999997
    agent-1: 196.49999999999997
    agent-2: 196.49999999999997
    agent-3: 196.49999999999997
    agent-4: 196.49999999999997
    agent-5: 196.49999999999997
  policy_reward_mean:
    agent-0: 167.18333333333322
    agent-1: 167.18333333333322
    agent-2: 167.18333333333322
    agent-3: 167.18333333333322
    agent-4: 167.18333333333322
    agent-5: 167.18333333333322
  policy_reward_min:
    agent-0: 88.00000000000003
    agent-1: 88.00000000000003
    agent-2: 88.00000000000003
    agent-3: 88.00000000000003
    agent-4: 88.00000000000003
    agent-5: 88.00000000000003
  sampler_perf:
    mean_env_wait_ms: 31.008963245262926
    mean_inference_ms: 14.59618192812152
    mean_processing_ms: 65.79697417326597
  time_since_restore: 44405.01046872139
  time_this_iter_s: 175.17292881011963
  time_total_s: 56955.82734012604
  timestamp: 1637079641
  timesteps_since_restore: 26112000
  timesteps_this_iter: 96000
  timesteps_total: 33792000
  training_iteration: 352
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    352 |          56955.8 | 33792000 |   1003.1 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 65
    apples_agent-0_mean: 2.69
    apples_agent-0_min: 0
    apples_agent-1_max: 128
    apples_agent-1_mean: 29.85
    apples_agent-1_min: 0
    apples_agent-2_max: 189
    apples_agent-2_mean: 8.64
    apples_agent-2_min: 0
    apples_agent-3_max: 209
    apples_agent-3_mean: 128.15
    apples_agent-3_min: 69
    apples_agent-4_max: 76
    apples_agent-4_mean: 1.75
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 86.42
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 566
    cleaning_beam_agent-0_mean: 430.4
    cleaning_beam_agent-0_min: 254
    cleaning_beam_agent-1_max: 459
    cleaning_beam_agent-1_mean: 221.12
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 545
    cleaning_beam_agent-2_mean: 382.85
    cleaning_beam_agent-2_min: 162
    cleaning_beam_agent-3_max: 79
    cleaning_beam_agent-3_mean: 19.64
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 503
    cleaning_beam_agent-4_mean: 394.67
    cleaning_beam_agent-4_min: 260
    cleaning_beam_agent-5_max: 219
    cleaning_beam_agent-5_mean: 16.53
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-23-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1207.00000000001
  episode_reward_mean: 1010.6099999999913
  episode_reward_min: 448.00000000001046
  episodes_this_iter: 96
  episodes_total: 33888
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12989.04
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9868713617324829
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011903916019946337
        model: {}
        policy_loss: -0.0026386920362710953
        total_loss: -0.0020493604242801666
        vf_explained_var: 0.04737366735935211
        vf_loss: 23.26224708557129
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1627793312072754
        entropy_coeff: 0.0017600000137463212
        kl: 0.001650021644309163
        model: {}
        policy_loss: -0.0036950907669961452
        total_loss: -0.0031969244591891766
        vf_explained_var: -0.0367298424243927
        vf_loss: 25.44659996032715
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0665843486785889
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016068653203547
        model: {}
        policy_loss: -0.003565582912415266
        total_loss: -0.0030128033831715584
        vf_explained_var: 0.01185213029384613
        vf_loss: 24.299713134765625
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4379892945289612
        entropy_coeff: 0.0017600000137463212
        kl: 0.001104134600609541
        model: {}
        policy_loss: -0.0024067233316600323
        total_loss: -0.0010477453470230103
        vf_explained_var: 0.1259220391511917
        vf_loss: 21.298398971557617
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9754776954650879
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023938454687595367
        model: {}
        policy_loss: -0.00380226643756032
        total_loss: -0.0031794942915439606
        vf_explained_var: 0.04106120765209198
        vf_loss: 23.3961181640625
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5187458992004395
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013560709776356816
        model: {}
        policy_loss: -0.0023078029043972492
        total_loss: -0.0010403968626633286
        vf_explained_var: 0.10343211889266968
        vf_loss: 21.803993225097656
    load_time_ms: 16102.597
    num_steps_sampled: 33888000
    num_steps_trained: 33888000
    sample_time_ms: 128879.081
    update_time_ms: 30.57
  iterations_since_restore: 273
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.786666666666665
    ram_util_percent: 15.718666666666667
  pid: 14340
  policy_reward_max:
    agent-0: 201.16666666666626
    agent-1: 201.16666666666626
    agent-2: 201.16666666666626
    agent-3: 201.16666666666626
    agent-4: 201.16666666666626
    agent-5: 201.16666666666626
  policy_reward_mean:
    agent-0: 168.4349999999999
    agent-1: 168.4349999999999
    agent-2: 168.4349999999999
    agent-3: 168.4349999999999
    agent-4: 168.4349999999999
    agent-5: 168.4349999999999
  policy_reward_min:
    agent-0: 74.66666666666667
    agent-1: 74.66666666666667
    agent-2: 74.66666666666667
    agent-3: 74.66666666666667
    agent-4: 74.66666666666667
    agent-5: 74.66666666666667
  sampler_perf:
    mean_env_wait_ms: 31.010881724272853
    mean_inference_ms: 14.59614018801798
    mean_processing_ms: 65.79771172547342
  time_since_restore: 44562.50510263443
  time_this_iter_s: 157.49463391304016
  time_total_s: 57113.32197403908
  timestamp: 1637079798
  timesteps_since_restore: 26208000
  timesteps_this_iter: 96000
  timesteps_total: 33888000
  training_iteration: 353
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    353 |          57113.3 | 33888000 |  1010.61 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 1.32
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 30.63
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 8.26
    apples_agent-2_min: 0
    apples_agent-3_max: 187
    apples_agent-3_mean: 125.16
    apples_agent-3_min: 28
    apples_agent-4_max: 65
    apples_agent-4_mean: 1.68
    apples_agent-4_min: 0
    apples_agent-5_max: 144
    apples_agent-5_mean: 85.75
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 627
    cleaning_beam_agent-0_mean: 451.71
    cleaning_beam_agent-0_min: 312
    cleaning_beam_agent-1_max: 494
    cleaning_beam_agent-1_mean: 227.18
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 516
    cleaning_beam_agent-2_mean: 371.55
    cleaning_beam_agent-2_min: 120
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 18.05
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 485
    cleaning_beam_agent-4_mean: 389.53
    cleaning_beam_agent-4_min: 273
    cleaning_beam_agent-5_max: 195
    cleaning_beam_agent-5_mean: 17.07
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-25-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1191.00000000002
  episode_reward_mean: 1011.539999999991
  episode_reward_min: 173.99999999999875
  episodes_this_iter: 96
  episodes_total: 33984
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12986.574
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9883825778961182
        entropy_coeff: 0.0017600000137463212
        kl: 0.00150575814768672
        model: {}
        policy_loss: -0.00275634927675128
        total_loss: -0.002337892074137926
        vf_explained_var: 0.05169796943664551
        vf_loss: 21.580097198486328
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1509300470352173
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018680425127968192
        model: {}
        policy_loss: -0.0038152604829519987
        total_loss: -0.0034839531872421503
        vf_explained_var: -0.026755422353744507
        vf_loss: 23.569398880004883
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0683661699295044
        entropy_coeff: 0.0017600000137463212
        kl: 0.002098201308399439
        model: {}
        policy_loss: -0.0038437554612755775
        total_loss: -0.0034167240373790264
        vf_explained_var: -0.0028040260076522827
        vf_loss: 23.073566436767578
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43122291564941406
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010647280141711235
        model: {}
        policy_loss: -0.0023102029226720333
        total_loss: -0.0010434011928737164
        vf_explained_var: 0.11130039393901825
        vf_loss: 20.257566452026367
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.975777268409729
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015611418057233095
        model: {}
        policy_loss: -0.003722995985299349
        total_loss: -0.003287848085165024
        vf_explained_var: 0.061931312084198
        vf_loss: 21.525165557861328
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5245622992515564
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010892172576859593
        model: {}
        policy_loss: -0.0025367222260683775
        total_loss: -0.0014151963405311108
        vf_explained_var: 0.09731554985046387
        vf_loss: 20.447559356689453
    load_time_ms: 16228.92
    num_steps_sampled: 33984000
    num_steps_trained: 33984000
    sample_time_ms: 128630.081
    update_time_ms: 35.858
  iterations_since_restore: 274
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.655405405405407
    ram_util_percent: 15.673423423423424
  pid: 14340
  policy_reward_max:
    agent-0: 198.49999999999991
    agent-1: 198.49999999999991
    agent-2: 198.49999999999991
    agent-3: 198.49999999999991
    agent-4: 198.49999999999991
    agent-5: 198.49999999999991
  policy_reward_mean:
    agent-0: 168.5899999999999
    agent-1: 168.5899999999999
    agent-2: 168.5899999999999
    agent-3: 168.5899999999999
    agent-4: 168.5899999999999
    agent-5: 168.5899999999999
  policy_reward_min:
    agent-0: 29.000000000000053
    agent-1: 29.000000000000053
    agent-2: 29.000000000000053
    agent-3: 29.000000000000053
    agent-4: 29.000000000000053
    agent-5: 29.000000000000053
  sampler_perf:
    mean_env_wait_ms: 31.012779737613936
    mean_inference_ms: 14.596261289702234
    mean_processing_ms: 65.79668460502509
  time_since_restore: 44718.184386730194
  time_this_iter_s: 155.67928409576416
  time_total_s: 57269.00125813484
  timestamp: 1637079954
  timesteps_since_restore: 26304000
  timesteps_this_iter: 96000
  timesteps_total: 33984000
  training_iteration: 354
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    354 |            57269 | 33984000 |  1011.54 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 103
    apples_agent-0_mean: 3.36
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 26.64
    apples_agent-1_min: 0
    apples_agent-2_max: 218
    apples_agent-2_mean: 9.12
    apples_agent-2_min: 0
    apples_agent-3_max: 196
    apples_agent-3_mean: 125.79
    apples_agent-3_min: 52
    apples_agent-4_max: 36
    apples_agent-4_mean: 1.1
    apples_agent-4_min: 0
    apples_agent-5_max: 161
    apples_agent-5_mean: 87.57
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 565
    cleaning_beam_agent-0_mean: 454.18
    cleaning_beam_agent-0_min: 293
    cleaning_beam_agent-1_max: 371
    cleaning_beam_agent-1_mean: 210.44
    cleaning_beam_agent-1_min: 87
    cleaning_beam_agent-2_max: 619
    cleaning_beam_agent-2_mean: 370.31
    cleaning_beam_agent-2_min: 138
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 18.46
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 507
    cleaning_beam_agent-4_mean: 397.9
    cleaning_beam_agent-4_min: 273
    cleaning_beam_agent-5_max: 59
    cleaning_beam_agent-5_mean: 10.8
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-28-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1207.999999999996
  episode_reward_mean: 1019.8399999999904
  episode_reward_min: 475.0000000000072
  episodes_this_iter: 96
  episodes_total: 34080
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12997.611
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9706138968467712
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011052396148443222
        model: {}
        policy_loss: -0.0028604967519640923
        total_loss: -0.0023093644995242357
        vf_explained_var: 0.021030738949775696
        vf_loss: 22.594158172607422
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1652426719665527
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022018798626959324
        model: {}
        policy_loss: -0.00353118684142828
        total_loss: -0.0031683449633419514
        vf_explained_var: -0.03949016332626343
        vf_loss: 24.13669204711914
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.06862211227417
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012951516546308994
        model: {}
        policy_loss: -0.0030671777203679085
        total_loss: -0.0024996220599859953
        vf_explained_var: -0.047187745571136475
        vf_loss: 24.483291625976562
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42755556106567383
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006963174673728645
        model: {}
        policy_loss: -0.002084248699247837
        total_loss: -0.0007910968270152807
        vf_explained_var: 0.11320152878761292
        vf_loss: 20.45648956298828
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9677245616912842
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025510427076369524
        model: {}
        policy_loss: -0.004117516800761223
        total_loss: -0.0035969442687928677
        vf_explained_var: 0.036786794662475586
        vf_loss: 22.237682342529297
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5026431083679199
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005901838303543627
        model: {}
        policy_loss: -0.002282636472955346
        total_loss: -0.001131359371356666
        vf_explained_var: 0.11549964547157288
        vf_loss: 20.359277725219727
    load_time_ms: 16324.597
    num_steps_sampled: 34080000
    num_steps_trained: 34080000
    sample_time_ms: 128638.555
    update_time_ms: 35.778
  iterations_since_restore: 275
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.799999999999997
    ram_util_percent: 15.660089686098658
  pid: 14340
  policy_reward_max:
    agent-0: 201.33333333333346
    agent-1: 201.33333333333346
    agent-2: 201.33333333333346
    agent-3: 201.33333333333346
    agent-4: 201.33333333333346
    agent-5: 201.33333333333346
  policy_reward_mean:
    agent-0: 169.9733333333332
    agent-1: 169.9733333333332
    agent-2: 169.9733333333332
    agent-3: 169.9733333333332
    agent-4: 169.9733333333332
    agent-5: 169.9733333333332
  policy_reward_min:
    agent-0: 79.16666666666653
    agent-1: 79.16666666666653
    agent-2: 79.16666666666653
    agent-3: 79.16666666666653
    agent-4: 79.16666666666653
    agent-5: 79.16666666666653
  sampler_perf:
    mean_env_wait_ms: 31.014467950207045
    mean_inference_ms: 14.596270032922183
    mean_processing_ms: 65.79577333104224
  time_since_restore: 44874.39832687378
  time_this_iter_s: 156.2139401435852
  time_total_s: 57425.21519827843
  timestamp: 1637080111
  timesteps_since_restore: 26400000
  timesteps_this_iter: 96000
  timesteps_total: 34080000
  training_iteration: 355
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    355 |          57425.2 | 34080000 |  1019.84 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 3.01
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 22.03
    apples_agent-1_min: 0
    apples_agent-2_max: 165
    apples_agent-2_mean: 7.55
    apples_agent-2_min: 0
    apples_agent-3_max: 232
    apples_agent-3_mean: 126.11
    apples_agent-3_min: 26
    apples_agent-4_max: 48
    apples_agent-4_mean: 2.18
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 84.64
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 552
    cleaning_beam_agent-0_mean: 447.3
    cleaning_beam_agent-0_min: 314
    cleaning_beam_agent-1_max: 540
    cleaning_beam_agent-1_mean: 207.92
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 545
    cleaning_beam_agent-2_mean: 374.17
    cleaning_beam_agent-2_min: 138
    cleaning_beam_agent-3_max: 137
    cleaning_beam_agent-3_mean: 18.6
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 486
    cleaning_beam_agent-4_mean: 399.42
    cleaning_beam_agent-4_min: 269
    cleaning_beam_agent-5_max: 81
    cleaning_beam_agent-5_mean: 11.07
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-31-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1184.000000000003
  episode_reward_mean: 1026.019999999992
  episode_reward_min: 535.0000000000099
  episodes_this_iter: 96
  episodes_total: 34176
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13041.438
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9757925868034363
        entropy_coeff: 0.0017600000137463212
        kl: 0.001840492244809866
        model: {}
        policy_loss: -0.003059146925806999
        total_loss: -0.0023439640644937754
        vf_explained_var: -0.0006921589374542236
        vf_loss: 24.32577133178711
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1556416749954224
        entropy_coeff: 0.0017600000137463212
        kl: 0.002117434050887823
        model: {}
        policy_loss: -0.0033832918852567673
        total_loss: -0.002870364813134074
        vf_explained_var: -0.04258596897125244
        vf_loss: 25.46857452392578
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0672624111175537
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019545804243534803
        model: {}
        policy_loss: -0.003477234859019518
        total_loss: -0.0029402822256088257
        vf_explained_var: 0.01701018214225769
        vf_loss: 24.153364181518555
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41903722286224365
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008136478718370199
        model: {}
        policy_loss: -0.00229641143232584
        total_loss: -0.0009125289507210255
        vf_explained_var: 0.12443645298480988
        vf_loss: 21.2138729095459
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9749752283096313
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018559546442702413
        model: {}
        policy_loss: -0.0035904136020690203
        total_loss: -0.0030864723958075047
        vf_explained_var: 0.09548182785511017
        vf_loss: 22.198986053466797
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49658894538879395
        entropy_coeff: 0.0017600000137463212
        kl: 0.001383973634801805
        model: {}
        policy_loss: -0.0023476218339055777
        total_loss: -0.0010518280323594809
        vf_explained_var: 0.10152041912078857
        vf_loss: 21.697879791259766
    load_time_ms: 16438.886
    num_steps_sampled: 34176000
    num_steps_trained: 34176000
    sample_time_ms: 128790.778
    update_time_ms: 36.029
  iterations_since_restore: 276
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.886995515695062
    ram_util_percent: 15.664125560538118
  pid: 14340
  policy_reward_max:
    agent-0: 197.33333333333306
    agent-1: 197.33333333333306
    agent-2: 197.33333333333306
    agent-3: 197.33333333333306
    agent-4: 197.33333333333306
    agent-5: 197.33333333333306
  policy_reward_mean:
    agent-0: 171.0033333333332
    agent-1: 171.0033333333332
    agent-2: 171.0033333333332
    agent-3: 171.0033333333332
    agent-4: 171.0033333333332
    agent-5: 171.0033333333332
  policy_reward_min:
    agent-0: 89.16666666666706
    agent-1: 89.16666666666706
    agent-2: 89.16666666666706
    agent-3: 89.16666666666706
    agent-4: 89.16666666666706
    agent-5: 89.16666666666706
  sampler_perf:
    mean_env_wait_ms: 31.016858051078366
    mean_inference_ms: 14.59618032442049
    mean_processing_ms: 65.79682521728775
  time_since_restore: 45031.01662540436
  time_this_iter_s: 156.6182985305786
  time_total_s: 57581.833496809006
  timestamp: 1637080268
  timesteps_since_restore: 26496000
  timesteps_this_iter: 96000
  timesteps_total: 34176000
  training_iteration: 356
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    356 |          57581.8 | 34176000 |  1026.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 3.97
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 29.34
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 6.05
    apples_agent-2_min: 0
    apples_agent-3_max: 228
    apples_agent-3_mean: 120.91
    apples_agent-3_min: 46
    apples_agent-4_max: 44
    apples_agent-4_mean: 1.75
    apples_agent-4_min: 0
    apples_agent-5_max: 128
    apples_agent-5_mean: 82.27
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 588
    cleaning_beam_agent-0_mean: 457.0
    cleaning_beam_agent-0_min: 231
    cleaning_beam_agent-1_max: 339
    cleaning_beam_agent-1_mean: 201.79
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 543
    cleaning_beam_agent-2_mean: 373.19
    cleaning_beam_agent-2_min: 221
    cleaning_beam_agent-3_max: 64
    cleaning_beam_agent-3_mean: 20.2
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 464
    cleaning_beam_agent-4_mean: 397.78
    cleaning_beam_agent-4_min: 269
    cleaning_beam_agent-5_max: 62
    cleaning_beam_agent-5_mean: 14.06
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-33-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1189.0000000000084
  episode_reward_mean: 1008.8899999999896
  episode_reward_min: 465.00000000000614
  episodes_this_iter: 96
  episodes_total: 34272
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13026.977
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9924461841583252
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023779000621289015
        model: {}
        policy_loss: -0.003289589425548911
        total_loss: -0.0026954535860568285
        vf_explained_var: 0.03833530843257904
        vf_loss: 23.40847396850586
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1662030220031738
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018325162818655372
        model: {}
        policy_loss: -0.003830377012491226
        total_loss: -0.0033670663833618164
        vf_explained_var: -0.03188431262969971
        vf_loss: 25.158283233642578
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0823432207107544
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026417935732752085
        model: {}
        policy_loss: -0.0037127858959138393
        total_loss: -0.0031385421752929688
        vf_explained_var: -0.018251121044158936
        vf_loss: 24.791650772094727
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4320870339870453
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009202887304127216
        model: {}
        policy_loss: -0.002226817887276411
        total_loss: -0.0008937865495681763
        vf_explained_var: 0.13826897740364075
        vf_loss: 20.93500518798828
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9754946827888489
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025814841501414776
        model: {}
        policy_loss: -0.004019088111817837
        total_loss: -0.00347524369135499
        vf_explained_var: 0.06970307230949402
        vf_loss: 22.607160568237305
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49531328678131104
        entropy_coeff: 0.0017600000137463212
        kl: 0.0004329554212745279
        model: {}
        policy_loss: -0.002116510644555092
        total_loss: -0.0008672615513205528
        vf_explained_var: 0.12404698133468628
        vf_loss: 21.210046768188477
    load_time_ms: 16412.022
    num_steps_sampled: 34272000
    num_steps_trained: 34272000
    sample_time_ms: 128723.266
    update_time_ms: 35.46
  iterations_since_restore: 277
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.938461538461542
    ram_util_percent: 15.657466063348418
  pid: 14340
  policy_reward_max:
    agent-0: 198.16666666666657
    agent-1: 198.16666666666657
    agent-2: 198.16666666666657
    agent-3: 198.16666666666657
    agent-4: 198.16666666666657
    agent-5: 198.16666666666657
  policy_reward_mean:
    agent-0: 168.14833333333317
    agent-1: 168.14833333333317
    agent-2: 168.14833333333317
    agent-3: 168.14833333333317
    agent-4: 168.14833333333317
    agent-5: 168.14833333333317
  policy_reward_min:
    agent-0: 77.50000000000003
    agent-1: 77.50000000000003
    agent-2: 77.50000000000003
    agent-3: 77.50000000000003
    agent-4: 77.50000000000003
    agent-5: 77.50000000000003
  sampler_perf:
    mean_env_wait_ms: 31.018061257105042
    mean_inference_ms: 14.596274110516081
    mean_processing_ms: 65.79671926115233
  time_since_restore: 45185.36447811127
  time_this_iter_s: 154.34785270690918
  time_total_s: 57736.181349515915
  timestamp: 1637080423
  timesteps_since_restore: 26592000
  timesteps_this_iter: 96000
  timesteps_total: 34272000
  training_iteration: 357
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    357 |          57736.2 | 34272000 |  1008.89 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 496
    apples_agent-0_mean: 8.4
    apples_agent-0_min: 0
    apples_agent-1_max: 151
    apples_agent-1_mean: 30.61
    apples_agent-1_min: 0
    apples_agent-2_max: 250
    apples_agent-2_mean: 8.29
    apples_agent-2_min: 0
    apples_agent-3_max: 325
    apples_agent-3_mean: 121.58
    apples_agent-3_min: 58
    apples_agent-4_max: 61
    apples_agent-4_mean: 3.08
    apples_agent-4_min: 0
    apples_agent-5_max: 175
    apples_agent-5_mean: 82.46
    apples_agent-5_min: 54
    cleaning_beam_agent-0_max: 577
    cleaning_beam_agent-0_mean: 441.19
    cleaning_beam_agent-0_min: 141
    cleaning_beam_agent-1_max: 438
    cleaning_beam_agent-1_mean: 210.14
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 570
    cleaning_beam_agent-2_mean: 383.69
    cleaning_beam_agent-2_min: 146
    cleaning_beam_agent-3_max: 113
    cleaning_beam_agent-3_mean: 22.7
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 495
    cleaning_beam_agent-4_mean: 401.09
    cleaning_beam_agent-4_min: 294
    cleaning_beam_agent-5_max: 81
    cleaning_beam_agent-5_mean: 12.03
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-36-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1225.9999999999973
  episode_reward_mean: 1000.9599999999901
  episode_reward_min: 468.9999999999991
  episodes_this_iter: 96
  episodes_total: 34368
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13027.628
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9863272905349731
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024285377003252506
        model: {}
        policy_loss: -0.0032152608036994934
        total_loss: -0.0027285474352538586
        vf_explained_var: 0.07819944620132446
        vf_loss: 22.22649383544922
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1730667352676392
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017078804085031152
        model: {}
        policy_loss: -0.003779139369726181
        total_loss: -0.003292584791779518
        vf_explained_var: -0.05955857038497925
        vf_loss: 25.51154327392578
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.057348370552063
        entropy_coeff: 0.0017600000137463212
        kl: 0.002061136532574892
        model: {}
        policy_loss: -0.0034193070605397224
        total_loss: -0.002807033248245716
        vf_explained_var: -0.027893424034118652
        vf_loss: 24.732091903686523
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43616023659706116
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012836514506489038
        model: {}
        policy_loss: -0.0024404923897236586
        total_loss: -0.0010385727509856224
        vf_explained_var: 0.09666857123374939
        vf_loss: 21.69563865661621
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9787732362747192
        entropy_coeff: 0.0017600000137463212
        kl: 0.002076981822028756
        model: {}
        policy_loss: -0.0043008774518966675
        total_loss: -0.0038028527051210403
        vf_explained_var: 0.07664582133293152
        vf_loss: 22.206647872924805
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.497165322303772
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007337976130656898
        model: {}
        policy_loss: -0.002217511646449566
        total_loss: -0.0009038522839546204
        vf_explained_var: 0.08666551113128662
        vf_loss: 21.886707305908203
    load_time_ms: 16384.729
    num_steps_sampled: 34368000
    num_steps_trained: 34368000
    sample_time_ms: 128753.357
    update_time_ms: 30.513
  iterations_since_restore: 278
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.866071428571427
    ram_util_percent: 15.663839285714287
  pid: 14340
  policy_reward_max:
    agent-0: 204.33333333333348
    agent-1: 204.33333333333348
    agent-2: 204.33333333333348
    agent-3: 204.33333333333348
    agent-4: 204.33333333333348
    agent-5: 204.33333333333348
  policy_reward_mean:
    agent-0: 166.82666666666657
    agent-1: 166.82666666666657
    agent-2: 166.82666666666657
    agent-3: 166.82666666666657
    agent-4: 166.82666666666657
    agent-5: 166.82666666666657
  policy_reward_min:
    agent-0: 78.16666666666673
    agent-1: 78.16666666666673
    agent-2: 78.16666666666673
    agent-3: 78.16666666666673
    agent-4: 78.16666666666673
    agent-5: 78.16666666666673
  sampler_perf:
    mean_env_wait_ms: 31.020203990015847
    mean_inference_ms: 14.599083735096743
    mean_processing_ms: 65.79776743042562
  time_since_restore: 45343.04521679878
  time_this_iter_s: 157.68073868751526
  time_total_s: 57893.86208820343
  timestamp: 1637080581
  timesteps_since_restore: 26688000
  timesteps_this_iter: 96000
  timesteps_total: 34368000
  training_iteration: 358
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    358 |          57893.9 | 34368000 |  1000.96 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 570
    apples_agent-0_mean: 10.51
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 35.2
    apples_agent-1_min: 0
    apples_agent-2_max: 60
    apples_agent-2_mean: 3.22
    apples_agent-2_min: 0
    apples_agent-3_max: 473
    apples_agent-3_mean: 126.66
    apples_agent-3_min: 48
    apples_agent-4_max: 48
    apples_agent-4_mean: 1.32
    apples_agent-4_min: 0
    apples_agent-5_max: 212
    apples_agent-5_mean: 84.84
    apples_agent-5_min: 46
    cleaning_beam_agent-0_max: 582
    cleaning_beam_agent-0_mean: 421.06
    cleaning_beam_agent-0_min: 181
    cleaning_beam_agent-1_max: 392
    cleaning_beam_agent-1_mean: 205.6
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 541
    cleaning_beam_agent-2_mean: 375.66
    cleaning_beam_agent-2_min: 208
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 20.66
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 493
    cleaning_beam_agent-4_mean: 394.33
    cleaning_beam_agent-4_min: 295
    cleaning_beam_agent-5_max: 58
    cleaning_beam_agent-5_mean: 10.32
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-38-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1188.999999999997
  episode_reward_mean: 1027.0399999999902
  episode_reward_min: 193.99999999999835
  episodes_this_iter: 96
  episodes_total: 34464
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 13026.826
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9968825578689575
        entropy_coeff: 0.0017600000137463212
        kl: 0.001689037773758173
        model: {}
        policy_loss: -0.0031603574752807617
        total_loss: -0.002438650932163
        vf_explained_var: 0.05829063057899475
        vf_loss: 24.762210845947266
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1787937879562378
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014771092683076859
        model: {}
        policy_loss: -0.0037481021136045456
        total_loss: -0.0030072429217398167
        vf_explained_var: -0.07327103614807129
        vf_loss: 28.15540885925293
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0767768621444702
        entropy_coeff: 0.0017600000137463212
        kl: 0.002044467255473137
        model: {}
        policy_loss: -0.0032627214677631855
        total_loss: -0.0025510601699352264
        vf_explained_var: -0.0024072229862213135
        vf_loss: 26.067886352539062
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42943674325942993
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008597890846431255
        model: {}
        policy_loss: -0.002182446885854006
        total_loss: -0.0006163336802273989
        vf_explained_var: 0.1091836541891098
        vf_loss: 23.2192440032959
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.973088264465332
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013032841961830854
        model: {}
        policy_loss: -0.003630564548075199
        total_loss: -0.002866944298148155
        vf_explained_var: 0.052310019731521606
        vf_loss: 24.76256561279297
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48314544558525085
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007034192094579339
        model: {}
        policy_loss: -0.0022304682061076164
        total_loss: -0.0008086315356194973
        vf_explained_var: 0.12390002608299255
        vf_loss: 22.721755981445312
    load_time_ms: 16571.191
    num_steps_sampled: 34464000
    num_steps_trained: 34464000
    sample_time_ms: 128782.535
    update_time_ms: 43.896
  iterations_since_restore: 279
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.673333333333332
    ram_util_percent: 15.677333333333335
  pid: 14340
  policy_reward_max:
    agent-0: 198.16666666666637
    agent-1: 198.16666666666637
    agent-2: 198.16666666666637
    agent-3: 198.16666666666637
    agent-4: 198.16666666666637
    agent-5: 198.16666666666637
  policy_reward_mean:
    agent-0: 171.17333333333326
    agent-1: 171.17333333333326
    agent-2: 171.17333333333326
    agent-3: 171.17333333333326
    agent-4: 171.17333333333326
    agent-5: 171.17333333333326
  policy_reward_min:
    agent-0: 32.33333333333339
    agent-1: 32.33333333333339
    agent-2: 32.33333333333339
    agent-3: 32.33333333333339
    agent-4: 32.33333333333339
    agent-5: 32.33333333333339
  sampler_perf:
    mean_env_wait_ms: 31.02140586992349
    mean_inference_ms: 14.600726078685131
    mean_processing_ms: 65.79705136351899
  time_since_restore: 45501.12644815445
  time_this_iter_s: 158.08123135566711
  time_total_s: 58051.9433195591
  timestamp: 1637080739
  timesteps_since_restore: 26784000
  timesteps_this_iter: 96000
  timesteps_total: 34464000
  training_iteration: 359
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    359 |          58051.9 | 34464000 |  1027.04 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 100
    apples_agent-0_mean: 3.34
    apples_agent-0_min: 0
    apples_agent-1_max: 116
    apples_agent-1_mean: 30.13
    apples_agent-1_min: 0
    apples_agent-2_max: 256
    apples_agent-2_mean: 7.07
    apples_agent-2_min: 0
    apples_agent-3_max: 302
    apples_agent-3_mean: 127.19
    apples_agent-3_min: 19
    apples_agent-4_max: 42
    apples_agent-4_mean: 1.19
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 81.71
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 560
    cleaning_beam_agent-0_mean: 430.26
    cleaning_beam_agent-0_min: 213
    cleaning_beam_agent-1_max: 474
    cleaning_beam_agent-1_mean: 213.0
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 596
    cleaning_beam_agent-2_mean: 377.25
    cleaning_beam_agent-2_min: 201
    cleaning_beam_agent-3_max: 80
    cleaning_beam_agent-3_mean: 23.08
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 515
    cleaning_beam_agent-4_mean: 398.48
    cleaning_beam_agent-4_min: 243
    cleaning_beam_agent-5_max: 109
    cleaning_beam_agent-5_mean: 14.94
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-41-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1223.0000000000075
  episode_reward_mean: 1016.6199999999903
  episode_reward_min: 202.9999999999974
  episodes_this_iter: 96
  episodes_total: 34560
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12955.133
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9928480386734009
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016849711537361145
        model: {}
        policy_loss: -0.0031098274048417807
        total_loss: -0.002486740006133914
        vf_explained_var: 0.053349390625953674
        vf_loss: 23.704988479614258
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1571521759033203
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016015885630622506
        model: {}
        policy_loss: -0.0038294345140457153
        total_loss: -0.0032842273358255625
        vf_explained_var: -0.028333187103271484
        vf_loss: 25.817951202392578
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0784424543380737
        entropy_coeff: 0.0017600000137463212
        kl: 0.002105667721480131
        model: {}
        policy_loss: -0.003475372912362218
        total_loss: -0.0028711564373224974
        vf_explained_var: -0.0005718022584915161
        vf_loss: 25.022708892822266
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43103042244911194
        entropy_coeff: 0.0017600000137463212
        kl: 0.000764297554269433
        model: {}
        policy_loss: -0.002398550510406494
        total_loss: -0.001004053745418787
        vf_explained_var: 0.1369713544845581
        vf_loss: 21.53111457824707
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9812586903572083
        entropy_coeff: 0.0017600000137463212
        kl: 0.002167015802115202
        model: {}
        policy_loss: -0.00427800789475441
        total_loss: -0.003645395627245307
        vf_explained_var: 0.05819395184516907
        vf_loss: 23.596290588378906
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48011690378189087
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005985104362480342
        model: {}
        policy_loss: -0.002282308880239725
        total_loss: -0.0009705815464258194
        vf_explained_var: 0.13577860593795776
        vf_loss: 21.56732177734375
    load_time_ms: 16432.914
    num_steps_sampled: 34560000
    num_steps_trained: 34560000
    sample_time_ms: 128675.97
    update_time_ms: 55.853
  iterations_since_restore: 280
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.033031674208143
    ram_util_percent: 15.661085972850682
  pid: 14340
  policy_reward_max:
    agent-0: 203.8333333333326
    agent-1: 203.8333333333326
    agent-2: 203.8333333333326
    agent-3: 203.8333333333326
    agent-4: 203.8333333333326
    agent-5: 203.8333333333326
  policy_reward_mean:
    agent-0: 169.43666666666653
    agent-1: 169.43666666666653
    agent-2: 169.43666666666653
    agent-3: 169.43666666666653
    agent-4: 169.43666666666653
    agent-5: 169.43666666666653
  policy_reward_min:
    agent-0: 33.83333333333335
    agent-1: 33.83333333333335
    agent-2: 33.83333333333335
    agent-3: 33.83333333333335
    agent-4: 33.83333333333335
    agent-5: 33.83333333333335
  sampler_perf:
    mean_env_wait_ms: 31.023147070899928
    mean_inference_ms: 14.601286102630425
    mean_processing_ms: 65.79809799040092
  time_since_restore: 45655.5972571373
  time_this_iter_s: 154.47080898284912
  time_total_s: 58206.41412854195
  timestamp: 1637080895
  timesteps_since_restore: 26880000
  timesteps_this_iter: 96000
  timesteps_total: 34560000
  training_iteration: 360
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    360 |          58206.4 | 34560000 |  1016.62 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 575
    apples_agent-0_mean: 8.51
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 32.59
    apples_agent-1_min: 0
    apples_agent-2_max: 131
    apples_agent-2_mean: 5.86
    apples_agent-2_min: 0
    apples_agent-3_max: 187
    apples_agent-3_mean: 123.39
    apples_agent-3_min: 41
    apples_agent-4_max: 61
    apples_agent-4_mean: 3.18
    apples_agent-4_min: 0
    apples_agent-5_max: 422
    apples_agent-5_mean: 83.66
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 566
    cleaning_beam_agent-0_mean: 432.89
    cleaning_beam_agent-0_min: 83
    cleaning_beam_agent-1_max: 382
    cleaning_beam_agent-1_mean: 203.47
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 510
    cleaning_beam_agent-2_mean: 355.32
    cleaning_beam_agent-2_min: 176
    cleaning_beam_agent-3_max: 71
    cleaning_beam_agent-3_mean: 23.24
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 483
    cleaning_beam_agent-4_mean: 386.59
    cleaning_beam_agent-4_min: 214
    cleaning_beam_agent-5_max: 134
    cleaning_beam_agent-5_mean: 15.86
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-44-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1238.9999999999966
  episode_reward_mean: 994.3799999999912
  episode_reward_min: 374.00000000000404
  episodes_this_iter: 96
  episodes_total: 34656
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12973.433
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9835784435272217
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011851235758513212
        model: {}
        policy_loss: -0.002906765788793564
        total_loss: -0.002300945343449712
        vf_explained_var: 0.041210949420928955
        vf_loss: 23.36914825439453
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1698288917541504
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023538910318166018
        model: {}
        policy_loss: -0.003660617396235466
        total_loss: -0.0032333810813724995
        vf_explained_var: -0.020791471004486084
        vf_loss: 24.8613338470459
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0903847217559814
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018882201984524727
        model: {}
        policy_loss: -0.0033036847598850727
        total_loss: -0.0027828982565551996
        vf_explained_var: -0.0016945302486419678
        vf_loss: 24.39865493774414
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45410171151161194
        entropy_coeff: 0.0017600000137463212
        kl: 0.001029226928949356
        model: {}
        policy_loss: -0.0024498007260262966
        total_loss: -0.0011222749017179012
        vf_explained_var: 0.12630432844161987
        vf_loss: 21.26742935180664
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9852789640426636
        entropy_coeff: 0.0017600000137463212
        kl: 0.001469181734137237
        model: {}
        policy_loss: -0.003631817875429988
        total_loss: -0.00307313259691
        vf_explained_var: 0.0597824901342392
        vf_loss: 22.927753448486328
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4909805655479431
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006561593618243933
        model: {}
        policy_loss: -0.002125405939295888
        total_loss: -0.0007660140981897712
        vf_explained_var: 0.08682714402675629
        vf_loss: 22.235170364379883
    load_time_ms: 16415.413
    num_steps_sampled: 34656000
    num_steps_trained: 34656000
    sample_time_ms: 128616.766
    update_time_ms: 55.111
  iterations_since_restore: 281
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.056696428571428
    ram_util_percent: 14.900892857142859
  pid: 14340
  policy_reward_max:
    agent-0: 206.49999999999983
    agent-1: 206.49999999999983
    agent-2: 206.49999999999983
    agent-3: 206.49999999999983
    agent-4: 206.49999999999983
    agent-5: 206.49999999999983
  policy_reward_mean:
    agent-0: 165.72999999999985
    agent-1: 165.72999999999985
    agent-2: 165.72999999999985
    agent-3: 165.72999999999985
    agent-4: 165.72999999999985
    agent-5: 165.72999999999985
  policy_reward_min:
    agent-0: 62.333333333333066
    agent-1: 62.333333333333066
    agent-2: 62.333333333333066
    agent-3: 62.333333333333066
    agent-4: 62.333333333333066
    agent-5: 62.333333333333066
  sampler_perf:
    mean_env_wait_ms: 31.024093922803086
    mean_inference_ms: 14.60098721964452
    mean_processing_ms: 65.797592887229
  time_since_restore: 45811.40231537819
  time_this_iter_s: 155.8050582408905
  time_total_s: 58362.21918678284
  timestamp: 1637081052
  timesteps_since_restore: 26976000
  timesteps_this_iter: 96000
  timesteps_total: 34656000
  training_iteration: 361
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    361 |          58362.2 | 34656000 |   994.38 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 3.31
    apples_agent-0_min: 0
    apples_agent-1_max: 115
    apples_agent-1_mean: 29.08
    apples_agent-1_min: 0
    apples_agent-2_max: 128
    apples_agent-2_mean: 6.18
    apples_agent-2_min: 0
    apples_agent-3_max: 283
    apples_agent-3_mean: 120.78
    apples_agent-3_min: 25
    apples_agent-4_max: 53
    apples_agent-4_mean: 3.37
    apples_agent-4_min: 0
    apples_agent-5_max: 317
    apples_agent-5_mean: 82.41
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 551
    cleaning_beam_agent-0_mean: 444.2
    cleaning_beam_agent-0_min: 210
    cleaning_beam_agent-1_max: 382
    cleaning_beam_agent-1_mean: 217.29
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 513
    cleaning_beam_agent-2_mean: 365.33
    cleaning_beam_agent-2_min: 207
    cleaning_beam_agent-3_max: 141
    cleaning_beam_agent-3_mean: 25.41
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 484
    cleaning_beam_agent-4_mean: 393.12
    cleaning_beam_agent-4_min: 227
    cleaning_beam_agent-5_max: 133
    cleaning_beam_agent-5_mean: 15.27
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-46-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1210.0000000000105
  episode_reward_mean: 1001.6499999999919
  episode_reward_min: 441.99999999999847
  episodes_this_iter: 96
  episodes_total: 34752
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12947.464
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9874991774559021
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011857047211378813
        model: {}
        policy_loss: -0.002920161932706833
        total_loss: -0.002050925511866808
        vf_explained_var: 0.04402567446231842
        vf_loss: 26.07235336303711
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1621623039245605
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019564530812203884
        model: {}
        policy_loss: -0.00390750914812088
        total_loss: -0.0031618187204003334
        vf_explained_var: -0.022986531257629395
        vf_loss: 27.910993576049805
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0840331315994263
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015558574814349413
        model: {}
        policy_loss: -0.0033585568889975548
        total_loss: -0.002614367753267288
        vf_explained_var: 0.027614042162895203
        vf_loss: 26.520889282226562
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44386714696884155
        entropy_coeff: 0.0017600000137463212
        kl: 0.001090274890884757
        model: {}
        policy_loss: -0.002439832780510187
        total_loss: -0.0009536693105474114
        vf_explained_var: 0.16933134198188782
        vf_loss: 22.673681259155273
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9747636914253235
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017601933795958757
        model: {}
        policy_loss: -0.004007514100521803
        total_loss: -0.0032563272397965193
        vf_explained_var: 0.10184401273727417
        vf_loss: 24.667722702026367
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4816272556781769
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007370775565505028
        model: {}
        policy_loss: -0.0023601101711392403
        total_loss: -0.0009115284774452448
        vf_explained_var: 0.15725179016590118
        vf_loss: 22.962459564208984
    load_time_ms: 14444.388
    num_steps_sampled: 34752000
    num_steps_trained: 34752000
    sample_time_ms: 128556.463
    update_time_ms: 55.433
  iterations_since_restore: 282
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.861818181818183
    ram_util_percent: 14.729090909090912
  pid: 14340
  policy_reward_max:
    agent-0: 201.66666666666669
    agent-1: 201.66666666666669
    agent-2: 201.66666666666669
    agent-3: 201.66666666666669
    agent-4: 201.66666666666669
    agent-5: 201.66666666666669
  policy_reward_mean:
    agent-0: 166.94166666666652
    agent-1: 166.94166666666652
    agent-2: 166.94166666666652
    agent-3: 166.94166666666652
    agent-4: 166.94166666666652
    agent-5: 166.94166666666652
  policy_reward_min:
    agent-0: 73.66666666666664
    agent-1: 73.66666666666664
    agent-2: 73.66666666666664
    agent-3: 73.66666666666664
    agent-4: 73.66666666666664
    agent-5: 73.66666666666664
  sampler_perf:
    mean_env_wait_ms: 31.025207033595795
    mean_inference_ms: 14.600706705869506
    mean_processing_ms: 65.79689275214902
  time_since_restore: 45965.92970728874
  time_this_iter_s: 154.52739191055298
  time_total_s: 58516.74657869339
  timestamp: 1637081206
  timesteps_since_restore: 27072000
  timesteps_this_iter: 96000
  timesteps_total: 34752000
  training_iteration: 362
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    362 |          58516.7 | 34752000 |  1001.65 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 220
    apples_agent-0_mean: 4.22
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 24.41
    apples_agent-1_min: 0
    apples_agent-2_max: 166
    apples_agent-2_mean: 11.3
    apples_agent-2_min: 0
    apples_agent-3_max: 280
    apples_agent-3_mean: 128.58
    apples_agent-3_min: 25
    apples_agent-4_max: 35
    apples_agent-4_mean: 1.57
    apples_agent-4_min: 0
    apples_agent-5_max: 217
    apples_agent-5_mean: 79.23
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 577
    cleaning_beam_agent-0_mean: 440.91
    cleaning_beam_agent-0_min: 226
    cleaning_beam_agent-1_max: 458
    cleaning_beam_agent-1_mean: 215.6
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 554
    cleaning_beam_agent-2_mean: 337.52
    cleaning_beam_agent-2_min: 105
    cleaning_beam_agent-3_max: 141
    cleaning_beam_agent-3_mean: 20.25
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 482
    cleaning_beam_agent-4_mean: 397.69
    cleaning_beam_agent-4_min: 262
    cleaning_beam_agent-5_max: 94
    cleaning_beam_agent-5_mean: 17.33
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-49-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1213.0000000000102
  episode_reward_mean: 983.5199999999935
  episode_reward_min: 394.00000000000557
  episodes_this_iter: 96
  episodes_total: 34848
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12943.775
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9847989082336426
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009680838556960225
        model: {}
        policy_loss: -0.0027750711888074875
        total_loss: -0.0019215447828173637
        vf_explained_var: 0.032931819558143616
        vf_loss: 25.867748260498047
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1593611240386963
        entropy_coeff: 0.0017600000137463212
        kl: 0.001617192872799933
        model: {}
        policy_loss: -0.003442138899117708
        total_loss: -0.002798296045511961
        vf_explained_var: -0.0029871612787246704
        vf_loss: 26.843212127685547
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.099969506263733
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018990131793543696
        model: {}
        policy_loss: -0.0035885022953152657
        total_loss: -0.002934078685939312
        vf_explained_var: 0.036031574010849
        vf_loss: 25.903722763061523
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.43886739015579224
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010762286838144064
        model: {}
        policy_loss: -0.0023246570490300655
        total_loss: -0.0008955291123129427
        vf_explained_var: 0.17619159817695618
        vf_loss: 22.015329360961914
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9803074598312378
        entropy_coeff: 0.0017600000137463212
        kl: 0.002225190168246627
        model: {}
        policy_loss: -0.004024194553494453
        total_loss: -0.0033292516600340605
        vf_explained_var: 0.09657995402812958
        vf_loss: 24.202863693237305
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49627286195755005
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005400355439633131
        model: {}
        policy_loss: -0.002153748646378517
        total_loss: -0.0007905811071395874
        vf_explained_var: 0.16406971216201782
        vf_loss: 22.36606216430664
    load_time_ms: 14519.528
    num_steps_sampled: 34848000
    num_steps_trained: 34848000
    sample_time_ms: 128242.77
    update_time_ms: 49.759
  iterations_since_restore: 283
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.882805429864252
    ram_util_percent: 14.704072398190046
  pid: 14340
  policy_reward_max:
    agent-0: 202.1666666666666
    agent-1: 202.1666666666666
    agent-2: 202.1666666666666
    agent-3: 202.1666666666666
    agent-4: 202.1666666666666
    agent-5: 202.1666666666666
  policy_reward_mean:
    agent-0: 163.91999999999993
    agent-1: 163.91999999999993
    agent-2: 163.91999999999993
    agent-3: 163.91999999999993
    agent-4: 163.91999999999993
    agent-5: 163.91999999999993
  policy_reward_min:
    agent-0: 65.66666666666647
    agent-1: 65.66666666666647
    agent-2: 65.66666666666647
    agent-3: 65.66666666666647
    agent-4: 65.66666666666647
    agent-5: 65.66666666666647
  sampler_perf:
    mean_env_wait_ms: 31.02601885654536
    mean_inference_ms: 14.600693021644764
    mean_processing_ms: 65.79597890719873
  time_since_restore: 46121.0063021183
  time_this_iter_s: 155.07659482955933
  time_total_s: 58671.82317352295
  timestamp: 1637081362
  timesteps_since_restore: 27168000
  timesteps_this_iter: 96000
  timesteps_total: 34848000
  training_iteration: 363
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    363 |          58671.8 | 34848000 |   983.52 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 103
    apples_agent-0_mean: 3.74
    apples_agent-0_min: 0
    apples_agent-1_max: 130
    apples_agent-1_mean: 26.72
    apples_agent-1_min: 0
    apples_agent-2_max: 151
    apples_agent-2_mean: 5.1
    apples_agent-2_min: 0
    apples_agent-3_max: 199
    apples_agent-3_mean: 127.04
    apples_agent-3_min: 65
    apples_agent-4_max: 34
    apples_agent-4_mean: 0.89
    apples_agent-4_min: 0
    apples_agent-5_max: 192
    apples_agent-5_mean: 80.04
    apples_agent-5_min: 52
    cleaning_beam_agent-0_max: 551
    cleaning_beam_agent-0_mean: 443.29
    cleaning_beam_agent-0_min: 321
    cleaning_beam_agent-1_max: 391
    cleaning_beam_agent-1_mean: 215.16
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 540
    cleaning_beam_agent-2_mean: 346.66
    cleaning_beam_agent-2_min: 170
    cleaning_beam_agent-3_max: 76
    cleaning_beam_agent-3_mean: 18.19
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 505
    cleaning_beam_agent-4_mean: 397.24
    cleaning_beam_agent-4_min: 279
    cleaning_beam_agent-5_max: 69
    cleaning_beam_agent-5_mean: 11.05
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-51-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1218.000000000007
  episode_reward_mean: 1043.5199999999923
  episode_reward_min: 622.9999999999967
  episodes_this_iter: 96
  episodes_total: 34944
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12942.624
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9935272336006165
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018950302619487047
        model: {}
        policy_loss: -0.00302166142500937
        total_loss: -0.002364878077059984
        vf_explained_var: 0.012484624981880188
        vf_loss: 24.053918838500977
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1549205780029297
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023137887474149466
        model: {}
        policy_loss: -0.0041169109754264355
        total_loss: -0.003615270834416151
        vf_explained_var: -0.03513157367706299
        vf_loss: 25.343002319335938
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0995633602142334
        entropy_coeff: 0.0017600000137463212
        kl: 0.00152679649181664
        model: {}
        policy_loss: -0.003475781297311187
        total_loss: -0.002960223238915205
        vf_explained_var: 0.00550590455532074
        vf_loss: 24.507896423339844
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.41471654176712036
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008099224651232362
        model: {}
        policy_loss: -0.0022084563970565796
        total_loss: -0.000802372582256794
        vf_explained_var: 0.11528746783733368
        vf_loss: 21.359859466552734
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9678179025650024
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016895040171220899
        model: {}
        policy_loss: -0.003712320700287819
        total_loss: -0.0031234705820679665
        vf_explained_var: 0.062375664710998535
        vf_loss: 22.922101974487305
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4521656334400177
        entropy_coeff: 0.0017600000137463212
        kl: 0.0005849216249771416
        model: {}
        policy_loss: -0.002007578033953905
        total_loss: -0.0006689056754112244
        vf_explained_var: 0.11591890454292297
        vf_loss: 21.34485626220703
    load_time_ms: 14411.131
    num_steps_sampled: 34944000
    num_steps_trained: 34944000
    sample_time_ms: 128269.056
    update_time_ms: 65.666
  iterations_since_restore: 284
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.795022624434388
    ram_util_percent: 14.643891402714935
  pid: 14340
  policy_reward_max:
    agent-0: 202.9999999999998
    agent-1: 202.9999999999998
    agent-2: 202.9999999999998
    agent-3: 202.9999999999998
    agent-4: 202.9999999999998
    agent-5: 202.9999999999998
  policy_reward_mean:
    agent-0: 173.91999999999985
    agent-1: 173.91999999999985
    agent-2: 173.91999999999985
    agent-3: 173.91999999999985
    agent-4: 173.91999999999985
    agent-5: 173.91999999999985
  policy_reward_min:
    agent-0: 103.83333333333344
    agent-1: 103.83333333333344
    agent-2: 103.83333333333344
    agent-3: 103.83333333333344
    agent-4: 103.83333333333344
    agent-5: 103.83333333333344
  sampler_perf:
    mean_env_wait_ms: 31.026933459136636
    mean_inference_ms: 14.600616405380052
    mean_processing_ms: 65.79553545812085
  time_since_restore: 46275.99374914169
  time_this_iter_s: 154.98744702339172
  time_total_s: 58826.81062054634
  timestamp: 1637081517
  timesteps_since_restore: 27264000
  timesteps_this_iter: 96000
  timesteps_total: 34944000
  training_iteration: 364
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    364 |          58826.8 | 34944000 |  1043.52 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 70
    apples_agent-0_mean: 1.96
    apples_agent-0_min: 0
    apples_agent-1_max: 115
    apples_agent-1_mean: 29.72
    apples_agent-1_min: 0
    apples_agent-2_max: 66
    apples_agent-2_mean: 5.99
    apples_agent-2_min: 0
    apples_agent-3_max: 425
    apples_agent-3_mean: 129.24
    apples_agent-3_min: 55
    apples_agent-4_max: 71
    apples_agent-4_mean: 2.77
    apples_agent-4_min: 0
    apples_agent-5_max: 269
    apples_agent-5_mean: 82.93
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 608
    cleaning_beam_agent-0_mean: 438.53
    cleaning_beam_agent-0_min: 239
    cleaning_beam_agent-1_max: 358
    cleaning_beam_agent-1_mean: 214.33
    cleaning_beam_agent-1_min: 71
    cleaning_beam_agent-2_max: 543
    cleaning_beam_agent-2_mean: 328.27
    cleaning_beam_agent-2_min: 114
    cleaning_beam_agent-3_max: 76
    cleaning_beam_agent-3_mean: 19.25
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 484
    cleaning_beam_agent-4_mean: 388.23
    cleaning_beam_agent-4_min: 217
    cleaning_beam_agent-5_max: 66
    cleaning_beam_agent-5_mean: 12.97
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-54-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1230.0000000000057
  episode_reward_mean: 1014.8899999999908
  episode_reward_min: 459.0000000000081
  episodes_this_iter: 96
  episodes_total: 35040
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12969.836
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0000786781311035
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019177679205313325
        model: {}
        policy_loss: -0.0028915870934724808
        total_loss: -0.0021632863208651543
        vf_explained_var: 0.015074729919433594
        vf_loss: 24.884424209594727
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.158900260925293
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014552546199411154
        model: {}
        policy_loss: -0.003587287850677967
        total_loss: -0.002969873370602727
        vf_explained_var: -0.05045345425605774
        vf_loss: 26.57074546813965
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1003167629241943
        entropy_coeff: 0.0017600000137463212
        kl: 0.002771559404209256
        model: {}
        policy_loss: -0.003869122825562954
        total_loss: -0.0032420032657682896
        vf_explained_var: -0.009057104587554932
        vf_loss: 25.636764526367188
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4260772168636322
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015842665452510118
        model: {}
        policy_loss: -0.0023437063209712505
        total_loss: -0.0008806842379271984
        vf_explained_var: 0.1227605938911438
        vf_loss: 22.12916374206543
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9803900718688965
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019855836872011423
        model: {}
        policy_loss: -0.003762977896258235
        total_loss: -0.0031671260949224234
        vf_explained_var: 0.08389672636985779
        vf_loss: 23.21339988708496
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46044495701789856
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009121921029873192
        model: {}
        policy_loss: -0.002140249125659466
        total_loss: -0.0007395432330667973
        vf_explained_var: 0.12623006105422974
        vf_loss: 22.110898971557617
    load_time_ms: 14482.907
    num_steps_sampled: 35040000
    num_steps_trained: 35040000
    sample_time_ms: 128202.769
    update_time_ms: 65.501
  iterations_since_restore: 285
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.67130044843049
    ram_util_percent: 14.696412556053813
  pid: 14340
  policy_reward_max:
    agent-0: 204.9999999999998
    agent-1: 204.9999999999998
    agent-2: 204.9999999999998
    agent-3: 204.9999999999998
    agent-4: 204.9999999999998
    agent-5: 204.9999999999998
  policy_reward_mean:
    agent-0: 169.1483333333332
    agent-1: 169.1483333333332
    agent-2: 169.1483333333332
    agent-3: 169.1483333333332
    agent-4: 169.1483333333332
    agent-5: 169.1483333333332
  policy_reward_min:
    agent-0: 76.49999999999986
    agent-1: 76.49999999999986
    agent-2: 76.49999999999986
    agent-3: 76.49999999999986
    agent-4: 76.49999999999986
    agent-5: 76.49999999999986
  sampler_perf:
    mean_env_wait_ms: 31.02779623847267
    mean_inference_ms: 14.600687280151948
    mean_processing_ms: 65.7972727204761
  time_since_restore: 46432.765852212906
  time_this_iter_s: 156.77210307121277
  time_total_s: 58983.582723617554
  timestamp: 1637081674
  timesteps_since_restore: 27360000
  timesteps_this_iter: 96000
  timesteps_total: 35040000
  training_iteration: 365
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    365 |          58983.6 | 35040000 |  1014.89 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 3.19
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 26.4
    apples_agent-1_min: 0
    apples_agent-2_max: 253
    apples_agent-2_mean: 7.88
    apples_agent-2_min: 0
    apples_agent-3_max: 190
    apples_agent-3_mean: 125.23
    apples_agent-3_min: 59
    apples_agent-4_max: 63
    apples_agent-4_mean: 2.94
    apples_agent-4_min: 0
    apples_agent-5_max: 122
    apples_agent-5_mean: 74.08
    apples_agent-5_min: 47
    cleaning_beam_agent-0_max: 545
    cleaning_beam_agent-0_mean: 432.73
    cleaning_beam_agent-0_min: 236
    cleaning_beam_agent-1_max: 347
    cleaning_beam_agent-1_mean: 224.42
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 555
    cleaning_beam_agent-2_mean: 341.13
    cleaning_beam_agent-2_min: 126
    cleaning_beam_agent-3_max: 96
    cleaning_beam_agent-3_mean: 20.82
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 489
    cleaning_beam_agent-4_mean: 396.34
    cleaning_beam_agent-4_min: 265
    cleaning_beam_agent-5_max: 90
    cleaning_beam_agent-5_mean: 15.01
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-57-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1200.0000000000007
  episode_reward_mean: 1019.5799999999891
  episode_reward_min: 604.9999999999928
  episodes_this_iter: 96
  episodes_total: 35136
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12912.263
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0007604360580444
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016948225675150752
        model: {}
        policy_loss: -0.0029538411181420088
        total_loss: -0.0023462865501642227
        vf_explained_var: 0.027353256940841675
        vf_loss: 23.688928604125977
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1612656116485596
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016775612020865083
        model: {}
        policy_loss: -0.003364889184013009
        total_loss: -0.00301587232388556
        vf_explained_var: 0.009720206260681152
        vf_loss: 23.9284610748291
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0965983867645264
        entropy_coeff: 0.0017600000137463212
        kl: 0.00164346257224679
        model: {}
        policy_loss: -0.0034418636932969093
        total_loss: -0.0028382614254951477
        vf_explained_var: -0.03302544355392456
        vf_loss: 25.33612823486328
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.42398256063461304
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008209465304389596
        model: {}
        policy_loss: -0.0021045105531811714
        total_loss: -0.0007764031179249287
        vf_explained_var: 0.14018557965755463
        vf_loss: 20.743160247802734
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9745899438858032
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018396233208477497
        model: {}
        policy_loss: -0.004223618656396866
        total_loss: -0.003687960561364889
        vf_explained_var: 0.07356584072113037
        vf_loss: 22.50934600830078
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4797725975513458
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013202755944803357
        model: {}
        policy_loss: -0.002435153815895319
        total_loss: -0.001136337174102664
        vf_explained_var: 0.11330549418926239
        vf_loss: 21.43215560913086
    load_time_ms: 14487.178
    num_steps_sampled: 35136000
    num_steps_trained: 35136000
    sample_time_ms: 128143.804
    update_time_ms: 81.699
  iterations_since_restore: 286
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.574660633484164
    ram_util_percent: 14.69638009049774
  pid: 14340
  policy_reward_max:
    agent-0: 199.9999999999999
    agent-1: 199.9999999999999
    agent-2: 199.9999999999999
    agent-3: 199.9999999999999
    agent-4: 199.9999999999999
    agent-5: 199.9999999999999
  policy_reward_mean:
    agent-0: 169.92999999999992
    agent-1: 169.92999999999992
    agent-2: 169.92999999999992
    agent-3: 169.92999999999992
    agent-4: 169.92999999999992
    agent-5: 169.92999999999992
  policy_reward_min:
    agent-0: 100.8333333333336
    agent-1: 100.8333333333336
    agent-2: 100.8333333333336
    agent-3: 100.8333333333336
    agent-4: 100.8333333333336
    agent-5: 100.8333333333336
  sampler_perf:
    mean_env_wait_ms: 31.02842973137483
    mean_inference_ms: 14.600401348794383
    mean_processing_ms: 65.79594003393771
  time_since_restore: 46588.45920085907
  time_this_iter_s: 155.69334864616394
  time_total_s: 59139.27607226372
  timestamp: 1637081830
  timesteps_since_restore: 27456000
  timesteps_this_iter: 96000
  timesteps_total: 35136000
  training_iteration: 366
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    366 |          59139.3 | 35136000 |  1019.58 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 83
    apples_agent-0_mean: 2.9
    apples_agent-0_min: 0
    apples_agent-1_max: 129
    apples_agent-1_mean: 31.58
    apples_agent-1_min: 0
    apples_agent-2_max: 77
    apples_agent-2_mean: 5.56
    apples_agent-2_min: 0
    apples_agent-3_max: 215
    apples_agent-3_mean: 124.73
    apples_agent-3_min: 13
    apples_agent-4_max: 49
    apples_agent-4_mean: 1.26
    apples_agent-4_min: 0
    apples_agent-5_max: 197
    apples_agent-5_mean: 80.82
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 584
    cleaning_beam_agent-0_mean: 425.52
    cleaning_beam_agent-0_min: 270
    cleaning_beam_agent-1_max: 496
    cleaning_beam_agent-1_mean: 233.47
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 482
    cleaning_beam_agent-2_mean: 340.56
    cleaning_beam_agent-2_min: 158
    cleaning_beam_agent-3_max: 102
    cleaning_beam_agent-3_mean: 20.34
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 490
    cleaning_beam_agent-4_mean: 411.93
    cleaning_beam_agent-4_min: 334
    cleaning_beam_agent-5_max: 185
    cleaning_beam_agent-5_mean: 14.08
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_11-59-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1226.9999999999943
  episode_reward_mean: 1035.8499999999917
  episode_reward_min: 489.00000000001705
  episodes_this_iter: 96
  episodes_total: 35232
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12916.995
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0186396837234497
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019832157995551825
        model: {}
        policy_loss: -0.0031815713737159967
        total_loss: -0.0026800627820193768
        vf_explained_var: 0.03437615931034088
        vf_loss: 22.943132400512695
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1433966159820557
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018410914344713092
        model: {}
        policy_loss: -0.0034332009963691235
        total_loss: -0.003096682485193014
        vf_explained_var: 0.007436811923980713
        vf_loss: 23.488967895507812
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0989811420440674
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012481232406571507
        model: {}
        policy_loss: -0.0032908469438552856
        total_loss: -0.0027445945888757706
        vf_explained_var: -0.016156703233718872
        vf_loss: 24.804595947265625
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40717995166778564
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008979265694506466
        model: {}
        policy_loss: -0.0020658746361732483
        total_loss: -0.0006939750164747238
        vf_explained_var: 0.11020965874195099
        vf_loss: 20.88536834716797
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9655386209487915
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015466988552361727
        model: {}
        policy_loss: -0.0035322702024132013
        total_loss: -0.003021571319550276
        vf_explained_var: 0.06653502583503723
        vf_loss: 22.1004581451416
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4666072130203247
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006971310940571129
        model: {}
        policy_loss: -0.002214013831689954
        total_loss: -0.0008933763019740582
        vf_explained_var: 0.08731819689273834
        vf_loss: 21.418678283691406
    load_time_ms: 14473.432
    num_steps_sampled: 35232000
    num_steps_trained: 35232000
    sample_time_ms: 128297.696
    update_time_ms: 85.064
  iterations_since_restore: 287
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.704035874439462
    ram_util_percent: 14.678923766816146
  pid: 14340
  policy_reward_max:
    agent-0: 204.5000000000003
    agent-1: 204.5000000000003
    agent-2: 204.5000000000003
    agent-3: 204.5000000000003
    agent-4: 204.5000000000003
    agent-5: 204.5000000000003
  policy_reward_mean:
    agent-0: 172.6416666666666
    agent-1: 172.6416666666666
    agent-2: 172.6416666666666
    agent-3: 172.6416666666666
    agent-4: 172.6416666666666
    agent-5: 172.6416666666666
  policy_reward_min:
    agent-0: 81.5000000000001
    agent-1: 81.5000000000001
    agent-2: 81.5000000000001
    agent-3: 81.5000000000001
    agent-4: 81.5000000000001
    agent-5: 81.5000000000001
  sampler_perf:
    mean_env_wait_ms: 31.029966834486103
    mean_inference_ms: 14.600302137495838
    mean_processing_ms: 65.79580993120057
  time_since_restore: 46744.595786333084
  time_this_iter_s: 156.13658547401428
  time_total_s: 59295.41265773773
  timestamp: 1637081987
  timesteps_since_restore: 27552000
  timesteps_this_iter: 96000
  timesteps_total: 35232000
  training_iteration: 367
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    367 |          59295.4 | 35232000 |  1035.85 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 1.66
    apples_agent-0_min: 0
    apples_agent-1_max: 110
    apples_agent-1_mean: 25.92
    apples_agent-1_min: 0
    apples_agent-2_max: 251
    apples_agent-2_mean: 8.32
    apples_agent-2_min: 0
    apples_agent-3_max: 201
    apples_agent-3_mean: 130.62
    apples_agent-3_min: 45
    apples_agent-4_max: 38
    apples_agent-4_mean: 1.71
    apples_agent-4_min: 0
    apples_agent-5_max: 282
    apples_agent-5_mean: 85.57
    apples_agent-5_min: 57
    cleaning_beam_agent-0_max: 536
    cleaning_beam_agent-0_mean: 431.36
    cleaning_beam_agent-0_min: 324
    cleaning_beam_agent-1_max: 427
    cleaning_beam_agent-1_mean: 238.28
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 557
    cleaning_beam_agent-2_mean: 359.32
    cleaning_beam_agent-2_min: 145
    cleaning_beam_agent-3_max: 86
    cleaning_beam_agent-3_mean: 16.2
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 509
    cleaning_beam_agent-4_mean: 413.49
    cleaning_beam_agent-4_min: 344
    cleaning_beam_agent-5_max: 98
    cleaning_beam_agent-5_mean: 10.03
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_12-02-21
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1204.0000000000123
  episode_reward_mean: 1048.299999999991
  episode_reward_min: 497.0000000000137
  episodes_this_iter: 96
  episodes_total: 35328
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12907.172
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.995076596736908
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012379144318401814
        model: {}
        policy_loss: -0.0028829993680119514
        total_loss: -0.002174727153033018
        vf_explained_var: 0.021842509508132935
        vf_loss: 24.596065521240234
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.139143705368042
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014402715023607016
        model: {}
        policy_loss: -0.0034664091654121876
        total_loss: -0.0029538962990045547
        vf_explained_var: 0.003749147057533264
        vf_loss: 25.174076080322266
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0928077697753906
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016031883424147964
        model: {}
        policy_loss: -0.0033508557826280594
        total_loss: -0.002669103443622589
        vf_explained_var: -0.012898415327072144
        vf_loss: 26.050947189331055
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.394437313079834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009007963235490024
        model: {}
        policy_loss: -0.0021001631394028664
        total_loss: -0.0005463329143822193
        vf_explained_var: 0.09617637097835541
        vf_loss: 22.480424880981445
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9681622982025146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013770703226327896
        model: {}
        policy_loss: -0.0038829380646348
        total_loss: -0.003198135644197464
        vf_explained_var: 0.05636252462863922
        vf_loss: 23.887723922729492
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45526960492134094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008411221788264811
        model: {}
        policy_loss: -0.002052553463727236
        total_loss: -0.000566321425139904
        vf_explained_var: 0.08390015363693237
        vf_loss: 22.875062942504883
    load_time_ms: 14360.805
    num_steps_sampled: 35328000
    num_steps_trained: 35328000
    sample_time_ms: 128050.826
    update_time_ms: 84.407
  iterations_since_restore: 288
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.90727272727273
    ram_util_percent: 14.740909090909094
  pid: 14340
  policy_reward_max:
    agent-0: 200.66666666666643
    agent-1: 200.66666666666643
    agent-2: 200.66666666666643
    agent-3: 200.66666666666643
    agent-4: 200.66666666666643
    agent-5: 200.66666666666643
  policy_reward_mean:
    agent-0: 174.7166666666665
    agent-1: 174.7166666666665
    agent-2: 174.7166666666665
    agent-3: 174.7166666666665
    agent-4: 174.7166666666665
    agent-5: 174.7166666666665
  policy_reward_min:
    agent-0: 82.83333333333346
    agent-1: 82.83333333333346
    agent-2: 82.83333333333346
    agent-3: 82.83333333333346
    agent-4: 82.83333333333346
    agent-5: 82.83333333333346
  sampler_perf:
    mean_env_wait_ms: 31.031439068763813
    mean_inference_ms: 14.600283668567117
    mean_processing_ms: 65.7937358868033
  time_since_restore: 46898.55278682709
  time_this_iter_s: 153.9570004940033
  time_total_s: 59449.369658231735
  timestamp: 1637082141
  timesteps_since_restore: 27648000
  timesteps_this_iter: 96000
  timesteps_total: 35328000
  training_iteration: 368
  trial_id: '00000'
  
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    368 |          59449.4 | 35328000 |   1048.3 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 73
    apples_agent-0_mean: 4.02
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 26.15
    apples_agent-1_min: 0
    apples_agent-2_max: 121
    apples_agent-2_mean: 7.98
    apples_agent-2_min: 0
    apples_agent-3_max: 239
    apples_agent-3_mean: 128.16
    apples_agent-3_min: 66
    apples_agent-4_max: 78
    apples_agent-4_mean: 1.96
    apples_agent-4_min: 0
    apples_agent-5_max: 188
    apples_agent-5_mean: 80.5
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 576
    cleaning_beam_agent-0_mean: 416.42
    cleaning_beam_agent-0_min: 241
    cleaning_beam_agent-1_max: 570
    cleaning_beam_agent-1_mean: 225.86
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 591
    cleaning_beam_agent-2_mean: 354.08
    cleaning_beam_agent-2_min: 150
    cleaning_beam_agent-3_max: 118
    cleaning_beam_agent-3_mean: 21.41
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 498
    cleaning_beam_agent-4_mean: 404.26
    cleaning_beam_agent-4_min: 257
    cleaning_beam_agent-5_max: 126
    cleaning_beam_agent-5_mean: 11.29
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_12-04-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1196.000000000008
  episode_reward_mean: 1018.079999999992
  episode_reward_min: 544.9999999999977
  episodes_this_iter: 96
  episodes_total: 35424
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12897.111
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0113000869750977
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011686744401231408
        model: {}
        policy_loss: -0.0030491012148559093
        total_loss: -0.0024480652064085007
        vf_explained_var: 0.04416215419769287
        vf_loss: 23.809249877929688
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1565600633621216
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017410602886229753
        model: {}
        policy_loss: -0.003599719610065222
        total_loss: -0.003097220091149211
        vf_explained_var: -0.02217864990234375
        vf_loss: 25.380441665649414
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0944324731826782
        entropy_coeff: 0.0017600000137463212
        kl: 0.001193624921143055
        model: {}
        policy_loss: -0.0034066676162183285
        total_loss: -0.002689124783501029
        vf_explained_var: -0.04836133122444153
        vf_loss: 26.437440872192383
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4242309331893921
        entropy_coeff: 0.0017600000137463212
        kl: 0.001333614345639944
        model: {}
        policy_loss: -0.0021705888211727142
        total_loss: -0.0007512816227972507
        vf_explained_var: 0.1255052387714386
        vf_loss: 21.65952491760254
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9639477133750916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014622870367020369
        model: {}
        policy_loss: -0.0036687476094812155
        total_loss: -0.003021877259016037
        vf_explained_var: 0.053103864192962646
        vf_loss: 23.434202194213867
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47284868359565735
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007193707278929651
        model: {}
        policy_loss: -0.00232452224008739
        total_loss: -0.0010160066885873675
        vf_explained_var: 0.1359272450208664
        vf_loss: 21.40727996826172
    load_time_ms: 14171.384
    num_steps_sampled: 35424000
    num_steps_trained: 35424000
    sample_time_ms: 127883.137
    update_time_ms: 71.163
  iterations_since_restore: 289
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.917272727272724
    ram_util_percent: 14.685454545454549
  pid: 14340
  policy_reward_max:
    agent-0: 199.33333333333243
    agent-1: 199.33333333333243
    agent-2: 199.33333333333243
    agent-3: 199.33333333333243
    agent-4: 199.33333333333243
    agent-5: 199.33333333333243
  policy_reward_mean:
    agent-0: 169.67999999999992
    agent-1: 169.67999999999992
    agent-2: 169.67999999999992
    agent-3: 169.67999999999992
    agent-4: 169.67999999999992
    agent-5: 169.67999999999992
  policy_reward_min:
    agent-0: 90.8333333333334
    agent-1: 90.8333333333334
    agent-2: 90.8333333333334
    agent-3: 90.8333333333334
    agent-4: 90.8333333333334
    agent-5: 90.8333333333334
  sampler_perf:
    mean_env_wait_ms: 31.032057625819615
    mean_inference_ms: 14.60014020578643
    mean_processing_ms: 65.79322880580207
  time_since_restore: 47052.88229012489
  time_this_iter_s: 154.3295032978058
  time_total_s: 59603.69916152954
  timestamp: 1637082295
  timesteps_since_restore: 27744000
  timesteps_this_iter: 96000
  timesteps_total: 35424000
  training_iteration: 369
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    369 |          59603.7 | 35424000 |  1018.08 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 2.61
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 30.54
    apples_agent-1_min: 0
    apples_agent-2_max: 198
    apples_agent-2_mean: 7.79
    apples_agent-2_min: 0
    apples_agent-3_max: 196
    apples_agent-3_mean: 124.04
    apples_agent-3_min: 68
    apples_agent-4_max: 38
    apples_agent-4_mean: 0.84
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 78.12
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 534
    cleaning_beam_agent-0_mean: 425.11
    cleaning_beam_agent-0_min: 265
    cleaning_beam_agent-1_max: 552
    cleaning_beam_agent-1_mean: 228.33
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 524
    cleaning_beam_agent-2_mean: 352.59
    cleaning_beam_agent-2_min: 150
    cleaning_beam_agent-3_max: 73
    cleaning_beam_agent-3_mean: 18.13
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 502
    cleaning_beam_agent-4_mean: 421.83
    cleaning_beam_agent-4_min: 265
    cleaning_beam_agent-5_max: 177
    cleaning_beam_agent-5_mean: 12.38
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_12-07-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1214.0000000000018
  episode_reward_mean: 1042.7499999999898
  episode_reward_min: 490.0000000000013
  episodes_this_iter: 96
  episodes_total: 35520
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12900.816
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0089623928070068
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015812834026291966
        model: {}
        policy_loss: -0.0031117284670472145
        total_loss: -0.002594208810478449
        vf_explained_var: 0.05955798923969269
        vf_loss: 22.93289566040039
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1536232233047485
        entropy_coeff: 0.0017600000137463212
        kl: 0.002343993168324232
        model: {}
        policy_loss: -0.003969159442931414
        total_loss: -0.003466835245490074
        vf_explained_var: -0.03249427676200867
        vf_loss: 25.32697868347168
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0959479808807373
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012893910752609372
        model: {}
        policy_loss: -0.0034010966774076223
        total_loss: -0.002774098888039589
        vf_explained_var: -0.00793464481830597
        vf_loss: 25.558692932128906
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.39231857657432556
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006688721477985382
        model: {}
        policy_loss: -0.002070173854008317
        total_loss: -0.0006030004005879164
        vf_explained_var: 0.10903239250183105
        vf_loss: 21.576547622680664
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9667101502418518
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012149207759648561
        model: {}
        policy_loss: -0.0033847028389573097
        total_loss: -0.0027625197544693947
        vf_explained_var: 0.04779431223869324
        vf_loss: 23.23592758178711
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46912965178489685
        entropy_coeff: 0.0017600000137463212
        kl: 0.000743373588193208
        model: {}
        policy_loss: -0.002252392005175352
        total_loss: -0.0009267341811209917
        vf_explained_var: 0.11681893467903137
        vf_loss: 21.51325798034668
    load_time_ms: 15490.034
    num_steps_sampled: 35520000
    num_steps_trained: 35520000
    sample_time_ms: 128078.956
    update_time_ms: 58.798
  iterations_since_restore: 290
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.556846473029044
    ram_util_percent: 14.847717842323652
  pid: 14340
  policy_reward_max:
    agent-0: 202.333333333333
    agent-1: 202.333333333333
    agent-2: 202.333333333333
    agent-3: 202.333333333333
    agent-4: 202.333333333333
    agent-5: 202.333333333333
  policy_reward_mean:
    agent-0: 173.7916666666665
    agent-1: 173.7916666666665
    agent-2: 173.7916666666665
    agent-3: 173.7916666666665
    agent-4: 173.7916666666665
    agent-5: 173.7916666666665
  policy_reward_min:
    agent-0: 81.66666666666664
    agent-1: 81.66666666666664
    agent-2: 81.66666666666664
    agent-3: 81.66666666666664
    agent-4: 81.66666666666664
    agent-5: 81.66666666666664
  sampler_perf:
    mean_env_wait_ms: 31.033661063186965
    mean_inference_ms: 14.602907437198924
    mean_processing_ms: 65.8002632069369
  time_since_restore: 47222.4112598896
  time_this_iter_s: 169.52896976470947
  time_total_s: 59773.22813129425
  timestamp: 1637082465
  timesteps_since_restore: 27840000
  timesteps_this_iter: 96000
  timesteps_total: 35520000
  training_iteration: 370
  trial_id: '00000'
  
[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    370 |          59773.2 | 35520000 |  1042.75 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 2.01
    apples_agent-0_min: 0
    apples_agent-1_max: 145
    apples_agent-1_mean: 34.76
    apples_agent-1_min: 0
    apples_agent-2_max: 115
    apples_agent-2_mean: 9.91
    apples_agent-2_min: 0
    apples_agent-3_max: 216
    apples_agent-3_mean: 126.46
    apples_agent-3_min: 62
    apples_agent-4_max: 88
    apples_agent-4_mean: 2.23
    apples_agent-4_min: 0
    apples_agent-5_max: 132
    apples_agent-5_mean: 76.86
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 552
    cleaning_beam_agent-0_mean: 417.95
    cleaning_beam_agent-0_min: 284
    cleaning_beam_agent-1_max: 384
    cleaning_beam_agent-1_mean: 228.55
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 493
    cleaning_beam_agent-2_mean: 353.05
    cleaning_beam_agent-2_min: 190
    cleaning_beam_agent-3_max: 62
    cleaning_beam_agent-3_mean: 16.0
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 540
    cleaning_beam_agent-4_mean: 423.24
    cleaning_beam_agent-4_min: 306
    cleaning_beam_agent-5_max: 140
    cleaning_beam_agent-5_mean: 13.78
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_12-10-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1204.0000000000127
  episode_reward_mean: 1037.2399999999934
  episode_reward_min: 587.9999999999984
  episodes_this_iter: 96
  episodes_total: 35616
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12903.726
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.008906364440918
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016699889674782753
        model: {}
        policy_loss: -0.003073409665375948
        total_loss: -0.002432813635095954
        vf_explained_var: 0.014618679881095886
        vf_loss: 24.162723541259766
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1471219062805176
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013567068381235003
        model: {}
        policy_loss: -0.0034242752008140087
        total_loss: -0.0028466503135859966
        vf_explained_var: -0.05435481667518616
        vf_loss: 25.96556854248047
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0998998880386353
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023235902190208435
        model: {}
        policy_loss: -0.0034626543056219816
        total_loss: -0.0027903011068701744
        vf_explained_var: -0.029205262660980225
        vf_loss: 26.081748962402344
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.40795719623565674
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010601361282169819
        model: {}
        policy_loss: -0.002173337386921048
        total_loss: -0.0007932228036224842
        vf_explained_var: 0.14076846837997437
        vf_loss: 20.981168746948242
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9617468118667603
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021364521235227585
        model: {}
        policy_loss: -0.003844479564577341
        total_loss: -0.0031648026779294014
        vf_explained_var: 0.03342717885971069
        vf_loss: 23.723556518554688
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47440722584724426
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006626265239901841
        model: {}
        policy_loss: -0.0021246392279863358
        total_loss: -0.0008076047524809837
        vf_explained_var: 0.12120011448860168
        vf_loss: 21.519939422607422
    load_time_ms: 15488.589
    num_steps_sampled: 35616000
    num_steps_trained: 35616000
    sample_time_ms: 128212.833
    update_time_ms: 58.581
  iterations_since_restore: 291
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.568749999999998
    ram_util_percent: 14.75089285714286
  pid: 14340
  policy_reward_max:
    agent-0: 200.66666666666666
    agent-1: 200.66666666666666
    agent-2: 200.66666666666666
    agent-3: 200.66666666666666
    agent-4: 200.66666666666666
    agent-5: 200.66666666666666
  policy_reward_mean:
    agent-0: 172.87333333333322
    agent-1: 172.87333333333322
    agent-2: 172.87333333333322
    agent-3: 172.87333333333322
    agent-4: 172.87333333333322
    agent-5: 172.87333333333322
  policy_reward_min:
    agent-0: 98.00000000000001
    agent-1: 98.00000000000001
    agent-2: 98.00000000000001
    agent-3: 98.00000000000001
    agent-4: 98.00000000000001
    agent-5: 98.00000000000001
  sampler_perf:
    mean_env_wait_ms: 31.034711299158012
    mean_inference_ms: 14.602597489248456
    mean_processing_ms: 65.80032040311109
  time_since_restore: 47379.5456905365
  time_this_iter_s: 157.13443064689636
  time_total_s: 59930.36256194115
  timestamp: 1637082622
  timesteps_since_restore: 27936000
  timesteps_this_iter: 96000
  timesteps_total: 35616000
  training_iteration: 371
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    371 |          59930.4 | 35616000 |  1037.24 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 1.77
    apples_agent-0_min: 0
    apples_agent-1_max: 114
    apples_agent-1_mean: 32.2
    apples_agent-1_min: 0
    apples_agent-2_max: 71
    apples_agent-2_mean: 3.9
    apples_agent-2_min: 0
    apples_agent-3_max: 180
    apples_agent-3_mean: 123.01
    apples_agent-3_min: 67
    apples_agent-4_max: 64
    apples_agent-4_mean: 1.89
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 78.41
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 537
    cleaning_beam_agent-0_mean: 424.59
    cleaning_beam_agent-0_min: 302
    cleaning_beam_agent-1_max: 509
    cleaning_beam_agent-1_mean: 243.3
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 532
    cleaning_beam_agent-2_mean: 360.95
    cleaning_beam_agent-2_min: 229
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 15.83
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 543
    cleaning_beam_agent-4_mean: 417.69
    cleaning_beam_agent-4_min: 209
    cleaning_beam_agent-5_max: 104
    cleaning_beam_agent-5_mean: 10.3
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 2
    fire_beam_agent-2_mean: 0.03
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_12-13-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1190.0000000000175
  episode_reward_mean: 1049.7699999999925
  episode_reward_min: 594.0000000000078
  episodes_this_iter: 96
  episodes_total: 35712
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12909.63
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0095789432525635
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010168509325012565
        model: {}
        policy_loss: -0.002973353024572134
        total_loss: -0.002388041466474533
        vf_explained_var: 0.015605419874191284
        vf_loss: 23.621707916259766
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1363849639892578
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018813028000295162
        model: {}
        policy_loss: -0.003718070685863495
        total_loss: -0.0031765056774020195
        vf_explained_var: -0.04832488298416138
        vf_loss: 25.416027069091797
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1054799556732178
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014794388553127646
        model: {}
        policy_loss: -0.003231409937143326
        total_loss: -0.002702335361391306
        vf_explained_var: -0.004633232951164246
        vf_loss: 24.747169494628906
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.400882750749588
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007980000227689743
        model: {}
        policy_loss: -0.0018691092263907194
        total_loss: -0.00047508423449471593
        vf_explained_var: 0.1234917938709259
        vf_loss: 20.995792388916016
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9663839340209961
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017290450632572174
        model: {}
        policy_loss: -0.0035746628418564796
        total_loss: -0.0029961136169731617
        vf_explained_var: 0.05909867584705353
        vf_loss: 22.793851852416992
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45339512825012207
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010010844562202692
        model: {}
        policy_loss: -0.002272056881338358
        total_loss: -0.0008881702087819576
        vf_explained_var: 0.09042301774024963
        vf_loss: 21.81861686706543
    load_time_ms: 15610.722
    num_steps_sampled: 35712000
    num_steps_trained: 35712000
    sample_time_ms: 128313.668
    update_time_ms: 68.89
  iterations_since_restore: 292
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.429017857142856
    ram_util_percent: 14.708482142857145
  pid: 14340
  policy_reward_max:
    agent-0: 198.33333333333334
    agent-1: 198.33333333333334
    agent-2: 198.33333333333334
    agent-3: 198.33333333333334
    agent-4: 198.33333333333334
    agent-5: 198.33333333333334
  policy_reward_mean:
    agent-0: 174.9616666666665
    agent-1: 174.9616666666665
    agent-2: 174.9616666666665
    agent-3: 174.9616666666665
    agent-4: 174.9616666666665
    agent-5: 174.9616666666665
  policy_reward_min:
    agent-0: 99.00000000000017
    agent-1: 99.00000000000017
    agent-2: 99.00000000000017
    agent-3: 99.00000000000017
    agent-4: 99.00000000000017
    agent-5: 99.00000000000017
  sampler_perf:
    mean_env_wait_ms: 31.03684536689708
    mean_inference_ms: 14.602407054344978
    mean_processing_ms: 65.80021770192228
  time_since_restore: 47536.56626033783
  time_this_iter_s: 157.02056980133057
  time_total_s: 60087.38313174248
  timestamp: 1637082780
  timesteps_since_restore: 28032000
  timesteps_this_iter: 96000
  timesteps_total: 35712000
  training_iteration: 372
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/36.96 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.35:14340 |    372 |          60087.4 | 35712000 |  1049.77 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=14340)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd56dd1b588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 3.84
    apples_agent-0_min: 0
    apples_agent-1_max: 133
    apples_agent-1_mean: 24.83
    apples_agent-1_min: 0
    apples_agent-2_max: 124
    apples_agent-2_mean: 5.73
    apples_agent-2_min: 0
    apples_agent-3_max: 224
    apples_agent-3_mean: 124.89
    apples_agent-3_min: 21
    apples_agent-4_max: 83
    apples_agent-4_mean: 2.11
    apples_agent-4_min: 0
    apples_agent-5_max: 135
    apples_agent-5_mean: 78.53
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 559
    cleaning_beam_agent-0_mean: 419.9
    cleaning_beam_agent-0_min: 200
    cleaning_beam_agent-1_max: 530
    cleaning_beam_agent-1_mean: 237.47
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 521
    cleaning_beam_agent-2_mean: 355.94
    cleaning_beam_agent-2_min: 159
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 17.76
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 519
    cleaning_beam_agent-4_mean: 427.71
    cleaning_beam_agent-4_min: 249
    cleaning_beam_agent-5_max: 104
    cleaning_beam_agent-5_mean: 13.72
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_12-15-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1212.9999999999823
  episode_reward_mean: 1030.1999999999903
  episode_reward_min: 141.00000000000014
  episodes_this_iter: 96
  episodes_total: 35808
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu035
  info:
    grad_time_ms: 12909.598
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9937422871589661
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017225940246134996
        model: {}
        policy_loss: -0.0032645398750901222
        total_loss: -0.0026106350123882294
        vf_explained_var: 0.056268125772476196
        vf_loss: 24.028911590576172
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1619235277175903
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017890636809170246
        model: {}
        policy_loss: -0.003812624141573906
        total_loss: -0.0032433136366307735
        vf_explained_var: -0.02810913324356079
        vf_loss: 26.142963409423828
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1007542610168457
        entropy_coeff: 0.0017600000137463212
        kl: 0.002026702044531703
        model: {}
        policy_loss: -0.0036476117093116045
        total_loss: -0.0029656910337507725
        vf_explained_var: -0.010428011417388916
        vf_loss: 26.192485809326172
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4093959331512451
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009398850379511714
        model: {}
        policy_loss: -0.0024462249130010605
        total_loss: -0.0010013938881456852
        vf_explained_var: 0.14787657558918
        vf_loss: 21.65370750427246
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9564869403839111
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014766198582947254
        model: {}
        policy_loss: -0.0033688463736325502
        total_loss: -0.002771505620330572
        vf_explained_var: 0.1016467958688736
        vf_loss: 22.80758285522461
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4596642255783081
        entropy_coeff: 0.0017600000137463212
        kl: 0.000329265691107139
        model: {}
        policy_loss: -0.0020675810519605875
        total_loss: -0.0006471810629591346
        vf_explained_var: 0.1232159286737442
        vf_loss: 22.29407501220703
    load_time_ms: 15400.907
    num_steps_sampled: 35808000
    num_steps_trained: 35808000
    sample_time_ms: 128558.994
    update_time_ms: 77.295
  iterations_since_restore: 293
  node_ip: 172.17.8.35
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.735135135135135
    ram_util_percent: 14.643693693693697
  pid: 14340
  policy_reward_max:
    agent-0: 202.1666666666665
    agent-1: 202.1666666666665
    agent-2: 202.1666666666665
    agent-3: 202.1666666666665
    agent-4: 202.1666666666665
    agent-5: 202.1666666666665
  policy_reward_mean:
    agent-0: 171.69999999999993
    agent-1: 171.69999999999993
    agent-2: 171.69999999999993
    agent-3: 171.69999999999993
    agent-4: 171.69999999999993
    agent-5: 171.69999999999993
  policy_reward_min:
    agent-0: 23.500000000000025
    agent-1: 23.500000000000025
    agent-2: 23.500000000000025
    agent-3: 23.500000000000025
    agent-4: 23.500000000000025
    agent-5: 23.500000000000025
  sampler_perf:
    mean_env_wait_ms: 31.038967748475592
    mean_inference_ms: 14.602343995967745
    mean_processing_ms: 65.79959350454826
  time_since_restore: 47692.021555900574
  time_this_iter_s: 155.45529556274414
  time_total_s: 60242.83842730522
  timestamp: 1637082936
  timesteps_since_restore: 28128000
  timesteps_this_iter: 96000
  timesteps_total: 35808000
  training_iteration: 373
  trial_id: '00000'
  >>> RESTORING FROM:  /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-15_17-00-45_bf59qjr/checkpoint_500
== Status ==
Memory usage on this node: 14.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    440 |          69193.5 | 42240000 |  1095.76 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=38704)[0m 2021-11-16 16:37:34,005	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=38704)[0m 2021-11-16 16:37:34,019	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=38704)[0m 2021-11-16 16:39:20,014	INFO trainable.py:180 -- _setup took 106.009 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=38704)[0m 2021-11-16 16:39:20,014	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=38704)[0m 2021-11-16 16:39:20,015	WARNING util.py:37 -- Install gputil for GPU system monitoring.
[2m[36m(pid=38704)[0m 2021-11-16 16:39:23,487	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=38704)[0m 2021-11-16 16:39:23,488	INFO trainable.py:423 -- Restored on 172.17.8.131 from checkpoint: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-15_17-00-45_bf59qjr/tmphn__us3trestore_from_object/checkpoint-440
[2m[36m(pid=38704)[0m 2021-11-16 16:39:23,488	INFO trainable.py:430 -- Current state after restoring: {'_iteration': 440, '_timesteps_total': 42240000, '_time_total': 69193.48181247711, '_episodes_total': 42240}
== Status ==
Memory usage on this node: 26.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+----------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |    440 |          69193.5 | 42240000 |  1095.76 |
+--------------------------------------+----------+-------+--------+------------------+----------+----------+


[2m[36m(pid=38704)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbc8255f518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 118
    apples_agent-0_mean: 2.46875
    apples_agent-0_min: 0
    apples_agent-1_max: 121
    apples_agent-1_mean: 26.895833333333332
    apples_agent-1_min: 0
    apples_agent-2_max: 60
    apples_agent-2_mean: 2.6458333333333335
    apples_agent-2_min: 0
    apples_agent-3_max: 255
    apples_agent-3_mean: 96.05208333333333
    apples_agent-3_min: 50
    apples_agent-4_max: 70
    apples_agent-4_mean: 1.3645833333333333
    apples_agent-4_min: 0
    apples_agent-5_max: 295
    apples_agent-5_mean: 79.4375
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 510
    cleaning_beam_agent-0_mean: 390.4583333333333
    cleaning_beam_agent-0_min: 201
    cleaning_beam_agent-1_max: 376
    cleaning_beam_agent-1_mean: 240.73958333333334
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 551
    cleaning_beam_agent-2_mean: 408.28125
    cleaning_beam_agent-2_min: 192
    cleaning_beam_agent-3_max: 76
    cleaning_beam_agent-3_mean: 9.71875
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 535
    cleaning_beam_agent-4_mean: 442.15625
    cleaning_beam_agent-4_min: 195
    cleaning_beam_agent-5_max: 56
    cleaning_beam_agent-5_mean: 12.46875
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.010416666666666666
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_16-43-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1256.0000000000155
  episode_reward_mean: 1101.2291666666608
  episode_reward_min: 593.9999999999989
  episodes_this_iter: 96
  episodes_total: 42336
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu131.cluster.local
  info:
    grad_time_ms: 29205.582
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0316423177719116
        entropy_coeff: 0.0017600000137463212
        kl: 0.001612824504263699
        model: {}
        policy_loss: -0.003728484269231558
        total_loss: -0.0027878896798938513
        vf_explained_var: 0.006023332476615906
        vf_loss: 24.337202072143555
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1421388387680054
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012467759661376476
        model: {}
        policy_loss: -0.004352489951997995
        total_loss: -0.003601823467761278
        vf_explained_var: -0.02059626579284668
        vf_loss: 25.11475944519043
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0860742330551147
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009945804486051202
        model: {}
        policy_loss: -0.0037436606362462044
        total_loss: -0.0029173255898058414
        vf_explained_var: -0.024338752031326294
        vf_loss: 25.389080047607422
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3279964029788971
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006166536477394402
        model: {}
        policy_loss: -0.002098817378282547
        total_loss: -0.00026795174926519394
        vf_explained_var: 0.0648600310087204
        vf_loss: 22.84807777404785
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9416651725769043
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015350799076259136
        model: {}
        policy_loss: -0.004320508800446987
        total_loss: -0.003320764284580946
        vf_explained_var: 0.04051460325717926
        vf_loss: 23.5006103515625
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4573230743408203
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007384748896583915
        model: {}
        policy_loss: -0.0026252102106809616
        total_loss: -0.001102757640182972
        vf_explained_var: 0.11533434689044952
        vf_loss: 21.796457290649414
    load_time_ms: 106230.003
    num_steps_sampled: 42336000
    num_steps_trained: 42336000
    sample_time_ms: 103745.293
    update_time_ms: 3570.553
  iterations_since_restore: 1
  node_ip: 172.17.8.131
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.475133689839573
    ram_util_percent: 18.466310160427806
  pid: 38704
  policy_reward_max:
    agent-0: 209.33333333333286
    agent-1: 209.33333333333286
    agent-2: 209.33333333333286
    agent-3: 209.33333333333286
    agent-4: 209.33333333333286
    agent-5: 209.33333333333286
  policy_reward_mean:
    agent-0: 183.53819444444434
    agent-1: 183.53819444444434
    agent-2: 183.53819444444434
    agent-3: 183.53819444444434
    agent-4: 183.53819444444434
    agent-5: 183.53819444444434
  policy_reward_min:
    agent-0: 99.00000000000016
    agent-1: 99.00000000000016
    agent-2: 99.00000000000016
    agent-3: 99.00000000000016
    agent-4: 99.00000000000016
    agent-5: 99.00000000000016
  sampler_perf:
    mean_env_wait_ms: 26.47148594235405
    mean_inference_ms: 14.445105553308489
    mean_processing_ms: 54.010830122432914
  time_since_restore: 252.73602485656738
  time_this_iter_s: 252.73602485656738
  time_total_s: 69446.21783733368
  timestamp: 1637099022
  timesteps_since_restore: 96000
  timesteps_this_iter: 96000
  timesteps_total: 42336000
  training_iteration: 441
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.131:38704 |    441 |          69446.2 | 42336000 |  1101.23 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=38704)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbc8255f518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 2.09
    apples_agent-0_min: 0
    apples_agent-1_max: 113
    apples_agent-1_mean: 22.5
    apples_agent-1_min: 0
    apples_agent-2_max: 129
    apples_agent-2_mean: 4.24
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 94.79
    apples_agent-3_min: 44
    apples_agent-4_max: 30
    apples_agent-4_mean: 0.74
    apples_agent-4_min: 0
    apples_agent-5_max: 132
    apples_agent-5_mean: 76.79
    apples_agent-5_min: 53
    cleaning_beam_agent-0_max: 454
    cleaning_beam_agent-0_mean: 373.02
    cleaning_beam_agent-0_min: 203
    cleaning_beam_agent-1_max: 411
    cleaning_beam_agent-1_mean: 253.23
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 483
    cleaning_beam_agent-2_mean: 386.04
    cleaning_beam_agent-2_min: 185
    cleaning_beam_agent-3_max: 66
    cleaning_beam_agent-3_mean: 8.43
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 576
    cleaning_beam_agent-4_mean: 449.87
    cleaning_beam_agent-4_min: 321
    cleaning_beam_agent-5_max: 98
    cleaning_beam_agent-5_mean: 14.24
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_16-48-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1270.999999999989
  episode_reward_mean: 1108.3599999999947
  episode_reward_min: 460.0000000000071
  episodes_this_iter: 96
  episodes_total: 42432
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu131.cluster.local
  info:
    grad_time_ms: 23979.874
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 1.030419945716858
        entropy_coeff: 0.0017600000137463212
        kl: 0.001787582878023386
        model: {}
        policy_loss: -0.003336830995976925
        total_loss: -0.00270643038675189
        vf_explained_var: 0.01059800386428833
        vf_loss: 22.65180015563965
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1490037441253662
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013424570206552744
        model: {}
        policy_loss: -0.0034341788850724697
        total_loss: -0.00298587279394269
        vf_explained_var: -0.008800968527793884
        vf_loss: 23.363056182861328
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1000453233718872
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012727010762318969
        model: {}
        policy_loss: -0.0035742903128266335
        total_loss: -0.0030542518943548203
        vf_explained_var: 0.006452009081840515
        vf_loss: 23.288490295410156
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3306181728839874
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008130512433126569
        model: {}
        policy_loss: -0.002086434280499816
        total_loss: -0.00048723595682531595
        vf_explained_var: 0.08303578197956085
        vf_loss: 20.997827529907227
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9333463907241821
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012677937047556043
        model: {}
        policy_loss: -0.0033961841836571693
        total_loss: -0.0026163116563111544
        vf_explained_var: 0.0015153437852859497
        vf_loss: 22.957828521728516
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45354172587394714
        entropy_coeff: 0.0017600000137463212
        kl: 0.00065195607021451
        model: {}
        policy_loss: -0.0022147209383547306
        total_loss: -0.0009366037556901574
        vf_explained_var: 0.12622468173503876
        vf_loss: 20.111513137817383
    load_time_ms: 85208.295
    num_steps_sampled: 42432000
    num_steps_trained: 42432000
    sample_time_ms: 136382.383
    update_time_ms: 2039.013
  iterations_since_restore: 2
  node_ip: 172.17.8.131
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.74791086350975
    ram_util_percent: 19.200835654596098
  pid: 38704
  policy_reward_max:
    agent-0: 211.8333333333332
    agent-1: 211.8333333333332
    agent-2: 211.8333333333332
    agent-3: 211.8333333333332
    agent-4: 211.8333333333332
    agent-5: 211.8333333333332
  policy_reward_mean:
    agent-0: 184.72666666666655
    agent-1: 184.72666666666655
    agent-2: 184.72666666666655
    agent-3: 184.72666666666655
    agent-4: 184.72666666666655
    agent-5: 184.72666666666655
  policy_reward_min:
    agent-0: 76.6666666666666
    agent-1: 76.6666666666666
    agent-2: 76.6666666666666
    agent-3: 76.6666666666666
    agent-4: 76.6666666666666
    agent-5: 76.6666666666666
  sampler_perf:
    mean_env_wait_ms: 27.82798202702047
    mean_inference_ms: 14.252684563589183
    mean_processing_ms: 55.744870641214405
  time_since_restore: 505.47191095352173
  time_this_iter_s: 252.73588609695435
  time_total_s: 69698.95372343063
  timestamp: 1637099291
  timesteps_since_restore: 192000
  timesteps_this_iter: 96000
  timesteps_total: 42432000
  training_iteration: 442
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 32.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.131:38704 |    442 |            69699 | 42432000 |  1108.36 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=38704)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbc8255f518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 125
    apples_agent-0_mean: 3.46
    apples_agent-0_min: 0
    apples_agent-1_max: 110
    apples_agent-1_mean: 27.26
    apples_agent-1_min: 0
    apples_agent-2_max: 150
    apples_agent-2_mean: 5.45
    apples_agent-2_min: 0
    apples_agent-3_max: 148
    apples_agent-3_mean: 92.55
    apples_agent-3_min: 18
    apples_agent-4_max: 52
    apples_agent-4_mean: 1.79
    apples_agent-4_min: 0
    apples_agent-5_max: 125
    apples_agent-5_mean: 73.23
    apples_agent-5_min: 11
    cleaning_beam_agent-0_max: 490
    cleaning_beam_agent-0_mean: 374.97
    cleaning_beam_agent-0_min: 145
    cleaning_beam_agent-1_max: 424
    cleaning_beam_agent-1_mean: 234.67
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 584
    cleaning_beam_agent-2_mean: 400.97
    cleaning_beam_agent-2_min: 197
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 9.7
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 561
    cleaning_beam_agent-4_mean: 452.69
    cleaning_beam_agent-4_min: 174
    cleaning_beam_agent-5_max: 92
    cleaning_beam_agent-5_mean: 16.07
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_16-51-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1255.0000000000061
  episode_reward_mean: 1072.4799999999925
  episode_reward_min: 174.9999999999986
  episodes_this_iter: 96
  episodes_total: 42528
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu131.cluster.local
  info:
    grad_time_ms: 22223.271
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0317094326019287
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016178510850295424
        model: {}
        policy_loss: -0.003202088875696063
        total_loss: -0.002400042023509741
        vf_explained_var: 0.015767857432365417
        vf_loss: 25.369630813598633
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1456516981124878
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014058214146643877
        model: {}
        policy_loss: -0.00372020760551095
        total_loss: -0.003069019876420498
        vf_explained_var: -0.008011922240257263
        vf_loss: 25.972434997558594
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0838474035263062
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018098435830324888
        model: {}
        policy_loss: -0.003774608951061964
        total_loss: -0.0028983871452510357
        vf_explained_var: -0.0438133180141449
        vf_loss: 26.932994842529297
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.35011693835258484
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007142722606658936
        model: {}
        policy_loss: -0.0019207338336855173
        total_loss: -0.00016531720757484436
        vf_explained_var: 0.09304273128509521
        vf_loss: 23.359102249145508
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9525183439254761
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018051032675430179
        model: {}
        policy_loss: -0.003825798165053129
        total_loss: -0.0029900656081736088
        vf_explained_var: 0.06056876480579376
        vf_loss: 24.219114303588867
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48260799050331116
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006233555031940341
        model: {}
        policy_loss: -0.002372400602325797
        total_loss: -0.000981431920081377
        vf_explained_var: 0.14279092848300934
        vf_loss: 22.091934204101562
    load_time_ms: 74157.4
    num_steps_sampled: 42528000
    num_steps_trained: 42528000
    sample_time_ms: 137434.192
    update_time_ms: 1372.547
  iterations_since_restore: 3
  node_ip: 172.17.8.131
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.37840531561462
    ram_util_percent: 18.92691029900332
  pid: 38704
  policy_reward_max:
    agent-0: 209.1666666666661
    agent-1: 209.1666666666661
    agent-2: 209.1666666666661
    agent-3: 209.1666666666661
    agent-4: 209.1666666666661
    agent-5: 209.1666666666661
  policy_reward_mean:
    agent-0: 178.7466666666666
    agent-1: 178.7466666666666
    agent-2: 178.7466666666666
    agent-3: 178.7466666666666
    agent-4: 178.7466666666666
    agent-5: 178.7466666666666
  policy_reward_min:
    agent-0: 29.166666666666746
    agent-1: 29.166666666666746
    agent-2: 29.166666666666746
    agent-3: 29.166666666666746
    agent-4: 29.166666666666746
    agent-5: 29.166666666666746
  sampler_perf:
    mean_env_wait_ms: 27.28381126314244
    mean_inference_ms: 13.603377835650049
    mean_processing_ms: 54.71313550936219
  time_since_restore: 715.9343392848969
  time_this_iter_s: 210.46242833137512
  time_total_s: 69909.41615176201
  timestamp: 1637099502
  timesteps_since_restore: 288000
  timesteps_this_iter: 96000
  timesteps_total: 42528000
  training_iteration: 443
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.131:38704 |    443 |          69909.4 | 42528000 |  1072.48 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=38704)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbc8255f518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 64
    apples_agent-0_mean: 2.05
    apples_agent-0_min: 0
    apples_agent-1_max: 121
    apples_agent-1_mean: 29.71
    apples_agent-1_min: 0
    apples_agent-2_max: 109
    apples_agent-2_mean: 3.6
    apples_agent-2_min: 0
    apples_agent-3_max: 144
    apples_agent-3_mean: 93.08
    apples_agent-3_min: 29
    apples_agent-4_max: 47
    apples_agent-4_mean: 1.51
    apples_agent-4_min: 0
    apples_agent-5_max: 110
    apples_agent-5_mean: 73.13
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 462
    cleaning_beam_agent-0_mean: 364.21
    cleaning_beam_agent-0_min: 237
    cleaning_beam_agent-1_max: 395
    cleaning_beam_agent-1_mean: 222.06
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 548
    cleaning_beam_agent-2_mean: 403.33
    cleaning_beam_agent-2_min: 264
    cleaning_beam_agent-3_max: 53
    cleaning_beam_agent-3_mean: 10.1
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 524
    cleaning_beam_agent-4_mean: 443.58
    cleaning_beam_agent-4_min: 179
    cleaning_beam_agent-5_max: 244
    cleaning_beam_agent-5_mean: 17.73
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_16-55-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1252.9999999999923
  episode_reward_mean: 1076.5999999999935
  episode_reward_min: 377.0000000000038
  episodes_this_iter: 96
  episodes_total: 42624
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu131.cluster.local
  info:
    grad_time_ms: 21400.873
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0435315370559692
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019761871080845594
        model: {}
        policy_loss: -0.00319790281355381
        total_loss: -0.0026956703513860703
        vf_explained_var: 0.0298444926738739
        vf_loss: 22.89441680908203
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1527677774429321
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016948661068454385
        model: {}
        policy_loss: -0.003851943649351597
        total_loss: -0.003413305152207613
        vf_explained_var: -0.025047123432159424
        vf_loss: 24.251399993896484
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0866878032684326
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016332519007846713
        model: {}
        policy_loss: -0.003511910093948245
        total_loss: -0.0029883556999266148
        vf_explained_var: -0.013670921325683594
        vf_loss: 23.95292091369629
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3354078233242035
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008709829417057335
        model: {}
        policy_loss: -0.0020422390662133694
        total_loss: -0.00046818135888315737
        vf_explained_var: 0.092394158244133
        vf_loss: 21.426002502441406
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9487155079841614
        entropy_coeff: 0.0017600000137463212
        kl: 0.001815869938582182
        model: {}
        policy_loss: -0.0034834067337214947
        total_loss: -0.0028334767557680607
        vf_explained_var: 0.03667518496513367
        vf_loss: 22.742738723754883
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4621541500091553
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012854640372097492
        model: {}
        policy_loss: -0.0024000098928809166
        total_loss: -0.0010240748524665833
        vf_explained_var: 0.08789223432540894
        vf_loss: 21.571924209594727
    load_time_ms: 70179.238
    num_steps_sampled: 42624000
    num_steps_trained: 42624000
    sample_time_ms: 142320.384
    update_time_ms: 1048.686
  iterations_since_restore: 4
  node_ip: 172.17.8.131
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.232335329341318
    ram_util_percent: 18.697904191616765
  pid: 38704
  policy_reward_max:
    agent-0: 208.83333333333294
    agent-1: 208.83333333333294
    agent-2: 208.83333333333294
    agent-3: 208.83333333333294
    agent-4: 208.83333333333294
    agent-5: 208.83333333333294
  policy_reward_mean:
    agent-0: 179.43333333333314
    agent-1: 179.43333333333314
    agent-2: 179.43333333333314
    agent-3: 179.43333333333314
    agent-4: 179.43333333333314
    agent-5: 179.43333333333314
  policy_reward_min:
    agent-0: 62.833333333333165
    agent-1: 62.833333333333165
    agent-2: 62.833333333333165
    agent-3: 62.833333333333165
    agent-4: 62.833333333333165
    agent-5: 62.833333333333165
  sampler_perf:
    mean_env_wait_ms: 26.972730933009462
    mean_inference_ms: 13.277297745354156
    mean_processing_ms: 54.20491893031732
  time_since_restore: 950.2690930366516
  time_this_iter_s: 234.33475375175476
  time_total_s: 70143.75090551376
  timestamp: 1637099737
  timesteps_since_restore: 384000
  timesteps_this_iter: 96000
  timesteps_total: 42624000
  training_iteration: 444
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 32.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.131:38704 |    444 |          70143.8 | 42624000 |   1076.6 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=38704)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbc8255f518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 1.51
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 28.02
    apples_agent-1_min: 0
    apples_agent-2_max: 261
    apples_agent-2_mean: 6.03
    apples_agent-2_min: 0
    apples_agent-3_max: 323
    apples_agent-3_mean: 95.67
    apples_agent-3_min: 51
    apples_agent-4_max: 36
    apples_agent-4_mean: 0.47
    apples_agent-4_min: 0
    apples_agent-5_max: 125
    apples_agent-5_mean: 76.23
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 496
    cleaning_beam_agent-0_mean: 380.12
    cleaning_beam_agent-0_min: 194
    cleaning_beam_agent-1_max: 388
    cleaning_beam_agent-1_mean: 229.12
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 622
    cleaning_beam_agent-2_mean: 416.42
    cleaning_beam_agent-2_min: 201
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 7.45
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 526
    cleaning_beam_agent-4_mean: 441.43
    cleaning_beam_agent-4_min: 307
    cleaning_beam_agent-5_max: 100
    cleaning_beam_agent-5_mean: 11.85
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_16-59-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1272.9999999999966
  episode_reward_mean: 1108.4199999999948
  episode_reward_min: 385.0000000000066
  episodes_this_iter: 96
  episodes_total: 42720
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu131.cluster.local
  info:
    grad_time_ms: 20900.301
    learner:
      agent-0:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 1.039928674697876
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018488398054614663
        model: {}
        policy_loss: -0.0032590841874480247
        total_loss: -0.0027214298024773598
        vf_explained_var: 0.015533611178398132
        vf_loss: 23.44817352294922
      agent-1:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1586610078811646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014424939872696996
        model: {}
        policy_loss: -0.0037505270447582006
        total_loss: -0.0032826687674969435
        vf_explained_var: -0.032623738050460815
        vf_loss: 24.890724182128906
      agent-2:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0756516456604004
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017034104093909264
        model: {}
        policy_loss: -0.003480621613562107
        total_loss: -0.0029312828555703163
        vf_explained_var: -0.0045881569385528564
        vf_loss: 24.211925506591797
      agent-3:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3285575211048126
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007006879895925522
        model: {}
        policy_loss: -0.0017194445244967937
        total_loss: -7.810443639755249e-05
        vf_explained_var: 0.07756252586841583
        vf_loss: 22.108444213867188
      agent-4:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9490585327148438
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015872371150180697
        model: {}
        policy_loss: -0.0033873755019158125
        total_loss: -0.002792306710034609
        vf_explained_var: 0.06487773358821869
        vf_loss: 22.455738067626953
      agent-5:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4487927556037903
        entropy_coeff: 0.0017600000137463212
        kl: 0.000987231032922864
        model: {}
        policy_loss: -0.002296768594533205
        total_loss: -0.0009469185024499893
        vf_explained_var: 0.11674882471561432
        vf_loss: 21.27388572692871
    load_time_ms: 68322.978
    num_steps_sampled: 42720000
    num_steps_trained: 42720000
    sample_time_ms: 146082.242
    update_time_ms: 849.96
  iterations_since_restore: 5
  node_ip: 172.17.8.131
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.15261627906977
    ram_util_percent: 18.590988372093022
  pid: 38704
  policy_reward_max:
    agent-0: 212.16666666666652
    agent-1: 212.16666666666652
    agent-2: 212.16666666666652
    agent-3: 212.16666666666652
    agent-4: 212.16666666666652
    agent-5: 212.16666666666652
  policy_reward_mean:
    agent-0: 184.73666666666654
    agent-1: 184.73666666666654
    agent-2: 184.73666666666654
    agent-3: 184.73666666666654
    agent-4: 184.73666666666654
    agent-5: 184.73666666666654
  policy_reward_min:
    agent-0: 64.16666666666644
    agent-1: 64.16666666666644
    agent-2: 64.16666666666644
    agent-3: 64.16666666666644
    agent-4: 64.16666666666644
    agent-5: 64.16666666666644
  sampler_perf:
    mean_env_wait_ms: 26.797542512600945
    mean_inference_ms: 13.07801188935454
    mean_processing_ms: 53.88561681376698
  time_since_restore: 1191.3838229179382
  time_this_iter_s: 241.11472988128662
  time_total_s: 70384.86563539505
  timestamp: 1637099978
  timesteps_since_restore: 480000
  timesteps_this_iter: 96000
  timesteps_total: 42720000
  training_iteration: 445
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.131:38704 |    445 |          70384.9 | 42720000 |  1108.42 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=38704)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbc8255f518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 1.63
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 25.01
    apples_agent-1_min: 0
    apples_agent-2_max: 261
    apples_agent-2_mean: 5.66
    apples_agent-2_min: 0
    apples_agent-3_max: 323
    apples_agent-3_mean: 97.05
    apples_agent-3_min: 47
    apples_agent-4_max: 81
    apples_agent-4_mean: 4.57
    apples_agent-4_min: 0
    apples_agent-5_max: 143
    apples_agent-5_mean: 73.03
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 492
    cleaning_beam_agent-0_mean: 382.11
    cleaning_beam_agent-0_min: 265
    cleaning_beam_agent-1_max: 398
    cleaning_beam_agent-1_mean: 229.34
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 548
    cleaning_beam_agent-2_mean: 397.16
    cleaning_beam_agent-2_min: 201
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 9.77
    cleaning_beam_agent-3_min: 1
    cleaning_beam_agent-4_max: 545
    cleaning_beam_agent-4_mean: 425.35
    cleaning_beam_agent-4_min: 250
    cleaning_beam_agent-5_max: 117
    cleaning_beam_agent-5_mean: 19.68
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_17-03-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1259.0000000000064
  episode_reward_mean: 1065.4799999999932
  episode_reward_min: 427.99999999999915
  episodes_this_iter: 96
  episodes_total: 42816
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu131.cluster.local
  info:
    grad_time_ms: 20571.308
    learner:
      agent-0:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 1.029477596282959
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022787796333432198
        model: {}
        policy_loss: -0.0032413932494819164
        total_loss: -0.002438182942569256
        vf_explained_var: 0.02930028736591339
        vf_loss: 26.008506774902344
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1375317573547363
        entropy_coeff: 0.0017600000137463212
        kl: 0.001438248553313315
        model: {}
        policy_loss: -0.0038773345295339823
        total_loss: -0.0031045833602547646
        vf_explained_var: -0.03896011412143707
        vf_loss: 27.6582088470459
      agent-2:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1030535697937012
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016498336335644126
        model: {}
        policy_loss: -0.0033483433071523905
        total_loss: -0.0026254570111632347
        vf_explained_var: 0.003698289394378662
        vf_loss: 26.53948402404785
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.351483553647995
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011022691614925861
        model: {}
        policy_loss: -0.0021732430905103683
        total_loss: -0.0004992997273802757
        vf_explained_var: 0.143247589468956
        vf_loss: 22.856700897216797
      agent-4:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9552146196365356
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016817896394059062
        model: {}
        policy_loss: -0.0039863744750618935
        total_loss: -0.0032325575593858957
        vf_explained_var: 0.09043958783149719
        vf_loss: 24.24486541748047
      agent-5:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4692063331604004
        entropy_coeff: 0.0017600000137463212
        kl: 0.001312124659307301
        model: {}
        policy_loss: -0.0025315112434327602
        total_loss: -0.0010823984630405903
        vf_explained_var: 0.14928843080997467
        vf_loss: 22.667171478271484
    load_time_ms: 65941.136
    num_steps_sampled: 42816000
    num_steps_trained: 42816000
    sample_time_ms: 145324.106
    update_time_ms: 720.771
  iterations_since_restore: 6
  node_ip: 172.17.8.131
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.87508196721312
    ram_util_percent: 18.552131147540983
  pid: 38704
  policy_reward_max:
    agent-0: 209.833333333333
    agent-1: 209.833333333333
    agent-2: 209.833333333333
    agent-3: 209.833333333333
    agent-4: 209.833333333333
    agent-5: 209.833333333333
  policy_reward_mean:
    agent-0: 177.57999999999984
    agent-1: 177.57999999999984
    agent-2: 177.57999999999984
    agent-3: 177.57999999999984
    agent-4: 177.57999999999984
    agent-5: 177.57999999999984
  policy_reward_min:
    agent-0: 71.33333333333336
    agent-1: 71.33333333333336
    agent-2: 71.33333333333336
    agent-3: 71.33333333333336
    agent-4: 71.33333333333336
    agent-5: 71.33333333333336
  sampler_perf:
    mean_env_wait_ms: 26.65732005049426
    mean_inference_ms: 12.939514119514296
    mean_processing_ms: 53.648719740105236
  time_since_restore: 1406.0412349700928
  time_this_iter_s: 214.65741205215454
  time_total_s: 70599.5230474472
  timestamp: 1637100193
  timesteps_since_restore: 576000
  timesteps_this_iter: 96000
  timesteps_total: 42816000
  training_iteration: 446
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.131:38704 |    446 |          70599.5 | 42816000 |  1065.48 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[36m(pid=38704)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbc8255f518> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 1.28
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 25.1
    apples_agent-1_min: 0
    apples_agent-2_max: 149
    apples_agent-2_mean: 4.91
    apples_agent-2_min: 0
    apples_agent-3_max: 146
    apples_agent-3_mean: 91.16
    apples_agent-3_min: 37
    apples_agent-4_max: 70
    apples_agent-4_mean: 3.84
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 70.95
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 489
    cleaning_beam_agent-0_mean: 371.87
    cleaning_beam_agent-0_min: 199
    cleaning_beam_agent-1_max: 410
    cleaning_beam_agent-1_mean: 236.17
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 528
    cleaning_beam_agent-2_mean: 393.91
    cleaning_beam_agent-2_min: 158
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 7.79
    cleaning_beam_agent-3_min: 0
    cleaning_beam_agent-4_max: 535
    cleaning_beam_agent-4_mean: 431.03
    cleaning_beam_agent-4_min: 222
    cleaning_beam_agent-5_max: 94
    cleaning_beam_agent-5_mean: 18.16
    cleaning_beam_agent-5_min: 0
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_17-06-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1254.000000000021
  episode_reward_mean: 1069.1199999999956
  episode_reward_min: 329.0000000000032
  episodes_this_iter: 96
  episodes_total: 42912
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu131.cluster.local
  info:
    grad_time_ms: 20347.269
    learner:
      agent-0:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 1.02738618850708
        entropy_coeff: 0.0017600000137463212
        kl: 0.001642429968342185
        model: {}
        policy_loss: -0.0030884945299476385
        total_loss: -0.0022041001357138157
        vf_explained_var: 0.01719975471496582
        vf_loss: 26.874597549438477
      agent-1:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1325993537902832
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016096741892397404
        model: {}
        policy_loss: -0.0040350803174078465
        total_loss: -0.003170357085764408
        vf_explained_var: -0.04139408469200134
        vf_loss: 28.53069305419922
      agent-2:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1089870929718018
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016936870524659753
        model: {}
        policy_loss: -0.003579101525247097
        total_loss: -0.0028241174295544624
        vf_explained_var: 0.015555113554000854
        vf_loss: 27.015090942382812
      agent-3:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.3364604711532593
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008908822201192379
        model: {}
        policy_loss: -0.0020540482364594936
        total_loss: -0.00032831350108608603
        vf_explained_var: 0.1537579447031021
        vf_loss: 23.151254653930664
      agent-4:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9542511701583862
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010194960050284863
        model: {}
        policy_loss: -0.003251108340919018
        total_loss: -0.0024766158312559128
        vf_explained_var: 0.10533498227596283
        vf_loss: 24.507898330688477
      agent-5:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45677947998046875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009937507566064596
        model: {}
        policy_loss: -0.002797058317810297
        total_loss: -0.0013694800436496735
        vf_explained_var: 0.1850665658712387
        vf_loss: 22.2840518951416
    load_time_ms: 64376.28
    num_steps_sampled: 42912000
    num_steps_trained: 42912000
    sample_time_ms: 144656.432
    update_time_ms: 628.971
  iterations_since_restore: 7
  node_ip: 172.17.8.131
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.19967426710098
    ram_util_percent: 18.470358306188924
  pid: 38704
  policy_reward_max:
    agent-0: 208.99999999999994
    agent-1: 208.99999999999994
    agent-2: 208.99999999999994
    agent-3: 208.99999999999994
    agent-4: 208.99999999999994
    agent-5: 208.99999999999994
  policy_reward_mean:
    agent-0: 178.18666666666647
    agent-1: 178.18666666666647
    agent-2: 178.18666666666647
    agent-3: 178.18666666666647
    agent-4: 178.18666666666647
    agent-5: 178.18666666666647
  policy_reward_min:
    agent-0: 54.83333333333322
    agent-1: 54.83333333333322
    agent-2: 54.83333333333322
    agent-3: 54.83333333333322
    agent-4: 54.83333333333322
    agent-5: 54.83333333333322
  sampler_perf:
    mean_env_wait_ms: 26.551707471960007
    mean_inference_ms: 12.841761673719605
    mean_processing_ms: 53.47899499204893
  time_since_restore: 1620.8578658103943
  time_this_iter_s: 214.8166308403015
  time_total_s: 70814.3396782875
  timestamp: 1637100408
  timesteps_since_restore: 672000
  timesteps_this_iter: 96000
  timesteps_total: 42912000
  training_iteration: 447
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/40 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.01 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc                |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+--------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.131:38704 |    447 |          70814.3 | 42912000 |  1069.12 |
+--------------------------------------+----------+--------------------+--------+------------------+----------+----------+


[2m[33m(pid=raylet)[0m F1116 17:09:42.527376 38570 node_manager.cc:559]  Check failed: node_id != self_node_id_ Exiting because this node manager has mistakenly been marked dead by the monitor.
[2m[33m(pid=raylet)[0m *** Check failure stack trace: ***
[2m[33m(pid=raylet)[0m     @     0x5559b663676d  google::LogMessage::Fail()
[2m[33m(pid=raylet)[0m     @     0x5559b6637bdc  google::LogMessage::SendToLog()
[2m[33m(pid=raylet)[0m     @     0x5559b6636449  google::LogMessage::Flush()
[2m[33m(pid=raylet)[0m     @     0x5559b6636661  google::LogMessage::~LogMessage()
[2m[33m(pid=raylet)[0m     @     0x5559b62f7029  ray::RayLog::~RayLog()
[2m[33m(pid=raylet)[0m     @     0x5559b60d6810  ray::raylet::NodeManager::NodeRemoved()
[2m[33m(pid=raylet)[0m     @     0x5559b60d69ec  _ZNSt17_Function_handlerIFvRKN3ray8ClientIDERKNS0_3rpc11GcsNodeInfoEEZNS0_6raylet11NodeManager11RegisterGcsEvEUlS3_S7_E0_E9_M_invokeERKSt9_Any_dataS3_S7_
[2m[33m(pid=raylet)[0m     @     0x5559b6190682  ray::gcs::ClientTable::HandleNotification()
[2m[33m(pid=raylet)[0m     @     0x5559b6190b6b  _ZNSt17_Function_handlerIFvPN3ray3gcs14RedisGcsClientERKNS0_8ClientIDERKSt6vectorINS0_3rpc11GcsNodeInfoESaIS9_EEEZNS1_11ClientTable21SubscribeToNodeChangeERKSt8functionIFvS6_RKS9_EERKSG_IFvNS0_6StatusEEEEUlS3_RKNS0_8UniqueIDESD_E_E9_M_invokeERKSt9_Any_dataS3_S6_SD_
[2m[33m(pid=raylet)[0m     @     0x5559b6193908  _ZNSt17_Function_handlerIFvPN3ray3gcs14RedisGcsClientERKNS0_8ClientIDENS0_3rpc13GcsChangeModeERKSt6vectorINS7_11GcsNodeInfoESaISA_EEEZNS1_3LogIS4_SA_E9SubscribeERKNS0_5JobIDES6_RKSt8functionIFvS3_S6_SE_EERKSL_IFvS3_EEEUlS3_S6_S8_SE_E_E9_M_invokeERKSt9_Any_dataS3_S6_S8_SE_
[2m[33m(pid=raylet)[0m     @     0x5559b619118e  _ZNSt17_Function_handlerIFvSt10shared_ptrIN3ray3gcs13CallbackReplyEEEZNS2_3LogINS1_8ClientIDENS1_3rpc11GcsNodeInfoEE9SubscribeERKNS1_5JobIDERKS7_RKSt8functionIFvPNS2_14RedisGcsClientESF_NS8_13GcsChangeModeERKSt6vectorIS9_SaIS9_EEEERKSG_IFvSI_EEEUlS4_E_E9_M_invokeERKSt9_Any_dataS4_
[2m[33m(pid=raylet)[0m     @     0x5559b616dd0b  _ZN5boost4asio6detail18completion_handlerIZN3ray3gcs20RedisCallbackManager12CallbackItem8DispatchERSt10shared_ptrINS4_13CallbackReplyEEEUlvE_E11do_completeEPvPNS1_19scheduler_operationERKNS_6system10error_codeEm
[2m[33m(pid=raylet)[0m     @     0x5559b65c841f  boost::asio::detail::scheduler::do_run_one()
[2m[33m(pid=raylet)[0m     @     0x5559b65c9921  boost::asio::detail::scheduler::run()
[2m[33m(pid=raylet)[0m     @     0x5559b65ca7c2  boost::asio::io_context::run()
[2m[33m(pid=raylet)[0m     @     0x5559b6046669  main
[2m[33m(pid=raylet)[0m     @     0x7f318810bbf7  __libc_start_main
[2m[33m(pid=raylet)[0m     @     0x5559b6057331  (unknown)
[2m[36m(pid=38704)[0m E1116 17:09:50.081598 39465 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38673)[0m E1116 17:09:49.775739 38850 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38664)[0m E1116 17:09:49.854336 38901 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38724)[0m E1116 17:09:50.080438 39459 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38684)[0m E1116 17:09:50.080706 39460 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38713)[0m E1116 17:09:50.038048 39371 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38702)[0m E1116 17:09:50.081177 39463 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38654)[0m E1116 17:09:49.836818 38883 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38667)[0m E1116 17:09:49.835371 38879 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38660)[0m E1116 17:09:49.842046 38887 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38675)[0m E1116 17:09:49.838119 38884 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38658)[0m E1116 17:09:49.851778 38899 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38656)[0m E1116 17:09:49.929544 39001 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38648)[0m E1116 17:09:49.931416 39012 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38679)[0m E1116 17:09:49.936056 39027 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38677)[0m E1116 17:09:49.931469 39013 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38670)[0m E1116 17:09:49.936111 39028 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38696)[0m E1116 17:09:49.936154 39029 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38682)[0m E1116 17:09:49.939843 39044 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38700)[0m E1116 17:09:49.936298 39030 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38669)[0m E1116 17:09:50.079442 39454 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38710)[0m E1116 17:09:50.077366 39448 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38690)[0m E1116 17:09:50.083505 39467 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38666)[0m E1116 17:09:50.078989 39452 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38688)[0m E1116 17:09:50.077811 39451 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38686)[0m E1116 17:09:50.080868 39461 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38715)[0m E1116 17:09:50.079866 39456 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38720)[0m E1116 17:09:50.038380 39369 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38652)[0m E1116 17:09:50.080009 39457 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38699)[0m E1116 17:09:50.081315 39464 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38717)[0m E1116 17:09:50.038049 39373 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38671)[0m E1116 17:09:50.038194 39367 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38662)[0m E1116 17:09:50.038162 39366 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38707)[0m E1116 17:09:50.039517 39375 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38692)[0m E1116 17:09:50.079205 39453 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38650)[0m E1116 17:09:50.080271 39458 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38694)[0m E1116 17:09:50.036998 39350 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38718)[0m E1116 17:09:50.079597 39455 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38706)[0m E1116 17:09:50.038295 39370 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38722)[0m E1116 17:09:50.038136 39368 core_worker.cc:646] Raylet failed. Shutting down.
[2m[36m(pid=38704)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fbc8255f518> -> 0 episodes
[2m[36m(pid=38704)[0m 2021-11-16 17:18:50,328	WARNING metrics.py:70 -- WARNING: collected no metrics in 180 seconds
