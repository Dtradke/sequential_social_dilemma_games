 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 17:39:23,402	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.23 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 17:39:23,699	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21028261888 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
slurmstepd: error: *** JOB 5006748 ON gpu147 CANCELLED AT 2021-11-17T18:46:55 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 18:49:06,957	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.99 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 18:49:07,241	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21459050496 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 18:49:07,939	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 18:49:08,078	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 18:49:08,078	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 18:49:08,229	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006748 ON gpu005 CANCELLED AT 2021-11-17T19:49:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 19:52:04,816	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.17 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 19:52:05,082	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21168754688 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 19:52:05,805	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 19:52:05,985	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 19:52:05,985	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 19:52:06,163	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006748 ON gpu051 CANCELLED AT 2021-11-17T20:52:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 20:55:08,136	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 44.51 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 20:55:08,405	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21474070528 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 20:55:09,357	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 20:55:09,710	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 20:55:09,710	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 20:55:10,041	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-17 21:18:09,157	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/experiment_state-2021-11-17_20-55-09.json'
slurmstepd: error: *** JOB 5006748 ON gpu013 CANCELLED AT 2021-11-17T21:56:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 21:59:05,599	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 50.2 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 21:59:05,862	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21168623616 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 21:59:06,676	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 21:59:06,969	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 21:59:06,969	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 21:59:07,271	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006748 ON gpu051 CANCELLED AT 2021-11-17T22:59:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 23:02:07,122	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.34 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 23:02:07,422	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21304827904 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 23:02:08,417	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 23:02:08,935	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 23:02:08,935	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 23:02:09,379	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006748 ON gpu008 CANCELLED AT 2021-11-18T00:04:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 00:08:08,130	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 45.81 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 00:08:08,447	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21411115008 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 00:08:09,368	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 00:08:09,814	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 00:08:09,814	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 00:08:10,232	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006748 ON gpu036 CANCELLED AT 2021-11-18T01:12:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 01:15:05,374	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.84 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 01:15:05,640	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21406953472 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 01:15:06,572	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 01:15:06,981	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 01:15:06,982	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 01:15:07,422	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006748 ON gpu014 CANCELLED AT 2021-11-18T02:16:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 02:19:05,416	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.62 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 02:19:05,684	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21454835712 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 02:19:06,660	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 02:19:07,150	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 02:19:07,151	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 02:19:07,619	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-18 02:49:35,037	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.6027388572692871 seconds to complete, which may be a performance bottleneck.
2021-11-18 04:03:33,102	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/experiment_state-2021-11-18_02-19-07.json'
2021-11-18 04:26:48,265	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/experiment_state-2021-11-18_02-19-07.json'
2021-11-18 05:26:04,113	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5120477676391602 seconds to complete, which may be a performance bottleneck.
2021-11-18 06:48:24,376	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5603830814361572 seconds to complete, which may be a performance bottleneck.
2021-11-18 08:06:37,651	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.575474739074707 seconds to complete, which may be a performance bottleneck.
2021-11-18 08:21:23,738	WARNING util.py:137 -- The `process_trial_save` operation took 0.5068776607513428 seconds to complete, which may be a performance bottleneck.
2021-11-18 08:21:23,739	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-18 08:42:30,774	WARNING util.py:137 -- The `process_trial_save` operation took 0.6990797519683838 seconds to complete, which may be a performance bottleneck.
2021-11-18 08:42:30,774	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-18 09:03:43,559	WARNING util.py:137 -- The `process_trial_save` operation took 0.5438895225524902 seconds to complete, which may be a performance bottleneck.
2021-11-18 09:03:43,560	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-18 09:24:55,394	WARNING util.py:137 -- The `process_trial_save` operation took 0.557795524597168 seconds to complete, which may be a performance bottleneck.
2021-11-18 09:24:55,394	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 5006748 ON gpu011 CANCELLED AT 2021-11-18T09:36:56 ***
