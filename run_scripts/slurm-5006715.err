 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 17:38:55,564	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.0 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 17:38:55,850	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 18499690496 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
slurmstepd: error: *** JOB 5006715 ON gpu024 CANCELLED AT 2021-11-17T18:39:22 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 18:41:50,464	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.82 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 18:41:50,824	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 20750696448 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 18:41:51,543	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 18:41:51,706	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 18:41:51,706	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 18:41:51,831	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006715 ON gpu058 CANCELLED AT 2021-11-17T19:43:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 19:46:08,107	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.78 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 19:46:08,375	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21471625216 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 19:46:09,199	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 19:46:09,448	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 19:46:09,449	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 19:46:09,687	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006715 ON gpu004 CANCELLED AT 2021-11-17T20:46:01 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 20:49:07,752	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 45.25 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 20:49:08,054	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 18202406912 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 20:49:08,901	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 20:49:09,276	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 20:49:09,276	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 20:49:09,571	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006715 ON gpu124 CANCELLED AT 2021-11-17T21:49:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 21:52:07,237	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 54.13 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 21:52:07,505	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21425614848 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 21:52:08,382	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 21:52:08,792	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 21:52:08,793	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 21:52:09,281	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006715 ON gpu049 CANCELLED AT 2021-11-17T22:52:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 22:55:08,678	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 50.3 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 22:55:09,007	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21340205056 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 22:55:09,994	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 22:55:10,628	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 22:55:10,628	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 22:55:11,055	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006715 ON gpu072 CANCELLED AT 2021-11-17T23:57:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 00:00:08,023	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.49 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 00:00:08,318	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21334913024 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 00:00:09,246	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 00:00:09,623	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 00:00:09,624	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 00:00:09,990	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006715 ON gpu014 CANCELLED AT 2021-11-18T01:10:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 01:13:07,057	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.99 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 01:13:07,327	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21471653888 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 01:13:08,438	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 01:13:09,165	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 01:13:09,165	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 01:13:09,477	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006715 ON gpu003 CANCELLED AT 2021-11-18T02:13:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 02:16:07,487	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 49.01 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 02:16:07,766	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21367185408 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 02:16:08,782	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 02:16:09,271	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 02:16:09,272	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 02:16:09,756	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-18 03:47:09,491	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/experiment_state-2021-11-18_02-16-09.json'
2021-11-18 06:05:20,606	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.57108473777771 seconds to complete, which may be a performance bottleneck.
2021-11-18 06:47:03,438	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5918667316436768 seconds to complete, which may be a performance bottleneck.
2021-11-18 07:22:23,793	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5678048133850098 seconds to complete, which may be a performance bottleneck.
2021-11-18 07:57:48,293	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5764434337615967 seconds to complete, which may be a performance bottleneck.
2021-11-18 08:35:16,649	WARNING util.py:137 -- The `process_trial_save` operation took 0.6720728874206543 seconds to complete, which may be a performance bottleneck.
2021-11-18 08:35:16,650	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-18 08:54:01,459	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/experiment_state-2021-11-18_02-16-09.json'
2021-11-18 08:56:07,695	WARNING util.py:137 -- The `process_trial_save` operation took 0.5188436508178711 seconds to complete, which may be a performance bottleneck.
2021-11-18 08:56:07,696	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-18 09:12:47,154	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5766260623931885 seconds to complete, which may be a performance bottleneck.
2021-11-18 09:16:55,248	WARNING util.py:137 -- The `process_trial_save` operation took 0.5296564102172852 seconds to complete, which may be a performance bottleneck.
2021-11-18 09:16:55,248	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 5006715 ON gpu042 CANCELLED AT 2021-11-18T09:37:05 ***
