 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-15 17:00:20,566	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 55.42 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-15 17:00:20,854	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 15730991104 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-15 17:06:35,457	WARNING util.py:137 -- The `fetch_result` operation took 1.168508768081665 seconds to complete, which may be a performance bottleneck.
2021-11-15 17:06:36,789	WARNING util.py:137 -- The `process_trial` operation took 2.6330418586730957 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 4985096 ON gpu049 CANCELLED AT 2021-11-15T18:00:14 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-15 18:02:57,435	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.57 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-15 18:02:57,735	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 14716534784 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-15 18:02:58,495	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-15 18:02:58,686	INFO trial_runner.py:169 -- Resuming trial.
2021-11-15 18:02:58,686	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-15 18:02:58,822	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-15 18:08:37,572	WARNING util.py:137 -- The `fetch_result` operation took 2.036168336868286 seconds to complete, which may be a performance bottleneck.
2021-11-15 18:08:39,257	WARNING util.py:137 -- The `process_trial` operation took 3.7881999015808105 seconds to complete, which may be a performance bottleneck.
2021-11-15 18:08:43,205	WARNING util.py:137 -- The `experiment_checkpoint` operation took 3.940924882888794 seconds to complete, which may be a performance bottleneck.
2021-11-15 18:11:47,824	WARNING util.py:137 -- The `process_trial` operation took 0.6560838222503662 seconds to complete, which may be a performance bottleneck.
2021-11-15 19:15:59,483	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.8588604927062988 seconds to complete, which may be a performance bottleneck.
2021-11-15 19:16:00,052	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/experiment_state-2021-11-15_18-02-58.json'
2021-11-15 21:19:31,914	WARNING util.py:137 -- The `experiment_checkpoint` operation took 2.2626678943634033 seconds to complete, which may be a performance bottleneck.
2021-11-15 22:38:29,373	WARNING util.py:137 -- The `process_trial` operation took 13.973789930343628 seconds to complete, which may be a performance bottleneck.
2021-11-15 23:19:12,179	WARNING util.py:137 -- The `process_trial` operation took 0.5845246315002441 seconds to complete, which may be a performance bottleneck.
2021-11-15 23:54:38,505	WARNING util.py:137 -- The `process_trial_save` operation took 0.9250655174255371 seconds to complete, which may be a performance bottleneck.
2021-11-15 23:54:38,505	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-16 00:54:38,527	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/experiment_state-2021-11-15_18-02-58.json'
2021-11-16 01:44:44,779	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.9885900020599365 seconds to complete, which may be a performance bottleneck.
2021-11-16 02:24:54,717	WARNING util.py:137 -- The `experiment_checkpoint` operation took 21.816819429397583 seconds to complete, which may be a performance bottleneck.
2021-11-16 02:24:54,718	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/experiment_state-2021-11-15_18-02-58.json'
2021-11-16 06:40:17,591	WARNING util.py:137 -- The `experiment_checkpoint` operation took 13.9485023021698 seconds to complete, which may be a performance bottleneck.
2021-11-16 08:17:01,802	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/experiment_state-2021-11-15_18-02-58.json'
2021-11-16 08:34:52,342	WARNING util.py:137 -- The `process_trial_save` operation took 0.7710120677947998 seconds to complete, which may be a performance bottleneck.
2021-11-16 08:34:52,343	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-16 09:20:11,218	WARNING util.py:137 -- The `process_trial_save` operation took 1.5533833503723145 seconds to complete, which may be a performance bottleneck.
2021-11-16 09:20:11,219	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 4985096 ON gpu124 CANCELLED AT 2021-11-16T11:20:04 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-16 11:22:16,817	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 54.42 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-16 11:22:17,070	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21474836480 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-16 11:22:18,288	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-16 11:22:18,981	INFO trial_runner.py:169 -- Resuming trial.
2021-11-16 11:22:18,982	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-16 11:22:19,293	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-16 11:27:36,571	WARNING util.py:137 -- The `fetch_result` operation took 1.075711727142334 seconds to complete, which may be a performance bottleneck.
2021-11-16 11:27:37,386	WARNING util.py:137 -- The `process_trial` operation took 1.9707567691802979 seconds to complete, which may be a performance bottleneck.
2021-11-16 11:27:40,360	WARNING util.py:137 -- The `experiment_checkpoint` operation took 2.969090700149536 seconds to complete, which may be a performance bottleneck.
2021-11-16 12:17:28,794	WARNING util.py:137 -- The `process_trial` operation took 1.6355645656585693 seconds to complete, which may be a performance bottleneck.
2021-11-16 12:18:07,364	WARNING util.py:137 -- The `process_trial_save` operation took 34.696043968200684 seconds to complete, which may be a performance bottleneck.
2021-11-16 12:18:07,364	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-16 13:08:04,094	WARNING util.py:137 -- The `process_trial_save` operation took 0.7817020416259766 seconds to complete, which may be a performance bottleneck.
2021-11-16 13:08:04,094	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-16 13:53:11,715	WARNING util.py:137 -- The `process_trial_save` operation took 0.8159420490264893 seconds to complete, which may be a performance bottleneck.
2021-11-16 13:53:11,716	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-16 14:26:28,832	WARNING util.py:137 -- The `experiment_checkpoint` operation took 7.7067413330078125 seconds to complete, which may be a performance bottleneck.
2021-11-16 15:21:33,678	WARNING util.py:137 -- The `process_trial_save` operation took 0.9410080909729004 seconds to complete, which may be a performance bottleneck.
2021-11-16 15:21:33,679	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 4985096 ON gpu037 CANCELLED AT 2021-11-16T16:35:10 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-16 16:37:27,532	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.7 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-16 16:37:27,831	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21349416960 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-16 16:37:28,991	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-16 16:37:29,714	INFO trial_runner.py:169 -- Resuming trial.
2021-11-16 16:37:29,714	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-16 16:37:30,225	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-16 16:43:48,930	WARNING util.py:137 -- The `fetch_result` operation took 6.155066967010498 seconds to complete, which may be a performance bottleneck.
2021-11-16 16:43:54,197	WARNING util.py:137 -- The `process_trial` operation took 11.740225553512573 seconds to complete, which may be a performance bottleneck.
2021-11-16 16:44:12,936	WARNING util.py:137 -- The `experiment_checkpoint` operation took 18.715378046035767 seconds to complete, which may be a performance bottleneck.
2021-11-16 17:09:42,775	WARNING worker.py:1090 -- The node with node id 7376a5e49b004c4278db754d881cc952cceb52d9 has been marked dead because the detector has missed too many heartbeats from it.
E1116 17:15:40.149969 38460 raylet_client.cc:90] IOError: [RayletClient] Connection closed unexpectedly. [RayletClient] Failed to disconnect from raylet.
Traceback (most recent call last):
  File "train.py", line 432, in <module>
    run(parsed_args, experiment)
  File "train.py", line 403, in run
    reuse_actors=args.tune_hparams,
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/tune.py", line 325, in run
    runner.step()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 339, in step
    self._process_events()  # blocking
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 434, in _process_events
    trial = self.trial_executor.get_next_available_trial()  # blocking
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 406, in get_next_available_trial
    [result_id], _ = ray.wait(shuffled_results)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/worker.py", line 1648, in wait
    worker.current_task_id,
  File "python/ray/_raylet.pyx", line 821, in ray._raylet.CoreWorker.wait
  File "python/ray/_raylet.pyx", line 142, in ray._raylet.check_status
ray.exceptions.RayletError: The Raylet died with this message: [RayletClient] Connection closed unexpectedly.
