>>> RESTORING FROM:  /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-15_17-00-45_bf59qjr/checkpoint_80
== Status ==
Memory usage on this node: 21.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+---------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |     60 |           9077.4 | 5760000 |   408.57 |
+--------------------------------------+----------+-------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m 2021-11-15 20:42:01,791	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=30948)[0m 2021-11-15 20:42:01,808	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=30948)[0m 2021-11-15 20:44:01,055	INFO trainable.py:180 -- _setup took 119.264 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=30948)[0m 2021-11-15 20:44:01,055	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=30948)[0m 2021-11-15 20:44:01,055	WARNING util.py:37 -- Install gputil for GPU system monitoring.
[2m[36m(pid=30948)[0m 2021-11-15 20:44:04,575	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=30948)[0m 2021-11-15 20:44:04,575	INFO trainable.py:423 -- Restored on 172.17.8.43 from checkpoint: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics/BaselinePPOTrainer_cleanup_env_0_2021-11-15_17-00-45_bf59qjr/tmp3j5o8gfarestore_from_object/checkpoint-60
[2m[36m(pid=30948)[0m 2021-11-15 20:44:04,575	INFO trainable.py:430 -- Current state after restoring: {'_iteration': 60, '_timesteps_total': 5760000, '_time_total': 9126.011813879013, '_episodes_total': 5760}
== Status ==
Memory usage on this node: 31.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+--------+------------------+---------+----------+
| Trial name                           | status   | loc   |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |     60 |           9077.4 | 5760000 |   408.57 |
+--------------------------------------+----------+-------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 8.34375
    apples_agent-0_min: 0
    apples_agent-1_max: 144
    apples_agent-1_mean: 15.958333333333334
    apples_agent-1_min: 0
    apples_agent-2_max: 122
    apples_agent-2_mean: 15.604166666666666
    apples_agent-2_min: 0
    apples_agent-3_max: 153
    apples_agent-3_mean: 89.95833333333333
    apples_agent-3_min: 10
    apples_agent-4_max: 129
    apples_agent-4_mean: 2.7604166666666665
    apples_agent-4_min: 0
    apples_agent-5_max: 102
    apples_agent-5_mean: 59.572916666666664
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 399
    cleaning_beam_agent-0_mean: 264.2395833333333
    cleaning_beam_agent-0_min: 163
    cleaning_beam_agent-1_max: 559
    cleaning_beam_agent-1_mean: 339.1666666666667
    cleaning_beam_agent-1_min: 59
    cleaning_beam_agent-2_max: 613
    cleaning_beam_agent-2_mean: 311.2395833333333
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 69.4375
    cleaning_beam_agent-3_min: 32
    cleaning_beam_agent-4_max: 495
    cleaning_beam_agent-4_mean: 340.78125
    cleaning_beam_agent-4_min: 157
    cleaning_beam_agent-5_max: 153
    cleaning_beam_agent-5_mean: 63.760416666666664
    cleaning_beam_agent-5_min: 26
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.020833333333333332
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.052083333333333336
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.010416666666666666
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.010416666666666666
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.0625
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.0625
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-47-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 717.9999999999893
  episode_reward_mean: 453.8020833333397
  episode_reward_min: 138.00000000000017
  episodes_this_iter: 96
  episodes_total: 5856
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 25195.337
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0009005760075524449
        entropy: 1.366586685180664
        entropy_coeff: 0.0017600000137463212
        kl: 0.008129761554300785
        model: {}
        policy_loss: -0.01701544225215912
        total_loss: -0.016842033714056015
        vf_explained_var: 0.08655868470668793
        vf_loss: 9.52650260925293
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0009005760075524449
        entropy: 1.2762346267700195
        entropy_coeff: 0.0017600000137463212
        kl: 0.01132552046328783
        model: {}
        policy_loss: -0.024428915232419968
        total_loss: -0.023429986089468002
        vf_explained_var: 0.05963209271430969
        vf_loss: 9.799964904785156
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0009005760075524449
        entropy: 1.3287405967712402
        entropy_coeff: 0.0017600000137463212
        kl: 0.010241090320050716
        model: {}
        policy_loss: -0.02296048402786255
        total_loss: -0.022313561290502548
        vf_explained_var: 0.10058657824993134
        vf_loss: 9.372882843017578
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0009005760075524449
        entropy: 1.167936086654663
        entropy_coeff: 0.0017600000137463212
        kl: 0.00794209260493517
        model: {}
        policy_loss: -0.01511215977370739
        total_loss: -0.014743311330676079
        vf_explained_var: 0.19819499552249908
        vf_loss: 8.359963417053223
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0009005760075524449
        entropy: 1.2194924354553223
        entropy_coeff: 0.0017600000137463212
        kl: 0.009535647928714752
        model: {}
        policy_loss: -0.023692987859249115
        total_loss: -0.023001372814178467
        vf_explained_var: 0.10675959289073944
        vf_loss: 9.307963371276855
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0009005760075524449
        entropy: 1.286784291267395
        entropy_coeff: 0.0017600000137463212
        kl: 0.011390464380383492
        model: {}
        policy_loss: -0.026085641235113144
        total_loss: -0.02520935609936714
        vf_explained_var: 0.17301394045352936
        vf_loss: 8.629295349121094
    load_time_ms: 63796.914
    num_steps_sampled: 5856000
    num_steps_trained: 5856000
    sample_time_ms: 115661.397
    update_time_ms: 3367.387
  iterations_since_restore: 1
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.1794952681388
    ram_util_percent: 18.988643533123028
  pid: 30948
  policy_reward_max:
    agent-0: 119.66666666666701
    agent-1: 119.66666666666701
    agent-2: 119.66666666666701
    agent-3: 119.66666666666701
    agent-4: 119.66666666666701
    agent-5: 119.66666666666701
  policy_reward_mean:
    agent-0: 75.6336805555556
    agent-1: 75.6336805555556
    agent-2: 75.6336805555556
    agent-3: 75.6336805555556
    agent-4: 75.6336805555556
    agent-5: 75.6336805555556
  policy_reward_min:
    agent-0: 23.000000000000004
    agent-1: 23.000000000000004
    agent-2: 23.000000000000004
    agent-3: 23.000000000000004
    agent-4: 23.000000000000004
    agent-5: 23.000000000000004
  sampler_perf:
    mean_env_wait_ms: 30.209855163172808
    mean_inference_ms: 15.139726730255214
    mean_processing_ms: 65.9413305950133
  time_since_restore: 212.29094791412354
  time_this_iter_s: 212.29094791412354
  time_total_s: 9338.302761793137
  timestamp: 1637027263
  timesteps_since_restore: 96000
  timesteps_this_iter: 96000
  timesteps_total: 5856000
  training_iteration: 61
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     61 |           9338.3 | 5856000 |  453.802 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 8.87
    apples_agent-0_min: 0
    apples_agent-1_max: 128
    apples_agent-1_mean: 19.88
    apples_agent-1_min: 0
    apples_agent-2_max: 177
    apples_agent-2_mean: 15.08
    apples_agent-2_min: 0
    apples_agent-3_max: 197
    apples_agent-3_mean: 86.87
    apples_agent-3_min: 3
    apples_agent-4_max: 90
    apples_agent-4_mean: 5.85
    apples_agent-4_min: 0
    apples_agent-5_max: 101
    apples_agent-5_mean: 59.13
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 412
    cleaning_beam_agent-0_mean: 262.27
    cleaning_beam_agent-0_min: 147
    cleaning_beam_agent-1_max: 541
    cleaning_beam_agent-1_mean: 326.18
    cleaning_beam_agent-1_min: 164
    cleaning_beam_agent-2_max: 478
    cleaning_beam_agent-2_mean: 303.32
    cleaning_beam_agent-2_min: 102
    cleaning_beam_agent-3_max: 169
    cleaning_beam_agent-3_mean: 69.83
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 555
    cleaning_beam_agent-4_mean: 345.02
    cleaning_beam_agent-4_min: 121
    cleaning_beam_agent-5_max: 180
    cleaning_beam_agent-5_mean: 67.86
    cleaning_beam_agent-5_min: 24
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.1
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.1
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-51-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 728.999999999989
  episode_reward_mean: 464.0600000000045
  episode_reward_min: 135.00000000000097
  episodes_this_iter: 96
  episodes_total: 5952
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 18517.224
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008945856243371964
        entropy: 1.345367193222046
        entropy_coeff: 0.0017600000137463212
        kl: 0.008729715831577778
        model: {}
        policy_loss: -0.01849142462015152
        total_loss: -0.01802147552371025
        vf_explained_var: 0.08095350861549377
        vf_loss: 10.91849136352539
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008945856243371964
        entropy: 1.2735264301300049
        entropy_coeff: 0.0017600000137463212
        kl: 0.011176595464348793
        model: {}
        policy_loss: -0.027907101437449455
        total_loss: -0.026799900457262993
        vf_explained_var: 0.06371214985847473
        vf_loss: 11.132862091064453
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008945856243371964
        entropy: 1.346313714981079
        entropy_coeff: 0.0017600000137463212
        kl: 0.010082731954753399
        model: {}
        policy_loss: -0.02273767814040184
        total_loss: -0.022011714056134224
        vf_explained_var: 0.09220729768276215
        vf_loss: 10.789301872253418
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008945856243371964
        entropy: 1.1399335861206055
        entropy_coeff: 0.0017600000137463212
        kl: 0.008180136792361736
        model: {}
        policy_loss: -0.017080850899219513
        total_loss: -0.016501329839229584
        vf_explained_var: 0.2008703649044037
        vf_loss: 9.49774169921875
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008945856243371964
        entropy: 1.2268338203430176
        entropy_coeff: 0.0017600000137463212
        kl: 0.010592440143227577
        model: {}
        policy_loss: -0.02713620662689209
        total_loss: -0.02613556757569313
        vf_explained_var: 0.1237591803073883
        vf_loss: 10.413800239562988
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008945856243371964
        entropy: 1.3134644031524658
        entropy_coeff: 0.0017600000137463212
        kl: 0.011480966582894325
        model: {}
        policy_loss: -0.027162209153175354
        total_loss: -0.026171833276748657
        vf_explained_var: 0.15429648756980896
        vf_loss: 10.058799743652344
    load_time_ms: 55060.511
    num_steps_sampled: 5952000
    num_steps_trained: 5952000
    sample_time_ms: 126840.021
    update_time_ms: 1720.262
  iterations_since_restore: 2
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.67950530035336
    ram_util_percent: 19.99363957597173
  pid: 30948
  policy_reward_max:
    agent-0: 121.50000000000064
    agent-1: 121.50000000000064
    agent-2: 121.50000000000064
    agent-3: 121.50000000000064
    agent-4: 121.50000000000064
    agent-5: 121.50000000000064
  policy_reward_mean:
    agent-0: 77.34333333333343
    agent-1: 77.34333333333343
    agent-2: 77.34333333333343
    agent-3: 77.34333333333343
    agent-4: 77.34333333333343
    agent-5: 77.34333333333343
  policy_reward_min:
    agent-0: 22.50000000000002
    agent-1: 22.50000000000002
    agent-2: 22.50000000000002
    agent-3: 22.50000000000002
    agent-4: 22.50000000000002
    agent-5: 22.50000000000002
  sampler_perf:
    mean_env_wait_ms: 30.699163109895217
    mean_inference_ms: 14.633856741532895
    mean_processing_ms: 66.83187340720274
  time_since_restore: 408.71640157699585
  time_this_iter_s: 196.42545366287231
  time_total_s: 9534.728215456009
  timestamp: 1637027462
  timesteps_since_restore: 192000
  timesteps_this_iter: 96000
  timesteps_total: 5952000
  training_iteration: 62
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 36.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     62 |          9534.73 | 5952000 |   464.06 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 66
    apples_agent-0_mean: 5.1
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 21.12
    apples_agent-1_min: 0
    apples_agent-2_max: 130
    apples_agent-2_mean: 10.42
    apples_agent-2_min: 0
    apples_agent-3_max: 179
    apples_agent-3_mean: 92.52
    apples_agent-3_min: 25
    apples_agent-4_max: 137
    apples_agent-4_mean: 7.65
    apples_agent-4_min: 0
    apples_agent-5_max: 110
    apples_agent-5_mean: 66.04
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 440
    cleaning_beam_agent-0_mean: 303.39
    cleaning_beam_agent-0_min: 142
    cleaning_beam_agent-1_max: 587
    cleaning_beam_agent-1_mean: 290.84
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 491
    cleaning_beam_agent-2_mean: 327.3
    cleaning_beam_agent-2_min: 145
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 63.79
    cleaning_beam_agent-3_min: 31
    cleaning_beam_agent-4_max: 503
    cleaning_beam_agent-4_mean: 332.41
    cleaning_beam_agent-4_min: 163
    cleaning_beam_agent-5_max: 150
    cleaning_beam_agent-5_mean: 59.83
    cleaning_beam_agent-5_min: 25
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.09
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 10
    fire_beam_agent-5_mean: 0.14
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-54-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 743.9999999999898
  episode_reward_mean: 511.040000000005
  episode_reward_min: 163.9999999999996
  episodes_this_iter: 96
  episodes_total: 6048
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 16301.558
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008885951829142869
        entropy: 1.3929283618927002
        entropy_coeff: 0.0017600000137463212
        kl: 0.009194358251988888
        model: {}
        policy_loss: -0.02001000940799713
        total_loss: -0.019566567614674568
        vf_explained_var: 0.06105904281139374
        vf_loss: 10.561253547668457
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008885951829142869
        entropy: 1.3018547296524048
        entropy_coeff: 0.0017600000137463212
        kl: 0.011644205078482628
        model: {}
        policy_loss: -0.028124967589974403
        total_loss: -0.02702656015753746
        vf_explained_var: 0.05779518187046051
        vf_loss: 10.608339309692383
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008885951829142869
        entropy: 1.329505205154419
        entropy_coeff: 0.0017600000137463212
        kl: 0.011778661049902439
        model: {}
        policy_loss: -0.023914629593491554
        total_loss: -0.022884147241711617
        vf_explained_var: 0.09863528609275818
        vf_loss: 10.146793365478516
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008885951829142869
        entropy: 1.1079471111297607
        entropy_coeff: 0.0017600000137463212
        kl: 0.007918666116893291
        model: {}
        policy_loss: -0.01755591481924057
        total_loss: -0.017004115507006645
        vf_explained_var: 0.18364039063453674
        vf_loss: 9.180529594421387
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008885951829142869
        entropy: 1.2150983810424805
        entropy_coeff: 0.0017600000137463212
        kl: 0.011077392846345901
        model: {}
        policy_loss: -0.02773149125277996
        total_loss: -0.02663113921880722
        vf_explained_var: 0.09126932919025421
        vf_loss: 10.234503746032715
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008885951829142869
        entropy: 1.266677975654602
        entropy_coeff: 0.0017600000137463212
        kl: 0.011091156862676144
        model: {}
        policy_loss: -0.028313465416431427
        total_loss: -0.02731754072010517
        vf_explained_var: 0.10488662123680115
        vf_loss: 10.070449829101562
    load_time_ms: 47251.935
    num_steps_sampled: 6048000
    num_steps_trained: 6048000
    sample_time_ms: 129757.503
    update_time_ms: 1165.239
  iterations_since_restore: 3
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.971093749999998
    ram_util_percent: 19.728125
  pid: 30948
  policy_reward_max:
    agent-0: 124.00000000000041
    agent-1: 124.00000000000041
    agent-2: 124.00000000000041
    agent-3: 124.00000000000041
    agent-4: 124.00000000000041
    agent-5: 124.00000000000041
  policy_reward_mean:
    agent-0: 85.17333333333345
    agent-1: 85.17333333333345
    agent-2: 85.17333333333345
    agent-3: 85.17333333333345
    agent-4: 85.17333333333345
    agent-5: 85.17333333333345
  policy_reward_min:
    agent-0: 27.33333333333338
    agent-1: 27.33333333333338
    agent-2: 27.33333333333338
    agent-3: 27.33333333333338
    agent-4: 27.33333333333338
    agent-5: 27.33333333333338
  sampler_perf:
    mean_env_wait_ms: 30.89449879392835
    mean_inference_ms: 14.446835491875234
    mean_processing_ms: 67.02830454033533
  time_since_restore: 588.0571584701538
  time_this_iter_s: 179.34075689315796
  time_total_s: 9714.068972349167
  timestamp: 1637027641
  timesteps_since_restore: 288000
  timesteps_this_iter: 96000
  timesteps_total: 6048000
  training_iteration: 63
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     63 |          9714.07 | 6048000 |   511.04 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 78
    apples_agent-0_mean: 5.16
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 14.91
    apples_agent-1_min: 0
    apples_agent-2_max: 304
    apples_agent-2_mean: 22.97
    apples_agent-2_min: 0
    apples_agent-3_max: 186
    apples_agent-3_mean: 92.08
    apples_agent-3_min: 28
    apples_agent-4_max: 88
    apples_agent-4_mean: 4.71
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 62.22
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 460
    cleaning_beam_agent-0_mean: 304.92
    cleaning_beam_agent-0_min: 156
    cleaning_beam_agent-1_max: 551
    cleaning_beam_agent-1_mean: 301.15
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 464
    cleaning_beam_agent-2_mean: 277.25
    cleaning_beam_agent-2_min: 76
    cleaning_beam_agent-3_max: 208
    cleaning_beam_agent-3_mean: 60.73
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 475
    cleaning_beam_agent-4_mean: 315.64
    cleaning_beam_agent-4_min: 135
    cleaning_beam_agent-5_max: 195
    cleaning_beam_agent-5_mean: 58.29
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.09
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 3
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_20-57-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 722.9999999999832
  episode_reward_mean: 491.4000000000046
  episode_reward_min: 126.00000000000186
  episodes_this_iter: 96
  episodes_total: 6144
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 15206.898
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008826047996990383
        entropy: 1.362518310546875
        entropy_coeff: 0.0017600000137463212
        kl: 0.009559981524944305
        model: {}
        policy_loss: -0.020524153485894203
        total_loss: -0.019911710172891617
        vf_explained_var: 0.08420330286026001
        vf_loss: 10.984771728515625
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008826047996990383
        entropy: 1.2532989978790283
        entropy_coeff: 0.0017600000137463212
        kl: 0.012441600672900677
        model: {}
        policy_loss: -0.028480391949415207
        total_loss: -0.02704096958041191
        vf_explained_var: 0.03502383828163147
        vf_loss: 11.569093704223633
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008826047996990383
        entropy: 1.3457067012786865
        entropy_coeff: 0.0017600000137463212
        kl: 0.010767747648060322
        model: {}
        policy_loss: -0.02463068813085556
        total_loss: -0.023708844557404518
        vf_explained_var: 0.05186200141906738
        vf_loss: 11.367427825927734
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008826047996990383
        entropy: 1.0960478782653809
        entropy_coeff: 0.0017600000137463212
        kl: 0.008419709280133247
        model: {}
        policy_loss: -0.018311072140932083
        total_loss: -0.017582517117261887
        vf_explained_var: 0.18764054775238037
        vf_loss: 9.736543655395508
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008826047996990383
        entropy: 1.2266504764556885
        entropy_coeff: 0.0017600000137463212
        kl: 0.012005736120045185
        model: {}
        policy_loss: -0.028283417224884033
        total_loss: -0.02696864865720272
        vf_explained_var: 0.10563667118549347
        vf_loss: 10.725290298461914
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008826047996990383
        entropy: 1.243457317352295
        entropy_coeff: 0.0017600000137463212
        kl: 0.01098062563687563
        model: {}
        policy_loss: -0.028924478217959404
        total_loss: -0.027872508391737938
        vf_explained_var: 0.12900495529174805
        vf_loss: 10.4433012008667
    load_time_ms: 44830.035
    num_steps_sampled: 6144000
    num_steps_trained: 6144000
    sample_time_ms: 130992.896
    update_time_ms: 887.794
  iterations_since_restore: 4
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.087072243346007
    ram_util_percent: 19.480228136882126
  pid: 30948
  policy_reward_max:
    agent-0: 120.50000000000057
    agent-1: 120.50000000000057
    agent-2: 120.50000000000057
    agent-3: 120.50000000000057
    agent-4: 120.50000000000057
    agent-5: 120.50000000000057
  policy_reward_mean:
    agent-0: 81.9000000000001
    agent-1: 81.9000000000001
    agent-2: 81.9000000000001
    agent-3: 81.9000000000001
    agent-4: 81.9000000000001
    agent-5: 81.9000000000001
  policy_reward_min:
    agent-0: 20.999999999999986
    agent-1: 20.999999999999986
    agent-2: 20.999999999999986
    agent-3: 20.999999999999986
    agent-4: 20.999999999999986
    agent-5: 20.999999999999986
  sampler_perf:
    mean_env_wait_ms: 30.879411691537133
    mean_inference_ms: 14.36379614724603
    mean_processing_ms: 67.22095502312817
  time_since_restore: 772.416576385498
  time_this_iter_s: 184.35941791534424
  time_total_s: 9898.428390264511
  timestamp: 1637027826
  timesteps_since_restore: 384000
  timesteps_this_iter: 96000
  timesteps_total: 6144000
  training_iteration: 64
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     64 |          9898.43 | 6144000 |    491.4 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 5.01
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 20.33
    apples_agent-1_min: 0
    apples_agent-2_max: 304
    apples_agent-2_mean: 16.31
    apples_agent-2_min: 0
    apples_agent-3_max: 186
    apples_agent-3_mean: 90.31
    apples_agent-3_min: 23
    apples_agent-4_max: 100
    apples_agent-4_mean: 5.76
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 68.79
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 500
    cleaning_beam_agent-0_mean: 313.01
    cleaning_beam_agent-0_min: 169
    cleaning_beam_agent-1_max: 545
    cleaning_beam_agent-1_mean: 295.08
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 503
    cleaning_beam_agent-2_mean: 299.59
    cleaning_beam_agent-2_min: 126
    cleaning_beam_agent-3_max: 113
    cleaning_beam_agent-3_mean: 58.77
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 435
    cleaning_beam_agent-4_mean: 305.09
    cleaning_beam_agent-4_min: 92
    cleaning_beam_agent-5_max: 123
    cleaning_beam_agent-5_mean: 55.94
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.04
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-00-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 707.9999999999836
  episode_reward_mean: 505.81000000000546
  episode_reward_min: 126.00000000000186
  episodes_this_iter: 96
  episodes_total: 6240
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 14599.533
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008766144164837897
        entropy: 1.35598623752594
        entropy_coeff: 0.0017600000137463212
        kl: 0.009974569082260132
        model: {}
        policy_loss: -0.02072981186211109
        total_loss: -0.020116252824664116
        vf_explained_var: 0.07303836941719055
        vf_loss: 10.051836967468262
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008766144164837897
        entropy: 1.2368004322052002
        entropy_coeff: 0.0017600000137463212
        kl: 0.012829049490392208
        model: {}
        policy_loss: -0.02972550131380558
        total_loss: -0.028278125450015068
        vf_explained_var: 0.025841638445854187
        vf_loss: 10.583331108093262
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008766144164837897
        entropy: 1.3388158082962036
        entropy_coeff: 0.0017600000137463212
        kl: 0.011871583759784698
        model: {}
        policy_loss: -0.02607995830476284
        total_loss: -0.025049038231372833
        vf_explained_var: 0.06602701544761658
        vf_loss: 10.129240989685059
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008766144164837897
        entropy: 1.0842188596725464
        entropy_coeff: 0.0017600000137463212
        kl: 0.008835749700665474
        model: {}
        policy_loss: -0.020297767594456673
        total_loss: -0.019517147913575172
        vf_explained_var: 0.15027354657649994
        vf_loss: 9.216955184936523
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008766144164837897
        entropy: 1.2083590030670166
        entropy_coeff: 0.0017600000137463212
        kl: 0.011734388768672943
        model: {}
        policy_loss: -0.028736744076013565
        total_loss: -0.027587462216615677
        vf_explained_var: 0.14362362027168274
        vf_loss: 9.291156768798828
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008766144164837897
        entropy: 1.214726448059082
        entropy_coeff: 0.0017600000137463212
        kl: 0.012475598603487015
        model: {}
        policy_loss: -0.028332751244306564
        total_loss: -0.02703351527452469
        vf_explained_var: 0.13158655166625977
        vf_loss: 9.420328140258789
    load_time_ms: 43540.74
    num_steps_sampled: 6240000
    num_steps_trained: 6240000
    sample_time_ms: 131815.814
    update_time_ms: 720.724
  iterations_since_restore: 5
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.56679245283019
    ram_util_percent: 18.493207547169806
  pid: 30948
  policy_reward_max:
    agent-0: 118.00000000000047
    agent-1: 118.00000000000047
    agent-2: 118.00000000000047
    agent-3: 118.00000000000047
    agent-4: 118.00000000000047
    agent-5: 118.00000000000047
  policy_reward_mean:
    agent-0: 84.30166666666682
    agent-1: 84.30166666666682
    agent-2: 84.30166666666682
    agent-3: 84.30166666666682
    agent-4: 84.30166666666682
    agent-5: 84.30166666666682
  policy_reward_min:
    agent-0: 20.999999999999986
    agent-1: 20.999999999999986
    agent-2: 20.999999999999986
    agent-3: 20.999999999999986
    agent-4: 20.999999999999986
    agent-5: 20.999999999999986
  sampler_perf:
    mean_env_wait_ms: 30.96206298679104
    mean_inference_ms: 14.32183017366824
    mean_processing_ms: 67.41914800235216
  time_since_restore: 958.2484419345856
  time_this_iter_s: 185.83186554908752
  time_total_s: 10084.260255813599
  timestamp: 1637028013
  timesteps_since_restore: 480000
  timesteps_this_iter: 96000
  timesteps_total: 6240000
  training_iteration: 65
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     65 |          10084.3 | 6240000 |   505.81 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 3.47
    apples_agent-0_min: 0
    apples_agent-1_max: 134
    apples_agent-1_mean: 21.32
    apples_agent-1_min: 0
    apples_agent-2_max: 147
    apples_agent-2_mean: 13.87
    apples_agent-2_min: 0
    apples_agent-3_max: 172
    apples_agent-3_mean: 95.32
    apples_agent-3_min: 19
    apples_agent-4_max: 71
    apples_agent-4_mean: 5.49
    apples_agent-4_min: 0
    apples_agent-5_max: 120
    apples_agent-5_mean: 71.57
    apples_agent-5_min: 11
    cleaning_beam_agent-0_max: 498
    cleaning_beam_agent-0_mean: 322.5
    cleaning_beam_agent-0_min: 197
    cleaning_beam_agent-1_max: 595
    cleaning_beam_agent-1_mean: 284.33
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 554
    cleaning_beam_agent-2_mean: 303.08
    cleaning_beam_agent-2_min: 130
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 57.5
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 443
    cleaning_beam_agent-4_mean: 313.23
    cleaning_beam_agent-4_min: 121
    cleaning_beam_agent-5_max: 148
    cleaning_beam_agent-5_mean: 46.54
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-03-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 767.9999999999828
  episode_reward_mean: 524.4000000000024
  episode_reward_min: 87.00000000000044
  episodes_this_iter: 96
  episodes_total: 6336
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 14317.962
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008706239750608802
        entropy: 1.3907976150512695
        entropy_coeff: 0.0017600000137463212
        kl: 0.009571332484483719
        model: {}
        policy_loss: -0.02160780504345894
        total_loss: -0.020908571779727936
        vf_explained_var: 0.06502631306648254
        vf_loss: 12.327726364135742
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008706239750608802
        entropy: 1.2795246839523315
        entropy_coeff: 0.0017600000137463212
        kl: 0.012802296318113804
        model: {}
        policy_loss: -0.030116181820631027
        total_loss: -0.028565362095832825
        vf_explained_var: 0.057887569069862366
        vf_loss: 12.42320728302002
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008706239750608802
        entropy: 1.3441495895385742
        entropy_coeff: 0.0017600000137463212
        kl: 0.013037161901593208
        model: {}
        policy_loss: -0.027785886079072952
        total_loss: -0.02634546346962452
        vf_explained_var: 0.09082832932472229
        vf_loss: 11.986945152282715
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008706239750608802
        entropy: 1.063407063484192
        entropy_coeff: 0.0017600000137463212
        kl: 0.010088073089718819
        model: {}
        policy_loss: -0.020441677421331406
        total_loss: -0.019212637096643448
        vf_explained_var: 0.17873743176460266
        vf_loss: 10.830215454101562
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008706239750608802
        entropy: 1.2229313850402832
        entropy_coeff: 0.0017600000137463212
        kl: 0.014144260436296463
        model: {}
        policy_loss: -0.02767619676887989
        total_loss: -0.025855984538793564
        vf_explained_var: 0.13279522955417633
        vf_loss: 11.437210083007812
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008706239750608802
        entropy: 1.183804988861084
        entropy_coeff: 0.0017600000137463212
        kl: 0.0120753925293684
        model: {}
        policy_loss: -0.026892444118857384
        total_loss: -0.025479378178715706
        vf_explained_var: 0.17948830127716064
        vf_loss: 10.814836502075195
    load_time_ms: 42611.869
    num_steps_sampled: 6336000
    num_steps_trained: 6336000
    sample_time_ms: 131465.959
    update_time_ms: 608.843
  iterations_since_restore: 6
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.8031007751938
    ram_util_percent: 18.312403100775196
  pid: 30948
  policy_reward_max:
    agent-0: 128.00000000000054
    agent-1: 128.00000000000054
    agent-2: 128.00000000000054
    agent-3: 128.00000000000054
    agent-4: 128.00000000000054
    agent-5: 128.00000000000054
  policy_reward_mean:
    agent-0: 87.40000000000018
    agent-1: 87.40000000000018
    agent-2: 87.40000000000018
    agent-3: 87.40000000000018
    agent-4: 87.40000000000018
    agent-5: 87.40000000000018
  policy_reward_min:
    agent-0: 14.49999999999998
    agent-1: 14.49999999999998
    agent-2: 14.49999999999998
    agent-3: 14.49999999999998
    agent-4: 14.49999999999998
    agent-5: 14.49999999999998
  sampler_perf:
    mean_env_wait_ms: 30.998275260703892
    mean_inference_ms: 14.286562606098155
    mean_processing_ms: 67.50989902337771
  time_since_restore: 1139.0110702514648
  time_this_iter_s: 180.76262831687927
  time_total_s: 10265.022884130478
  timestamp: 1637028193
  timesteps_since_restore: 576000
  timesteps_this_iter: 96000
  timesteps_total: 6336000
  training_iteration: 66
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     66 |            10265 | 6336000 |    524.4 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 6.48
    apples_agent-0_min: 0
    apples_agent-1_max: 74
    apples_agent-1_mean: 16.11
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 11.83
    apples_agent-2_min: 0
    apples_agent-3_max: 200
    apples_agent-3_mean: 95.45
    apples_agent-3_min: 30
    apples_agent-4_max: 64
    apples_agent-4_mean: 5.41
    apples_agent-4_min: 0
    apples_agent-5_max: 116
    apples_agent-5_mean: 67.93
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 487
    cleaning_beam_agent-0_mean: 310.57
    cleaning_beam_agent-0_min: 171
    cleaning_beam_agent-1_max: 584
    cleaning_beam_agent-1_mean: 301.08
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 557
    cleaning_beam_agent-2_mean: 325.3
    cleaning_beam_agent-2_min: 145
    cleaning_beam_agent-3_max: 132
    cleaning_beam_agent-3_mean: 63.27
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 489
    cleaning_beam_agent-4_mean: 333.49
    cleaning_beam_agent-4_min: 188
    cleaning_beam_agent-5_max: 189
    cleaning_beam_agent-5_mean: 55.17
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-06-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 716.9999999999865
  episode_reward_mean: 516.6000000000046
  episode_reward_min: 207.99999999999778
  episodes_this_iter: 96
  episodes_total: 6432
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 13915.46
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008646335918456316
        entropy: 1.4017484188079834
        entropy_coeff: 0.0017600000137463212
        kl: 0.010239630937576294
        model: {}
        policy_loss: -0.023614073172211647
        total_loss: -0.022881176322698593
        vf_explained_var: 0.07330910861492157
        vf_loss: 11.520471572875977
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008646335918456316
        entropy: 1.2560408115386963
        entropy_coeff: 0.0017600000137463212
        kl: 0.012427730485796928
        model: {}
        policy_loss: -0.029710624366998672
        total_loss: -0.02825198322534561
        vf_explained_var: 0.0476551353931427
        vf_loss: 11.837308883666992
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008646335918456316
        entropy: 1.313586950302124
        entropy_coeff: 0.0017600000137463212
        kl: 0.011417766101658344
        model: {}
        policy_loss: -0.026450205594301224
        total_loss: -0.02534281462430954
        vf_explained_var: 0.08670637011528015
        vf_loss: 11.35746955871582
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008646335918456316
        entropy: 1.067673921585083
        entropy_coeff: 0.0017600000137463212
        kl: 0.009260877035558224
        model: {}
        policy_loss: -0.021010972559452057
        total_loss: -0.020015783607959747
        vf_explained_var: 0.17764602601528168
        vf_loss: 10.221227645874023
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008646335918456316
        entropy: 1.1802250146865845
        entropy_coeff: 0.0017600000137463212
        kl: 0.011049873195588589
        model: {}
        policy_loss: -0.02755824476480484
        total_loss: -0.026370717212557793
        vf_explained_var: 0.15128222107887268
        vf_loss: 10.54749584197998
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008646335918456316
        entropy: 1.1884584426879883
        entropy_coeff: 0.0017600000137463212
        kl: 0.012102529406547546
        model: {}
        policy_loss: -0.029220592230558395
        total_loss: -0.02786898985505104
        vf_explained_var: 0.1779126673936844
        vf_loss: 10.22785758972168
    load_time_ms: 43066.993
    num_steps_sampled: 6432000
    num_steps_trained: 6432000
    sample_time_ms: 131121.755
    update_time_ms: 532.376
  iterations_since_restore: 7
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.864794007490634
    ram_util_percent: 18.40486891385768
  pid: 30948
  policy_reward_max:
    agent-0: 119.50000000000041
    agent-1: 119.50000000000041
    agent-2: 119.50000000000041
    agent-3: 119.50000000000041
    agent-4: 119.50000000000041
    agent-5: 119.50000000000041
  policy_reward_mean:
    agent-0: 86.10000000000015
    agent-1: 86.10000000000015
    agent-2: 86.10000000000015
    agent-3: 86.10000000000015
    agent-4: 86.10000000000015
    agent-5: 86.10000000000015
  policy_reward_min:
    agent-0: 34.666666666666735
    agent-1: 34.666666666666735
    agent-2: 34.666666666666735
    agent-3: 34.666666666666735
    agent-4: 34.666666666666735
    agent-5: 34.666666666666735
  sampler_perf:
    mean_env_wait_ms: 31.10989396957271
    mean_inference_ms: 14.292192865468895
    mean_processing_ms: 67.69773006961013
  time_since_restore: 1325.6520223617554
  time_this_iter_s: 186.64095211029053
  time_total_s: 10451.663836240768
  timestamp: 1637028380
  timesteps_since_restore: 672000
  timesteps_this_iter: 96000
  timesteps_total: 6432000
  training_iteration: 67
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     67 |          10451.7 | 6432000 |    516.6 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 92
    apples_agent-0_mean: 6.82
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 18.98
    apples_agent-1_min: 0
    apples_agent-2_max: 133
    apples_agent-2_mean: 14.15
    apples_agent-2_min: 0
    apples_agent-3_max: 181
    apples_agent-3_mean: 99.03
    apples_agent-3_min: 15
    apples_agent-4_max: 81
    apples_agent-4_mean: 6.17
    apples_agent-4_min: 0
    apples_agent-5_max: 131
    apples_agent-5_mean: 67.79
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 485
    cleaning_beam_agent-0_mean: 296.83
    cleaning_beam_agent-0_min: 176
    cleaning_beam_agent-1_max: 541
    cleaning_beam_agent-1_mean: 284.8
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 546
    cleaning_beam_agent-2_mean: 317.07
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 129
    cleaning_beam_agent-3_mean: 58.33
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 573
    cleaning_beam_agent-4_mean: 359.37
    cleaning_beam_agent-4_min: 163
    cleaning_beam_agent-5_max: 213
    cleaning_beam_agent-5_mean: 58.92
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.07
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 5
    fire_beam_agent-4_mean: 0.1
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-09-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 766.9999999999798
  episode_reward_mean: 527.6800000000032
  episode_reward_min: 139.00000000000088
  episodes_this_iter: 96
  episodes_total: 6528
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 13601.444
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000858643208630383
        entropy: 1.4472317695617676
        entropy_coeff: 0.0017600000137463212
        kl: 0.01019793376326561
        model: {}
        policy_loss: -0.0232342928647995
        total_loss: -0.02256149798631668
        vf_explained_var: 0.08805179595947266
        vf_loss: 11.803364753723145
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000858643208630383
        entropy: 1.2557201385498047
        entropy_coeff: 0.0017600000137463212
        kl: 0.012222104705870152
        model: {}
        policy_loss: -0.03038291446864605
        total_loss: -0.028874387964606285
        vf_explained_var: 0.016871020197868347
        vf_loss: 12.741724967956543
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000858643208630383
        entropy: 1.2977612018585205
        entropy_coeff: 0.0017600000137463212
        kl: 0.013790931552648544
        model: {}
        policy_loss: -0.027018189430236816
        total_loss: -0.02537444420158863
        vf_explained_var: 0.09697818756103516
        vf_loss: 11.696229934692383
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000858643208630383
        entropy: 1.0498228073120117
        entropy_coeff: 0.0017600000137463212
        kl: 0.009695454500615597
        model: {}
        policy_loss: -0.022692857310175896
        total_loss: -0.02153852954506874
        vf_explained_var: 0.1784566342830658
        vf_loss: 10.629249572753906
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000858643208630383
        entropy: 1.143376111984253
        entropy_coeff: 0.0017600000137463212
        kl: 0.011470583267509937
        model: {}
        policy_loss: -0.028297308832406998
        total_loss: -0.02687661536037922
        vf_explained_var: 0.1198955774307251
        vf_loss: 11.389174461364746
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000858643208630383
        entropy: 1.187273383140564
        entropy_coeff: 0.0017600000137463212
        kl: 0.01219475269317627
        model: {}
        policy_loss: -0.02977665513753891
        total_loss: -0.028355447575449944
        vf_explained_var: 0.17254747450351715
        vf_loss: 10.718610763549805
    load_time_ms: 42487.89
    num_steps_sampled: 6528000
    num_steps_trained: 6528000
    sample_time_ms: 130645.371
    update_time_ms: 473.27
  iterations_since_restore: 8
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.26825396825397
    ram_util_percent: 19.136507936507936
  pid: 30948
  policy_reward_max:
    agent-0: 127.83333333333407
    agent-1: 127.83333333333407
    agent-2: 127.83333333333407
    agent-3: 127.83333333333407
    agent-4: 127.83333333333407
    agent-5: 127.83333333333407
  policy_reward_mean:
    agent-0: 87.94666666666683
    agent-1: 87.94666666666683
    agent-2: 87.94666666666683
    agent-3: 87.94666666666683
    agent-4: 87.94666666666683
    agent-5: 87.94666666666683
  policy_reward_min:
    agent-0: 23.166666666666682
    agent-1: 23.166666666666682
    agent-2: 23.166666666666682
    agent-3: 23.166666666666682
    agent-4: 23.166666666666682
    agent-5: 23.166666666666682
  sampler_perf:
    mean_env_wait_ms: 31.174845968943554
    mean_inference_ms: 14.277695055853947
    mean_processing_ms: 67.81275623017159
  time_since_restore: 1502.9751200675964
  time_this_iter_s: 177.32309770584106
  time_total_s: 10628.98693394661
  timestamp: 1637028558
  timesteps_since_restore: 768000
  timesteps_this_iter: 96000
  timesteps_total: 6528000
  training_iteration: 68
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     68 |            10629 | 6528000 |   527.68 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 76
    apples_agent-0_mean: 4.49
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 23.36
    apples_agent-1_min: 0
    apples_agent-2_max: 94
    apples_agent-2_mean: 13.0
    apples_agent-2_min: 0
    apples_agent-3_max: 148
    apples_agent-3_mean: 87.59
    apples_agent-3_min: 29
    apples_agent-4_max: 78
    apples_agent-4_mean: 3.21
    apples_agent-4_min: 0
    apples_agent-5_max: 147
    apples_agent-5_mean: 71.02
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 438
    cleaning_beam_agent-0_mean: 286.57
    cleaning_beam_agent-0_min: 165
    cleaning_beam_agent-1_max: 544
    cleaning_beam_agent-1_mean: 291.46
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 612
    cleaning_beam_agent-2_mean: 355.38
    cleaning_beam_agent-2_min: 146
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 50.5
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 544
    cleaning_beam_agent-4_mean: 374.35
    cleaning_beam_agent-4_min: 223
    cleaning_beam_agent-5_max: 172
    cleaning_beam_agent-5_mean: 56.19
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-12-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 782.9999999999799
  episode_reward_mean: 551.9300000000032
  episode_reward_min: 138.00000000000085
  episodes_this_iter: 96
  episodes_total: 6624
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 13451.058
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008526528254151344
        entropy: 1.4508798122406006
        entropy_coeff: 0.0017600000137463212
        kl: 0.011693925596773624
        model: {}
        policy_loss: -0.023762686178088188
        total_loss: -0.022859668359160423
        vf_explained_var: 0.09361506998538971
        vf_loss: 11.177840232849121
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008526528254151344
        entropy: 1.2558317184448242
        entropy_coeff: 0.0017600000137463212
        kl: 0.012553493492305279
        model: {}
        policy_loss: -0.029536057263612747
        total_loss: -0.02804817073047161
        vf_explained_var: 0.03712700307369232
        vf_loss: 11.87451171875
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008526528254151344
        entropy: 1.2718228101730347
        entropy_coeff: 0.0017600000137463212
        kl: 0.01177801750600338
        model: {}
        policy_loss: -0.02600652165710926
        total_loss: -0.02474934421479702
        vf_explained_var: 0.07541355490684509
        vf_loss: 11.399874687194824
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008526528254151344
        entropy: 1.0076674222946167
        entropy_coeff: 0.0017600000137463212
        kl: 0.00939290039241314
        model: {}
        policy_loss: -0.02064776048064232
        total_loss: -0.019489353522658348
        vf_explained_var: 0.14509806036949158
        vf_loss: 10.533195495605469
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008526528254151344
        entropy: 1.1536751985549927
        entropy_coeff: 0.0017600000137463212
        kl: 0.011916086077690125
        model: {}
        policy_loss: -0.02957926318049431
        total_loss: -0.028088418766856194
        vf_explained_var: 0.0775429755449295
        vf_loss: 11.380935668945312
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008526528254151344
        entropy: 1.1543828248977661
        entropy_coeff: 0.0017600000137463212
        kl: 0.011806084774434566
        model: {}
        policy_loss: -0.029749687761068344
        total_loss: -0.02838498167693615
        vf_explained_var: 0.1601705402135849
        vf_loss: 10.35205078125
    load_time_ms: 41401.362
    num_steps_sampled: 6624000
    num_steps_trained: 6624000
    sample_time_ms: 130157.345
    update_time_ms: 429.145
  iterations_since_restore: 9
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.83032786885246
    ram_util_percent: 19.048770491803285
  pid: 30948
  policy_reward_max:
    agent-0: 130.50000000000063
    agent-1: 130.50000000000063
    agent-2: 130.50000000000063
    agent-3: 130.50000000000063
    agent-4: 130.50000000000063
    agent-5: 130.50000000000063
  policy_reward_mean:
    agent-0: 91.98833333333354
    agent-1: 91.98833333333354
    agent-2: 91.98833333333354
    agent-3: 91.98833333333354
    agent-4: 91.98833333333354
    agent-5: 91.98833333333354
  policy_reward_min:
    agent-0: 23.000000000000014
    agent-1: 23.000000000000014
    agent-2: 23.000000000000014
    agent-3: 23.000000000000014
    agent-4: 23.000000000000014
    agent-5: 23.000000000000014
  sampler_perf:
    mean_env_wait_ms: 31.2479419693403
    mean_inference_ms: 14.265548911723448
    mean_processing_ms: 67.87146673916448
  time_since_restore: 1674.3696336746216
  time_this_iter_s: 171.39451360702515
  time_total_s: 10800.381447553635
  timestamp: 1637028729
  timesteps_since_restore: 864000
  timesteps_this_iter: 96000
  timesteps_total: 6624000
  training_iteration: 69
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     69 |          10800.4 | 6624000 |   551.93 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 4.66
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 17.45
    apples_agent-1_min: 0
    apples_agent-2_max: 91
    apples_agent-2_mean: 11.52
    apples_agent-2_min: 0
    apples_agent-3_max: 197
    apples_agent-3_mean: 94.64
    apples_agent-3_min: 26
    apples_agent-4_max: 61
    apples_agent-4_mean: 3.93
    apples_agent-4_min: 0
    apples_agent-5_max: 133
    apples_agent-5_mean: 77.32
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 491
    cleaning_beam_agent-0_mean: 277.82
    cleaning_beam_agent-0_min: 134
    cleaning_beam_agent-1_max: 547
    cleaning_beam_agent-1_mean: 306.53
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 568
    cleaning_beam_agent-2_mean: 334.38
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 88
    cleaning_beam_agent-3_mean: 41.36
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 524
    cleaning_beam_agent-4_mean: 353.95
    cleaning_beam_agent-4_min: 144
    cleaning_beam_agent-5_max: 156
    cleaning_beam_agent-5_mean: 54.31
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-14-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 756.9999999999847
  episode_reward_mean: 556.3200000000016
  episode_reward_min: 186.9999999999987
  episodes_this_iter: 96
  episodes_total: 6720
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 13312.125
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008466623839922249
        entropy: 1.4066252708435059
        entropy_coeff: 0.0017600000137463212
        kl: 0.010454623028635979
        model: {}
        policy_loss: -0.02288721688091755
        total_loss: -0.02204637974500656
        vf_explained_var: 0.10428322851657867
        vf_loss: 12.255777359008789
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008466623839922249
        entropy: 1.23920738697052
        entropy_coeff: 0.0017600000137463212
        kl: 0.01352296955883503
        model: {}
        policy_loss: -0.027959313243627548
        total_loss: -0.026128185912966728
        vf_explained_var: 0.04373583197593689
        vf_loss: 13.075386047363281
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008466623839922249
        entropy: 1.2993234395980835
        entropy_coeff: 0.0017600000137463212
        kl: 0.012505050748586655
        model: {}
        policy_loss: -0.02782651036977768
        total_loss: -0.026309596374630928
        vf_explained_var: 0.04791545867919922
        vf_loss: 13.027143478393555
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008466623839922249
        entropy: 0.9716155529022217
        entropy_coeff: 0.0017600000137463212
        kl: 0.010258149355649948
        model: {}
        policy_loss: -0.020655179396271706
        total_loss: -0.019181935116648674
        vf_explained_var: 0.17250359058380127
        vf_loss: 11.316584587097168
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008466623839922249
        entropy: 1.1649607419967651
        entropy_coeff: 0.0017600000137463212
        kl: 0.011915040202438831
        model: {}
        policy_loss: -0.030691832304000854
        total_loss: -0.029118763282895088
        vf_explained_var: 0.093238964676857
        vf_loss: 12.403905868530273
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008466623839922249
        entropy: 1.1372283697128296
        entropy_coeff: 0.0017600000137463212
        kl: 0.01234690472483635
        model: {}
        policy_loss: -0.030924895778298378
        total_loss: -0.029334265738725662
        vf_explained_var: 0.17924512922763824
        vf_loss: 11.227714538574219
    load_time_ms: 40326.358
    num_steps_sampled: 6720000
    num_steps_trained: 6720000
    sample_time_ms: 129358.004
    update_time_ms: 392.559
  iterations_since_restore: 10
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.14957627118644
    ram_util_percent: 18.59915254237288
  pid: 30948
  policy_reward_max:
    agent-0: 126.16666666666731
    agent-1: 126.16666666666731
    agent-2: 126.16666666666731
    agent-3: 126.16666666666731
    agent-4: 126.16666666666731
    agent-5: 126.16666666666731
  policy_reward_mean:
    agent-0: 92.72000000000024
    agent-1: 92.72000000000024
    agent-2: 92.72000000000024
    agent-3: 92.72000000000024
    agent-4: 92.72000000000024
    agent-5: 92.72000000000024
  policy_reward_min:
    agent-0: 31.166666666666725
    agent-1: 31.166666666666725
    agent-2: 31.166666666666725
    agent-3: 31.166666666666725
    agent-4: 31.166666666666725
    agent-5: 31.166666666666725
  sampler_perf:
    mean_env_wait_ms: 31.279832704819693
    mean_inference_ms: 14.254571309060033
    mean_processing_ms: 67.84754071277426
  time_since_restore: 1839.4973571300507
  time_this_iter_s: 165.12772345542908
  time_total_s: 10965.509171009064
  timestamp: 1637028895
  timesteps_since_restore: 960000
  timesteps_this_iter: 96000
  timesteps_total: 6720000
  training_iteration: 70
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 33.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     70 |          10965.5 | 6720000 |   556.32 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 4.67
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 24.54
    apples_agent-1_min: 0
    apples_agent-2_max: 167
    apples_agent-2_mean: 10.92
    apples_agent-2_min: 0
    apples_agent-3_max: 191
    apples_agent-3_mean: 94.33
    apples_agent-3_min: 31
    apples_agent-4_max: 87
    apples_agent-4_mean: 5.42
    apples_agent-4_min: 0
    apples_agent-5_max: 124
    apples_agent-5_mean: 74.49
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 464
    cleaning_beam_agent-0_mean: 315.58
    cleaning_beam_agent-0_min: 139
    cleaning_beam_agent-1_max: 587
    cleaning_beam_agent-1_mean: 327.36
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 602
    cleaning_beam_agent-2_mean: 362.36
    cleaning_beam_agent-2_min: 149
    cleaning_beam_agent-3_max: 104
    cleaning_beam_agent-3_mean: 43.73
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 530
    cleaning_beam_agent-4_mean: 347.76
    cleaning_beam_agent-4_min: 131
    cleaning_beam_agent-5_max: 170
    cleaning_beam_agent-5_mean: 61.19
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-17-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 808.9999999999878
  episode_reward_mean: 573.8200000000002
  episode_reward_min: 201.99999999999656
  episodes_this_iter: 96
  episodes_total: 6816
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12041.204
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008406720007769763
        entropy: 1.3668453693389893
        entropy_coeff: 0.0017600000137463212
        kl: 0.010650364682078362
        model: {}
        policy_loss: -0.023980766534805298
        total_loss: -0.023092230781912804
        vf_explained_var: 0.12727156281471252
        vf_loss: 11.641081809997559
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008406720007769763
        entropy: 1.2214062213897705
        entropy_coeff: 0.0017600000137463212
        kl: 0.012759232893586159
        model: {}
        policy_loss: -0.028780851513147354
        total_loss: -0.027139296755194664
        vf_explained_var: 0.0710332840681076
        vf_loss: 12.393828392028809
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008406720007769763
        entropy: 1.2484707832336426
        entropy_coeff: 0.0017600000137463212
        kl: 0.01232786662876606
        model: {}
        policy_loss: -0.028277330100536346
        total_loss: -0.026780791580677032
        vf_explained_var: 0.0789356380701065
        vf_loss: 12.282739639282227
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008406720007769763
        entropy: 0.943149209022522
        entropy_coeff: 0.0017600000137463212
        kl: 0.009303677827119827
        model: {}
        policy_loss: -0.020433487370610237
        total_loss: -0.01908290386199951
        vf_explained_var: 0.1369759440422058
        vf_loss: 11.497928619384766
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008406720007769763
        entropy: 1.1703245639801025
        entropy_coeff: 0.0017600000137463212
        kl: 0.012137951329350471
        model: {}
        policy_loss: -0.031109429895877838
        total_loss: -0.029561467468738556
        vf_explained_var: 0.11514715850353241
        vf_loss: 11.801403045654297
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008406720007769763
        entropy: 1.1145693063735962
        entropy_coeff: 0.0017600000137463212
        kl: 0.013121198862791061
        model: {}
        policy_loss: -0.03066989965736866
        total_loss: -0.0288456492125988
        vf_explained_var: 0.12907367944717407
        vf_loss: 11.61652660369873
    load_time_ms: 37038.793
    num_steps_sampled: 6816000
    num_steps_trained: 6816000
    sample_time_ms: 128890.437
    update_time_ms: 59.041
  iterations_since_restore: 11
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.972272727272724
    ram_util_percent: 19.15318181818182
  pid: 30948
  policy_reward_max:
    agent-0: 134.83333333333346
    agent-1: 134.83333333333346
    agent-2: 134.83333333333346
    agent-3: 134.83333333333346
    agent-4: 134.83333333333346
    agent-5: 134.83333333333346
  policy_reward_mean:
    agent-0: 95.63666666666691
    agent-1: 95.63666666666691
    agent-2: 95.63666666666691
    agent-3: 95.63666666666691
    agent-4: 95.63666666666691
    agent-5: 95.63666666666691
  policy_reward_min:
    agent-0: 33.666666666666735
    agent-1: 33.666666666666735
    agent-2: 33.666666666666735
    agent-3: 33.666666666666735
    agent-4: 33.666666666666735
    agent-5: 33.666666666666735
  sampler_perf:
    mean_env_wait_ms: 31.042957604014187
    mean_inference_ms: 14.169751978802665
    mean_processing_ms: 67.2476713909118
  time_since_restore: 1994.0181548595428
  time_this_iter_s: 154.5207977294922
  time_total_s: 11120.029968738556
  timestamp: 1637029049
  timesteps_since_restore: 1056000
  timesteps_this_iter: 96000
  timesteps_total: 6816000
  training_iteration: 71
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     71 |            11120 | 6816000 |   573.82 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 83
    apples_agent-0_mean: 7.29
    apples_agent-0_min: 0
    apples_agent-1_max: 128
    apples_agent-1_mean: 17.13
    apples_agent-1_min: 0
    apples_agent-2_max: 167
    apples_agent-2_mean: 10.08
    apples_agent-2_min: 0
    apples_agent-3_max: 196
    apples_agent-3_mean: 92.74
    apples_agent-3_min: 15
    apples_agent-4_max: 69
    apples_agent-4_mean: 4.59
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 76.95
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 361
    cleaning_beam_agent-0_mean: 268.75
    cleaning_beam_agent-0_min: 152
    cleaning_beam_agent-1_max: 543
    cleaning_beam_agent-1_mean: 304.6
    cleaning_beam_agent-1_min: 63
    cleaning_beam_agent-2_max: 601
    cleaning_beam_agent-2_mean: 397.55
    cleaning_beam_agent-2_min: 183
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 49.73
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 594
    cleaning_beam_agent-4_mean: 376.2
    cleaning_beam_agent-4_min: 135
    cleaning_beam_agent-5_max: 127
    cleaning_beam_agent-5_mean: 57.06
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-19-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 730.9999999999984
  episode_reward_mean: 531.7600000000034
  episode_reward_min: 108.00000000000209
  episodes_this_iter: 96
  episodes_total: 6912
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12089.955
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008346816175617278
        entropy: 1.4072943925857544
        entropy_coeff: 0.0017600000137463212
        kl: 0.010336250066757202
        model: {}
        policy_loss: -0.025192297995090485
        total_loss: -0.02441607601940632
        vf_explained_var: 0.12019695341587067
        vf_loss: 11.85805892944336
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008346816175617278
        entropy: 1.2104285955429077
        entropy_coeff: 0.0017600000137463212
        kl: 0.013577466830611229
        model: {}
        policy_loss: -0.029600422829389572
        total_loss: -0.02776673063635826
        vf_explained_var: 0.07327331602573395
        vf_loss: 12.485547065734863
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008346816175617278
        entropy: 1.2326380014419556
        entropy_coeff: 0.0017600000137463212
        kl: 0.011789780110120773
        model: {}
        policy_loss: -0.02698873169720173
        total_loss: -0.0255170539021492
        vf_explained_var: 0.04797044396400452
        vf_loss: 12.831626892089844
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008346816175617278
        entropy: 0.9665844440460205
        entropy_coeff: 0.0017600000137463212
        kl: 0.010041790083050728
        model: {}
        policy_loss: -0.022672470659017563
        total_loss: -0.021234264597296715
        vf_explained_var: 0.16009189188480377
        vf_loss: 11.310357093811035
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008346816175617278
        entropy: 1.1633135080337524
        entropy_coeff: 0.0017600000137463212
        kl: 0.012636483646929264
        model: {}
        policy_loss: -0.03266996890306473
        total_loss: -0.03099253587424755
        vf_explained_var: 0.11107805371284485
        vf_loss: 11.975666046142578
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008346816175617278
        entropy: 1.1533315181732178
        entropy_coeff: 0.0017600000137463212
        kl: 0.012472117319703102
        model: {}
        policy_loss: -0.031994760036468506
        total_loss: -0.030438203364610672
        vf_explained_var: 0.18798786401748657
        vf_loss: 10.919923782348633
    load_time_ms: 34512.999
    num_steps_sampled: 6912000
    num_steps_trained: 6912000
    sample_time_ms: 126420.573
    update_time_ms: 58.309
  iterations_since_restore: 12
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.628095238095238
    ram_util_percent: 19.068571428571428
  pid: 30948
  policy_reward_max:
    agent-0: 121.83333333333381
    agent-1: 121.83333333333381
    agent-2: 121.83333333333381
    agent-3: 121.83333333333381
    agent-4: 121.83333333333381
    agent-5: 121.83333333333381
  policy_reward_mean:
    agent-0: 88.62666666666688
    agent-1: 88.62666666666688
    agent-2: 88.62666666666688
    agent-3: 88.62666666666688
    agent-4: 88.62666666666688
    agent-5: 88.62666666666688
  policy_reward_min:
    agent-0: 17.99999999999998
    agent-1: 17.99999999999998
    agent-2: 17.99999999999998
    agent-3: 17.99999999999998
    agent-4: 17.99999999999998
    agent-5: 17.99999999999998
  sampler_perf:
    mean_env_wait_ms: 30.789936313566333
    mean_inference_ms: 14.177668210322768
    mean_processing_ms: 66.84357867862424
  time_since_restore: 2141.391926050186
  time_this_iter_s: 147.3737711906433
  time_total_s: 11267.4037399292
  timestamp: 1637029197
  timesteps_since_restore: 1152000
  timesteps_this_iter: 96000
  timesteps_total: 6912000
  training_iteration: 72
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     72 |          11267.4 | 6912000 |   531.76 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 89
    apples_agent-0_mean: 6.86
    apples_agent-0_min: 0
    apples_agent-1_max: 117
    apples_agent-1_mean: 18.73
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 10.99
    apples_agent-2_min: 0
    apples_agent-3_max: 177
    apples_agent-3_mean: 94.34
    apples_agent-3_min: 26
    apples_agent-4_max: 77
    apples_agent-4_mean: 4.66
    apples_agent-4_min: 0
    apples_agent-5_max: 127
    apples_agent-5_mean: 70.33
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 380
    cleaning_beam_agent-0_mean: 259.94
    cleaning_beam_agent-0_min: 113
    cleaning_beam_agent-1_max: 558
    cleaning_beam_agent-1_mean: 287.32
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 577
    cleaning_beam_agent-2_mean: 393.05
    cleaning_beam_agent-2_min: 186
    cleaning_beam_agent-3_max: 116
    cleaning_beam_agent-3_mean: 50.13
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 520
    cleaning_beam_agent-4_mean: 349.57
    cleaning_beam_agent-4_min: 122
    cleaning_beam_agent-5_max: 195
    cleaning_beam_agent-5_mean: 64.91
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 6
    fire_beam_agent-5_mean: 0.13
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-22-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 789.9999999999802
  episode_reward_mean: 515.5700000000021
  episode_reward_min: 156.00000000000017
  episodes_this_iter: 96
  episodes_total: 7008
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12137.062
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008286911761388183
        entropy: 1.3807425498962402
        entropy_coeff: 0.0017600000137463212
        kl: 0.011224491521716118
        model: {}
        policy_loss: -0.025564897805452347
        total_loss: -0.02455856278538704
        vf_explained_var: 0.14830689132213593
        vf_loss: 11.915456771850586
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008286911761388183
        entropy: 1.2181764841079712
        entropy_coeff: 0.0017600000137463212
        kl: 0.01327039860188961
        model: {}
        policy_loss: -0.03043130598962307
        total_loss: -0.028556067496538162
        vf_explained_var: 0.02453617751598358
        vf_loss: 13.651545524597168
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008286911761388183
        entropy: 1.2331774234771729
        entropy_coeff: 0.0017600000137463212
        kl: 0.012330032885074615
        model: {}
        policy_loss: -0.02849799022078514
        total_loss: -0.02687271311879158
        vf_explained_var: 0.04990983009338379
        vf_loss: 13.29659652709961
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008286911761388183
        entropy: 0.9897764921188354
        entropy_coeff: 0.0017600000137463212
        kl: 0.009905301034450531
        model: {}
        policy_loss: -0.022615525871515274
        total_loss: -0.0212289746850729
        vf_explained_var: 0.17927755415439606
        vf_loss: 11.474974632263184
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008286911761388183
        entropy: 1.1688677072525024
        entropy_coeff: 0.0017600000137463212
        kl: 0.013234669342637062
        model: {}
        policy_loss: -0.03356541320681572
        total_loss: -0.03180274739861488
        vf_explained_var: 0.16173389554023743
        vf_loss: 11.729397773742676
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008286911761388183
        entropy: 1.1895338296890259
        entropy_coeff: 0.0017600000137463212
        kl: 0.014015835709869862
        model: {}
        policy_loss: -0.03413262218236923
        total_loss: -0.03228110074996948
        vf_explained_var: 0.18363893032073975
        vf_loss: 11.419339179992676
    load_time_ms: 34202.07
    num_steps_sampled: 7008000
    num_steps_trained: 7008000
    sample_time_ms: 123735.407
    update_time_ms: 67.372
  iterations_since_restore: 13
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.21627906976744
    ram_util_percent: 19.082325581395356
  pid: 30948
  policy_reward_max:
    agent-0: 131.66666666666728
    agent-1: 131.66666666666728
    agent-2: 131.66666666666728
    agent-3: 131.66666666666728
    agent-4: 131.66666666666728
    agent-5: 131.66666666666728
  policy_reward_mean:
    agent-0: 85.92833333333353
    agent-1: 85.92833333333353
    agent-2: 85.92833333333353
    agent-3: 85.92833333333353
    agent-4: 85.92833333333353
    agent-5: 85.92833333333353
  policy_reward_min:
    agent-0: 26.000000000000032
    agent-1: 26.000000000000032
    agent-2: 26.000000000000032
    agent-3: 26.000000000000032
    agent-4: 26.000000000000032
    agent-5: 26.000000000000032
  sampler_perf:
    mean_env_wait_ms: 30.562932405594978
    mean_inference_ms: 14.086838294207132
    mean_processing_ms: 66.39197088756768
  time_since_restore: 2291.266113758087
  time_this_iter_s: 149.874187707901
  time_total_s: 11417.2779276371
  timestamp: 1637029348
  timesteps_since_restore: 1248000
  timesteps_this_iter: 96000
  timesteps_total: 7008000
  training_iteration: 73
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     73 |          11417.3 | 7008000 |   515.57 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 3.89
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 21.33
    apples_agent-1_min: 0
    apples_agent-2_max: 70
    apples_agent-2_mean: 9.63
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 98.18
    apples_agent-3_min: 29
    apples_agent-4_max: 96
    apples_agent-4_mean: 5.29
    apples_agent-4_min: 0
    apples_agent-5_max: 145
    apples_agent-5_mean: 74.89
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 499
    cleaning_beam_agent-0_mean: 295.41
    cleaning_beam_agent-0_min: 148
    cleaning_beam_agent-1_max: 527
    cleaning_beam_agent-1_mean: 274.36
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 636
    cleaning_beam_agent-2_mean: 407.61
    cleaning_beam_agent-2_min: 126
    cleaning_beam_agent-3_max: 129
    cleaning_beam_agent-3_mean: 48.38
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 592
    cleaning_beam_agent-4_mean: 358.74
    cleaning_beam_agent-4_min: 118
    cleaning_beam_agent-5_max: 202
    cleaning_beam_agent-5_mean: 64.29
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 4
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-24-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 777.999999999984
  episode_reward_mean: 549.1300000000016
  episode_reward_min: 176.9999999999986
  episodes_this_iter: 96
  episodes_total: 7104
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12202.173
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008227007929235697
        entropy: 1.3446242809295654
        entropy_coeff: 0.0017600000137463212
        kl: 0.011539671570062637
        model: {}
        policy_loss: -0.023762578144669533
        total_loss: -0.0226115882396698
        vf_explained_var: 0.10384458303451538
        vf_loss: 12.095954895019531
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008227007929235697
        entropy: 1.1861156225204468
        entropy_coeff: 0.0017600000137463212
        kl: 0.013042265549302101
        model: {}
        policy_loss: -0.031736891716718674
        total_loss: -0.02992381528019905
        vf_explained_var: 0.0426286906003952
        vf_loss: 12.921855926513672
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008227007929235697
        entropy: 1.2197827100753784
        entropy_coeff: 0.0017600000137463212
        kl: 0.012418585829436779
        model: {}
        policy_loss: -0.02771541103720665
        total_loss: -0.026091624051332474
        vf_explained_var: 0.04729302227497101
        vf_loss: 12.86884880065918
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008227007929235697
        entropy: 0.9654738306999207
        entropy_coeff: 0.0017600000137463212
        kl: 0.009835886768996716
        model: {}
        policy_loss: -0.022626124322414398
        total_loss: -0.021280784159898758
        vf_explained_var: 0.20198339223861694
        vf_loss: 10.77393627166748
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008227007929235697
        entropy: 1.1782066822052002
        entropy_coeff: 0.0017600000137463212
        kl: 0.012893839739263058
        model: {}
        policy_loss: -0.033263906836509705
        total_loss: -0.031582582741975784
        vf_explained_var: 0.12845030426979065
        vf_loss: 11.761960983276367
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008227007929235697
        entropy: 1.177290439605713
        entropy_coeff: 0.0017600000137463212
        kl: 0.013353928923606873
        model: {}
        policy_loss: -0.03181303292512894
        total_loss: -0.03008711338043213
        vf_explained_var: 0.16435465216636658
        vf_loss: 11.271627426147461
    load_time_ms: 33036.282
    num_steps_sampled: 7104000
    num_steps_trained: 7104000
    sample_time_ms: 120914.57
    update_time_ms: 64.094
  iterations_since_restore: 14
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.852884615384614
    ram_util_percent: 18.96923076923077
  pid: 30948
  policy_reward_max:
    agent-0: 129.66666666666728
    agent-1: 129.66666666666728
    agent-2: 129.66666666666728
    agent-3: 129.66666666666728
    agent-4: 129.66666666666728
    agent-5: 129.66666666666728
  policy_reward_mean:
    agent-0: 91.52166666666692
    agent-1: 91.52166666666692
    agent-2: 91.52166666666692
    agent-3: 91.52166666666692
    agent-4: 91.52166666666692
    agent-5: 91.52166666666692
  policy_reward_min:
    agent-0: 29.500000000000053
    agent-1: 29.500000000000053
    agent-2: 29.500000000000053
    agent-3: 29.500000000000053
    agent-4: 29.500000000000053
    agent-5: 29.500000000000053
  sampler_perf:
    mean_env_wait_ms: 30.40068501535622
    mean_inference_ms: 14.016067563673863
    mean_processing_ms: 65.97192574881339
  time_since_restore: 2436.9599664211273
  time_this_iter_s: 145.69385266304016
  time_total_s: 11562.97178030014
  timestamp: 1637029494
  timesteps_since_restore: 1344000
  timesteps_this_iter: 96000
  timesteps_total: 7104000
  training_iteration: 74
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.5/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     74 |            11563 | 7104000 |   549.13 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 72
    apples_agent-0_mean: 4.47
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 23.51
    apples_agent-1_min: 0
    apples_agent-2_max: 305
    apples_agent-2_mean: 13.07
    apples_agent-2_min: 0
    apples_agent-3_max: 181
    apples_agent-3_mean: 95.32
    apples_agent-3_min: 23
    apples_agent-4_max: 74
    apples_agent-4_mean: 6.73
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 76.86
    apples_agent-5_min: 14
    cleaning_beam_agent-0_max: 485
    cleaning_beam_agent-0_mean: 325.19
    cleaning_beam_agent-0_min: 157
    cleaning_beam_agent-1_max: 593
    cleaning_beam_agent-1_mean: 269.97
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 666
    cleaning_beam_agent-2_mean: 423.95
    cleaning_beam_agent-2_min: 111
    cleaning_beam_agent-3_max: 116
    cleaning_beam_agent-3_mean: 48.91
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 497
    cleaning_beam_agent-4_mean: 344.42
    cleaning_beam_agent-4_min: 149
    cleaning_beam_agent-5_max: 217
    cleaning_beam_agent-5_mean: 61.25
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-27-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 805.9999999999782
  episode_reward_mean: 572.0200000000006
  episode_reward_min: 171.99999999999986
  episodes_this_iter: 96
  episodes_total: 7200
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12206.677
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008167104097083211
        entropy: 1.3577091693878174
        entropy_coeff: 0.0017600000137463212
        kl: 0.01077503152191639
        model: {}
        policy_loss: -0.025264039635658264
        total_loss: -0.024235015735030174
        vf_explained_var: 0.08785241842269897
        vf_loss: 12.635831832885742
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008167104097083211
        entropy: 1.1677422523498535
        entropy_coeff: 0.0017600000137463212
        kl: 0.015170741826295853
        model: {}
        policy_loss: -0.03166236728429794
        total_loss: -0.029347877949476242
        vf_explained_var: 0.036790311336517334
        vf_loss: 13.355740547180176
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008167104097083211
        entropy: 1.1765360832214355
        entropy_coeff: 0.0017600000137463212
        kl: 0.012423263862729073
        model: {}
        policy_loss: -0.027972353622317314
        total_loss: -0.02623644843697548
        vf_explained_var: 0.04656487703323364
        vf_loss: 13.219575881958008
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008167104097083211
        entropy: 0.9384860396385193
        entropy_coeff: 0.0017600000137463212
        kl: 0.009374317713081837
        model: {}
        policy_loss: -0.02192782610654831
        total_loss: -0.020565006881952286
        vf_explained_var: 0.17749032378196716
        vf_loss: 11.396934509277344
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008167104097083211
        entropy: 1.197004795074463
        entropy_coeff: 0.0017600000137463212
        kl: 0.012953960336744785
        model: {}
        policy_loss: -0.03330428898334503
        total_loss: -0.03163149952888489
        vf_explained_var: 0.14192430675029755
        vf_loss: 11.887248039245605
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008167104097083211
        entropy: 1.1473122835159302
        entropy_coeff: 0.0017600000137463212
        kl: 0.013147605583071709
        model: {}
        policy_loss: -0.03226682171225548
        total_loss: -0.03049395978450775
        vf_explained_var: 0.16191579401493073
        vf_loss: 11.626115798950195
    load_time_ms: 32062.308
    num_steps_sampled: 7200000
    num_steps_trained: 7200000
    sample_time_ms: 118075.702
    update_time_ms: 90.878
  iterations_since_restore: 15
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.578672985781992
    ram_util_percent: 18.66445497630332
  pid: 30948
  policy_reward_max:
    agent-0: 134.33333333333363
    agent-1: 134.33333333333363
    agent-2: 134.33333333333363
    agent-3: 134.33333333333363
    agent-4: 134.33333333333363
    agent-5: 134.33333333333363
  policy_reward_mean:
    agent-0: 95.33666666666691
    agent-1: 95.33666666666691
    agent-2: 95.33666666666691
    agent-3: 95.33666666666691
    agent-4: 95.33666666666691
    agent-5: 95.33666666666691
  policy_reward_min:
    agent-0: 28.666666666666707
    agent-1: 28.666666666666707
    agent-2: 28.666666666666707
    agent-3: 28.666666666666707
    agent-4: 28.666666666666707
    agent-5: 28.666666666666707
  sampler_perf:
    mean_env_wait_ms: 30.26232756996592
    mean_inference_ms: 13.950861223861066
    mean_processing_ms: 65.58050897434886
  time_since_restore: 2585.0475573539734
  time_this_iter_s: 148.08759093284607
  time_total_s: 11711.059371232986
  timestamp: 1637029642
  timesteps_since_restore: 1440000
  timesteps_this_iter: 96000
  timesteps_total: 7200000
  training_iteration: 75
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     75 |          11711.1 | 7200000 |   572.02 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 5.27
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 25.33
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 10.55
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 95.29
    apples_agent-3_min: 17
    apples_agent-4_max: 79
    apples_agent-4_mean: 3.87
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 81.99
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 453
    cleaning_beam_agent-0_mean: 329.67
    cleaning_beam_agent-0_min: 120
    cleaning_beam_agent-1_max: 537
    cleaning_beam_agent-1_mean: 282.47
    cleaning_beam_agent-1_min: 78
    cleaning_beam_agent-2_max: 710
    cleaning_beam_agent-2_mean: 428.2
    cleaning_beam_agent-2_min: 153
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 48.98
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 495
    cleaning_beam_agent-4_mean: 362.26
    cleaning_beam_agent-4_min: 127
    cleaning_beam_agent-5_max: 162
    cleaning_beam_agent-5_mean: 58.18
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.06
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-29-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 840.9999999999767
  episode_reward_mean: 592.3599999999992
  episode_reward_min: 254.99999999999457
  episodes_this_iter: 96
  episodes_total: 7296
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12178.049
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008107200264930725
        entropy: 1.3080378770828247
        entropy_coeff: 0.0017600000137463212
        kl: 0.011163481511175632
        model: {}
        policy_loss: -0.023392509669065475
        total_loss: -0.02217290550470352
        vf_explained_var: 0.13106143474578857
        vf_loss: 12.890569686889648
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008107200264930725
        entropy: 1.2084767818450928
        entropy_coeff: 0.0017600000137463212
        kl: 0.01260743010789156
        model: {}
        policy_loss: -0.031889937818050385
        total_loss: -0.03003625012934208
        vf_explained_var: 0.01734013855457306
        vf_loss: 14.591228485107422
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008107200264930725
        entropy: 1.1744122505187988
        entropy_coeff: 0.0017600000137463212
        kl: 0.012593742460012436
        model: {}
        policy_loss: -0.02904948592185974
        total_loss: -0.027212079614400864
        vf_explained_var: 0.06792683899402618
        vf_loss: 13.856260299682617
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008107200264930725
        entropy: 0.9326244592666626
        entropy_coeff: 0.0017600000137463212
        kl: 0.011255187913775444
        model: {}
        policy_loss: -0.02166159451007843
        total_loss: -0.019867341965436935
        vf_explained_var: 0.20239107310771942
        vf_loss: 11.846342086791992
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008107200264930725
        entropy: 1.1582069396972656
        entropy_coeff: 0.0017600000137463212
        kl: 0.012984526343643665
        model: {}
        policy_loss: -0.03183708339929581
        total_loss: -0.030005665495991707
        vf_explained_var: 0.14161141216754913
        vf_loss: 12.729595184326172
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0008107200264930725
        entropy: 1.130578875541687
        entropy_coeff: 0.0017600000137463212
        kl: 0.014037967659533024
        model: {}
        policy_loss: -0.031150149181485176
        total_loss: -0.02912634238600731
        vf_explained_var: 0.18881066143512726
        vf_loss: 12.06029224395752
    load_time_ms: 30562.587
    num_steps_sampled: 7296000
    num_steps_trained: 7296000
    sample_time_ms: 115988.951
    update_time_ms: 87.673
  iterations_since_restore: 16
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.93188405797101
    ram_util_percent: 18.873913043478254
  pid: 30948
  policy_reward_max:
    agent-0: 140.1666666666673
    agent-1: 140.1666666666673
    agent-2: 140.1666666666673
    agent-3: 140.1666666666673
    agent-4: 140.1666666666673
    agent-5: 140.1666666666673
  policy_reward_mean:
    agent-0: 98.72666666666697
    agent-1: 98.72666666666697
    agent-2: 98.72666666666697
    agent-3: 98.72666666666697
    agent-4: 98.72666666666697
    agent-5: 98.72666666666697
  policy_reward_min:
    agent-0: 42.49999999999995
    agent-1: 42.49999999999995
    agent-2: 42.49999999999995
    agent-3: 42.49999999999995
    agent-4: 42.49999999999995
    agent-5: 42.49999999999995
  sampler_perf:
    mean_env_wait_ms: 30.153848042378687
    mean_inference_ms: 13.904834320752736
    mean_processing_ms: 65.2319144683056
  time_since_restore: 2730.083108186722
  time_this_iter_s: 145.0355508327484
  time_total_s: 11856.094922065735
  timestamp: 1637029788
  timesteps_since_restore: 1536000
  timesteps_this_iter: 96000
  timesteps_total: 7296000
  training_iteration: 76
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     76 |          11856.1 | 7296000 |   592.36 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 4.68
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 24.41
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 10.47
    apples_agent-2_min: 0
    apples_agent-3_max: 251
    apples_agent-3_mean: 93.14
    apples_agent-3_min: 45
    apples_agent-4_max: 78
    apples_agent-4_mean: 4.82
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 78.01
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 556
    cleaning_beam_agent-0_mean: 326.45
    cleaning_beam_agent-0_min: 196
    cleaning_beam_agent-1_max: 553
    cleaning_beam_agent-1_mean: 284.85
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 681
    cleaning_beam_agent-2_mean: 428.49
    cleaning_beam_agent-2_min: 73
    cleaning_beam_agent-3_max: 92
    cleaning_beam_agent-3_mean: 45.4
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 541
    cleaning_beam_agent-4_mean: 384.54
    cleaning_beam_agent-4_min: 155
    cleaning_beam_agent-5_max: 170
    cleaning_beam_agent-5_mean: 60.14
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 9
    fire_beam_agent-5_mean: 0.11
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-32-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 797.9999999999881
  episode_reward_mean: 588.1200000000001
  episode_reward_min: 328.0000000000027
  episodes_this_iter: 96
  episodes_total: 7392
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12287.469
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000804729585070163
        entropy: 1.2911630868911743
        entropy_coeff: 0.0017600000137463212
        kl: 0.010371287353336811
        model: {}
        policy_loss: -0.02588922157883644
        total_loss: -0.02483213134109974
        vf_explained_var: 0.08008861541748047
        vf_loss: 12.552755355834961
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000804729585070163
        entropy: 1.190423607826233
        entropy_coeff: 0.0017600000137463212
        kl: 0.013539868406951427
        model: {}
        policy_loss: -0.03212526813149452
        total_loss: -0.030176356434822083
        vf_explained_var: 0.021004050970077515
        vf_loss: 13.360872268676758
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000804729585070163
        entropy: 1.1743805408477783
        entropy_coeff: 0.0017600000137463212
        kl: 0.01289902813732624
        model: {}
        policy_loss: -0.02743244729936123
        total_loss: -0.025590796023607254
        vf_explained_var: 0.026362702250480652
        vf_loss: 13.287588119506836
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000804729585070163
        entropy: 0.9149819016456604
        entropy_coeff: 0.0017600000137463212
        kl: 0.009635929949581623
        model: {}
        policy_loss: -0.02205628715455532
        total_loss: -0.02058502845466137
        vf_explained_var: 0.15406303107738495
        vf_loss: 11.544381141662598
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000804729585070163
        entropy: 1.1454217433929443
        entropy_coeff: 0.0017600000137463212
        kl: 0.012385649606585503
        model: {}
        policy_loss: -0.03227616474032402
        total_loss: -0.03058038465678692
        vf_explained_var: 0.09520025551319122
        vf_loss: 12.345978736877441
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000804729585070163
        entropy: 1.1236889362335205
        entropy_coeff: 0.0017600000137463212
        kl: 0.013503961265087128
        model: {}
        policy_loss: -0.03122788853943348
        total_loss: -0.0293673537671566
        vf_explained_var: 0.16653160750865936
        vf_loss: 11.374349594116211
    load_time_ms: 29230.161
    num_steps_sampled: 7392000
    num_steps_trained: 7392000
    sample_time_ms: 113720.583
    update_time_ms: 109.626
  iterations_since_restore: 17
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.509677419354837
    ram_util_percent: 18.727188940092166
  pid: 30948
  policy_reward_max:
    agent-0: 133.00000000000028
    agent-1: 133.00000000000028
    agent-2: 133.00000000000028
    agent-3: 133.00000000000028
    agent-4: 133.00000000000028
    agent-5: 133.00000000000028
  policy_reward_mean:
    agent-0: 98.02000000000025
    agent-1: 98.02000000000025
    agent-2: 98.02000000000025
    agent-3: 98.02000000000025
    agent-4: 98.02000000000025
    agent-5: 98.02000000000025
  policy_reward_min:
    agent-0: 54.66666666666646
    agent-1: 54.66666666666646
    agent-2: 54.66666666666646
    agent-3: 54.66666666666646
    agent-4: 54.66666666666646
    agent-5: 54.66666666666646
  sampler_perf:
    mean_env_wait_ms: 30.03017754574138
    mean_inference_ms: 13.861460805039995
    mean_processing_ms: 64.85280558404344
  time_since_restore: 2881.9422924518585
  time_this_iter_s: 151.85918426513672
  time_total_s: 12007.954106330872
  timestamp: 1637029940
  timesteps_since_restore: 1632000
  timesteps_this_iter: 96000
  timesteps_total: 7392000
  training_iteration: 77
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     77 |            12008 | 7392000 |   588.12 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 4.54
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 18.29
    apples_agent-1_min: 0
    apples_agent-2_max: 72
    apples_agent-2_mean: 9.54
    apples_agent-2_min: 0
    apples_agent-3_max: 155
    apples_agent-3_mean: 84.49
    apples_agent-3_min: 26
    apples_agent-4_max: 85
    apples_agent-4_mean: 5.17
    apples_agent-4_min: 0
    apples_agent-5_max: 121
    apples_agent-5_mean: 73.01
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 451
    cleaning_beam_agent-0_mean: 324.9
    cleaning_beam_agent-0_min: 172
    cleaning_beam_agent-1_max: 493
    cleaning_beam_agent-1_mean: 281.12
    cleaning_beam_agent-1_min: 78
    cleaning_beam_agent-2_max: 681
    cleaning_beam_agent-2_mean: 429.36
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 45.73
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 539
    cleaning_beam_agent-4_mean: 357.01
    cleaning_beam_agent-4_min: 151
    cleaning_beam_agent-5_max: 216
    cleaning_beam_agent-5_mean: 62.11
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-34-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 822.9999999999826
  episode_reward_mean: 596.3999999999988
  episode_reward_min: 251.99999999999648
  episodes_this_iter: 96
  episodes_total: 7488
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12295.18
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007987392018549144
        entropy: 1.2938001155853271
        entropy_coeff: 0.0017600000137463212
        kl: 0.01060680951923132
        model: {}
        policy_loss: -0.025926334783434868
        total_loss: -0.024728184565901756
        vf_explained_var: 0.12996576726436615
        vf_loss: 13.538793563842773
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007987392018549144
        entropy: 1.1912543773651123
        entropy_coeff: 0.0017600000137463212
        kl: 0.014402320608496666
        model: {}
        policy_loss: -0.031264886260032654
        total_loss: -0.028956551104784012
        vf_explained_var: 0.021625280380249023
        vf_loss: 15.244848251342773
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007987392018549144
        entropy: 1.1771645545959473
        entropy_coeff: 0.0017600000137463212
        kl: 0.01705370843410492
        model: {}
        policy_loss: -0.023940281942486763
        total_loss: -0.021172352135181427
        vf_explained_var: 0.08139528334140778
        vf_loss: 14.289983749389648
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007987392018549144
        entropy: 0.8771681785583496
        entropy_coeff: 0.0017600000137463212
        kl: 0.009514981880784035
        model: {}
        policy_loss: -0.021371016278862953
        total_loss: -0.019772004336118698
        vf_explained_var: 0.20360085368156433
        vf_loss: 12.398345947265625
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007987392018549144
        entropy: 1.1723783016204834
        entropy_coeff: 0.0017600000137463212
        kl: 0.014993617311120033
        model: {}
        policy_loss: -0.033691681921482086
        total_loss: -0.03138352558016777
        vf_explained_var: 0.11780942976474762
        vf_loss: 13.728144645690918
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007987392018549144
        entropy: 1.0909911394119263
        entropy_coeff: 0.0017600000137463212
        kl: 0.011958679184317589
        model: {}
        policy_loss: -0.03059205785393715
        total_loss: -0.02882123924791813
        vf_explained_var: 0.16471846401691437
        vf_loss: 12.992310523986816
    load_time_ms: 26789.587
    num_steps_sampled: 7488000
    num_steps_trained: 7488000
    sample_time_ms: 111454.071
    update_time_ms: 105.104
  iterations_since_restore: 18
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.380645161290317
    ram_util_percent: 16.522043010752686
  pid: 30948
  policy_reward_max:
    agent-0: 137.1666666666669
    agent-1: 137.1666666666669
    agent-2: 137.1666666666669
    agent-3: 137.1666666666669
    agent-4: 137.1666666666669
    agent-5: 137.1666666666669
  policy_reward_mean:
    agent-0: 99.40000000000029
    agent-1: 99.40000000000029
    agent-2: 99.40000000000029
    agent-3: 99.40000000000029
    agent-4: 99.40000000000029
    agent-5: 99.40000000000029
  policy_reward_min:
    agent-0: 42.00000000000001
    agent-1: 42.00000000000001
    agent-2: 42.00000000000001
    agent-3: 42.00000000000001
    agent-4: 42.00000000000001
    agent-5: 42.00000000000001
  sampler_perf:
    mean_env_wait_ms: 29.910364522999206
    mean_inference_ms: 13.818140495759591
    mean_processing_ms: 64.48037035214054
  time_since_restore: 3012.1877133846283
  time_this_iter_s: 130.24542093276978
  time_total_s: 12138.199527263641
  timestamp: 1637030071
  timesteps_since_restore: 1728000
  timesteps_this_iter: 96000
  timesteps_total: 7488000
  training_iteration: 78
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     78 |          12138.2 | 7488000 |    596.4 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 5.32
    apples_agent-0_min: 0
    apples_agent-1_max: 149
    apples_agent-1_mean: 24.11
    apples_agent-1_min: 0
    apples_agent-2_max: 173
    apples_agent-2_mean: 9.67
    apples_agent-2_min: 0
    apples_agent-3_max: 222
    apples_agent-3_mean: 90.4
    apples_agent-3_min: 39
    apples_agent-4_max: 71
    apples_agent-4_mean: 4.17
    apples_agent-4_min: 0
    apples_agent-5_max: 125
    apples_agent-5_mean: 73.96
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 540
    cleaning_beam_agent-0_mean: 312.21
    cleaning_beam_agent-0_min: 124
    cleaning_beam_agent-1_max: 428
    cleaning_beam_agent-1_mean: 247.99
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 660
    cleaning_beam_agent-2_mean: 475.89
    cleaning_beam_agent-2_min: 218
    cleaning_beam_agent-3_max: 89
    cleaning_beam_agent-3_mean: 46.19
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 506
    cleaning_beam_agent-4_mean: 373.89
    cleaning_beam_agent-4_min: 155
    cleaning_beam_agent-5_max: 169
    cleaning_beam_agent-5_mean: 57.7
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-36-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 834.9999999999822
  episode_reward_mean: 604.5499999999984
  episode_reward_min: 261.99999999999545
  episodes_this_iter: 96
  episodes_total: 7584
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12263.534
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007927488186396658
        entropy: 1.299321174621582
        entropy_coeff: 0.0017600000137463212
        kl: 0.011563903652131557
        model: {}
        policy_loss: -0.02641136571764946
        total_loss: -0.025053992867469788
        vf_explained_var: 0.11634460091590881
        vf_loss: 13.313965797424316
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007927488186396658
        entropy: 1.1957378387451172
        entropy_coeff: 0.0017600000137463212
        kl: 0.013198548927903175
        model: {}
        policy_loss: -0.03097020462155342
        total_loss: -0.028970174491405487
        vf_explained_var: 0.028280094265937805
        vf_loss: 14.64816665649414
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007927488186396658
        entropy: 1.1067030429840088
        entropy_coeff: 0.0017600000137463212
        kl: 0.012450875714421272
        model: {}
        policy_loss: -0.02732912078499794
        total_loss: -0.025326751172542572
        vf_explained_var: 0.03072899580001831
        vf_loss: 14.599958419799805
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007927488186396658
        entropy: 0.888155460357666
        entropy_coeff: 0.0017600000137463212
        kl: 0.009212692268192768
        model: {}
        policy_loss: -0.020503900945186615
        total_loss: -0.019028473645448685
        vf_explained_var: 0.2062208652496338
        vf_loss: 11.960413932800293
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007927488186396658
        entropy: 1.1791960000991821
        entropy_coeff: 0.0017600000137463212
        kl: 0.013065515086054802
        model: {}
        policy_loss: -0.03186139464378357
        total_loss: -0.0299905426800251
        vf_explained_var: 0.11542342603206635
        vf_loss: 13.331336975097656
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007927488186396658
        entropy: 1.0409890413284302
        entropy_coeff: 0.0017600000137463212
        kl: 0.012698980048298836
        model: {}
        policy_loss: -0.030180811882019043
        total_loss: -0.02816193923354149
        vf_explained_var: 0.1304817646741867
        vf_loss: 13.112229347229004
    load_time_ms: 24936.086
    num_steps_sampled: 7584000
    num_steps_trained: 7584000
    sample_time_ms: 109119.124
    update_time_ms: 98.782
  iterations_since_restore: 19
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.843783783783785
    ram_util_percent: 16.84864864864865
  pid: 30948
  policy_reward_max:
    agent-0: 139.16666666666677
    agent-1: 139.16666666666677
    agent-2: 139.16666666666677
    agent-3: 139.16666666666677
    agent-4: 139.16666666666677
    agent-5: 139.16666666666677
  policy_reward_mean:
    agent-0: 100.75833333333362
    agent-1: 100.75833333333362
    agent-2: 100.75833333333362
    agent-3: 100.75833333333362
    agent-4: 100.75833333333362
    agent-5: 100.75833333333362
  policy_reward_min:
    agent-0: 43.66666666666664
    agent-1: 43.66666666666664
    agent-2: 43.66666666666664
    agent-3: 43.66666666666664
    agent-4: 43.66666666666664
    agent-5: 43.66666666666664
  sampler_perf:
    mean_env_wait_ms: 29.742184573235292
    mean_inference_ms: 13.756990605965832
    mean_processing_ms: 64.03144333407258
  time_since_restore: 3141.6033565998077
  time_this_iter_s: 129.41564321517944
  time_total_s: 12267.61517047882
  timestamp: 1637030201
  timesteps_since_restore: 1824000
  timesteps_this_iter: 96000
  timesteps_total: 7584000
  training_iteration: 79
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     79 |          12267.6 | 7584000 |   604.55 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 68
    apples_agent-0_mean: 4.24
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 28.14
    apples_agent-1_min: 0
    apples_agent-2_max: 65
    apples_agent-2_mean: 8.69
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 83.15
    apples_agent-3_min: 20
    apples_agent-4_max: 73
    apples_agent-4_mean: 4.78
    apples_agent-4_min: 0
    apples_agent-5_max: 131
    apples_agent-5_mean: 73.39
    apples_agent-5_min: 9
    cleaning_beam_agent-0_max: 466
    cleaning_beam_agent-0_mean: 317.74
    cleaning_beam_agent-0_min: 134
    cleaning_beam_agent-1_max: 495
    cleaning_beam_agent-1_mean: 246.62
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 665
    cleaning_beam_agent-2_mean: 454.83
    cleaning_beam_agent-2_min: 102
    cleaning_beam_agent-3_max: 98
    cleaning_beam_agent-3_mean: 54.04
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 579
    cleaning_beam_agent-4_mean: 380.05
    cleaning_beam_agent-4_min: 149
    cleaning_beam_agent-5_max: 234
    cleaning_beam_agent-5_mean: 64.3
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-38-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 795.9999999999767
  episode_reward_mean: 570.08
  episode_reward_min: 137.00000000000102
  episodes_this_iter: 96
  episodes_total: 7680
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12288.386
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007867583772167563
        entropy: 1.2914764881134033
        entropy_coeff: 0.0017600000137463212
        kl: 0.011237972415983677
        model: {}
        policy_loss: -0.02533021755516529
        total_loss: -0.02407475933432579
        vf_explained_var: 0.1139327883720398
        vf_loss: 12.80860710144043
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007867583772167563
        entropy: 1.2007737159729004
        entropy_coeff: 0.0017600000137463212
        kl: 0.01408892497420311
        model: {}
        policy_loss: -0.03511473536491394
        total_loss: -0.032984670251607895
        vf_explained_var: 0.013590589165687561
        vf_loss: 14.25643539428711
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007867583772167563
        entropy: 1.1266491413116455
        entropy_coeff: 0.0017600000137463212
        kl: 0.012607439421117306
        model: {}
        policy_loss: -0.027891498059034348
        total_loss: -0.025979414582252502
        vf_explained_var: 0.049667805433273315
        vf_loss: 13.734947204589844
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007867583772167563
        entropy: 0.9606174230575562
        entropy_coeff: 0.0017600000137463212
        kl: 0.010561700910329819
        model: {}
        policy_loss: -0.022355441004037857
        total_loss: -0.02077529765665531
        vf_explained_var: 0.19812443852424622
        vf_loss: 11.584894180297852
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007867583772167563
        entropy: 1.144358515739441
        entropy_coeff: 0.0017600000137463212
        kl: 0.012873226776719093
        model: {}
        policy_loss: -0.03232577443122864
        total_loss: -0.030554112046957016
        vf_explained_var: 0.16185471415519714
        vf_loss: 12.110907554626465
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007867583772167563
        entropy: 1.0812932252883911
        entropy_coeff: 0.0017600000137463212
        kl: 0.011993339285254478
        model: {}
        policy_loss: -0.03264434635639191
        total_loss: -0.030967624858021736
        vf_explained_var: 0.18276047706604004
        vf_loss: 11.811277389526367
    load_time_ms: 23220.392
    num_steps_sampled: 7680000
    num_steps_trained: 7680000
    sample_time_ms: 106956.114
    update_time_ms: 93.89
  iterations_since_restore: 20
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.134444444444448
    ram_util_percent: 16.81444444444444
  pid: 30948
  policy_reward_max:
    agent-0: 132.66666666666728
    agent-1: 132.66666666666728
    agent-2: 132.66666666666728
    agent-3: 132.66666666666728
    agent-4: 132.66666666666728
    agent-5: 132.66666666666728
  policy_reward_mean:
    agent-0: 95.01333333333358
    agent-1: 95.01333333333358
    agent-2: 95.01333333333358
    agent-3: 95.01333333333358
    agent-4: 95.01333333333358
    agent-5: 95.01333333333358
  policy_reward_min:
    agent-0: 22.833333333333353
    agent-1: 22.833333333333353
    agent-2: 22.833333333333353
    agent-3: 22.833333333333353
    agent-4: 22.833333333333353
    agent-5: 22.833333333333353
  sampler_perf:
    mean_env_wait_ms: 29.594152287336875
    mean_inference_ms: 13.69959962710605
    mean_processing_ms: 63.62331254105071
  time_since_restore: 3268.0560200214386
  time_this_iter_s: 126.45266342163086
  time_total_s: 12394.067833900452
  timestamp: 1637030328
  timesteps_since_restore: 1920000
  timesteps_this_iter: 96000
  timesteps_total: 7680000
  training_iteration: 80
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     80 |          12394.1 | 7680000 |   570.08 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 80
    apples_agent-0_mean: 5.75
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 22.7
    apples_agent-1_min: 0
    apples_agent-2_max: 96
    apples_agent-2_mean: 6.39
    apples_agent-2_min: 0
    apples_agent-3_max: 132
    apples_agent-3_mean: 85.55
    apples_agent-3_min: 30
    apples_agent-4_max: 57
    apples_agent-4_mean: 3.26
    apples_agent-4_min: 0
    apples_agent-5_max: 138
    apples_agent-5_mean: 72.6
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 497
    cleaning_beam_agent-0_mean: 327.64
    cleaning_beam_agent-0_min: 134
    cleaning_beam_agent-1_max: 519
    cleaning_beam_agent-1_mean: 254.69
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 651
    cleaning_beam_agent-2_mean: 432.21
    cleaning_beam_agent-2_min: 128
    cleaning_beam_agent-3_max: 93
    cleaning_beam_agent-3_mean: 45.48
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 585
    cleaning_beam_agent-4_mean: 378.5
    cleaning_beam_agent-4_min: 170
    cleaning_beam_agent-5_max: 239
    cleaning_beam_agent-5_mean: 63.15
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-40-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 803.9999999999792
  episode_reward_mean: 597.6499999999988
  episode_reward_min: 300.9999999999987
  episodes_this_iter: 96
  episodes_total: 7776
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12308.063
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 1.2813785076141357
        entropy_coeff: 0.0017600000137463212
        kl: 0.012385968118906021
        model: {}
        policy_loss: -0.02671981230378151
        total_loss: -0.025263356044888496
        vf_explained_var: 0.13428819179534912
        vf_loss: 12.344916343688965
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 1.1969900131225586
        entropy_coeff: 0.0017600000137463212
        kl: 0.013587495312094688
        model: {}
        policy_loss: -0.03308805823326111
        total_loss: -0.031083449721336365
        vf_explained_var: 0.024359598755836487
        vf_loss: 13.938129425048828
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 1.1870455741882324
        entropy_coeff: 0.0017600000137463212
        kl: 0.013152205385267735
        model: {}
        policy_loss: -0.029380623251199722
        total_loss: -0.027522455900907516
        vf_explained_var: 0.07633733749389648
        vf_loss: 13.169270515441895
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 0.8908767700195312
        entropy_coeff: 0.0017600000137463212
        kl: 0.010432424955070019
        model: {}
        policy_loss: -0.021962083876132965
        total_loss: -0.020295344293117523
        vf_explained_var: 0.19422681629657745
        vf_loss: 11.481992721557617
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 1.1351977586746216
        entropy_coeff: 0.0017600000137463212
        kl: 0.012957639060914516
        model: {}
        policy_loss: -0.0319511741399765
        total_loss: -0.030061816796660423
        vf_explained_var: 0.0916254073381424
        vf_loss: 12.95776081085205
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007807679940015078
        entropy: 1.080979824066162
        entropy_coeff: 0.0017600000137463212
        kl: 0.013524077832698822
        model: {}
        policy_loss: -0.03306376188993454
        total_loss: -0.031053515151143074
        vf_explained_var: 0.15301395952701569
        vf_loss: 12.079548835754395
    load_time_ms: 21467.549
    num_steps_sampled: 7776000
    num_steps_trained: 7776000
    sample_time_ms: 106097.015
    update_time_ms: 92.302
  iterations_since_restore: 21
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.022702702702706
    ram_util_percent: 16.823243243243244
  pid: 30948
  policy_reward_max:
    agent-0: 134.00000000000057
    agent-1: 134.00000000000057
    agent-2: 134.00000000000057
    agent-3: 134.00000000000057
    agent-4: 134.00000000000057
    agent-5: 134.00000000000057
  policy_reward_mean:
    agent-0: 99.60833333333359
    agent-1: 99.60833333333359
    agent-2: 99.60833333333359
    agent-3: 99.60833333333359
    agent-4: 99.60833333333359
    agent-5: 99.60833333333359
  policy_reward_min:
    agent-0: 50.16666666666666
    agent-1: 50.16666666666666
    agent-2: 50.16666666666666
    agent-3: 50.16666666666666
    agent-4: 50.16666666666666
    agent-5: 50.16666666666666
  sampler_perf:
    mean_env_wait_ms: 29.45723715559724
    mean_inference_ms: 13.649421283957393
    mean_processing_ms: 63.25733045342009
  time_since_restore: 3396.627015352249
  time_this_iter_s: 128.57099533081055
  time_total_s: 12522.638829231262
  timestamp: 1637030458
  timesteps_since_restore: 2016000
  timesteps_this_iter: 96000
  timesteps_total: 7776000
  training_iteration: 81
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     81 |          12522.6 | 7776000 |   597.65 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 4.15
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 26.7
    apples_agent-1_min: 0
    apples_agent-2_max: 177
    apples_agent-2_mean: 8.37
    apples_agent-2_min: 0
    apples_agent-3_max: 156
    apples_agent-3_mean: 89.4
    apples_agent-3_min: 34
    apples_agent-4_max: 52
    apples_agent-4_mean: 2.61
    apples_agent-4_min: 0
    apples_agent-5_max: 130
    apples_agent-5_mean: 74.18
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 452
    cleaning_beam_agent-0_mean: 324.17
    cleaning_beam_agent-0_min: 202
    cleaning_beam_agent-1_max: 430
    cleaning_beam_agent-1_mean: 235.04
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 724
    cleaning_beam_agent-2_mean: 416.18
    cleaning_beam_agent-2_min: 152
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 50.06
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 503
    cleaning_beam_agent-4_mean: 368.8
    cleaning_beam_agent-4_min: 180
    cleaning_beam_agent-5_max: 206
    cleaning_beam_agent-5_mean: 64.58
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-43-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 757.9999999999824
  episode_reward_mean: 585.3800000000006
  episode_reward_min: 286.99999999999966
  episodes_this_iter: 96
  episodes_total: 7872
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12242.755
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 1.2784178256988525
        entropy_coeff: 0.0017600000137463212
        kl: 0.012093497440218925
        model: {}
        policy_loss: -0.027235426008701324
        total_loss: -0.025880537927150726
        vf_explained_var: 0.08506141602993011
        vf_loss: 11.86207389831543
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 1.2000200748443604
        entropy_coeff: 0.0017600000137463212
        kl: 0.014298835769295692
        model: {}
        policy_loss: -0.03408590704202652
        total_loss: -0.03207796439528465
        vf_explained_var: 0.02846240997314453
        vf_loss: 12.602149963378906
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 1.1849894523620605
        entropy_coeff: 0.0017600000137463212
        kl: 0.013662518933415413
        model: {}
        policy_loss: -0.030334744602441788
        total_loss: -0.028438545763492584
        vf_explained_var: 0.036528438329696655
        vf_loss: 12.492779731750488
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 0.9034014940261841
        entropy_coeff: 0.0017600000137463212
        kl: 0.009357854723930359
        model: {}
        policy_loss: -0.022170383483171463
        total_loss: -0.02080295979976654
        vf_explained_var: 0.16252075135707855
        vf_loss: 10.858428955078125
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 1.15583336353302
        entropy_coeff: 0.0017600000137463212
        kl: 0.013966730795800686
        model: {}
        policy_loss: -0.03175315260887146
        total_loss: -0.029851702973246574
        vf_explained_var: 0.1185191422700882
        vf_loss: 11.423704147338867
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007747776107862592
        entropy: 1.09035325050354
        entropy_coeff: 0.0017600000137463212
        kl: 0.012919459491968155
        model: {}
        policy_loss: -0.03475523367524147
        total_loss: -0.03297264501452446
        vf_explained_var: 0.13869313895702362
        vf_loss: 11.177212715148926
    load_time_ms: 20696.539
    num_steps_sampled: 7872000
    num_steps_trained: 7872000
    sample_time_ms: 104822.739
    update_time_ms: 86.982
  iterations_since_restore: 22
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.25977653631285
    ram_util_percent: 16.81564245810056
  pid: 30948
  policy_reward_max:
    agent-0: 126.33333333333381
    agent-1: 126.33333333333381
    agent-2: 126.33333333333381
    agent-3: 126.33333333333381
    agent-4: 126.33333333333381
    agent-5: 126.33333333333381
  policy_reward_mean:
    agent-0: 97.56333333333363
    agent-1: 97.56333333333363
    agent-2: 97.56333333333363
    agent-3: 97.56333333333363
    agent-4: 97.56333333333363
    agent-5: 97.56333333333363
  policy_reward_min:
    agent-0: 47.833333333333236
    agent-1: 47.833333333333236
    agent-2: 47.833333333333236
    agent-3: 47.833333333333236
    agent-4: 47.833333333333236
    agent-5: 47.833333333333236
  sampler_perf:
    mean_env_wait_ms: 29.332356307061378
    mean_inference_ms: 13.600845943975774
    mean_processing_ms: 62.927146946331995
  time_since_restore: 3522.31729388237
  time_this_iter_s: 125.69027853012085
  time_total_s: 12648.329107761383
  timestamp: 1637030584
  timesteps_since_restore: 2112000
  timesteps_this_iter: 96000
  timesteps_total: 7872000
  training_iteration: 82
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     82 |          12648.3 | 7872000 |   585.38 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 7.27
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 25.01
    apples_agent-1_min: 0
    apples_agent-2_max: 64
    apples_agent-2_mean: 6.56
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 83.76
    apples_agent-3_min: 23
    apples_agent-4_max: 129
    apples_agent-4_mean: 5.17
    apples_agent-4_min: 0
    apples_agent-5_max: 117
    apples_agent-5_mean: 67.9
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 437
    cleaning_beam_agent-0_mean: 314.34
    cleaning_beam_agent-0_min: 143
    cleaning_beam_agent-1_max: 508
    cleaning_beam_agent-1_mean: 242.13
    cleaning_beam_agent-1_min: 55
    cleaning_beam_agent-2_max: 600
    cleaning_beam_agent-2_mean: 418.67
    cleaning_beam_agent-2_min: 210
    cleaning_beam_agent-3_max: 97
    cleaning_beam_agent-3_mean: 48.81
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 561
    cleaning_beam_agent-4_mean: 375.66
    cleaning_beam_agent-4_min: 170
    cleaning_beam_agent-5_max: 334
    cleaning_beam_agent-5_mean: 63.57
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-45-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 816.9999999999911
  episode_reward_mean: 556.1100000000005
  episode_reward_min: 197.99999999999832
  episodes_this_iter: 96
  episodes_total: 7968
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12252.956
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 1.2692251205444336
        entropy_coeff: 0.0017600000137463212
        kl: 0.012000499293208122
        model: {}
        policy_loss: -0.028202980756759644
        total_loss: -0.02674032375216484
        vf_explained_var: 0.10717688500881195
        vf_loss: 12.963937759399414
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 1.193581223487854
        entropy_coeff: 0.0017600000137463212
        kl: 0.01358924712985754
        model: {}
        policy_loss: -0.033401235938072205
        total_loss: -0.031369537115097046
        vf_explained_var: 0.025787636637687683
        vf_loss: 14.145522117614746
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 1.1695773601531982
        entropy_coeff: 0.0017600000137463212
        kl: 0.013927371241152287
        model: {}
        policy_loss: -0.030683187767863274
        total_loss: -0.028596952557563782
        vf_explained_var: 0.06371855735778809
        vf_loss: 13.592196464538574
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 0.9293069839477539
        entropy_coeff: 0.0017600000137463212
        kl: 0.01015540026128292
        model: {}
        policy_loss: -0.0236971415579319
        total_loss: -0.022191056981682777
        vf_explained_var: 0.2353137731552124
        vf_loss: 11.105846405029297
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 1.1774826049804688
        entropy_coeff: 0.0017600000137463212
        kl: 0.013788256794214249
        model: {}
        policy_loss: -0.03247440606355667
        total_loss: -0.030510274693369865
        vf_explained_var: 0.11950160562992096
        vf_loss: 12.788551330566406
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007687872275710106
        entropy: 1.1110968589782715
        entropy_coeff: 0.0017600000137463212
        kl: 0.013839127495884895
        model: {}
        policy_loss: -0.034622564911842346
        total_loss: -0.0326077826321125
        vf_explained_var: 0.17211943864822388
        vf_loss: 12.024881362915039
    load_time_ms: 19193.818
    num_steps_sampled: 7968000
    num_steps_trained: 7968000
    sample_time_ms: 103899.969
    update_time_ms: 73.715
  iterations_since_restore: 23
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.27444444444444
    ram_util_percent: 16.83111111111111
  pid: 30948
  policy_reward_max:
    agent-0: 136.1666666666672
    agent-1: 136.1666666666672
    agent-2: 136.1666666666672
    agent-3: 136.1666666666672
    agent-4: 136.1666666666672
    agent-5: 136.1666666666672
  policy_reward_mean:
    agent-0: 92.68500000000022
    agent-1: 92.68500000000022
    agent-2: 92.68500000000022
    agent-3: 92.68500000000022
    agent-4: 92.68500000000022
    agent-5: 92.68500000000022
  policy_reward_min:
    agent-0: 33.00000000000005
    agent-1: 33.00000000000005
    agent-2: 33.00000000000005
    agent-3: 33.00000000000005
    agent-4: 33.00000000000005
    agent-5: 33.00000000000005
  sampler_perf:
    mean_env_wait_ms: 29.19516548991349
    mean_inference_ms: 13.559439012938695
    mean_processing_ms: 62.608516910990176
  time_since_restore: 3648.20751285553
  time_this_iter_s: 125.89021897315979
  time_total_s: 12774.219326734543
  timestamp: 1637030710
  timesteps_since_restore: 2208000
  timesteps_this_iter: 96000
  timesteps_total: 7968000
  training_iteration: 83
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     83 |          12774.2 | 7968000 |   556.11 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 6.27
    apples_agent-0_min: 0
    apples_agent-1_max: 91
    apples_agent-1_mean: 22.93
    apples_agent-1_min: 0
    apples_agent-2_max: 137
    apples_agent-2_mean: 9.47
    apples_agent-2_min: 0
    apples_agent-3_max: 144
    apples_agent-3_mean: 83.69
    apples_agent-3_min: 34
    apples_agent-4_max: 77
    apples_agent-4_mean: 3.17
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 72.28
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 474
    cleaning_beam_agent-0_mean: 327.34
    cleaning_beam_agent-0_min: 157
    cleaning_beam_agent-1_max: 430
    cleaning_beam_agent-1_mean: 240.15
    cleaning_beam_agent-1_min: 49
    cleaning_beam_agent-2_max: 575
    cleaning_beam_agent-2_mean: 411.58
    cleaning_beam_agent-2_min: 151
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 48.48
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 592
    cleaning_beam_agent-4_mean: 366.86
    cleaning_beam_agent-4_min: 127
    cleaning_beam_agent-5_max: 258
    cleaning_beam_agent-5_mean: 73.37
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-47-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 863.999999999985
  episode_reward_mean: 579.9199999999996
  episode_reward_min: 308.00000000000165
  episodes_this_iter: 96
  episodes_total: 8064
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12204.597
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 1.2571122646331787
        entropy_coeff: 0.0017600000137463212
        kl: 0.011360116302967072
        model: {}
        policy_loss: -0.026991937309503555
        total_loss: -0.025509130209684372
        vf_explained_var: 0.11108671128749847
        vf_loss: 14.232992172241211
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 1.2097769975662231
        entropy_coeff: 0.0017600000137463212
        kl: 0.014262388460338116
        model: {}
        policy_loss: -0.03432384133338928
        total_loss: -0.03207123652100563
        vf_explained_var: 0.04577335715293884
        vf_loss: 15.293346405029297
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 1.186436414718628
        entropy_coeff: 0.0017600000137463212
        kl: 0.013675462454557419
        model: {}
        policy_loss: -0.030135951936244965
        total_loss: -0.027950357645750046
        vf_explained_var: 0.03992980718612671
        vf_loss: 15.386301040649414
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 0.900417685508728
        entropy_coeff: 0.0017600000137463212
        kl: 0.010075011290609837
        model: {}
        policy_loss: -0.023232348263263702
        total_loss: -0.021626079455018044
        vf_explained_var: 0.2660398483276367
        vf_loss: 11.760051727294922
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 1.1574023962020874
        entropy_coeff: 0.0017600000137463212
        kl: 0.013580923900008202
        model: {}
        policy_loss: -0.032284848392009735
        total_loss: -0.030272863805294037
        vf_explained_var: 0.16798293590545654
        vf_loss: 13.328266143798828
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007627967861481011
        entropy: 1.0758757591247559
        entropy_coeff: 0.0017600000137463212
        kl: 0.012605012394487858
        model: {}
        policy_loss: -0.03359702229499817
        total_loss: -0.03164701163768768
        vf_explained_var: 0.17426759004592896
        vf_loss: 13.225537300109863
    load_time_ms: 17951.944
    num_steps_sampled: 8064000
    num_steps_trained: 8064000
    sample_time_ms: 103505.884
    update_time_ms: 72.937
  iterations_since_restore: 24
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.1478021978022
    ram_util_percent: 16.77197802197802
  pid: 30948
  policy_reward_max:
    agent-0: 144.00000000000043
    agent-1: 144.00000000000043
    agent-2: 144.00000000000043
    agent-3: 144.00000000000043
    agent-4: 144.00000000000043
    agent-5: 144.00000000000043
  policy_reward_mean:
    agent-0: 96.65333333333358
    agent-1: 96.65333333333358
    agent-2: 96.65333333333358
    agent-3: 96.65333333333358
    agent-4: 96.65333333333358
    agent-5: 96.65333333333358
  policy_reward_min:
    agent-0: 51.33333333333321
    agent-1: 51.33333333333321
    agent-2: 51.33333333333321
    agent-3: 51.33333333333321
    agent-4: 51.33333333333321
    agent-5: 51.33333333333321
  sampler_perf:
    mean_env_wait_ms: 29.09310325823421
    mean_inference_ms: 13.526425329814725
    mean_processing_ms: 62.37618543860145
  time_since_restore: 3776.4118206501007
  time_this_iter_s: 128.20430779457092
  time_total_s: 12902.423634529114
  timestamp: 1637030838
  timesteps_since_restore: 2304000
  timesteps_this_iter: 96000
  timesteps_total: 8064000
  training_iteration: 84
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     84 |          12902.4 | 8064000 |   579.92 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 68
    apples_agent-0_mean: 7.13
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 24.44
    apples_agent-1_min: 0
    apples_agent-2_max: 116
    apples_agent-2_mean: 6.92
    apples_agent-2_min: 0
    apples_agent-3_max: 153
    apples_agent-3_mean: 89.19
    apples_agent-3_min: 25
    apples_agent-4_max: 58
    apples_agent-4_mean: 2.82
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 74.66
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 398
    cleaning_beam_agent-0_mean: 299.92
    cleaning_beam_agent-0_min: 84
    cleaning_beam_agent-1_max: 454
    cleaning_beam_agent-1_mean: 245.83
    cleaning_beam_agent-1_min: 49
    cleaning_beam_agent-2_max: 631
    cleaning_beam_agent-2_mean: 427.43
    cleaning_beam_agent-2_min: 205
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 48.29
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 557
    cleaning_beam_agent-4_mean: 374.27
    cleaning_beam_agent-4_min: 170
    cleaning_beam_agent-5_max: 214
    cleaning_beam_agent-5_mean: 60.75
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-49-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 841.9999999999975
  episode_reward_mean: 590.4099999999987
  episode_reward_min: 143.9999999999994
  episodes_this_iter: 96
  episodes_total: 8160
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12171.224
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 1.292479395866394
        entropy_coeff: 0.0017600000137463212
        kl: 0.012190236710011959
        model: {}
        policy_loss: -0.030828652903437614
        total_loss: -0.029203146696090698
        vf_explained_var: 0.09432417154312134
        vf_loss: 14.622251510620117
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 1.215065836906433
        entropy_coeff: 0.0017600000137463212
        kl: 0.014107412658631802
        model: {}
        policy_loss: -0.03389272466301918
        total_loss: -0.031662750989198685
        vf_explained_var: 0.04226435720920563
        vf_loss: 15.47004508972168
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 1.1709513664245605
        entropy_coeff: 0.0017600000137463212
        kl: 0.013307095505297184
        model: {}
        policy_loss: -0.03017570823431015
        total_loss: -0.0280507430434227
        vf_explained_var: 0.054660096764564514
        vf_loss: 15.244197845458984
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 0.9178135395050049
        entropy_coeff: 0.0017600000137463212
        kl: 0.00946937408298254
        model: {}
        policy_loss: -0.023503035306930542
        total_loss: -0.02193829044699669
        vf_explained_var: 0.20163953304290771
        vf_loss: 12.862232208251953
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 1.155184030532837
        entropy_coeff: 0.0017600000137463212
        kl: 0.01514425128698349
        model: {}
        policy_loss: -0.03241217881441116
        total_loss: -0.029995815828442574
        vf_explained_var: 0.11917135119438171
        vf_loss: 14.206387519836426
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007568064029328525
        entropy: 1.0526342391967773
        entropy_coeff: 0.0017600000137463212
        kl: 0.014217103831470013
        model: {}
        policy_loss: -0.03369650989770889
        total_loss: -0.031422749161720276
        vf_explained_var: 0.20476917922496796
        vf_loss: 12.8297119140625
    load_time_ms: 16419.501
    num_steps_sampled: 8160000
    num_steps_trained: 8160000
    sample_time_ms: 103208.281
    update_time_ms: 42.134
  iterations_since_restore: 25
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.321739130434775
    ram_util_percent: 16.441304347826087
  pid: 30948
  policy_reward_max:
    agent-0: 140.33333333333326
    agent-1: 140.33333333333326
    agent-2: 140.33333333333326
    agent-3: 140.33333333333326
    agent-4: 140.33333333333326
    agent-5: 140.33333333333326
  policy_reward_mean:
    agent-0: 98.40166666666696
    agent-1: 98.40166666666696
    agent-2: 98.40166666666696
    agent-3: 98.40166666666696
    agent-4: 98.40166666666696
    agent-5: 98.40166666666696
  policy_reward_min:
    agent-0: 24.000000000000014
    agent-1: 24.000000000000014
    agent-2: 24.000000000000014
    agent-3: 24.000000000000014
    agent-4: 24.000000000000014
    agent-5: 24.000000000000014
  sampler_perf:
    mean_env_wait_ms: 29.026409177989585
    mean_inference_ms: 13.4993152746207
    mean_processing_ms: 62.19653460768613
  time_since_restore: 3905.431926012039
  time_this_iter_s: 129.02010536193848
  time_total_s: 13031.443739891052
  timestamp: 1637030968
  timesteps_since_restore: 2400000
  timesteps_this_iter: 96000
  timesteps_total: 8160000
  training_iteration: 85
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     85 |          13031.4 | 8160000 |   590.41 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 5.19
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 28.87
    apples_agent-1_min: 0
    apples_agent-2_max: 104
    apples_agent-2_mean: 10.2
    apples_agent-2_min: 0
    apples_agent-3_max: 203
    apples_agent-3_mean: 87.45
    apples_agent-3_min: 7
    apples_agent-4_max: 82
    apples_agent-4_mean: 3.46
    apples_agent-4_min: 0
    apples_agent-5_max: 130
    apples_agent-5_mean: 73.44
    apples_agent-5_min: 11
    cleaning_beam_agent-0_max: 443
    cleaning_beam_agent-0_mean: 309.19
    cleaning_beam_agent-0_min: 88
    cleaning_beam_agent-1_max: 465
    cleaning_beam_agent-1_mean: 237.33
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 600
    cleaning_beam_agent-2_mean: 400.28
    cleaning_beam_agent-2_min: 169
    cleaning_beam_agent-3_max: 78
    cleaning_beam_agent-3_mean: 43.81
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 406.02
    cleaning_beam_agent-4_min: 156
    cleaning_beam_agent-5_max: 346
    cleaning_beam_agent-5_mean: 71.34
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-51-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 831.9999999999933
  episode_reward_mean: 601.4099999999978
  episode_reward_min: 101.00000000000055
  episodes_this_iter: 96
  episodes_total: 8256
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12129.54
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 1.2943423986434937
        entropy_coeff: 0.0017600000137463212
        kl: 0.01145870704203844
        model: {}
        policy_loss: -0.029286056756973267
        total_loss: -0.027948832139372826
        vf_explained_var: 0.1462961584329605
        vf_loss: 13.235262870788574
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 1.2242079973220825
        entropy_coeff: 0.0017600000137463212
        kl: 0.014274426735937595
        model: {}
        policy_loss: -0.034807443618774414
        total_loss: -0.032566916197538376
        vf_explained_var: 0.0067813098430633545
        vf_loss: 15.40246868133545
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 1.1781868934631348
        entropy_coeff: 0.0017600000137463212
        kl: 0.013055568560957909
        model: {}
        policy_loss: -0.03023630380630493
        total_loss: -0.028280407190322876
        vf_explained_var: 0.08454902470111847
        vf_loss: 14.183934211730957
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 0.8912838697433472
        entropy_coeff: 0.0017600000137463212
        kl: 0.009656013920903206
        model: {}
        policy_loss: -0.023747948929667473
        total_loss: -0.022125987336039543
        vf_explained_var: 0.18677303194999695
        vf_loss: 12.594216346740723
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 1.1324436664581299
        entropy_coeff: 0.0017600000137463212
        kl: 0.013462861999869347
        model: {}
        policy_loss: -0.0324365571141243
        total_loss: -0.03040885180234909
        vf_explained_var: 0.1430230587720871
        vf_loss: 13.28233528137207
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007508160197176039
        entropy: 1.0479658842086792
        entropy_coeff: 0.0017600000137463212
        kl: 0.01460809726268053
        model: {}
        policy_loss: -0.031439196318387985
        total_loss: -0.029059527441859245
        vf_explained_var: 0.15932703018188477
        vf_loss: 13.024736404418945
    load_time_ms: 15456.617
    num_steps_sampled: 8256000
    num_steps_trained: 8256000
    sample_time_ms: 102245.517
    update_time_ms: 41.632
  iterations_since_restore: 26
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.302247191011233
    ram_util_percent: 16.81067415730337
  pid: 30948
  policy_reward_max:
    agent-0: 138.66666666666688
    agent-1: 138.66666666666688
    agent-2: 138.66666666666688
    agent-3: 138.66666666666688
    agent-4: 138.66666666666688
    agent-5: 138.66666666666688
  policy_reward_mean:
    agent-0: 100.23500000000027
    agent-1: 100.23500000000027
    agent-2: 100.23500000000027
    agent-3: 100.23500000000027
    agent-4: 100.23500000000027
    agent-5: 100.23500000000027
  policy_reward_min:
    agent-0: 16.83333333333332
    agent-1: 16.83333333333332
    agent-2: 16.83333333333332
    agent-3: 16.83333333333332
    agent-4: 16.83333333333332
    agent-5: 16.83333333333332
  sampler_perf:
    mean_env_wait_ms: 28.930631929835677
    mean_inference_ms: 13.464427304190068
    mean_processing_ms: 61.94914852201856
  time_since_restore: 4030.2991194725037
  time_this_iter_s: 124.86719346046448
  time_total_s: 13156.310933351517
  timestamp: 1637031093
  timesteps_since_restore: 2496000
  timesteps_this_iter: 96000
  timesteps_total: 8256000
  training_iteration: 86
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     86 |          13156.3 | 8256000 |   601.41 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 81
    apples_agent-0_mean: 5.83
    apples_agent-0_min: 0
    apples_agent-1_max: 71
    apples_agent-1_mean: 20.73
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 7.12
    apples_agent-2_min: 0
    apples_agent-3_max: 145
    apples_agent-3_mean: 86.18
    apples_agent-3_min: 14
    apples_agent-4_max: 63
    apples_agent-4_mean: 2.85
    apples_agent-4_min: 0
    apples_agent-5_max: 133
    apples_agent-5_mean: 75.99
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 421
    cleaning_beam_agent-0_mean: 299.05
    cleaning_beam_agent-0_min: 73
    cleaning_beam_agent-1_max: 480
    cleaning_beam_agent-1_mean: 256.71
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 574
    cleaning_beam_agent-2_mean: 409.15
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 48.34
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 560
    cleaning_beam_agent-4_mean: 393.8
    cleaning_beam_agent-4_min: 134
    cleaning_beam_agent-5_max: 326
    cleaning_beam_agent-5_mean: 76.61
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-53-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 865.9999999999811
  episode_reward_mean: 611.1699999999976
  episode_reward_min: 155.9999999999998
  episodes_this_iter: 96
  episodes_total: 8352
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12106.191
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 1.2752411365509033
        entropy_coeff: 0.0017600000137463212
        kl: 0.012214191257953644
        model: {}
        policy_loss: -0.03209991753101349
        total_loss: -0.030428018420934677
        vf_explained_var: 0.11876852810382843
        vf_loss: 14.734793663024902
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 1.2367210388183594
        entropy_coeff: 0.0017600000137463212
        kl: 0.01363112311810255
        model: {}
        policy_loss: -0.03413769602775574
        total_loss: -0.03203196078538895
        vf_explained_var: 0.06839112937450409
        vf_loss: 15.561412811279297
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 1.1684346199035645
        entropy_coeff: 0.0017600000137463212
        kl: 0.013689784333109856
        model: {}
        policy_loss: -0.030926603823900223
        total_loss: -0.028646985068917274
        vf_explained_var: 0.0437026172876358
        vf_loss: 15.981078147888184
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 0.8764657974243164
        entropy_coeff: 0.0017600000137463212
        kl: 0.010086596012115479
        model: {}
        policy_loss: -0.024297039955854416
        total_loss: -0.022531364113092422
        vf_explained_var: 0.2270965725183487
        vf_loss: 12.909407615661621
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 1.1492981910705566
        entropy_coeff: 0.0017600000137463212
        kl: 0.014047706499695778
        model: {}
        policy_loss: -0.0330427885055542
        total_loss: -0.030827248468995094
        vf_explained_var: 0.14509494602680206
        vf_loss: 14.287616729736328
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007448255782946944
        entropy: 1.0492137670516968
        entropy_coeff: 0.0017600000137463212
        kl: 0.013315297663211823
        model: {}
        policy_loss: -0.03515032306313515
        total_loss: -0.03304493427276611
        vf_explained_var: 0.22896498441696167
        vf_loss: 12.889457702636719
    load_time_ms: 13538.807
    num_steps_sampled: 8352000
    num_steps_trained: 8352000
    sample_time_ms: 101513.282
    update_time_ms: 14.335
  iterations_since_restore: 27
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.474719101123597
    ram_util_percent: 16.82808988764045
  pid: 30948
  policy_reward_max:
    agent-0: 144.33333333333354
    agent-1: 144.33333333333354
    agent-2: 144.33333333333354
    agent-3: 144.33333333333354
    agent-4: 144.33333333333354
    agent-5: 144.33333333333354
  policy_reward_mean:
    agent-0: 101.86166666666696
    agent-1: 101.86166666666696
    agent-2: 101.86166666666696
    agent-3: 101.86166666666696
    agent-4: 101.86166666666696
    agent-5: 101.86166666666696
  policy_reward_min:
    agent-0: 26.000000000000043
    agent-1: 26.000000000000043
    agent-2: 26.000000000000043
    agent-3: 26.000000000000043
    agent-4: 26.000000000000043
    agent-5: 26.000000000000043
  sampler_perf:
    mean_env_wait_ms: 28.84490655829684
    mean_inference_ms: 13.432731343217087
    mean_processing_ms: 61.717681442399595
  time_since_restore: 4155.139828681946
  time_this_iter_s: 124.84070920944214
  time_total_s: 13281.151642560959
  timestamp: 1637031218
  timesteps_since_restore: 2592000
  timesteps_this_iter: 96000
  timesteps_total: 8352000
  training_iteration: 87
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     87 |          13281.2 | 8352000 |   611.17 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 85
    apples_agent-0_mean: 4.74
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 20.63
    apples_agent-1_min: 0
    apples_agent-2_max: 73
    apples_agent-2_mean: 9.08
    apples_agent-2_min: 0
    apples_agent-3_max: 127
    apples_agent-3_mean: 81.44
    apples_agent-3_min: 19
    apples_agent-4_max: 51
    apples_agent-4_mean: 4.55
    apples_agent-4_min: 0
    apples_agent-5_max: 169
    apples_agent-5_mean: 68.65
    apples_agent-5_min: 14
    cleaning_beam_agent-0_max: 438
    cleaning_beam_agent-0_mean: 296.95
    cleaning_beam_agent-0_min: 68
    cleaning_beam_agent-1_max: 522
    cleaning_beam_agent-1_mean: 241.36
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 629
    cleaning_beam_agent-2_mean: 402.06
    cleaning_beam_agent-2_min: 140
    cleaning_beam_agent-3_max: 76
    cleaning_beam_agent-3_mean: 47.06
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 601
    cleaning_beam_agent-4_mean: 408.79
    cleaning_beam_agent-4_min: 211
    cleaning_beam_agent-5_max: 314
    cleaning_beam_agent-5_mean: 79.63
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-55-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 834.9999999999851
  episode_reward_mean: 582.3899999999976
  episode_reward_min: 115.00000000000087
  episodes_this_iter: 96
  episodes_total: 8448
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12154.561
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 1.2675039768218994
        entropy_coeff: 0.0017600000137463212
        kl: 0.012219653464853764
        model: {}
        policy_loss: -0.03051912412047386
        total_loss: -0.028744205832481384
        vf_explained_var: 0.1051710695028305
        vf_loss: 15.617952346801758
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 1.1950221061706543
        entropy_coeff: 0.0017600000137463212
        kl: 0.014105459675192833
        model: {}
        policy_loss: -0.03365185856819153
        total_loss: -0.031235618516802788
        vf_explained_var: 0.0260484516620636
        vf_loss: 16.983835220336914
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 1.1668206453323364
        entropy_coeff: 0.0017600000137463212
        kl: 0.01378565188497305
        model: {}
        policy_loss: -0.03114720806479454
        total_loss: -0.028876973316073418
        vf_explained_var: 0.10157287120819092
        vf_loss: 15.667110443115234
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 0.9105364680290222
        entropy_coeff: 0.0017600000137463212
        kl: 0.010964931920170784
        model: {}
        policy_loss: -0.02427113614976406
        total_loss: -0.022345945239067078
        vf_explained_var: 0.23432479798793793
        vf_loss: 13.347480773925781
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 1.1485621929168701
        entropy_coeff: 0.0017600000137463212
        kl: 0.014198210090398788
        model: {}
        policy_loss: -0.034317705780267715
        total_loss: -0.03198462724685669
        vf_explained_var: 0.1308935135602951
        vf_loss: 15.149044036865234
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007388351950794458
        entropy: 1.0380476713180542
        entropy_coeff: 0.0017600000137463212
        kl: 0.012659478932619095
        model: {}
        policy_loss: -0.0325297936797142
        total_loss: -0.030549004673957825
        vf_explained_var: 0.26830506324768066
        vf_loss: 12.758598327636719
    load_time_ms: 13991.934
    num_steps_sampled: 8448000
    num_steps_trained: 8448000
    sample_time_ms: 101465.874
    update_time_ms: 15.123
  iterations_since_restore: 28
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.582291666666666
    ram_util_percent: 16.89635416666667
  pid: 30948
  policy_reward_max:
    agent-0: 139.16666666666677
    agent-1: 139.16666666666677
    agent-2: 139.16666666666677
    agent-3: 139.16666666666677
    agent-4: 139.16666666666677
    agent-5: 139.16666666666677
  policy_reward_mean:
    agent-0: 97.0650000000003
    agent-1: 97.0650000000003
    agent-2: 97.0650000000003
    agent-3: 97.0650000000003
    agent-4: 97.0650000000003
    agent-5: 97.0650000000003
  policy_reward_min:
    agent-0: 19.16666666666667
    agent-1: 19.16666666666667
    agent-2: 19.16666666666667
    agent-3: 19.16666666666667
    agent-4: 19.16666666666667
    agent-5: 19.16666666666667
  sampler_perf:
    mean_env_wait_ms: 28.78350455067086
    mean_inference_ms: 13.40911844330851
    mean_processing_ms: 61.55156273764666
  time_since_restore: 4289.933322429657
  time_this_iter_s: 134.79349374771118
  time_total_s: 13415.94513630867
  timestamp: 1637031353
  timesteps_since_restore: 2688000
  timesteps_this_iter: 96000
  timesteps_total: 8448000
  training_iteration: 88
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     88 |          13415.9 | 8448000 |   582.39 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 3.33
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 27.25
    apples_agent-1_min: 0
    apples_agent-2_max: 134
    apples_agent-2_mean: 9.13
    apples_agent-2_min: 0
    apples_agent-3_max: 149
    apples_agent-3_mean: 80.69
    apples_agent-3_min: 22
    apples_agent-4_max: 63
    apples_agent-4_mean: 3.97
    apples_agent-4_min: 0
    apples_agent-5_max: 122
    apples_agent-5_mean: 68.4
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 452
    cleaning_beam_agent-0_mean: 304.88
    cleaning_beam_agent-0_min: 104
    cleaning_beam_agent-1_max: 436
    cleaning_beam_agent-1_mean: 225.35
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 691
    cleaning_beam_agent-2_mean: 416.25
    cleaning_beam_agent-2_min: 183
    cleaning_beam_agent-3_max: 118
    cleaning_beam_agent-3_mean: 54.22
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 556
    cleaning_beam_agent-4_mean: 399.72
    cleaning_beam_agent-4_min: 149
    cleaning_beam_agent-5_max: 395
    cleaning_beam_agent-5_mean: 77.16
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.06
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_21-58-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 834.9999999999752
  episode_reward_mean: 585.7399999999984
  episode_reward_min: 222.9999999999966
  episodes_this_iter: 96
  episodes_total: 8544
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12202.519
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 1.2509276866912842
        entropy_coeff: 0.0017600000137463212
        kl: 0.01393043715506792
        model: {}
        policy_loss: -0.027994442731142044
        total_loss: -0.026034653186798096
        vf_explained_var: 0.12031283974647522
        vf_loss: 13.753317832946777
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 1.210664987564087
        entropy_coeff: 0.0017600000137463212
        kl: 0.013572607189416885
        model: {}
        policy_loss: -0.03472113609313965
        total_loss: -0.03263217955827713
        vf_explained_var: 0.03859563171863556
        vf_loss: 15.052018165588379
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 1.154664397239685
        entropy_coeff: 0.0017600000137463212
        kl: 0.013206561096012592
        model: {}
        policy_loss: -0.031764715909957886
        total_loss: -0.029670633375644684
        vf_explained_var: 0.05059508979320526
        vf_loss: 14.849822998046875
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 0.9212274551391602
        entropy_coeff: 0.0017600000137463212
        kl: 0.01208714209496975
        model: {}
        policy_loss: -0.022968733683228493
        total_loss: -0.020934760570526123
        vf_explained_var: 0.20888392627239227
        vf_loss: 12.3790922164917
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 1.1277486085891724
        entropy_coeff: 0.0017600000137463212
        kl: 0.014286484569311142
        model: {}
        policy_loss: -0.032112158834934235
        total_loss: -0.02991049736738205
        vf_explained_var: 0.1499072015285492
        vf_loss: 13.292028427124023
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007328448118641973
        entropy: 1.0260359048843384
        entropy_coeff: 0.0017600000137463212
        kl: 0.01351796556264162
        model: {}
        policy_loss: -0.03383009508252144
        total_loss: -0.03167860582470894
        vf_explained_var: 0.1976168155670166
        vf_loss: 12.537151336669922
    load_time_ms: 13934.463
    num_steps_sampled: 8544000
    num_steps_trained: 8544000
    sample_time_ms: 101394.158
    update_time_ms: 15.646
  iterations_since_restore: 29
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.127868852459013
    ram_util_percent: 16.384699453551917
  pid: 30948
  policy_reward_max:
    agent-0: 139.1666666666669
    agent-1: 139.1666666666669
    agent-2: 139.1666666666669
    agent-3: 139.1666666666669
    agent-4: 139.1666666666669
    agent-5: 139.1666666666669
  policy_reward_mean:
    agent-0: 97.62333333333359
    agent-1: 97.62333333333359
    agent-2: 97.62333333333359
    agent-3: 97.62333333333359
    agent-4: 97.62333333333359
    agent-5: 97.62333333333359
  policy_reward_min:
    agent-0: 37.16666666666667
    agent-1: 37.16666666666667
    agent-2: 37.16666666666667
    agent-3: 37.16666666666667
    agent-4: 37.16666666666667
    agent-5: 37.16666666666667
  sampler_perf:
    mean_env_wait_ms: 28.71354611556165
    mean_inference_ms: 13.380863245180054
    mean_processing_ms: 61.395235102380795
  time_since_restore: 4418.226762294769
  time_this_iter_s: 128.2934398651123
  time_total_s: 13544.238576173782
  timestamp: 1637031481
  timesteps_since_restore: 2784000
  timesteps_this_iter: 96000
  timesteps_total: 8544000
  training_iteration: 89
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.4/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     89 |          13544.2 | 8544000 |   585.74 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 4.02
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 26.3
    apples_agent-1_min: 0
    apples_agent-2_max: 134
    apples_agent-2_mean: 10.41
    apples_agent-2_min: 0
    apples_agent-3_max: 135
    apples_agent-3_mean: 81.15
    apples_agent-3_min: 23
    apples_agent-4_max: 50
    apples_agent-4_mean: 1.44
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 72.68
    apples_agent-5_min: 12
    cleaning_beam_agent-0_max: 400
    cleaning_beam_agent-0_mean: 276.4
    cleaning_beam_agent-0_min: 98
    cleaning_beam_agent-1_max: 462
    cleaning_beam_agent-1_mean: 220.85
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 682
    cleaning_beam_agent-2_mean: 419.31
    cleaning_beam_agent-2_min: 154
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 53.43
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 639
    cleaning_beam_agent-4_mean: 432.09
    cleaning_beam_agent-4_min: 183
    cleaning_beam_agent-5_max: 317
    cleaning_beam_agent-5_mean: 72.78
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 2
    fire_beam_agent-3_mean: 0.03
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-00-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 822.9999999999844
  episode_reward_mean: 627.6599999999984
  episode_reward_min: 238.99999999999525
  episodes_this_iter: 96
  episodes_total: 8640
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12126.692
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 1.2701365947723389
        entropy_coeff: 0.0017600000137463212
        kl: 0.012228714302182198
        model: {}
        policy_loss: -0.030956503003835678
        total_loss: -0.02945759892463684
        vf_explained_var: 0.10566279292106628
        vf_loss: 12.886043548583984
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 1.2090330123901367
        entropy_coeff: 0.0017600000137463212
        kl: 0.013566922396421432
        model: {}
        policy_loss: -0.034529246389865875
        total_loss: -0.032536283135414124
        vf_explained_var: 0.02372261881828308
        vf_loss: 14.074764251708984
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 1.1649624109268188
        entropy_coeff: 0.0017600000137463212
        kl: 0.014319056645035744
        model: {}
        policy_loss: -0.03108273819088936
        total_loss: -0.02892628125846386
        vf_explained_var: 0.06714971363544464
        vf_loss: 13.429791450500488
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 0.8587177395820618
        entropy_coeff: 0.0017600000137463212
        kl: 0.009417098015546799
        model: {}
        policy_loss: -0.02289608120918274
        total_loss: -0.021306540817022324
        vf_explained_var: 0.15401220321655273
        vf_loss: 12.174675941467285
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 1.0930287837982178
        entropy_coeff: 0.0017600000137463212
        kl: 0.012915614992380142
        model: {}
        policy_loss: -0.030966628342866898
        total_loss: -0.029091015458106995
        vf_explained_var: 0.15443992614746094
        vf_loss: 12.162200927734375
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007268544286489487
        entropy: 0.9981399178504944
        entropy_coeff: 0.0017600000137463212
        kl: 0.01211551669985056
        model: {}
        policy_loss: -0.031993355602025986
        total_loss: -0.030145205557346344
        vf_explained_var: 0.17882002890110016
        vf_loss: 11.817715644836426
    load_time_ms: 13912.6
    num_steps_sampled: 8640000
    num_steps_trained: 8640000
    sample_time_ms: 101512.862
    update_time_ms: 15.471
  iterations_since_restore: 30
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.43388888888889
    ram_util_percent: 16.77888888888889
  pid: 30948
  policy_reward_max:
    agent-0: 137.16666666666686
    agent-1: 137.16666666666686
    agent-2: 137.16666666666686
    agent-3: 137.16666666666686
    agent-4: 137.16666666666686
    agent-5: 137.16666666666686
  policy_reward_mean:
    agent-0: 104.61000000000031
    agent-1: 104.61000000000031
    agent-2: 104.61000000000031
    agent-3: 104.61000000000031
    agent-4: 104.61000000000031
    agent-5: 104.61000000000031
  policy_reward_min:
    agent-0: 39.8333333333333
    agent-1: 39.8333333333333
    agent-2: 39.8333333333333
    agent-3: 39.8333333333333
    agent-4: 39.8333333333333
    agent-5: 39.8333333333333
  sampler_perf:
    mean_env_wait_ms: 28.654656136739618
    mean_inference_ms: 13.358479896760327
    mean_processing_ms: 61.24071600061468
  time_since_restore: 4544.8509068489075
  time_this_iter_s: 126.62414455413818
  time_total_s: 13670.86272072792
  timestamp: 1637031608
  timesteps_since_restore: 2880000
  timesteps_this_iter: 96000
  timesteps_total: 8640000
  training_iteration: 90
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     90 |          13670.9 | 8640000 |   627.66 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 5.1
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 27.49
    apples_agent-1_min: 0
    apples_agent-2_max: 63
    apples_agent-2_mean: 6.89
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 79.14
    apples_agent-3_min: 32
    apples_agent-4_max: 74
    apples_agent-4_mean: 2.58
    apples_agent-4_min: 0
    apples_agent-5_max: 148
    apples_agent-5_mean: 75.04
    apples_agent-5_min: 3
    cleaning_beam_agent-0_max: 458
    cleaning_beam_agent-0_mean: 270.58
    cleaning_beam_agent-0_min: 103
    cleaning_beam_agent-1_max: 478
    cleaning_beam_agent-1_mean: 215.44
    cleaning_beam_agent-1_min: 57
    cleaning_beam_agent-2_max: 646
    cleaning_beam_agent-2_mean: 428.55
    cleaning_beam_agent-2_min: 184
    cleaning_beam_agent-3_max: 107
    cleaning_beam_agent-3_mean: 57.8
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 602
    cleaning_beam_agent-4_mean: 394.41
    cleaning_beam_agent-4_min: 106
    cleaning_beam_agent-5_max: 231
    cleaning_beam_agent-5_mean: 71.34
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 16
    fire_beam_agent-5_mean: 0.19
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-02-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 853.9999999999849
  episode_reward_mean: 600.8499999999977
  episode_reward_min: 263.99999999999835
  episodes_this_iter: 96
  episodes_total: 8736
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12025.581
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 1.268540620803833
        entropy_coeff: 0.0017600000137463212
        kl: 0.012201985344290733
        model: {}
        policy_loss: -0.031297631561756134
        total_loss: -0.02972625195980072
        vf_explained_var: 0.08194531500339508
        vf_loss: 13.636164665222168
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 1.2207931280136108
        entropy_coeff: 0.0017600000137463212
        kl: 0.014092367142438889
        model: {}
        policy_loss: -0.035141799598932266
        total_loss: -0.03303932771086693
        vf_explained_var: 0.03531457483768463
        vf_loss: 14.325957298278809
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 1.1764733791351318
        entropy_coeff: 0.0017600000137463212
        kl: 0.013547400012612343
        model: {}
        policy_loss: -0.031346939504146576
        total_loss: -0.029276054352521896
        vf_explained_var: 0.035036906599998474
        vf_loss: 14.319994926452637
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 0.869173526763916
        entropy_coeff: 0.0017600000137463212
        kl: 0.010772494599223137
        model: {}
        policy_loss: -0.02345351129770279
        total_loss: -0.02163267321884632
        vf_explained_var: 0.1939133107662201
        vf_loss: 11.960904121398926
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 1.1215914487838745
        entropy_coeff: 0.0017600000137463212
        kl: 0.01426132395863533
        model: {}
        policy_loss: -0.03488738462328911
        total_loss: -0.03277357295155525
        vf_explained_var: 0.16770769655704498
        vf_loss: 12.355438232421875
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007208639872260392
        entropy: 1.0389087200164795
        entropy_coeff: 0.0017600000137463212
        kl: 0.01343834400177002
        model: {}
        policy_loss: -0.03606503829360008
        total_loss: -0.033974576741456985
        vf_explained_var: 0.17049100995063782
        vf_loss: 12.312705993652344
    load_time_ms: 13925.511
    num_steps_sampled: 8736000
    num_steps_trained: 8736000
    sample_time_ms: 101209.273
    update_time_ms: 15.129
  iterations_since_restore: 31
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.62881355932204
    ram_util_percent: 16.805649717514125
  pid: 30948
  policy_reward_max:
    agent-0: 142.33333333333331
    agent-1: 142.33333333333331
    agent-2: 142.33333333333331
    agent-3: 142.33333333333331
    agent-4: 142.33333333333331
    agent-5: 142.33333333333331
  policy_reward_mean:
    agent-0: 100.14166666666695
    agent-1: 100.14166666666695
    agent-2: 100.14166666666695
    agent-3: 100.14166666666695
    agent-4: 100.14166666666695
    agent-5: 100.14166666666695
  policy_reward_min:
    agent-0: 43.99999999999996
    agent-1: 43.99999999999996
    agent-2: 43.99999999999996
    agent-3: 43.99999999999996
    agent-4: 43.99999999999996
    agent-5: 43.99999999999996
  sampler_perf:
    mean_env_wait_ms: 28.568879329765036
    mean_inference_ms: 13.331227648369039
    mean_processing_ms: 61.07619333442387
  time_since_restore: 4669.494130373001
  time_this_iter_s: 124.64322352409363
  time_total_s: 13795.505944252014
  timestamp: 1637031733
  timesteps_since_restore: 2976000
  timesteps_this_iter: 96000
  timesteps_total: 8736000
  training_iteration: 91
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     91 |          13795.5 | 8736000 |   600.85 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 4.86
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 28.63
    apples_agent-1_min: 0
    apples_agent-2_max: 83
    apples_agent-2_mean: 7.23
    apples_agent-2_min: 0
    apples_agent-3_max: 130
    apples_agent-3_mean: 76.29
    apples_agent-3_min: 29
    apples_agent-4_max: 63
    apples_agent-4_mean: 2.5
    apples_agent-4_min: 0
    apples_agent-5_max: 148
    apples_agent-5_mean: 72.53
    apples_agent-5_min: 9
    cleaning_beam_agent-0_max: 385
    cleaning_beam_agent-0_mean: 272.67
    cleaning_beam_agent-0_min: 103
    cleaning_beam_agent-1_max: 446
    cleaning_beam_agent-1_mean: 207.65
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 644
    cleaning_beam_agent-2_mean: 421.56
    cleaning_beam_agent-2_min: 111
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 51.19
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 571
    cleaning_beam_agent-4_mean: 416.72
    cleaning_beam_agent-4_min: 188
    cleaning_beam_agent-5_max: 296
    cleaning_beam_agent-5_mean: 85.59
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-04-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 811.9999999999878
  episode_reward_mean: 598.369999999999
  episode_reward_min: 194.99999999999838
  episodes_this_iter: 96
  episodes_total: 8832
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12124.064
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 1.2590528726577759
        entropy_coeff: 0.0017600000137463212
        kl: 0.012541186064481735
        model: {}
        policy_loss: -0.032061342149972916
        total_loss: -0.030407698825001717
        vf_explained_var: 0.10154981911182404
        vf_loss: 13.613388061523438
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 1.211789608001709
        entropy_coeff: 0.0017600000137463212
        kl: 0.014518080279231071
        model: {}
        policy_loss: -0.03553495183587074
        total_loss: -0.03327599912881851
        vf_explained_var: 0.01837746798992157
        vf_loss: 14.880847930908203
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 1.1701650619506836
        entropy_coeff: 0.0017600000137463212
        kl: 0.013333468697965145
        model: {}
        policy_loss: -0.031278096139431
        total_loss: -0.02928866446018219
        vf_explained_var: 0.0873786211013794
        vf_loss: 13.82227611541748
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 0.8274756669998169
        entropy_coeff: 0.0017600000137463212
        kl: 0.012682168744504452
        model: {}
        policy_loss: -0.02100532129406929
        total_loss: -0.01863545924425125
        vf_explained_var: 0.14886510372161865
        vf_loss: 12.897867202758789
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 1.1030322313308716
        entropy_coeff: 0.0017600000137463212
        kl: 0.014265930280089378
        model: {}
        policy_loss: -0.03275040537118912
        total_loss: -0.03057214803993702
        vf_explained_var: 0.16320806741714478
        vf_loss: 12.664093971252441
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007148736040107906
        entropy: 1.0430952310562134
        entropy_coeff: 0.0017600000137463212
        kl: 0.013659638352692127
        model: {}
        policy_loss: -0.034883566200733185
        total_loss: -0.03276383876800537
        vf_explained_var: 0.19176100194454193
        vf_loss: 12.236452102661133
    load_time_ms: 13952.716
    num_steps_sampled: 8832000
    num_steps_trained: 8832000
    sample_time_ms: 101030.466
    update_time_ms: 16.406
  iterations_since_restore: 32
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.341340782122902
    ram_util_percent: 16.830167597765364
  pid: 30948
  policy_reward_max:
    agent-0: 135.33333333333343
    agent-1: 135.33333333333343
    agent-2: 135.33333333333343
    agent-3: 135.33333333333343
    agent-4: 135.33333333333343
    agent-5: 135.33333333333343
  policy_reward_mean:
    agent-0: 99.72833333333361
    agent-1: 99.72833333333361
    agent-2: 99.72833333333361
    agent-3: 99.72833333333361
    agent-4: 99.72833333333361
    agent-5: 99.72833333333361
  policy_reward_min:
    agent-0: 32.50000000000006
    agent-1: 32.50000000000006
    agent-2: 32.50000000000006
    agent-3: 32.50000000000006
    agent-4: 32.50000000000006
    agent-5: 32.50000000000006
  sampler_perf:
    mean_env_wait_ms: 28.50489070889696
    mean_inference_ms: 13.308092422205174
    mean_processing_ms: 60.910833978709064
  time_since_restore: 4794.662855148315
  time_this_iter_s: 125.16872477531433
  time_total_s: 13920.674669027328
  timestamp: 1637031858
  timesteps_since_restore: 3072000
  timesteps_this_iter: 96000
  timesteps_total: 8832000
  training_iteration: 92
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     92 |          13920.7 | 8832000 |   598.37 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 68
    apples_agent-0_mean: 4.95
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 31.8
    apples_agent-1_min: 0
    apples_agent-2_max: 132
    apples_agent-2_mean: 8.93
    apples_agent-2_min: 0
    apples_agent-3_max: 183
    apples_agent-3_mean: 79.06
    apples_agent-3_min: 16
    apples_agent-4_max: 58
    apples_agent-4_mean: 1.95
    apples_agent-4_min: 0
    apples_agent-5_max: 143
    apples_agent-5_mean: 73.72
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 380
    cleaning_beam_agent-0_mean: 278.79
    cleaning_beam_agent-0_min: 80
    cleaning_beam_agent-1_max: 371
    cleaning_beam_agent-1_mean: 202.08
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 614
    cleaning_beam_agent-2_mean: 419.27
    cleaning_beam_agent-2_min: 196
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 56.74
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 570
    cleaning_beam_agent-4_mean: 412.49
    cleaning_beam_agent-4_min: 235
    cleaning_beam_agent-5_max: 248
    cleaning_beam_agent-5_mean: 71.31
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-07-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 851.9999999999776
  episode_reward_mean: 592.7599999999984
  episode_reward_min: 229.99999999999582
  episodes_this_iter: 96
  episodes_total: 8928
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12097.532
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 1.243720531463623
        entropy_coeff: 0.0017600000137463212
        kl: 0.012153524905443192
        model: {}
        policy_loss: -0.03035724349319935
        total_loss: -0.02871451899409294
        vf_explained_var: 0.10107438266277313
        vf_loss: 14.009692192077637
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 1.193303108215332
        entropy_coeff: 0.0017600000137463212
        kl: 0.014107134193181992
        model: {}
        policy_loss: -0.03521303832530975
        total_loss: -0.03293291851878166
        vf_explained_var: 1.3917684555053711e-05
        vf_loss: 15.589093208312988
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 1.1522977352142334
        entropy_coeff: 0.0017600000137463212
        kl: 0.012820192612707615
        model: {}
        policy_loss: -0.03134780377149582
        total_loss: -0.029369061812758446
        vf_explained_var: 0.07379767298698425
        vf_loss: 14.427492141723633
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 0.8359764218330383
        entropy_coeff: 0.0017600000137463212
        kl: 0.011297175660729408
        model: {}
        policy_loss: -0.024053700268268585
        total_loss: -0.02202662266790867
        vf_explained_var: 0.20395873486995697
        vf_loss: 12.389577865600586
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 1.1065040826797485
        entropy_coeff: 0.0017600000137463212
        kl: 0.013821275904774666
        model: {}
        policy_loss: -0.03315894305706024
        total_loss: -0.03100813925266266
        vf_explained_var: 0.143047034740448
        vf_loss: 13.339990615844727
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000708883220795542
        entropy: 1.018599510192871
        entropy_coeff: 0.0017600000137463212
        kl: 0.012966150417923927
        model: {}
        policy_loss: -0.034634217619895935
        total_loss: -0.03255375847220421
        vf_explained_var: 0.17802394926548004
        vf_loss: 12.799681663513184
    load_time_ms: 14076.902
    num_steps_sampled: 8928000
    num_steps_trained: 8928000
    sample_time_ms: 105055.767
    update_time_ms: 16.519
  iterations_since_restore: 33
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.093922651933703
    ram_util_percent: 16.855801104972375
  pid: 30948
  policy_reward_max:
    agent-0: 142.0000000000003
    agent-1: 142.0000000000003
    agent-2: 142.0000000000003
    agent-3: 142.0000000000003
    agent-4: 142.0000000000003
    agent-5: 142.0000000000003
  policy_reward_mean:
    agent-0: 98.7933333333336
    agent-1: 98.7933333333336
    agent-2: 98.7933333333336
    agent-3: 98.7933333333336
    agent-4: 98.7933333333336
    agent-5: 98.7933333333336
  policy_reward_min:
    agent-0: 38.33333333333331
    agent-1: 38.33333333333331
    agent-2: 38.33333333333331
    agent-3: 38.33333333333331
    agent-4: 38.33333333333331
    agent-5: 38.33333333333331
  sampler_perf:
    mean_env_wait_ms: 28.44994678146298
    mean_inference_ms: 13.286731401884861
    mean_processing_ms: 60.777553834286365
  time_since_restore: 4961.425142288208
  time_this_iter_s: 166.76228713989258
  time_total_s: 14087.436956167221
  timestamp: 1637032025
  timesteps_since_restore: 3168000
  timesteps_this_iter: 96000
  timesteps_total: 8928000
  training_iteration: 93
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     93 |          14087.4 | 8928000 |   592.76 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 4.63
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 24.0
    apples_agent-1_min: 0
    apples_agent-2_max: 83
    apples_agent-2_mean: 8.13
    apples_agent-2_min: 0
    apples_agent-3_max: 187
    apples_agent-3_mean: 84.94
    apples_agent-3_min: 7
    apples_agent-4_max: 48
    apples_agent-4_mean: 1.68
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 77.57
    apples_agent-5_min: 19
    cleaning_beam_agent-0_max: 387
    cleaning_beam_agent-0_mean: 287.02
    cleaning_beam_agent-0_min: 109
    cleaning_beam_agent-1_max: 437
    cleaning_beam_agent-1_mean: 229.19
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 634
    cleaning_beam_agent-2_mean: 419.36
    cleaning_beam_agent-2_min: 186
    cleaning_beam_agent-3_max: 111
    cleaning_beam_agent-3_mean: 50.26
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 503
    cleaning_beam_agent-4_mean: 395.14
    cleaning_beam_agent-4_min: 253
    cleaning_beam_agent-5_max: 215
    cleaning_beam_agent-5_mean: 61.19
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-09-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 882.9999999999825
  episode_reward_mean: 629.5299999999969
  episode_reward_min: 265.99999999999653
  episodes_this_iter: 96
  episodes_total: 9024
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12021.575
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 1.2374227046966553
        entropy_coeff: 0.0017600000137463212
        kl: 0.012504462152719498
        model: {}
        policy_loss: -0.03069356456398964
        total_loss: -0.029014786705374718
        vf_explained_var: 0.11247025430202484
        vf_loss: 13.557462692260742
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 1.2095587253570557
        entropy_coeff: 0.0017600000137463212
        kl: 0.01466041523963213
        model: {}
        policy_loss: -0.03498096391558647
        total_loss: -0.03270372003316879
        vf_explained_var: 0.03505069017410278
        vf_loss: 14.739864349365234
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 1.134233832359314
        entropy_coeff: 0.0017600000137463212
        kl: 0.013229526579380035
        model: {}
        policy_loss: -0.03148648515343666
        total_loss: -0.029406487941741943
        vf_explained_var: 0.06345765292644501
        vf_loss: 14.303457260131836
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 0.8106674551963806
        entropy_coeff: 0.0017600000137463212
        kl: 0.009135406464338303
        model: {}
        policy_loss: -0.023280560970306396
        total_loss: -0.021664392203092575
        vf_explained_var: 0.20416173338890076
        vf_loss: 12.158624649047852
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 1.114395260810852
        entropy_coeff: 0.0017600000137463212
        kl: 0.01548101007938385
        model: {}
        policy_loss: -0.03385283052921295
        total_loss: -0.031427279114723206
        vf_explained_var: 0.15470831096172333
        vf_loss: 12.906811714172363
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0007028927793726325
        entropy: 1.003085732460022
        entropy_coeff: 0.0017600000137463212
        kl: 0.013365868479013443
        model: {}
        policy_loss: -0.034663520753383636
        total_loss: -0.03246091306209564
        vf_explained_var: 0.15209631621837616
        vf_loss: 12.94861888885498
    load_time_ms: 14164.569
    num_steps_sampled: 9024000
    num_steps_trained: 9024000
    sample_time_ms: 105229.155
    update_time_ms: 17.416
  iterations_since_restore: 34
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.581720430107527
    ram_util_percent: 16.398387096774194
  pid: 30948
  policy_reward_max:
    agent-0: 147.1666666666667
    agent-1: 147.1666666666667
    agent-2: 147.1666666666667
    agent-3: 147.1666666666667
    agent-4: 147.1666666666667
    agent-5: 147.1666666666667
  policy_reward_mean:
    agent-0: 104.92166666666701
    agent-1: 104.92166666666701
    agent-2: 104.92166666666701
    agent-3: 104.92166666666701
    agent-4: 104.92166666666701
    agent-5: 104.92166666666701
  policy_reward_min:
    agent-0: 44.333333333333265
    agent-1: 44.333333333333265
    agent-2: 44.333333333333265
    agent-3: 44.333333333333265
    agent-4: 44.333333333333265
    agent-5: 44.333333333333265
  sampler_perf:
    mean_env_wait_ms: 28.409183719456806
    mean_inference_ms: 13.272508891334116
    mean_processing_ms: 60.677114166020544
  time_since_restore: 5091.498564004898
  time_this_iter_s: 130.07342171669006
  time_total_s: 14217.510377883911
  timestamp: 1637032155
  timesteps_since_restore: 3264000
  timesteps_this_iter: 96000
  timesteps_total: 9024000
  training_iteration: 94
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     94 |          14217.5 | 9024000 |   629.53 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 6.89
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 23.98
    apples_agent-1_min: 0
    apples_agent-2_max: 110
    apples_agent-2_mean: 12.84
    apples_agent-2_min: 0
    apples_agent-3_max: 169
    apples_agent-3_mean: 73.35
    apples_agent-3_min: 20
    apples_agent-4_max: 50
    apples_agent-4_mean: 2.48
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 73.08
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 382
    cleaning_beam_agent-0_mean: 273.2
    cleaning_beam_agent-0_min: 103
    cleaning_beam_agent-1_max: 414
    cleaning_beam_agent-1_mean: 224.66
    cleaning_beam_agent-1_min: 77
    cleaning_beam_agent-2_max: 591
    cleaning_beam_agent-2_mean: 397.05
    cleaning_beam_agent-2_min: 109
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 56.82
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 563
    cleaning_beam_agent-4_mean: 387.75
    cleaning_beam_agent-4_min: 144
    cleaning_beam_agent-5_max: 180
    cleaning_beam_agent-5_mean: 62.1
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-11-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 881.9999999999787
  episode_reward_mean: 573.4599999999994
  episode_reward_min: 205.99999999999864
  episodes_this_iter: 96
  episodes_total: 9120
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12084.188
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 1.2379851341247559
        entropy_coeff: 0.0017600000137463212
        kl: 0.013222049921751022
        model: {}
        policy_loss: -0.029942279681563377
        total_loss: -0.02812262624502182
        vf_explained_var: 0.1579914689064026
        vf_loss: 13.540987014770508
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 1.2037889957427979
        entropy_coeff: 0.0017600000137463212
        kl: 0.015220756642520428
        model: {}
        policy_loss: -0.03513508290052414
        total_loss: -0.032625142484903336
        vf_explained_var: 0.014092177152633667
        vf_loss: 15.844595909118652
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 1.1429073810577393
        entropy_coeff: 0.0017600000137463212
        kl: 0.013722600415349007
        model: {}
        policy_loss: -0.032718952745199203
        total_loss: -0.030497834086418152
        vf_explained_var: 0.07464058697223663
        vf_loss: 14.881128311157227
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 0.8560047149658203
        entropy_coeff: 0.0017600000137463212
        kl: 0.009982287883758545
        model: {}
        policy_loss: -0.026062991470098495
        total_loss: -0.024364862591028214
        vf_explained_var: 0.24840040504932404
        vf_loss: 12.082433700561523
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 1.15007746219635
        entropy_coeff: 0.0017600000137463212
        kl: 0.016662294045090675
        model: {}
        policy_loss: -0.03410777077078819
        total_loss: -0.031387586146593094
        vf_explained_var: 0.12289515137672424
        vf_loss: 14.118598937988281
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006969023961573839
        entropy: 1.0612605810165405
        entropy_coeff: 0.0017600000137463212
        kl: 0.013728352263569832
        model: {}
        policy_loss: -0.03471965342760086
        total_loss: -0.03256642818450928
        vf_explained_var: 0.20716552436351776
        vf_loss: 12.753734588623047
    load_time_ms: 14172.282
    num_steps_sampled: 9120000
    num_steps_trained: 9120000
    sample_time_ms: 105423.92
    update_time_ms: 23.105
  iterations_since_restore: 35
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.864171122994655
    ram_util_percent: 16.0855614973262
  pid: 30948
  policy_reward_max:
    agent-0: 147.00000000000026
    agent-1: 147.00000000000026
    agent-2: 147.00000000000026
    agent-3: 147.00000000000026
    agent-4: 147.00000000000026
    agent-5: 147.00000000000026
  policy_reward_mean:
    agent-0: 95.57666666666691
    agent-1: 95.57666666666691
    agent-2: 95.57666666666691
    agent-3: 95.57666666666691
    agent-4: 95.57666666666691
    agent-5: 95.57666666666691
  policy_reward_min:
    agent-0: 34.33333333333335
    agent-1: 34.33333333333335
    agent-2: 34.33333333333335
    agent-3: 34.33333333333335
    agent-4: 34.33333333333335
    agent-5: 34.33333333333335
  sampler_perf:
    mean_env_wait_ms: 28.38544031469176
    mean_inference_ms: 13.266622310503353
    mean_processing_ms: 60.62383318416756
  time_since_restore: 5223.225544691086
  time_this_iter_s: 131.72698068618774
  time_total_s: 14349.237358570099
  timestamp: 1637032287
  timesteps_since_restore: 3360000
  timesteps_this_iter: 96000
  timesteps_total: 9120000
  training_iteration: 95
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 31.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     95 |          14349.2 | 9120000 |   573.46 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 2.8
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 27.36
    apples_agent-1_min: 0
    apples_agent-2_max: 163
    apples_agent-2_mean: 14.04
    apples_agent-2_min: 0
    apples_agent-3_max: 207
    apples_agent-3_mean: 82.82
    apples_agent-3_min: 37
    apples_agent-4_max: 62
    apples_agent-4_mean: 3.48
    apples_agent-4_min: 0
    apples_agent-5_max: 143
    apples_agent-5_mean: 72.11
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 378
    cleaning_beam_agent-0_mean: 277.09
    cleaning_beam_agent-0_min: 106
    cleaning_beam_agent-1_max: 442
    cleaning_beam_agent-1_mean: 235.65
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 624
    cleaning_beam_agent-2_mean: 396.33
    cleaning_beam_agent-2_min: 200
    cleaning_beam_agent-3_max: 157
    cleaning_beam_agent-3_mean: 58.47
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 520
    cleaning_beam_agent-4_mean: 365.25
    cleaning_beam_agent-4_min: 154
    cleaning_beam_agent-5_max: 255
    cleaning_beam_agent-5_mean: 83.7
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 9
    fire_beam_agent-0_mean: 0.09
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-13-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 837.9999999999776
  episode_reward_mean: 601.4099999999996
  episode_reward_min: 294.9999999999958
  episodes_this_iter: 96
  episodes_total: 9216
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12031.772
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 1.24630606174469
        entropy_coeff: 0.0017600000137463212
        kl: 0.01237553358078003
        model: {}
        policy_loss: -0.032357797026634216
        total_loss: -0.030728567391633987
        vf_explained_var: 0.06539781391620636
        vf_loss: 13.47620964050293
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 1.2108968496322632
        entropy_coeff: 0.0017600000137463212
        kl: 0.014284073375165462
        model: {}
        policy_loss: -0.036643773317337036
        total_loss: -0.03449232503771782
        vf_explained_var: 0.010894253849983215
        vf_loss: 14.258097648620605
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 1.1447324752807617
        entropy_coeff: 0.0017600000137463212
        kl: 0.014338933862745762
        model: {}
        policy_loss: -0.032025039196014404
        total_loss: -0.029855482280254364
        vf_explained_var: 0.08617427945137024
        vf_loss: 13.165014266967773
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 0.825814962387085
        entropy_coeff: 0.0017600000137463212
        kl: 0.00999243650585413
        model: {}
        policy_loss: -0.02482541650533676
        total_loss: -0.02313237264752388
        vf_explained_var: 0.20342768728733063
        vf_loss: 11.47994613647461
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 1.1366369724273682
        entropy_coeff: 0.0017600000137463212
        kl: 0.015025678090751171
        model: {}
        policy_loss: -0.03701193630695343
        total_loss: -0.034786731004714966
        vf_explained_var: 0.15280400216579437
        vf_loss: 12.205460548400879
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006909120129421353
        entropy: 1.0381191968917847
        entropy_coeff: 0.0017600000137463212
        kl: 0.013929694890975952
        model: {}
        policy_loss: -0.03629954159259796
        total_loss: -0.03407604992389679
        vf_explained_var: 0.12184695899486542
        vf_loss: 12.646406173706055
    load_time_ms: 14175.033
    num_steps_sampled: 9216000
    num_steps_trained: 9216000
    sample_time_ms: 105546.58
    update_time_ms: 23.424
  iterations_since_restore: 36
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.532402234636873
    ram_util_percent: 16.810614525139663
  pid: 30948
  policy_reward_max:
    agent-0: 139.66666666666683
    agent-1: 139.66666666666683
    agent-2: 139.66666666666683
    agent-3: 139.66666666666683
    agent-4: 139.66666666666683
    agent-5: 139.66666666666683
  policy_reward_mean:
    agent-0: 100.2350000000003
    agent-1: 100.2350000000003
    agent-2: 100.2350000000003
    agent-3: 100.2350000000003
    agent-4: 100.2350000000003
    agent-5: 100.2350000000003
  policy_reward_min:
    agent-0: 49.166666666666664
    agent-1: 49.166666666666664
    agent-2: 49.166666666666664
    agent-3: 49.166666666666664
    agent-4: 49.166666666666664
    agent-5: 49.166666666666664
  sampler_perf:
    mean_env_wait_ms: 28.33759774833248
    mean_inference_ms: 13.250331364283152
    mean_processing_ms: 60.50664845239405
  time_since_restore: 5348.923852682114
  time_this_iter_s: 125.69830799102783
  time_total_s: 14474.935666561127
  timestamp: 1637032413
  timesteps_since_restore: 3456000
  timesteps_this_iter: 96000
  timesteps_total: 9216000
  training_iteration: 96
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 31.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     96 |          14474.9 | 9216000 |   601.41 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 5.66
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 28.89
    apples_agent-1_min: 0
    apples_agent-2_max: 75
    apples_agent-2_mean: 9.47
    apples_agent-2_min: 0
    apples_agent-3_max: 145
    apples_agent-3_mean: 79.19
    apples_agent-3_min: 28
    apples_agent-4_max: 43
    apples_agent-4_mean: 1.49
    apples_agent-4_min: 0
    apples_agent-5_max: 131
    apples_agent-5_mean: 72.57
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 467
    cleaning_beam_agent-0_mean: 270.82
    cleaning_beam_agent-0_min: 89
    cleaning_beam_agent-1_max: 484
    cleaning_beam_agent-1_mean: 232.43
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 594
    cleaning_beam_agent-2_mean: 415.26
    cleaning_beam_agent-2_min: 68
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 61.3
    cleaning_beam_agent-3_min: 28
    cleaning_beam_agent-4_max: 558
    cleaning_beam_agent-4_mean: 391.57
    cleaning_beam_agent-4_min: 145
    cleaning_beam_agent-5_max: 249
    cleaning_beam_agent-5_mean: 76.16
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-15-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 831.9999999999811
  episode_reward_mean: 597.8399999999997
  episode_reward_min: 184.99999999999952
  episodes_this_iter: 96
  episodes_total: 9312
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12068.346
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 1.2356886863708496
        entropy_coeff: 0.0017600000137463212
        kl: 0.012431396171450615
        model: {}
        policy_loss: -0.03153711184859276
        total_loss: -0.029942447319626808
        vf_explained_var: 0.11204452812671661
        vf_loss: 12.831995010375977
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 1.1914827823638916
        entropy_coeff: 0.0017600000137463212
        kl: 0.013991977088153362
        model: {}
        policy_loss: -0.03678692504763603
        total_loss: -0.0347132571041584
        vf_explained_var: 0.04952464997768402
        vf_loss: 13.722826957702637
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 1.1146247386932373
        entropy_coeff: 0.0017600000137463212
        kl: 0.015125313773751259
        model: {}
        policy_loss: -0.03253205493092537
        total_loss: -0.03011820651590824
        vf_explained_var: 0.06456141173839569
        vf_loss: 13.505277633666992
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 0.8530184626579285
        entropy_coeff: 0.0017600000137463212
        kl: 0.010253390297293663
        model: {}
        policy_loss: -0.024826135486364365
        total_loss: -0.023131804540753365
        vf_explained_var: 0.20806492865085602
        vf_loss: 11.44965648651123
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 1.1285767555236816
        entropy_coeff: 0.0017600000137463212
        kl: 0.014410010538995266
        model: {}
        policy_loss: -0.03573111444711685
        total_loss: -0.03363533318042755
        vf_explained_var: 0.1696418970823288
        vf_loss: 12.000741004943848
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006849215715192258
        entropy: 1.0426476001739502
        entropy_coeff: 0.0017600000137463212
        kl: 0.013943009078502655
        model: {}
        policy_loss: -0.036701153963804245
        total_loss: -0.03451220691204071
        vf_explained_var: 0.1449165791273117
        vf_loss: 12.353985786437988
    load_time_ms: 14206.032
    num_steps_sampled: 9312000
    num_steps_trained: 9312000
    sample_time_ms: 105778.627
    update_time_ms: 22.688
  iterations_since_restore: 37
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 26.281868131868134
    ram_util_percent: 16.492307692307698
  pid: 30948
  policy_reward_max:
    agent-0: 138.66666666666669
    agent-1: 138.66666666666669
    agent-2: 138.66666666666669
    agent-3: 138.66666666666669
    agent-4: 138.66666666666669
    agent-5: 138.66666666666669
  policy_reward_mean:
    agent-0: 99.64000000000031
    agent-1: 99.64000000000031
    agent-2: 99.64000000000031
    agent-3: 99.64000000000031
    agent-4: 99.64000000000031
    agent-5: 99.64000000000031
  policy_reward_min:
    agent-0: 30.83333333333338
    agent-1: 30.83333333333338
    agent-2: 30.83333333333338
    agent-3: 30.83333333333338
    agent-4: 30.83333333333338
    agent-5: 30.83333333333338
  sampler_perf:
    mean_env_wait_ms: 28.289450086338192
    mean_inference_ms: 13.238378491496782
    mean_processing_ms: 60.395222415373865
  time_since_restore: 5476.707969903946
  time_this_iter_s: 127.78411722183228
  time_total_s: 14602.719783782959
  timestamp: 1637032541
  timesteps_since_restore: 3552000
  timesteps_this_iter: 96000
  timesteps_total: 9312000
  training_iteration: 97
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 28.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     97 |          14602.7 | 9312000 |   597.84 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 5.35
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 28.33
    apples_agent-1_min: 0
    apples_agent-2_max: 172
    apples_agent-2_mean: 17.45
    apples_agent-2_min: 0
    apples_agent-3_max: 145
    apples_agent-3_mean: 77.39
    apples_agent-3_min: 24
    apples_agent-4_max: 50
    apples_agent-4_mean: 2.82
    apples_agent-4_min: 0
    apples_agent-5_max: 128
    apples_agent-5_mean: 69.17
    apples_agent-5_min: 14
    cleaning_beam_agent-0_max: 467
    cleaning_beam_agent-0_mean: 283.14
    cleaning_beam_agent-0_min: 103
    cleaning_beam_agent-1_max: 367
    cleaning_beam_agent-1_mean: 205.18
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 602
    cleaning_beam_agent-2_mean: 374.58
    cleaning_beam_agent-2_min: 129
    cleaning_beam_agent-3_max: 142
    cleaning_beam_agent-3_mean: 59.26
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 527
    cleaning_beam_agent-4_mean: 396.76
    cleaning_beam_agent-4_min: 150
    cleaning_beam_agent-5_max: 232
    cleaning_beam_agent-5_mean: 85.7
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-17-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 744.9999999999792
  episode_reward_mean: 560.0600000000014
  episode_reward_min: 231.99999999999574
  episodes_this_iter: 96
  episodes_total: 9408
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12092.305
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 1.2363193035125732
        entropy_coeff: 0.0017600000137463212
        kl: 0.012725336477160454
        model: {}
        policy_loss: -0.032538846135139465
        total_loss: -0.030894920229911804
        vf_explained_var: 0.12218828499317169
        vf_loss: 12.747842788696289
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 1.1900324821472168
        entropy_coeff: 0.0017600000137463212
        kl: 0.01511616725474596
        model: {}
        policy_loss: -0.036829251796007156
        total_loss: -0.034460023045539856
        vf_explained_var: 0.009852126240730286
        vf_loss: 14.404512405395508
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 1.1243717670440674
        entropy_coeff: 0.0017600000137463212
        kl: 0.015365174040198326
        model: {}
        policy_loss: -0.03179680556058884
        total_loss: -0.029340293258428574
        vf_explained_var: 0.06145881116390228
        vf_loss: 13.623689651489258
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 0.8625447750091553
        entropy_coeff: 0.0017600000137463212
        kl: 0.012139830738306046
        model: {}
        policy_loss: -0.026041807606816292
        total_loss: -0.023954741656780243
        vf_explained_var: 0.18964433670043945
        vf_loss: 11.771783828735352
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 1.1119558811187744
        entropy_coeff: 0.0017600000137463212
        kl: 0.014242163859307766
        model: {}
        policy_loss: -0.03629215061664581
        total_loss: -0.03420786187052727
        vf_explained_var: 0.1783941090106964
        vf_loss: 11.928995132446289
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006789311883039773
        entropy: 1.0650427341461182
        entropy_coeff: 0.0017600000137463212
        kl: 0.0144118070602417
        model: {}
        policy_loss: -0.03580452501773834
        total_loss: -0.03365190699696541
        vf_explained_var: 0.21204839646816254
        vf_loss: 11.447305679321289
    load_time_ms: 13704.731
    num_steps_sampled: 9408000
    num_steps_trained: 9408000
    sample_time_ms: 105192.391
    update_time_ms: 21.745
  iterations_since_restore: 38
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.622033898305087
    ram_util_percent: 15.635028248587572
  pid: 30948
  policy_reward_max:
    agent-0: 124.16666666666723
    agent-1: 124.16666666666723
    agent-2: 124.16666666666723
    agent-3: 124.16666666666723
    agent-4: 124.16666666666723
    agent-5: 124.16666666666723
  policy_reward_mean:
    agent-0: 93.34333333333359
    agent-1: 93.34333333333359
    agent-2: 93.34333333333359
    agent-3: 93.34333333333359
    agent-4: 93.34333333333359
    agent-5: 93.34333333333359
  policy_reward_min:
    agent-0: 38.66666666666664
    agent-1: 38.66666666666664
    agent-2: 38.66666666666664
    agent-3: 38.66666666666664
    agent-4: 38.66666666666664
    agent-5: 38.66666666666664
  sampler_perf:
    mean_env_wait_ms: 28.246802883961458
    mean_inference_ms: 13.223782773458435
    mean_processing_ms: 60.29199950596343
  time_since_restore: 5600.859824180603
  time_this_iter_s: 124.1518542766571
  time_total_s: 14726.871638059616
  timestamp: 1637032665
  timesteps_since_restore: 3648000
  timesteps_this_iter: 96000
  timesteps_total: 9408000
  training_iteration: 98
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     98 |          14726.9 | 9408000 |   560.06 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 4.89
    apples_agent-0_min: 0
    apples_agent-1_max: 153
    apples_agent-1_mean: 27.86
    apples_agent-1_min: 0
    apples_agent-2_max: 148
    apples_agent-2_mean: 13.8
    apples_agent-2_min: 0
    apples_agent-3_max: 162
    apples_agent-3_mean: 81.74
    apples_agent-3_min: 0
    apples_agent-4_max: 39
    apples_agent-4_mean: 2.11
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 71.42
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 408
    cleaning_beam_agent-0_mean: 275.77
    cleaning_beam_agent-0_min: 148
    cleaning_beam_agent-1_max: 501
    cleaning_beam_agent-1_mean: 242.34
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 598
    cleaning_beam_agent-2_mean: 385.88
    cleaning_beam_agent-2_min: 155
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 51.4
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 594
    cleaning_beam_agent-4_mean: 425.75
    cleaning_beam_agent-4_min: 191
    cleaning_beam_agent-5_max: 265
    cleaning_beam_agent-5_mean: 90.62
    cleaning_beam_agent-5_min: 22
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-20-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 849.9999999999769
  episode_reward_mean: 590.9100000000002
  episode_reward_min: 176.9999999999986
  episodes_this_iter: 96
  episodes_total: 9504
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12116.679
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 1.2112330198287964
        entropy_coeff: 0.0017600000137463212
        kl: 0.012368505820631981
        model: {}
        policy_loss: -0.030358128249645233
        total_loss: -0.02865782380104065
        vf_explained_var: 0.031586095690727234
        vf_loss: 13.583751678466797
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 1.1723501682281494
        entropy_coeff: 0.0017600000137463212
        kl: 0.015582308173179626
        model: {}
        policy_loss: -0.03533582389354706
        total_loss: -0.032934803515672684
        vf_explained_var: 0.04062804579734802
        vf_loss: 13.478967666625977
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 1.1313897371292114
        entropy_coeff: 0.0017600000137463212
        kl: 0.014462783932685852
        model: {}
        policy_loss: -0.033279888331890106
        total_loss: -0.031093623489141464
        vf_explained_var: 0.08359859883785248
        vf_loss: 12.849546432495117
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 0.8066496849060059
        entropy_coeff: 0.0017600000137463212
        kl: 0.009736958891153336
        model: {}
        policy_loss: -0.023709692060947418
        total_loss: -0.022028598934412003
        vf_explained_var: 0.1767989844083786
        vf_loss: 11.534072875976562
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 1.0871965885162354
        entropy_coeff: 0.0017600000137463212
        kl: 0.013781187124550343
        model: {}
        policy_loss: -0.034719616174697876
        total_loss: -0.03267005458474159
        vf_explained_var: 0.13915157318115234
        vf_loss: 12.067898750305176
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006729408050887287
        entropy: 1.0318855047225952
        entropy_coeff: 0.0017600000137463212
        kl: 0.013531715609133244
        model: {}
        policy_loss: -0.03462503105401993
        total_loss: -0.03254064917564392
        vf_explained_var: 0.1479734480381012
        vf_loss: 11.94156265258789
    load_time_ms: 14214.55
    num_steps_sampled: 9504000
    num_steps_trained: 9504000
    sample_time_ms: 106129.144
    update_time_ms: 67.624
  iterations_since_restore: 39
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.101463414634143
    ram_util_percent: 15.762926829268295
  pid: 30948
  policy_reward_max:
    agent-0: 141.66666666666697
    agent-1: 141.66666666666697
    agent-2: 141.66666666666697
    agent-3: 141.66666666666697
    agent-4: 141.66666666666697
    agent-5: 141.66666666666697
  policy_reward_mean:
    agent-0: 98.48500000000027
    agent-1: 98.48500000000027
    agent-2: 98.48500000000027
    agent-3: 98.48500000000027
    agent-4: 98.48500000000027
    agent-5: 98.48500000000027
  policy_reward_min:
    agent-0: 29.50000000000007
    agent-1: 29.50000000000007
    agent-2: 29.50000000000007
    agent-3: 29.50000000000007
    agent-4: 29.50000000000007
    agent-5: 29.50000000000007
  sampler_perf:
    mean_env_wait_ms: 28.265923320393313
    mean_inference_ms: 13.22472151019266
    mean_processing_ms: 60.30824462222183
  time_since_restore: 5744.34849858284
  time_this_iter_s: 143.48867440223694
  time_total_s: 14870.360312461853
  timestamp: 1637032809
  timesteps_since_restore: 3744000
  timesteps_this_iter: 96000
  timesteps_total: 9504000
  training_iteration: 99
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 28.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |     99 |          14870.4 | 9504000 |   590.91 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 3.97
    apples_agent-0_min: 0
    apples_agent-1_max: 138
    apples_agent-1_mean: 28.01
    apples_agent-1_min: 0
    apples_agent-2_max: 231
    apples_agent-2_mean: 14.82
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 83.24
    apples_agent-3_min: 14
    apples_agent-4_max: 47
    apples_agent-4_mean: 2.59
    apples_agent-4_min: 0
    apples_agent-5_max: 129
    apples_agent-5_mean: 73.76
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 366
    cleaning_beam_agent-0_mean: 266.02
    cleaning_beam_agent-0_min: 131
    cleaning_beam_agent-1_max: 521
    cleaning_beam_agent-1_mean: 250.14
    cleaning_beam_agent-1_min: 85
    cleaning_beam_agent-2_max: 629
    cleaning_beam_agent-2_mean: 398.29
    cleaning_beam_agent-2_min: 187
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 50.46
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 582
    cleaning_beam_agent-4_mean: 421.52
    cleaning_beam_agent-4_min: 190
    cleaning_beam_agent-5_max: 181
    cleaning_beam_agent-5_mean: 78.81
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-22-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 804.9999999999767
  episode_reward_mean: 588.3199999999999
  episode_reward_min: 188.99999999999866
  episodes_this_iter: 96
  episodes_total: 9600
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12091.373
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 1.2224972248077393
        entropy_coeff: 0.0017600000137463212
        kl: 0.012018310837447643
        model: {}
        policy_loss: -0.031759824603796005
        total_loss: -0.030237726867198944
        vf_explained_var: 0.09939058125019073
        vf_loss: 12.70034408569336
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 1.171259880065918
        entropy_coeff: 0.0017600000137463212
        kl: 0.014166625216603279
        model: {}
        policy_loss: -0.03485045209527016
        total_loss: -0.032667964696884155
        vf_explained_var: 0.0007241368293762207
        vf_loss: 14.105817794799805
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 1.1178021430969238
        entropy_coeff: 0.0017600000137463212
        kl: 0.014054704457521439
        model: {}
        policy_loss: -0.03328322246670723
        total_loss: -0.031132768839597702
        vf_explained_var: 0.07274742424488068
        vf_loss: 13.068480491638184
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 0.8236392736434937
        entropy_coeff: 0.0017600000137463212
        kl: 0.009838948957622051
        model: {}
        policy_loss: -0.024932842701673508
        total_loss: -0.023244917392730713
        vf_explained_var: 0.17055834829807281
        vf_loss: 11.697418212890625
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 1.0912771224975586
        entropy_coeff: 0.0017600000137463212
        kl: 0.01322499942034483
        model: {}
        policy_loss: -0.03363296762108803
        total_loss: -0.03170899301767349
        vf_explained_var: 0.14897401630878448
        vf_loss: 11.996255874633789
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006669504218734801
        entropy: 1.0373027324676514
        entropy_coeff: 0.0017600000137463212
        kl: 0.013288917951285839
        model: {}
        policy_loss: -0.034488383680582047
        total_loss: -0.03247354179620743
        vf_explained_var: 0.16108527779579163
        vf_loss: 11.827069282531738
    load_time_ms: 14226.748
    num_steps_sampled: 9600000
    num_steps_trained: 9600000
    sample_time_ms: 106212.614
    update_time_ms: 67.594
  iterations_since_restore: 40
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.332417582417584
    ram_util_percent: 15.672527472527475
  pid: 30948
  policy_reward_max:
    agent-0: 134.1666666666671
    agent-1: 134.1666666666671
    agent-2: 134.1666666666671
    agent-3: 134.1666666666671
    agent-4: 134.1666666666671
    agent-5: 134.1666666666671
  policy_reward_mean:
    agent-0: 98.05333333333361
    agent-1: 98.05333333333361
    agent-2: 98.05333333333361
    agent-3: 98.05333333333361
    agent-4: 98.05333333333361
    agent-5: 98.05333333333361
  policy_reward_min:
    agent-0: 31.500000000000068
    agent-1: 31.500000000000068
    agent-2: 31.500000000000068
    agent-3: 31.500000000000068
    agent-4: 31.500000000000068
    agent-5: 31.500000000000068
  sampler_perf:
    mean_env_wait_ms: 28.2478471607432
    mean_inference_ms: 13.213941849272697
    mean_processing_ms: 60.26014685423301
  time_since_restore: 5871.688423871994
  time_this_iter_s: 127.33992528915405
  time_total_s: 14997.700237751007
  timestamp: 1637032937
  timesteps_since_restore: 3840000
  timesteps_this_iter: 96000
  timesteps_total: 9600000
  training_iteration: 100
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    100 |          14997.7 | 9600000 |   588.32 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 129
    apples_agent-0_mean: 5.64
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 27.95
    apples_agent-1_min: 0
    apples_agent-2_max: 237
    apples_agent-2_mean: 13.91
    apples_agent-2_min: 0
    apples_agent-3_max: 176
    apples_agent-3_mean: 81.49
    apples_agent-3_min: 8
    apples_agent-4_max: 67
    apples_agent-4_mean: 4.55
    apples_agent-4_min: 0
    apples_agent-5_max: 143
    apples_agent-5_mean: 77.48
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 363
    cleaning_beam_agent-0_mean: 271.99
    cleaning_beam_agent-0_min: 135
    cleaning_beam_agent-1_max: 422
    cleaning_beam_agent-1_mean: 242.66
    cleaning_beam_agent-1_min: 68
    cleaning_beam_agent-2_max: 623
    cleaning_beam_agent-2_mean: 385.77
    cleaning_beam_agent-2_min: 150
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 51.22
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 399.33
    cleaning_beam_agent-4_min: 193
    cleaning_beam_agent-5_max: 218
    cleaning_beam_agent-5_mean: 71.36
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-24-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 916.9999999999799
  episode_reward_mean: 599.0999999999995
  episode_reward_min: 141.00000000000034
  episodes_this_iter: 96
  episodes_total: 9696
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12152.421
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 1.2074229717254639
        entropy_coeff: 0.0017600000137463212
        kl: 0.012415070086717606
        model: {}
        policy_loss: -0.03193226084113121
        total_loss: -0.030184179544448853
        vf_explained_var: 0.08995302021503448
        vf_loss: 13.901318550109863
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 1.1586558818817139
        entropy_coeff: 0.0017600000137463212
        kl: 0.014800012111663818
        model: {}
        policy_loss: -0.03679139167070389
        total_loss: -0.03434813395142555
        vf_explained_var: 0.005561083555221558
        vf_loss: 15.22490119934082
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 1.1533470153808594
        entropy_coeff: 0.0017600000137463212
        kl: 0.015067417174577713
        model: {}
        policy_loss: -0.03557552397251129
        total_loss: -0.03317267447710037
        vf_explained_var: 0.07104028761386871
        vf_loss: 14.192598342895508
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 0.8015214800834656
        entropy_coeff: 0.0017600000137463212
        kl: 0.010186109691858292
        model: {}
        policy_loss: -0.025925777852535248
        total_loss: -0.024035248905420303
        vf_explained_var: 0.17324303090572357
        vf_loss: 12.639850616455078
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 1.104105830192566
        entropy_coeff: 0.0017600000137463212
        kl: 0.01422680914402008
        model: {}
        policy_loss: -0.03602015599608421
        total_loss: -0.033761605620384216
        vf_explained_var: 0.11202621459960938
        vf_loss: 13.564180374145508
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006609599804505706
        entropy: 1.028375506401062
        entropy_coeff: 0.0017600000137463212
        kl: 0.014182677492499352
        model: {}
        policy_loss: -0.034971799701452255
        total_loss: -0.03262678161263466
        vf_explained_var: 0.1369776725769043
        vf_loss: 13.184221267700195
    load_time_ms: 14212.94
    num_steps_sampled: 9696000
    num_steps_trained: 9696000
    sample_time_ms: 107174.697
    update_time_ms: 67.602
  iterations_since_restore: 41
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.969270833333336
    ram_util_percent: 15.605729166666668
  pid: 30948
  policy_reward_max:
    agent-0: 152.83333333333337
    agent-1: 152.83333333333337
    agent-2: 152.83333333333337
    agent-3: 152.83333333333337
    agent-4: 152.83333333333337
    agent-5: 152.83333333333337
  policy_reward_mean:
    agent-0: 99.8500000000003
    agent-1: 99.8500000000003
    agent-2: 99.8500000000003
    agent-3: 99.8500000000003
    agent-4: 99.8500000000003
    agent-5: 99.8500000000003
  policy_reward_min:
    agent-0: 23.500000000000036
    agent-1: 23.500000000000036
    agent-2: 23.500000000000036
    agent-3: 23.500000000000036
    agent-4: 23.500000000000036
    agent-5: 23.500000000000036
  sampler_perf:
    mean_env_wait_ms: 28.258315674543834
    mean_inference_ms: 13.21342694336225
    mean_processing_ms: 60.26563959940947
  time_since_restore: 6006.410932064056
  time_this_iter_s: 134.72250819206238
  time_total_s: 15132.42274594307
  timestamp: 1637033072
  timesteps_since_restore: 3936000
  timesteps_this_iter: 96000
  timesteps_total: 9696000
  training_iteration: 101
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 28.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    101 |          15132.4 | 9696000 |    599.1 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 65
    apples_agent-0_mean: 4.72
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 27.67
    apples_agent-1_min: 0
    apples_agent-2_max: 129
    apples_agent-2_mean: 9.84
    apples_agent-2_min: 0
    apples_agent-3_max: 178
    apples_agent-3_mean: 83.77
    apples_agent-3_min: 17
    apples_agent-4_max: 71
    apples_agent-4_mean: 1.56
    apples_agent-4_min: 0
    apples_agent-5_max: 112
    apples_agent-5_mean: 71.88
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 402
    cleaning_beam_agent-0_mean: 268.79
    cleaning_beam_agent-0_min: 71
    cleaning_beam_agent-1_max: 400
    cleaning_beam_agent-1_mean: 227.65
    cleaning_beam_agent-1_min: 61
    cleaning_beam_agent-2_max: 584
    cleaning_beam_agent-2_mean: 404.18
    cleaning_beam_agent-2_min: 171
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 55.32
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 490
    cleaning_beam_agent-4_mean: 384.17
    cleaning_beam_agent-4_min: 137
    cleaning_beam_agent-5_max: 227
    cleaning_beam_agent-5_mean: 68.66
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-26-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 791.9999999999792
  episode_reward_mean: 585.1600000000007
  episode_reward_min: 255.9999999999968
  episodes_this_iter: 96
  episodes_total: 9792
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12035.415
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 1.2019646167755127
        entropy_coeff: 0.0017600000137463212
        kl: 0.012497534975409508
        model: {}
        policy_loss: -0.032401785254478455
        total_loss: -0.03085743822157383
        vf_explained_var: 0.11890758574008942
        vf_loss: 11.602970123291016
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 1.1519571542739868
        entropy_coeff: 0.0017600000137463212
        kl: 0.01435026340186596
        model: {}
        policy_loss: -0.0361374206840992
        total_loss: -0.0339791439473629
        vf_explained_var: 0.0021857768297195435
        vf_loss: 13.15664291381836
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 1.12474524974823
        entropy_coeff: 0.0017600000137463212
        kl: 0.013708053156733513
        model: {}
        policy_loss: -0.033059012144804
        total_loss: -0.03107433393597603
        vf_explained_var: 0.07155166566371918
        vf_loss: 12.226232528686523
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 0.8056409358978271
        entropy_coeff: 0.0017600000137463212
        kl: 0.00931557733565569
        model: {}
        policy_loss: -0.024184735491871834
        total_loss: -0.02261456474661827
        vf_explained_var: 0.14620433747768402
        vf_loss: 11.249898910522461
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 1.1239585876464844
        entropy_coeff: 0.0017600000137463212
        kl: 0.014341477304697037
        model: {}
        policy_loss: -0.03592293709516525
        total_loss: -0.03382899984717369
        vf_explained_var: 0.08584454655647278
        vf_loss: 12.03808307647705
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000654969597235322
        entropy: 1.0668412446975708
        entropy_coeff: 0.0017600000137463212
        kl: 0.014445466920733452
        model: {}
        policy_loss: -0.03397076204419136
        total_loss: -0.03177466616034508
        vf_explained_var: 0.10053317248821259
        vf_loss: 11.846439361572266
    load_time_ms: 14189.322
    num_steps_sampled: 9792000
    num_steps_trained: 9792000
    sample_time_ms: 107450.916
    update_time_ms: 66.392
  iterations_since_restore: 42
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.061666666666667
    ram_util_percent: 16.326666666666664
  pid: 30948
  policy_reward_max:
    agent-0: 132.0000000000004
    agent-1: 132.0000000000004
    agent-2: 132.0000000000004
    agent-3: 132.0000000000004
    agent-4: 132.0000000000004
    agent-5: 132.0000000000004
  policy_reward_mean:
    agent-0: 97.52666666666693
    agent-1: 97.52666666666693
    agent-2: 97.52666666666693
    agent-3: 97.52666666666693
    agent-4: 97.52666666666693
    agent-5: 97.52666666666693
  policy_reward_min:
    agent-0: 42.66666666666671
    agent-1: 42.66666666666671
    agent-2: 42.66666666666671
    agent-3: 42.66666666666671
    agent-4: 42.66666666666671
    agent-5: 42.66666666666671
  sampler_perf:
    mean_env_wait_ms: 28.234510557869417
    mean_inference_ms: 13.206087831061323
    mean_processing_ms: 60.20815707922789
  time_since_restore: 6132.9191081523895
  time_this_iter_s: 126.50817608833313
  time_total_s: 15258.930922031403
  timestamp: 1637033199
  timesteps_since_restore: 4032000
  timesteps_this_iter: 96000
  timesteps_total: 9792000
  training_iteration: 102
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    102 |          15258.9 | 9792000 |   585.16 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 77
    apples_agent-0_mean: 6.59
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 27.85
    apples_agent-1_min: 0
    apples_agent-2_max: 69
    apples_agent-2_mean: 8.81
    apples_agent-2_min: 0
    apples_agent-3_max: 174
    apples_agent-3_mean: 77.3
    apples_agent-3_min: 16
    apples_agent-4_max: 43
    apples_agent-4_mean: 1.69
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 69.97
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 404
    cleaning_beam_agent-0_mean: 268.16
    cleaning_beam_agent-0_min: 81
    cleaning_beam_agent-1_max: 395
    cleaning_beam_agent-1_mean: 235.32
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 661
    cleaning_beam_agent-2_mean: 385.69
    cleaning_beam_agent-2_min: 187
    cleaning_beam_agent-3_max: 127
    cleaning_beam_agent-3_mean: 58.86
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 535
    cleaning_beam_agent-4_mean: 393.67
    cleaning_beam_agent-4_min: 220
    cleaning_beam_agent-5_max: 276
    cleaning_beam_agent-5_mean: 73.87
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-28-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 826.9999999999786
  episode_reward_mean: 588.2299999999992
  episode_reward_min: 222.99999999999736
  episodes_this_iter: 96
  episodes_total: 9888
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12068.174
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 1.1909763813018799
        entropy_coeff: 0.0017600000137463212
        kl: 0.012288451194763184
        model: {}
        policy_loss: -0.030669691041111946
        total_loss: -0.029094820842146873
        vf_explained_var: 0.13529156148433685
        vf_loss: 12.132936477661133
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 1.1683509349822998
        entropy_coeff: 0.0017600000137463212
        kl: 0.014912903308868408
        model: {}
        policy_loss: -0.0364200659096241
        total_loss: -0.0340791717171669
        vf_explained_var: -0.008353739976882935
        vf_loss: 14.146106719970703
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 1.1356266736984253
        entropy_coeff: 0.0017600000137463212
        kl: 0.014054609462618828
        model: {}
        policy_loss: -0.03457839414477348
        total_loss: -0.032505251467227936
        vf_explained_var: 0.10041606426239014
        vf_loss: 12.609292030334473
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 0.8088715672492981
        entropy_coeff: 0.0017600000137463212
        kl: 0.010580487549304962
        model: {}
        policy_loss: -0.025746934115886688
        total_loss: -0.023901384323835373
        vf_explained_var: 0.17799393832683563
        vf_loss: 11.53068733215332
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 1.0983390808105469
        entropy_coeff: 0.0017600000137463212
        kl: 0.015338804572820663
        model: {}
        policy_loss: -0.035533979535102844
        total_loss: -0.03314635157585144
        vf_explained_var: 0.10729531943798065
        vf_loss: 12.529476165771484
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006489792140200734
        entropy: 1.0388716459274292
        entropy_coeff: 0.0017600000137463212
        kl: 0.013331773690879345
        model: {}
        policy_loss: -0.03742034360766411
        total_loss: -0.03536166995763779
        vf_explained_var: 0.13029323518276215
        vf_loss: 12.207326889038086
    load_time_ms: 14054.352
    num_steps_sampled: 9888000
    num_steps_trained: 9888000
    sample_time_ms: 104544.542
    update_time_ms: 66.34
  iterations_since_restore: 43
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.006153846153847
    ram_util_percent: 16.566153846153846
  pid: 30948
  policy_reward_max:
    agent-0: 137.83333333333414
    agent-1: 137.83333333333414
    agent-2: 137.83333333333414
    agent-3: 137.83333333333414
    agent-4: 137.83333333333414
    agent-5: 137.83333333333414
  policy_reward_mean:
    agent-0: 98.0383333333336
    agent-1: 98.0383333333336
    agent-2: 98.0383333333336
    agent-3: 98.0383333333336
    agent-4: 98.0383333333336
    agent-5: 98.0383333333336
  policy_reward_min:
    agent-0: 37.16666666666668
    agent-1: 37.16666666666668
    agent-2: 37.16666666666668
    agent-3: 37.16666666666668
    agent-4: 37.16666666666668
    agent-5: 37.16666666666668
  sampler_perf:
    mean_env_wait_ms: 28.24426852925015
    mean_inference_ms: 13.207219633779395
    mean_processing_ms: 60.21965095468464
  time_since_restore: 6269.591793298721
  time_this_iter_s: 136.6726851463318
  time_total_s: 15395.603607177734
  timestamp: 1637033336
  timesteps_since_restore: 4128000
  timesteps_this_iter: 96000
  timesteps_total: 9888000
  training_iteration: 103
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    103 |          15395.6 | 9888000 |   588.23 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 77
    apples_agent-0_mean: 6.77
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 25.37
    apples_agent-1_min: 0
    apples_agent-2_max: 131
    apples_agent-2_mean: 11.25
    apples_agent-2_min: 0
    apples_agent-3_max: 161
    apples_agent-3_mean: 76.44
    apples_agent-3_min: 16
    apples_agent-4_max: 41
    apples_agent-4_mean: 1.9
    apples_agent-4_min: 0
    apples_agent-5_max: 121
    apples_agent-5_mean: 68.19
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 407
    cleaning_beam_agent-0_mean: 279.99
    cleaning_beam_agent-0_min: 106
    cleaning_beam_agent-1_max: 425
    cleaning_beam_agent-1_mean: 225.17
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 661
    cleaning_beam_agent-2_mean: 402.98
    cleaning_beam_agent-2_min: 159
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 53.86
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 510
    cleaning_beam_agent-4_mean: 382.17
    cleaning_beam_agent-4_min: 178
    cleaning_beam_agent-5_max: 276
    cleaning_beam_agent-5_mean: 73.72
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-31-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 867.9999999999802
  episode_reward_mean: 602.3999999999992
  episode_reward_min: 162.99999999999932
  episodes_this_iter: 96
  episodes_total: 9984
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12208.606
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 1.1756641864776611
        entropy_coeff: 0.0017600000137463212
        kl: 0.012053113430738449
        model: {}
        policy_loss: -0.030677780508995056
        total_loss: -0.028990473598241806
        vf_explained_var: 0.07376693189144135
        vf_loss: 13.458587646484375
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 1.1695802211761475
        entropy_coeff: 0.0017600000137463212
        kl: 0.015022866427898407
        model: {}
        policy_loss: -0.036139532923698425
        total_loss: -0.03378499299287796
        vf_explained_var: 0.03108319640159607
        vf_loss: 14.084278106689453
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 1.1334450244903564
        entropy_coeff: 0.0017600000137463212
        kl: 0.014279353432357311
        model: {}
        policy_loss: -0.03441619127988815
        total_loss: -0.0322086401283741
        vf_explained_var: 0.07374696433544159
        vf_loss: 13.465460777282715
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 0.7828142046928406
        entropy_coeff: 0.0017600000137463212
        kl: 0.009637223556637764
        model: {}
        policy_loss: -0.024722907692193985
        total_loss: -0.022984180599451065
        vf_explained_var: 0.18203027546405792
        vf_loss: 11.890361785888672
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 1.1131787300109863
        entropy_coeff: 0.0017600000137463212
        kl: 0.014212505891919136
        model: {}
        policy_loss: -0.03719444200396538
        total_loss: -0.035013966262340546
        vf_explained_var: 0.10686357319355011
        vf_loss: 12.971694946289062
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006429887725971639
        entropy: 1.023496389389038
        entropy_coeff: 0.0017600000137463212
        kl: 0.014019074849784374
        model: {}
        policy_loss: -0.03700658679008484
        total_loss: -0.03477589413523674
        vf_explained_var: 0.15552352368831635
        vf_loss: 12.282306671142578
    load_time_ms: 13944.881
    num_steps_sampled: 9984000
    num_steps_trained: 9984000
    sample_time_ms: 104254.973
    update_time_ms: 65.269
  iterations_since_restore: 44
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.50164835164835
    ram_util_percent: 16.526373626373623
  pid: 30948
  policy_reward_max:
    agent-0: 144.66666666666694
    agent-1: 144.66666666666694
    agent-2: 144.66666666666694
    agent-3: 144.66666666666694
    agent-4: 144.66666666666694
    agent-5: 144.66666666666694
  policy_reward_mean:
    agent-0: 100.40000000000028
    agent-1: 100.40000000000028
    agent-2: 100.40000000000028
    agent-3: 100.40000000000028
    agent-4: 100.40000000000028
    agent-5: 100.40000000000028
  policy_reward_min:
    agent-0: 27.166666666666725
    agent-1: 27.166666666666725
    agent-2: 27.166666666666725
    agent-3: 27.166666666666725
    agent-4: 27.166666666666725
    agent-5: 27.166666666666725
  sampler_perf:
    mean_env_wait_ms: 28.21249455110613
    mean_inference_ms: 13.19699304329452
    mean_processing_ms: 60.16441665412408
  time_since_restore: 6397.064338684082
  time_this_iter_s: 127.47254538536072
  time_total_s: 15523.076152563095
  timestamp: 1637033464
  timesteps_since_restore: 4224000
  timesteps_this_iter: 96000
  timesteps_total: 9984000
  training_iteration: 104
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |      ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+---------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    104 |          15523.1 | 9984000 |    602.4 |
+--------------------------------------+----------+-------------------+--------+------------------+---------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 4.35
    apples_agent-0_min: 0
    apples_agent-1_max: 130
    apples_agent-1_mean: 26.06
    apples_agent-1_min: 0
    apples_agent-2_max: 129
    apples_agent-2_mean: 13.32
    apples_agent-2_min: 0
    apples_agent-3_max: 241
    apples_agent-3_mean: 83.59
    apples_agent-3_min: 23
    apples_agent-4_max: 60
    apples_agent-4_mean: 2.49
    apples_agent-4_min: 0
    apples_agent-5_max: 121
    apples_agent-5_mean: 68.81
    apples_agent-5_min: 11
    cleaning_beam_agent-0_max: 429
    cleaning_beam_agent-0_mean: 286.01
    cleaning_beam_agent-0_min: 130
    cleaning_beam_agent-1_max: 379
    cleaning_beam_agent-1_mean: 230.46
    cleaning_beam_agent-1_min: 66
    cleaning_beam_agent-2_max: 636
    cleaning_beam_agent-2_mean: 394.3
    cleaning_beam_agent-2_min: 201
    cleaning_beam_agent-3_max: 93
    cleaning_beam_agent-3_mean: 50.21
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 513
    cleaning_beam_agent-4_mean: 379.07
    cleaning_beam_agent-4_min: 135
    cleaning_beam_agent-5_max: 191
    cleaning_beam_agent-5_mean: 66.89
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-33-17
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 825.9999999999882
  episode_reward_mean: 596.239999999999
  episode_reward_min: 241.9999999999971
  episodes_this_iter: 96
  episodes_total: 10080
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12210.17
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 1.163110613822937
        entropy_coeff: 0.0017600000137463212
        kl: 0.012562265619635582
        model: {}
        policy_loss: -0.029839107766747475
        total_loss: -0.028093507513403893
        vf_explained_var: 0.07080332934856415
        vf_loss: 12.802204132080078
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 1.1629278659820557
        entropy_coeff: 0.0017600000137463212
        kl: 0.014027712866663933
        model: {}
        policy_loss: -0.036310434341430664
        total_loss: -0.034198641777038574
        vf_explained_var: 0.019126400351524353
        vf_loss: 13.530052185058594
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 1.1401360034942627
        entropy_coeff: 0.0017600000137463212
        kl: 0.014313817955553532
        model: {}
        policy_loss: -0.03473753109574318
        total_loss: -0.03258991986513138
        vf_explained_var: 0.06208693981170654
        vf_loss: 12.914902687072754
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 0.7732443809509277
        entropy_coeff: 0.0017600000137463212
        kl: 0.009630240499973297
        model: {}
        policy_loss: -0.025112487375736237
        total_loss: -0.02339107170701027
        vf_explained_var: 0.16155914962291718
        vf_loss: 11.56281852722168
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 1.1104570627212524
        entropy_coeff: 0.0017600000137463212
        kl: 0.014094585552811623
        model: {}
        policy_loss: -0.03678050637245178
        total_loss: -0.034721437841653824
        vf_explained_var: 0.1325894445180893
        vf_loss: 11.945621490478516
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006369983893819153
        entropy: 1.0434424877166748
        entropy_coeff: 0.0017600000137463212
        kl: 0.014366966672241688
        model: {}
        policy_loss: -0.03721734881401062
        total_loss: -0.034987665712833405
        vf_explained_var: 0.1343916803598404
        vf_loss: 11.927484512329102
    load_time_ms: 13981.99
    num_steps_sampled: 10080000
    num_steps_trained: 10080000
    sample_time_ms: 104398.777
    update_time_ms: 59.61
  iterations_since_restore: 45
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.524338624338625
    ram_util_percent: 15.71851851851852
  pid: 30948
  policy_reward_max:
    agent-0: 137.66666666666703
    agent-1: 137.66666666666703
    agent-2: 137.66666666666703
    agent-3: 137.66666666666703
    agent-4: 137.66666666666703
    agent-5: 137.66666666666703
  policy_reward_mean:
    agent-0: 99.37333333333363
    agent-1: 99.37333333333363
    agent-2: 99.37333333333363
    agent-3: 99.37333333333363
    agent-4: 99.37333333333363
    agent-5: 99.37333333333363
  policy_reward_min:
    agent-0: 40.33333333333327
    agent-1: 40.33333333333327
    agent-2: 40.33333333333327
    agent-3: 40.33333333333327
    agent-4: 40.33333333333327
    agent-5: 40.33333333333327
  sampler_perf:
    mean_env_wait_ms: 28.20791106690119
    mean_inference_ms: 13.19469146903179
    mean_processing_ms: 60.15446797289334
  time_since_restore: 6530.583093166351
  time_this_iter_s: 133.5187544822693
  time_total_s: 15656.594907045364
  timestamp: 1637033597
  timesteps_since_restore: 4320000
  timesteps_this_iter: 96000
  timesteps_total: 10080000
  training_iteration: 105
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    105 |          15656.6 | 10080000 |   596.24 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 3.86
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 26.18
    apples_agent-1_min: 0
    apples_agent-2_max: 161
    apples_agent-2_mean: 19.57
    apples_agent-2_min: 0
    apples_agent-3_max: 198
    apples_agent-3_mean: 81.23
    apples_agent-3_min: 21
    apples_agent-4_max: 45
    apples_agent-4_mean: 1.21
    apples_agent-4_min: 0
    apples_agent-5_max: 179
    apples_agent-5_mean: 72.64
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 471
    cleaning_beam_agent-0_mean: 314.65
    cleaning_beam_agent-0_min: 119
    cleaning_beam_agent-1_max: 509
    cleaning_beam_agent-1_mean: 231.88
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 564
    cleaning_beam_agent-2_mean: 373.03
    cleaning_beam_agent-2_min: 82
    cleaning_beam_agent-3_max: 101
    cleaning_beam_agent-3_mean: 52.7
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 481
    cleaning_beam_agent-4_mean: 374.54
    cleaning_beam_agent-4_min: 204
    cleaning_beam_agent-5_max: 152
    cleaning_beam_agent-5_mean: 67.22
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.06
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-35-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 825.9999999999837
  episode_reward_mean: 614.429999999998
  episode_reward_min: 219.9999999999964
  episodes_this_iter: 96
  episodes_total: 10176
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12312.072
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 1.1533994674682617
        entropy_coeff: 0.0017600000137463212
        kl: 0.011442717164754868
        model: {}
        policy_loss: -0.03031928464770317
        total_loss: -0.02880248799920082
        vf_explained_var: 0.09487290680408478
        vf_loss: 12.582340240478516
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 1.192694902420044
        entropy_coeff: 0.0017600000137463212
        kl: 0.014590077102184296
        model: {}
        policy_loss: -0.035246316343545914
        total_loss: -0.03302431106567383
        vf_explained_var: -0.008565142750740051
        vf_loss: 14.031327247619629
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 1.1154415607452393
        entropy_coeff: 0.0017600000137463212
        kl: 0.013144217431545258
        model: {}
        policy_loss: -0.03445714712142944
        total_loss: -0.032509781420230865
        vf_explained_var: 0.07790450751781464
        vf_loss: 12.817008018493652
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 0.7868921756744385
        entropy_coeff: 0.0017600000137463212
        kl: 0.010008685290813446
        model: {}
        policy_loss: -0.02627856656908989
        total_loss: -0.024488648399710655
        vf_explained_var: 0.15602989494800568
        vf_loss: 11.731130599975586
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 1.1137425899505615
        entropy_coeff: 0.0017600000137463212
        kl: 0.014918841421604156
        model: {}
        policy_loss: -0.03775162994861603
        total_loss: -0.035438165068626404
        vf_explained_var: 0.07196308672428131
        vf_loss: 12.898870468139648
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006310080061666667
        entropy: 1.0290498733520508
        entropy_coeff: 0.0017600000137463212
        kl: 0.013720345683395863
        model: {}
        policy_loss: -0.03504742309451103
        total_loss: -0.03286018222570419
        vf_explained_var: 0.09755747020244598
        vf_loss: 12.542970657348633
    load_time_ms: 14028.752
    num_steps_sampled: 10176000
    num_steps_trained: 10176000
    sample_time_ms: 105053.91
    update_time_ms: 59.515
  iterations_since_restore: 46
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.967187500000005
    ram_util_percent: 15.605729166666665
  pid: 30948
  policy_reward_max:
    agent-0: 137.66666666666723
    agent-1: 137.66666666666723
    agent-2: 137.66666666666723
    agent-3: 137.66666666666723
    agent-4: 137.66666666666723
    agent-5: 137.66666666666723
  policy_reward_mean:
    agent-0: 102.40500000000031
    agent-1: 102.40500000000031
    agent-2: 102.40500000000031
    agent-3: 102.40500000000031
    agent-4: 102.40500000000031
    agent-5: 102.40500000000031
  policy_reward_min:
    agent-0: 36.666666666666686
    agent-1: 36.666666666666686
    agent-2: 36.666666666666686
    agent-3: 36.666666666666686
    agent-4: 36.666666666666686
    agent-5: 36.666666666666686
  sampler_perf:
    mean_env_wait_ms: 28.206506171812986
    mean_inference_ms: 13.193161388156861
    mean_processing_ms: 60.14284221612938
  time_since_restore: 6664.200956583023
  time_this_iter_s: 133.61786341667175
  time_total_s: 15790.212770462036
  timestamp: 1637033731
  timesteps_since_restore: 4416000
  timesteps_this_iter: 96000
  timesteps_total: 10176000
  training_iteration: 106
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 28.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    106 |          15790.2 | 10176000 |   614.43 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 2.99
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 21.9
    apples_agent-1_min: 0
    apples_agent-2_max: 140
    apples_agent-2_mean: 17.57
    apples_agent-2_min: 0
    apples_agent-3_max: 192
    apples_agent-3_mean: 81.21
    apples_agent-3_min: 6
    apples_agent-4_max: 36
    apples_agent-4_mean: 1.5
    apples_agent-4_min: 0
    apples_agent-5_max: 135
    apples_agent-5_mean: 72.67
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 483
    cleaning_beam_agent-0_mean: 323.64
    cleaning_beam_agent-0_min: 137
    cleaning_beam_agent-1_max: 366
    cleaning_beam_agent-1_mean: 224.15
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 572
    cleaning_beam_agent-2_mean: 385.11
    cleaning_beam_agent-2_min: 69
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 50.86
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 483
    cleaning_beam_agent-4_mean: 355.8
    cleaning_beam_agent-4_min: 223
    cleaning_beam_agent-5_max: 229
    cleaning_beam_agent-5_mean: 74.24
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-37-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 849.9999999999885
  episode_reward_mean: 612.2299999999994
  episode_reward_min: 344.00000000000483
  episodes_this_iter: 96
  episodes_total: 10272
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12274.411
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 1.126863718032837
        entropy_coeff: 0.0017600000137463212
        kl: 0.011898868717253208
        model: {}
        policy_loss: -0.029648475348949432
        total_loss: -0.027960244566202164
        vf_explained_var: 0.05790312588214874
        vf_loss: 12.917319297790527
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 1.201257348060608
        entropy_coeff: 0.0017600000137463212
        kl: 0.014786265790462494
        model: {}
        policy_loss: -0.03408224135637283
        total_loss: -0.03191947937011719
        vf_explained_var: 0.0379636287689209
        vf_loss: 13.197163581848145
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 1.1030769348144531
        entropy_coeff: 0.0017600000137463212
        kl: 0.013807239010930061
        model: {}
        policy_loss: -0.03320658579468727
        total_loss: -0.031108099967241287
        vf_explained_var: 0.06775632500648499
        vf_loss: 12.784531593322754
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 0.7821179032325745
        entropy_coeff: 0.0017600000137463212
        kl: 0.009881325997412205
        model: {}
        policy_loss: -0.025333432480692863
        total_loss: -0.02359999157488346
        vf_explained_var: 0.1732090413570404
        vf_loss: 11.337034225463867
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 1.1429545879364014
        entropy_coeff: 0.0017600000137463212
        kl: 0.01482086256146431
        model: {}
        policy_loss: -0.03751381114125252
        total_loss: -0.03531332314014435
        vf_explained_var: 0.08988630771636963
        vf_loss: 12.479103088378906
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006250176229514182
        entropy: 1.0371917486190796
        entropy_coeff: 0.0017600000137463212
        kl: 0.013628889806568623
        model: {}
        policy_loss: -0.03522217273712158
        total_loss: -0.03311435505747795
        vf_explained_var: 0.11924843490123749
        vf_loss: 12.0750150680542
    load_time_ms: 14048.849
    num_steps_sampled: 10272000
    num_steps_trained: 10272000
    sample_time_ms: 105707.569
    update_time_ms: 59.595
  iterations_since_restore: 47
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.792670157068063
    ram_util_percent: 14.749738219895287
  pid: 30948
  policy_reward_max:
    agent-0: 141.66666666666686
    agent-1: 141.66666666666686
    agent-2: 141.66666666666686
    agent-3: 141.66666666666686
    agent-4: 141.66666666666686
    agent-5: 141.66666666666686
  policy_reward_mean:
    agent-0: 102.03833333333361
    agent-1: 102.03833333333361
    agent-2: 102.03833333333361
    agent-3: 102.03833333333361
    agent-4: 102.03833333333361
    agent-5: 102.03833333333361
  policy_reward_min:
    agent-0: 57.33333333333305
    agent-1: 57.33333333333305
    agent-2: 57.33333333333305
    agent-3: 57.33333333333305
    agent-4: 57.33333333333305
    agent-5: 57.33333333333305
  sampler_perf:
    mean_env_wait_ms: 28.195077676951705
    mean_inference_ms: 13.188018883518241
    mean_processing_ms: 60.11845865869401
  time_since_restore: 6798.393296957016
  time_this_iter_s: 134.19234037399292
  time_total_s: 15924.405110836029
  timestamp: 1637033866
  timesteps_since_restore: 4512000
  timesteps_this_iter: 96000
  timesteps_total: 10272000
  training_iteration: 107
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 27.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    107 |          15924.4 | 10272000 |   612.23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 1.93
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 27.81
    apples_agent-1_min: 0
    apples_agent-2_max: 94
    apples_agent-2_mean: 13.14
    apples_agent-2_min: 0
    apples_agent-3_max: 146
    apples_agent-3_mean: 75.42
    apples_agent-3_min: 17
    apples_agent-4_max: 76
    apples_agent-4_mean: 3.32
    apples_agent-4_min: 0
    apples_agent-5_max: 140
    apples_agent-5_mean: 67.7
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 435
    cleaning_beam_agent-0_mean: 302.36
    cleaning_beam_agent-0_min: 95
    cleaning_beam_agent-1_max: 355
    cleaning_beam_agent-1_mean: 220.72
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 641
    cleaning_beam_agent-2_mean: 388.85
    cleaning_beam_agent-2_min: 128
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 50.91
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 584
    cleaning_beam_agent-4_mean: 348.58
    cleaning_beam_agent-4_min: 118
    cleaning_beam_agent-5_max: 229
    cleaning_beam_agent-5_mean: 71.7
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-40-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 811.9999999999868
  episode_reward_mean: 581.8099999999996
  episode_reward_min: 135.0000000000009
  episodes_this_iter: 96
  episodes_total: 10368
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12298.88
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 1.1357700824737549
        entropy_coeff: 0.0017600000137463212
        kl: 0.012592854909598827
        model: {}
        policy_loss: -0.03108638897538185
        total_loss: -0.0291118286550045
        vf_explained_var: 0.07482585310935974
        vf_loss: 14.54940414428711
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 1.2171695232391357
        entropy_coeff: 0.0017600000137463212
        kl: 0.013934722170233727
        model: {}
        policy_loss: -0.03562729060649872
        total_loss: -0.03344077616930008
        vf_explained_var: 0.01971106231212616
        vf_loss: 15.417871475219727
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 1.1045243740081787
        entropy_coeff: 0.0017600000137463212
        kl: 0.013350090011954308
        model: {}
        policy_loss: -0.03524702042341232
        total_loss: -0.03310757875442505
        vf_explained_var: 0.1022210419178009
        vf_loss: 14.1338472366333
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 0.8031468391418457
        entropy_coeff: 0.0017600000137463212
        kl: 0.010032115504145622
        model: {}
        policy_loss: -0.026345297694206238
        total_loss: -0.024480629712343216
        vf_explained_var: 0.19128628075122833
        vf_loss: 12.717846870422363
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 1.1369669437408447
        entropy_coeff: 0.0017600000137463212
        kl: 0.015143873170018196
        model: {}
        policy_loss: -0.03647181764245033
        total_loss: -0.03411397337913513
        vf_explained_var: 0.15469075739383698
        vf_loss: 13.301359176635742
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006190271815285087
        entropy: 1.0456669330596924
        entropy_coeff: 0.0017600000137463212
        kl: 0.013557669706642628
        model: {}
        policy_loss: -0.03630104660987854
        total_loss: -0.034124549478292465
        vf_explained_var: 0.16930393874645233
        vf_loss: 13.053316116333008
    load_time_ms: 14054.603
    num_steps_sampled: 10368000
    num_steps_trained: 10368000
    sample_time_ms: 106663.073
    update_time_ms: 60.321
  iterations_since_restore: 48
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.682291666666668
    ram_util_percent: 14.7046875
  pid: 30948
  policy_reward_max:
    agent-0: 135.3333333333336
    agent-1: 135.3333333333336
    agent-2: 135.3333333333336
    agent-3: 135.3333333333336
    agent-4: 135.3333333333336
    agent-5: 135.3333333333336
  policy_reward_mean:
    agent-0: 96.96833333333362
    agent-1: 96.96833333333362
    agent-2: 96.96833333333362
    agent-3: 96.96833333333362
    agent-4: 96.96833333333362
    agent-5: 96.96833333333362
  policy_reward_min:
    agent-0: 22.50000000000002
    agent-1: 22.50000000000002
    agent-2: 22.50000000000002
    agent-3: 22.50000000000002
    agent-4: 22.50000000000002
    agent-5: 22.50000000000002
  sampler_perf:
    mean_env_wait_ms: 28.183521536660805
    mean_inference_ms: 13.183397851779155
    mean_processing_ms: 60.09298795345451
  time_since_restore: 6932.605547428131
  time_this_iter_s: 134.2122504711151
  time_total_s: 16058.617361307144
  timestamp: 1637034000
  timesteps_since_restore: 4608000
  timesteps_this_iter: 96000
  timesteps_total: 10368000
  training_iteration: 108
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 28.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    108 |          16058.6 | 10368000 |   581.81 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 74
    apples_agent-0_mean: 3.06
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 24.89
    apples_agent-1_min: 0
    apples_agent-2_max: 258
    apples_agent-2_mean: 17.97
    apples_agent-2_min: 0
    apples_agent-3_max: 232
    apples_agent-3_mean: 79.75
    apples_agent-3_min: 24
    apples_agent-4_max: 36
    apples_agent-4_mean: 2.42
    apples_agent-4_min: 0
    apples_agent-5_max: 152
    apples_agent-5_mean: 72.38
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 413
    cleaning_beam_agent-0_mean: 306.48
    cleaning_beam_agent-0_min: 167
    cleaning_beam_agent-1_max: 433
    cleaning_beam_agent-1_mean: 211.4
    cleaning_beam_agent-1_min: 87
    cleaning_beam_agent-2_max: 641
    cleaning_beam_agent-2_mean: 384.95
    cleaning_beam_agent-2_min: 139
    cleaning_beam_agent-3_max: 143
    cleaning_beam_agent-3_mean: 49.42
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 474
    cleaning_beam_agent-4_mean: 345.09
    cleaning_beam_agent-4_min: 129
    cleaning_beam_agent-5_max: 157
    cleaning_beam_agent-5_mean: 65.31
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-42-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 873.9999999999703
  episode_reward_mean: 577.3500000000008
  episode_reward_min: 168.99999999999997
  episodes_this_iter: 96
  episodes_total: 10464
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12304.357
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 1.1350678205490112
        entropy_coeff: 0.0017600000137463212
        kl: 0.013953828252851963
        model: {}
        policy_loss: -0.02998017705976963
        total_loss: -0.027816105633974075
        vf_explained_var: 0.06894262135028839
        vf_loss: 13.710237503051758
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 1.2004717588424683
        entropy_coeff: 0.0017600000137463212
        kl: 0.014414826408028603
        model: {}
        policy_loss: -0.034429483115673065
        total_loss: -0.03221850469708443
        vf_explained_var: 0.02142387628555298
        vf_loss: 14.408393859863281
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 1.10701584815979
        entropy_coeff: 0.0017600000137463212
        kl: 0.01396638061851263
        model: {}
        policy_loss: -0.03440754488110542
        total_loss: -0.03228816017508507
        vf_explained_var: 0.1340448409318924
        vf_loss: 12.744548797607422
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 0.7901445627212524
        entropy_coeff: 0.0017600000137463212
        kl: 0.01041452493518591
        model: {}
        policy_loss: -0.024958737194538116
        total_loss: -0.023055270314216614
        vf_explained_var: 0.17703646421432495
        vf_loss: 12.112163543701172
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 1.1334117650985718
        entropy_coeff: 0.0017600000137463212
        kl: 0.013824721798300743
        model: {}
        policy_loss: -0.03619462624192238
        total_loss: -0.03415495902299881
        vf_explained_var: 0.1375206708908081
        vf_loss: 12.695314407348633
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006130367983132601
        entropy: 1.0503206253051758
        entropy_coeff: 0.0017600000137463212
        kl: 0.013925906270742416
        model: {}
        policy_loss: -0.03613809123635292
        total_loss: -0.03397019952535629
        vf_explained_var: 0.16382767260074615
        vf_loss: 12.312677383422852
    load_time_ms: 13556.235
    num_steps_sampled: 10464000
    num_steps_trained: 10464000
    sample_time_ms: 105851.734
    update_time_ms: 14.045
  iterations_since_restore: 49
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.958152173913046
    ram_util_percent: 16.510326086956525
  pid: 30948
  policy_reward_max:
    agent-0: 145.66666666666686
    agent-1: 145.66666666666686
    agent-2: 145.66666666666686
    agent-3: 145.66666666666686
    agent-4: 145.66666666666686
    agent-5: 145.66666666666686
  policy_reward_mean:
    agent-0: 96.22500000000024
    agent-1: 96.22500000000024
    agent-2: 96.22500000000024
    agent-3: 96.22500000000024
    agent-4: 96.22500000000024
    agent-5: 96.22500000000024
  policy_reward_min:
    agent-0: 28.16666666666671
    agent-1: 28.16666666666671
    agent-2: 28.16666666666671
    agent-3: 28.16666666666671
    agent-4: 28.16666666666671
    agent-5: 28.16666666666671
  sampler_perf:
    mean_env_wait_ms: 28.165762189863525
    mean_inference_ms: 13.178268596291282
    mean_processing_ms: 60.05472307990818
  time_since_restore: 7062.6051461696625
  time_this_iter_s: 129.99959874153137
  time_total_s: 16188.616960048676
  timestamp: 1637034131
  timesteps_since_restore: 4704000
  timesteps_this_iter: 96000
  timesteps_total: 10464000
  training_iteration: 109
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 35.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    109 |          16188.6 | 10464000 |   577.35 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 3.1
    apples_agent-0_min: 0
    apples_agent-1_max: 148
    apples_agent-1_mean: 26.12
    apples_agent-1_min: 0
    apples_agent-2_max: 221
    apples_agent-2_mean: 19.52
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 77.67
    apples_agent-3_min: 24
    apples_agent-4_max: 74
    apples_agent-4_mean: 5.46
    apples_agent-4_min: 0
    apples_agent-5_max: 152
    apples_agent-5_mean: 74.25
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 425
    cleaning_beam_agent-0_mean: 294.1
    cleaning_beam_agent-0_min: 127
    cleaning_beam_agent-1_max: 433
    cleaning_beam_agent-1_mean: 224.6
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 582
    cleaning_beam_agent-2_mean: 378.18
    cleaning_beam_agent-2_min: 146
    cleaning_beam_agent-3_max: 145
    cleaning_beam_agent-3_mean: 51.61
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 525
    cleaning_beam_agent-4_mean: 346.06
    cleaning_beam_agent-4_min: 135
    cleaning_beam_agent-5_max: 193
    cleaning_beam_agent-5_mean: 69.87
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-44-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 810.9999999999852
  episode_reward_mean: 600.8500000000001
  episode_reward_min: 275.9999999999974
  episodes_this_iter: 96
  episodes_total: 10560
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12473.125
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 1.141753911972046
        entropy_coeff: 0.0017600000137463212
        kl: 0.011903480626642704
        model: {}
        policy_loss: -0.03049628809094429
        total_loss: -0.028762415051460266
        vf_explained_var: 0.05430199205875397
        vf_loss: 13.62667465209961
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 1.1993759870529175
        entropy_coeff: 0.0017600000137463212
        kl: 0.014233836904168129
        model: {}
        policy_loss: -0.034903667867183685
        total_loss: -0.0327298566699028
        vf_explained_var: 0.0021674782037734985
        vf_loss: 14.379447937011719
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 1.1047905683517456
        entropy_coeff: 0.0017600000137463212
        kl: 0.014280552975833416
        model: {}
        policy_loss: -0.032444313168525696
        total_loss: -0.030214659869670868
        vf_explained_var: 0.08509039878845215
        vf_loss: 13.179770469665527
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 0.7973134517669678
        entropy_coeff: 0.0017600000137463212
        kl: 0.009483659639954567
        model: {}
        policy_loss: -0.026047667488455772
        total_loss: -0.024340160191059113
        vf_explained_var: 0.1571444422006607
        vf_loss: 12.140463829040527
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 1.1163034439086914
        entropy_coeff: 0.0017600000137463212
        kl: 0.015224494971334934
        model: {}
        policy_loss: -0.03725680708885193
        total_loss: -0.03489706665277481
        vf_explained_var: 0.11061304807662964
        vf_loss: 12.795289993286133
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0006070464150980115
        entropy: 1.052219271659851
        entropy_coeff: 0.0017600000137463212
        kl: 0.014065560884773731
        model: {}
        policy_loss: -0.03702975809574127
        total_loss: -0.034860700368881226
        vf_explained_var: 0.16016581654548645
        vf_loss: 12.07851791381836
    load_time_ms: 13594.948
    num_steps_sampled: 10560000
    num_steps_trained: 10560000
    sample_time_ms: 106192.53
    update_time_ms: 19.58
  iterations_since_restore: 50
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.01894736842105
    ram_util_percent: 18.92315789473684
  pid: 30948
  policy_reward_max:
    agent-0: 135.16666666666748
    agent-1: 135.16666666666748
    agent-2: 135.16666666666748
    agent-3: 135.16666666666748
    agent-4: 135.16666666666748
    agent-5: 135.16666666666748
  policy_reward_mean:
    agent-0: 100.14166666666696
    agent-1: 100.14166666666696
    agent-2: 100.14166666666696
    agent-3: 100.14166666666696
    agent-4: 100.14166666666696
    agent-5: 100.14166666666696
  policy_reward_min:
    agent-0: 45.999999999999886
    agent-1: 45.999999999999886
    agent-2: 45.999999999999886
    agent-3: 45.999999999999886
    agent-4: 45.999999999999886
    agent-5: 45.999999999999886
  sampler_perf:
    mean_env_wait_ms: 28.14637616970917
    mean_inference_ms: 13.173295439587303
    mean_processing_ms: 60.01759230194112
  time_since_restore: 7195.477905750275
  time_this_iter_s: 132.87275958061218
  time_total_s: 16321.489719629288
  timestamp: 1637034264
  timesteps_since_restore: 4800000
  timesteps_this_iter: 96000
  timesteps_total: 10560000
  training_iteration: 110
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    110 |          16321.5 | 10560000 |   600.85 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 4.23
    apples_agent-0_min: 0
    apples_agent-1_max: 115
    apples_agent-1_mean: 24.44
    apples_agent-1_min: 0
    apples_agent-2_max: 153
    apples_agent-2_mean: 16.22
    apples_agent-2_min: 0
    apples_agent-3_max: 171
    apples_agent-3_mean: 83.02
    apples_agent-3_min: 24
    apples_agent-4_max: 63
    apples_agent-4_mean: 3.1
    apples_agent-4_min: 0
    apples_agent-5_max: 124
    apples_agent-5_mean: 69.49
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 446
    cleaning_beam_agent-0_mean: 288.41
    cleaning_beam_agent-0_min: 146
    cleaning_beam_agent-1_max: 421
    cleaning_beam_agent-1_mean: 222.35
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 533
    cleaning_beam_agent-2_mean: 371.43
    cleaning_beam_agent-2_min: 113
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 46.05
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 464
    cleaning_beam_agent-4_mean: 342.21
    cleaning_beam_agent-4_min: 84
    cleaning_beam_agent-5_max: 184
    cleaning_beam_agent-5_mean: 73.1
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-46-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 854.9999999999777
  episode_reward_mean: 608.0999999999984
  episode_reward_min: 264.9999999999957
  episodes_this_iter: 96
  episodes_total: 10656
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12479.256
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 1.1482551097869873
        entropy_coeff: 0.0017600000137463212
        kl: 0.01298313308507204
        model: {}
        policy_loss: -0.03066486492753029
        total_loss: -0.02874985709786415
        vf_explained_var: 0.0931607186794281
        vf_loss: 13.393075942993164
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 1.192420244216919
        entropy_coeff: 0.0017600000137463212
        kl: 0.014899609610438347
        model: {}
        policy_loss: -0.034234724938869476
        total_loss: -0.03191043436527252
        vf_explained_var: 0.02299763262271881
        vf_loss: 14.430310249328613
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 1.1086763143539429
        entropy_coeff: 0.0017600000137463212
        kl: 0.016073912382125854
        model: {}
        policy_loss: -0.03229358792304993
        total_loss: -0.02971482090651989
        vf_explained_var: 0.10928910970687866
        vf_loss: 13.152570724487305
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 0.7918558716773987
        entropy_coeff: 0.0017600000137463212
        kl: 0.010735131800174713
        model: {}
        policy_loss: -0.02515276148915291
        total_loss: -0.023157930001616478
        vf_explained_var: 0.1592024713754654
        vf_loss: 12.41473388671875
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 1.1213608980178833
        entropy_coeff: 0.0017600000137463212
        kl: 0.014642789959907532
        model: {}
        policy_loss: -0.03826268017292023
        total_loss: -0.03597370162606239
        vf_explained_var: 0.09568089246749878
        vf_loss: 13.340190887451172
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000601055973675102
        entropy: 1.044561743736267
        entropy_coeff: 0.0017600000137463212
        kl: 0.013286871835589409
        model: {}
        policy_loss: -0.036748435348272324
        total_loss: -0.034696064889431
        vf_explained_var: 0.1647300273180008
        vf_loss: 12.33426284790039
    load_time_ms: 13622.94
    num_steps_sampled: 10656000
    num_steps_trained: 10656000
    sample_time_ms: 105728.361
    update_time_ms: 19.944
  iterations_since_restore: 51
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.10698924731183
    ram_util_percent: 18.906989247311827
  pid: 30948
  policy_reward_max:
    agent-0: 142.5000000000001
    agent-1: 142.5000000000001
    agent-2: 142.5000000000001
    agent-3: 142.5000000000001
    agent-4: 142.5000000000001
    agent-5: 142.5000000000001
  policy_reward_mean:
    agent-0: 101.35000000000029
    agent-1: 101.35000000000029
    agent-2: 101.35000000000029
    agent-3: 101.35000000000029
    agent-4: 101.35000000000029
    agent-5: 101.35000000000029
  policy_reward_min:
    agent-0: 44.16666666666658
    agent-1: 44.16666666666658
    agent-2: 44.16666666666658
    agent-3: 44.16666666666658
    agent-4: 44.16666666666658
    agent-5: 44.16666666666658
  sampler_perf:
    mean_env_wait_ms: 28.11891397192677
    mean_inference_ms: 13.16846044545125
    mean_processing_ms: 59.974183973341304
  time_since_restore: 7325.957196235657
  time_this_iter_s: 130.47929048538208
  time_total_s: 16451.96901011467
  timestamp: 1637034395
  timesteps_since_restore: 4896000
  timesteps_this_iter: 96000
  timesteps_total: 10656000
  training_iteration: 111
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    111 |            16452 | 10656000 |    608.1 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 4.13
    apples_agent-0_min: 0
    apples_agent-1_max: 157
    apples_agent-1_mean: 24.65
    apples_agent-1_min: 0
    apples_agent-2_max: 186
    apples_agent-2_mean: 19.08
    apples_agent-2_min: 0
    apples_agent-3_max: 199
    apples_agent-3_mean: 81.07
    apples_agent-3_min: 28
    apples_agent-4_max: 66
    apples_agent-4_mean: 4.13
    apples_agent-4_min: 0
    apples_agent-5_max: 123
    apples_agent-5_mean: 65.92
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 439
    cleaning_beam_agent-0_mean: 281.74
    cleaning_beam_agent-0_min: 150
    cleaning_beam_agent-1_max: 302
    cleaning_beam_agent-1_mean: 202.52
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 567
    cleaning_beam_agent-2_mean: 356.35
    cleaning_beam_agent-2_min: 82
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 41.69
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 485
    cleaning_beam_agent-4_mean: 335.8
    cleaning_beam_agent-4_min: 119
    cleaning_beam_agent-5_max: 281
    cleaning_beam_agent-5_mean: 81.74
    cleaning_beam_agent-5_min: 25
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-48-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 802.9999999999908
  episode_reward_mean: 556.0600000000011
  episode_reward_min: 189.99999999999858
  episodes_this_iter: 96
  episodes_total: 10752
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12569.367
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 1.1619229316711426
        entropy_coeff: 0.0017600000137463212
        kl: 0.012399078346788883
        model: {}
        policy_loss: -0.031506676226854324
        total_loss: -0.029806721955537796
        vf_explained_var: 0.12082025408744812
        vf_loss: 12.651253700256348
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 1.2061591148376465
        entropy_coeff: 0.0017600000137463212
        kl: 0.014293686486780643
        model: {}
        policy_loss: -0.03686496987938881
        total_loss: -0.03474743664264679
        vf_explained_var: 0.04137133061885834
        vf_loss: 13.816391944885254
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 1.1212749481201172
        entropy_coeff: 0.0017600000137463212
        kl: 0.014012184925377369
        model: {}
        policy_loss: -0.034049805253744125
        total_loss: -0.031902946531772614
        vf_explained_var: 0.08621291816234589
        vf_loss: 13.178646087646484
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 0.7806558012962341
        entropy_coeff: 0.0017600000137463212
        kl: 0.010317007079720497
        model: {}
        policy_loss: -0.025805475190281868
        total_loss: -0.023987438529729843
        vf_explained_var: 0.21679989993572235
        vf_loss: 11.28591251373291
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 1.1407381296157837
        entropy_coeff: 0.0017600000137463212
        kl: 0.01518505159765482
        model: {}
        policy_loss: -0.03735817223787308
        total_loss: -0.0350775383412838
        vf_explained_var: 0.13255341351032257
        vf_loss: 12.513266563415527
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005950655904598534
        entropy: 1.0733213424682617
        entropy_coeff: 0.0017600000137463212
        kl: 0.017335422337055206
        model: {}
        policy_loss: -0.03425096347928047
        total_loss: -0.03147803992033005
        vf_explained_var: 0.17107245326042175
        vf_loss: 11.9488525390625
    load_time_ms: 13637.722
    num_steps_sampled: 10752000
    num_steps_trained: 10752000
    sample_time_ms: 106139.608
    update_time_ms: 20.146
  iterations_since_restore: 52
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.123936170212765
    ram_util_percent: 18.91010638297872
  pid: 30948
  policy_reward_max:
    agent-0: 133.83333333333377
    agent-1: 133.83333333333377
    agent-2: 133.83333333333377
    agent-3: 133.83333333333377
    agent-4: 133.83333333333377
    agent-5: 133.83333333333377
  policy_reward_mean:
    agent-0: 92.6766666666669
    agent-1: 92.6766666666669
    agent-2: 92.6766666666669
    agent-3: 92.6766666666669
    agent-4: 92.6766666666669
    agent-5: 92.6766666666669
  policy_reward_min:
    agent-0: 31.666666666666746
    agent-1: 31.666666666666746
    agent-2: 31.666666666666746
    agent-3: 31.666666666666746
    agent-4: 31.666666666666746
    agent-5: 31.666666666666746
  sampler_perf:
    mean_env_wait_ms: 28.096522078510887
    mean_inference_ms: 13.16383287141395
    mean_processing_ms: 59.936573035064164
  time_since_restore: 7457.649178981781
  time_this_iter_s: 131.69198274612427
  time_total_s: 16583.660992860794
  timestamp: 1637034526
  timesteps_since_restore: 4992000
  timesteps_this_iter: 96000
  timesteps_total: 10752000
  training_iteration: 112
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    112 |          16583.7 | 10752000 |   556.06 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 2.7
    apples_agent-0_min: 0
    apples_agent-1_max: 122
    apples_agent-1_mean: 28.41
    apples_agent-1_min: 0
    apples_agent-2_max: 170
    apples_agent-2_mean: 13.5
    apples_agent-2_min: 0
    apples_agent-3_max: 177
    apples_agent-3_mean: 81.77
    apples_agent-3_min: 5
    apples_agent-4_max: 29
    apples_agent-4_mean: 1.65
    apples_agent-4_min: 0
    apples_agent-5_max: 116
    apples_agent-5_mean: 67.74
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 406
    cleaning_beam_agent-0_mean: 289.71
    cleaning_beam_agent-0_min: 101
    cleaning_beam_agent-1_max: 354
    cleaning_beam_agent-1_mean: 196.02
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 580
    cleaning_beam_agent-2_mean: 362.02
    cleaning_beam_agent-2_min: 119
    cleaning_beam_agent-3_max: 101
    cleaning_beam_agent-3_mean: 39.75
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 480
    cleaning_beam_agent-4_mean: 352.15
    cleaning_beam_agent-4_min: 155
    cleaning_beam_agent-5_max: 243
    cleaning_beam_agent-5_mean: 82.13
    cleaning_beam_agent-5_min: 27
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-50-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 848.999999999984
  episode_reward_mean: 595.4699999999988
  episode_reward_min: 63.99999999999962
  episodes_this_iter: 96
  episodes_total: 10848
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12564.114
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 1.1420186758041382
        entropy_coeff: 0.0017600000137463212
        kl: 0.012590918689966202
        model: {}
        policy_loss: -0.03187257796525955
        total_loss: -0.030072785913944244
        vf_explained_var: 0.09031318128108978
        vf_loss: 12.915666580200195
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 1.1882593631744385
        entropy_coeff: 0.0017600000137463212
        kl: 0.014733205549418926
        model: {}
        policy_loss: -0.037031132727861404
        total_loss: -0.034767258912324905
        vf_explained_var: 0.007763415575027466
        vf_loss: 14.085737228393555
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 1.1303009986877441
        entropy_coeff: 0.0017600000137463212
        kl: 0.014352267608046532
        model: {}
        policy_loss: -0.03394312039017677
        total_loss: -0.031786054372787476
        vf_explained_var: 0.1012011468410492
        vf_loss: 12.75942611694336
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 0.7565904855728149
        entropy_coeff: 0.0017600000137463212
        kl: 0.009848137386143208
        model: {}
        policy_loss: -0.026420867070555687
        total_loss: -0.02461247704923153
        vf_explained_var: 0.1756696105003357
        vf_loss: 11.703615188598633
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 1.1217155456542969
        entropy_coeff: 0.0017600000137463212
        kl: 0.01447293721139431
        model: {}
        policy_loss: -0.036921098828315735
        total_loss: -0.03468585014343262
        vf_explained_var: 0.07328274846076965
        vf_loss: 13.148781776428223
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005890752072446048
        entropy: 1.0246167182922363
        entropy_coeff: 0.0017600000137463212
        kl: 0.014558406546711922
        model: {}
        policy_loss: -0.03531494736671448
        total_loss: -0.0329848974943161
        vf_explained_var: 0.1391046792268753
        vf_loss: 12.216978073120117
    load_time_ms: 13671.01
    num_steps_sampled: 10848000
    num_steps_trained: 10848000
    sample_time_ms: 105552.851
    update_time_ms: 20.205
  iterations_since_restore: 53
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.118716577540102
    ram_util_percent: 18.916042780748658
  pid: 30948
  policy_reward_max:
    agent-0: 141.50000000000006
    agent-1: 141.50000000000006
    agent-2: 141.50000000000006
    agent-3: 141.50000000000006
    agent-4: 141.50000000000006
    agent-5: 141.50000000000006
  policy_reward_mean:
    agent-0: 99.24500000000029
    agent-1: 99.24500000000029
    agent-2: 99.24500000000029
    agent-3: 99.24500000000029
    agent-4: 99.24500000000029
    agent-5: 99.24500000000029
  policy_reward_min:
    agent-0: 10.666666666666663
    agent-1: 10.666666666666663
    agent-2: 10.666666666666663
    agent-3: 10.666666666666663
    agent-4: 10.666666666666663
    agent-5: 10.666666666666663
  sampler_perf:
    mean_env_wait_ms: 28.07485955075099
    mean_inference_ms: 13.160551886949712
    mean_processing_ms: 59.895349553669675
  time_since_restore: 7588.79162311554
  time_this_iter_s: 131.14244413375854
  time_total_s: 16714.803436994553
  timestamp: 1637034658
  timesteps_since_restore: 5088000
  timesteps_this_iter: 96000
  timesteps_total: 10848000
  training_iteration: 113
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 35.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    113 |          16714.8 | 10848000 |   595.47 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 3.57
    apples_agent-0_min: 0
    apples_agent-1_max: 146
    apples_agent-1_mean: 27.68
    apples_agent-1_min: 0
    apples_agent-2_max: 112
    apples_agent-2_mean: 13.1
    apples_agent-2_min: 0
    apples_agent-3_max: 132
    apples_agent-3_mean: 72.65
    apples_agent-3_min: 13
    apples_agent-4_max: 40
    apples_agent-4_mean: 1.58
    apples_agent-4_min: 0
    apples_agent-5_max: 137
    apples_agent-5_mean: 69.39
    apples_agent-5_min: 11
    cleaning_beam_agent-0_max: 512
    cleaning_beam_agent-0_mean: 286.25
    cleaning_beam_agent-0_min: 92
    cleaning_beam_agent-1_max: 354
    cleaning_beam_agent-1_mean: 199.72
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 537
    cleaning_beam_agent-2_mean: 360.44
    cleaning_beam_agent-2_min: 89
    cleaning_beam_agent-3_max: 120
    cleaning_beam_agent-3_mean: 42.38
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 525
    cleaning_beam_agent-4_mean: 346.28
    cleaning_beam_agent-4_min: 211
    cleaning_beam_agent-5_max: 165
    cleaning_beam_agent-5_mean: 80.58
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-53-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 805.9999999999965
  episode_reward_mean: 568.3100000000009
  episode_reward_min: 174.99999999999818
  episodes_this_iter: 96
  episodes_total: 10944
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12547.837
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 1.1466748714447021
        entropy_coeff: 0.0017600000137463212
        kl: 0.011975051835179329
        model: {}
        policy_loss: -0.03132856637239456
        total_loss: -0.029597515240311623
        vf_explained_var: 0.0452953577041626
        vf_loss: 13.541852951049805
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 1.1588385105133057
        entropy_coeff: 0.0017600000137463212
        kl: 0.013545704074203968
        model: {}
        policy_loss: -0.03564697131514549
        total_loss: -0.033557791262865067
        vf_explained_var: 0.0007466822862625122
        vf_loss: 14.195914268493652
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 1.1122546195983887
        entropy_coeff: 0.0017600000137463212
        kl: 0.01322145201265812
        model: {}
        policy_loss: -0.03371795639395714
        total_loss: -0.03178686276078224
        vf_explained_var: 0.1236942708492279
        vf_loss: 12.443714141845703
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 0.7800657749176025
        entropy_coeff: 0.0017600000137463212
        kl: 0.010124417953193188
        model: {}
        policy_loss: -0.026661934331059456
        total_loss: -0.024826839566230774
        vf_explained_var: 0.16590073704719543
        vf_loss: 11.831275939941406
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 1.1189517974853516
        entropy_coeff: 0.0017600000137463212
        kl: 0.014527843333780766
        model: {}
        policy_loss: -0.0369945764541626
        total_loss: -0.03473815321922302
        vf_explained_var: 0.06835117936134338
        vf_loss: 13.202122688293457
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005830848240293562
        entropy: 1.039602279663086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0136514101177454
        model: {}
        policy_loss: -0.03623964637517929
        total_loss: -0.03411022573709488
        vf_explained_var: 0.13359837234020233
        vf_loss: 12.288396835327148
    load_time_ms: 13692.295
    num_steps_sampled: 10944000
    num_steps_trained: 10944000
    sample_time_ms: 106130.242
    update_time_ms: 37.188
  iterations_since_restore: 54
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.954736842105266
    ram_util_percent: 18.90947368421052
  pid: 30948
  policy_reward_max:
    agent-0: 134.3333333333336
    agent-1: 134.3333333333336
    agent-2: 134.3333333333336
    agent-3: 134.3333333333336
    agent-4: 134.3333333333336
    agent-5: 134.3333333333336
  policy_reward_mean:
    agent-0: 94.71833333333356
    agent-1: 94.71833333333356
    agent-2: 94.71833333333356
    agent-3: 94.71833333333356
    agent-4: 94.71833333333356
    agent-5: 94.71833333333356
  policy_reward_min:
    agent-0: 29.16666666666673
    agent-1: 29.16666666666673
    agent-2: 29.16666666666673
    agent-3: 29.16666666666673
    agent-4: 29.16666666666673
    agent-5: 29.16666666666673
  sampler_perf:
    mean_env_wait_ms: 28.052828539689962
    mean_inference_ms: 13.157351451280679
    mean_processing_ms: 59.861924531675726
  time_since_restore: 7722.268353700638
  time_this_iter_s: 133.47673058509827
  time_total_s: 16848.28016757965
  timestamp: 1637034792
  timesteps_since_restore: 5184000
  timesteps_this_iter: 96000
  timesteps_total: 10944000
  training_iteration: 114
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    114 |          16848.3 | 10944000 |   568.31 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 3.89
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 25.49
    apples_agent-1_min: 0
    apples_agent-2_max: 200
    apples_agent-2_mean: 13.47
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 77.72
    apples_agent-3_min: 13
    apples_agent-4_max: 42
    apples_agent-4_mean: 2.99
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 66.99
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 455
    cleaning_beam_agent-0_mean: 284.13
    cleaning_beam_agent-0_min: 92
    cleaning_beam_agent-1_max: 356
    cleaning_beam_agent-1_mean: 194.93
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 563
    cleaning_beam_agent-2_mean: 370.61
    cleaning_beam_agent-2_min: 59
    cleaning_beam_agent-3_max: 125
    cleaning_beam_agent-3_mean: 42.12
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 494
    cleaning_beam_agent-4_mean: 337.6
    cleaning_beam_agent-4_min: 101
    cleaning_beam_agent-5_max: 197
    cleaning_beam_agent-5_mean: 84.74
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-55-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 852.9999999999882
  episode_reward_mean: 568.0300000000007
  episode_reward_min: 159.0000000000002
  episodes_this_iter: 96
  episodes_total: 11040
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12518.763
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 1.132922887802124
        entropy_coeff: 0.0017600000137463212
        kl: 0.011326334439218044
        model: {}
        policy_loss: -0.03025343269109726
        total_loss: -0.028717083856463432
        vf_explained_var: 0.10643908381462097
        vf_loss: 12.650270462036133
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 1.1726893186569214
        entropy_coeff: 0.0017600000137463212
        kl: 0.014849827624857426
        model: {}
        policy_loss: -0.03754844516515732
        total_loss: -0.035216692835092545
        vf_explained_var: -0.00659160315990448
        vf_loss: 14.257207870483398
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 1.0944279432296753
        entropy_coeff: 0.0017600000137463212
        kl: 0.014053422026336193
        model: {}
        policy_loss: -0.03352087363600731
        total_loss: -0.0312437042593956
        vf_explained_var: 0.016847148537635803
        vf_loss: 13.926773071289062
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 0.7698458433151245
        entropy_coeff: 0.0017600000137463212
        kl: 0.010274814441800117
        model: {}
        policy_loss: -0.026648065075278282
        total_loss: -0.024813462048768997
        vf_explained_var: 0.19819708168506622
        vf_loss: 11.345701217651367
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 1.1187705993652344
        entropy_coeff: 0.0017600000137463212
        kl: 0.014803174883127213
        model: {}
        policy_loss: -0.03830726444721222
        total_loss: -0.03601787984371185
        vf_explained_var: 0.08363641798496246
        vf_loss: 12.977859497070312
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005770943826064467
        entropy: 1.0517252683639526
        entropy_coeff: 0.0017600000137463212
        kl: 0.013586891815066338
        model: {}
        policy_loss: -0.036540769040584564
        total_loss: -0.03443646803498268
        vf_explained_var: 0.12612390518188477
        vf_loss: 12.379583358764648
    load_time_ms: 13680.261
    num_steps_sampled: 11040000
    num_steps_trained: 11040000
    sample_time_ms: 105921.06
    update_time_ms: 37.35
  iterations_since_restore: 55
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.164171122994656
    ram_util_percent: 18.84331550802139
  pid: 30948
  policy_reward_max:
    agent-0: 142.16666666666688
    agent-1: 142.16666666666688
    agent-2: 142.16666666666688
    agent-3: 142.16666666666688
    agent-4: 142.16666666666688
    agent-5: 142.16666666666688
  policy_reward_mean:
    agent-0: 94.6716666666669
    agent-1: 94.6716666666669
    agent-2: 94.6716666666669
    agent-3: 94.6716666666669
    agent-4: 94.6716666666669
    agent-5: 94.6716666666669
  policy_reward_min:
    agent-0: 26.50000000000004
    agent-1: 26.50000000000004
    agent-2: 26.50000000000004
    agent-3: 26.50000000000004
    agent-4: 26.50000000000004
    agent-5: 26.50000000000004
  sampler_perf:
    mean_env_wait_ms: 28.034027050799356
    mean_inference_ms: 13.154134505045906
    mean_processing_ms: 59.82615896925858
  time_since_restore: 7853.320024967194
  time_this_iter_s: 131.0516712665558
  time_total_s: 16979.331838846207
  timestamp: 1637034923
  timesteps_since_restore: 5280000
  timesteps_this_iter: 96000
  timesteps_total: 11040000
  training_iteration: 115
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    115 |          16979.3 | 11040000 |   568.03 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 2.51
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 32.17
    apples_agent-1_min: 0
    apples_agent-2_max: 192
    apples_agent-2_mean: 16.6
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 85.21
    apples_agent-3_min: 43
    apples_agent-4_max: 57
    apples_agent-4_mean: 2.82
    apples_agent-4_min: 0
    apples_agent-5_max: 139
    apples_agent-5_mean: 72.44
    apples_agent-5_min: 7
    cleaning_beam_agent-0_max: 429
    cleaning_beam_agent-0_mean: 315.54
    cleaning_beam_agent-0_min: 205
    cleaning_beam_agent-1_max: 364
    cleaning_beam_agent-1_mean: 191.62
    cleaning_beam_agent-1_min: 74
    cleaning_beam_agent-2_max: 627
    cleaning_beam_agent-2_mean: 379.67
    cleaning_beam_agent-2_min: 28
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 44.9
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 530
    cleaning_beam_agent-4_mean: 342.52
    cleaning_beam_agent-4_min: 208
    cleaning_beam_agent-5_max: 301
    cleaning_beam_agent-5_mean: 73.79
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-57-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 879.9999999999882
  episode_reward_mean: 599.9599999999996
  episode_reward_min: 342.0000000000031
  episodes_this_iter: 96
  episodes_total: 11136
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12489.091
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 1.1273549795150757
        entropy_coeff: 0.0017600000137463212
        kl: 0.012122753076255322
        model: {}
        policy_loss: -0.030247226357460022
        total_loss: -0.02847689762711525
        vf_explained_var: 0.09446266293525696
        vf_loss: 13.299232482910156
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 1.1657991409301758
        entropy_coeff: 0.0017600000137463212
        kl: 0.014897355809807777
        model: {}
        policy_loss: -0.03664211183786392
        total_loss: -0.034248873591423035
        vf_explained_var: 0.003099799156188965
        vf_loss: 14.655723571777344
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 1.065411925315857
        entropy_coeff: 0.0017600000137463212
        kl: 0.014202122576534748
        model: {}
        policy_loss: -0.0322687067091465
        total_loss: -0.029936347156763077
        vf_explained_var: 0.06938181817531586
        vf_loss: 13.670634269714355
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 0.7815297842025757
        entropy_coeff: 0.0017600000137463212
        kl: 0.010647911578416824
        model: {}
        policy_loss: -0.026321042329072952
        total_loss: -0.024374792352318764
        vf_explained_var: 0.18896885216236115
        vf_loss: 11.92158031463623
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 1.1206821203231812
        entropy_coeff: 0.0017600000137463212
        kl: 0.014230500906705856
        model: {}
        policy_loss: -0.038007572293281555
        total_loss: -0.03578120842576027
        vf_explained_var: 0.07897476851940155
        vf_loss: 13.526644706726074
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005711039993911982
        entropy: 1.0272669792175293
        entropy_coeff: 0.0017600000137463212
        kl: 0.013426527380943298
        model: {}
        policy_loss: -0.03509032353758812
        total_loss: -0.0329325869679451
        vf_explained_var: 0.12874850630760193
        vf_loss: 12.804210662841797
    load_time_ms: 13658.91
    num_steps_sampled: 11136000
    num_steps_trained: 11136000
    sample_time_ms: 105802.16
    update_time_ms: 37.401
  iterations_since_restore: 56
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.06382978723404
    ram_util_percent: 18.91755319148936
  pid: 30948
  policy_reward_max:
    agent-0: 146.66666666666652
    agent-1: 146.66666666666652
    agent-2: 146.66666666666652
    agent-3: 146.66666666666652
    agent-4: 146.66666666666652
    agent-5: 146.66666666666652
  policy_reward_mean:
    agent-0: 99.9933333333336
    agent-1: 99.9933333333336
    agent-2: 99.9933333333336
    agent-3: 99.9933333333336
    agent-4: 99.9933333333336
    agent-5: 99.9933333333336
  policy_reward_min:
    agent-0: 56.999999999999844
    agent-1: 56.999999999999844
    agent-2: 56.999999999999844
    agent-3: 56.999999999999844
    agent-4: 56.999999999999844
    agent-5: 56.999999999999844
  sampler_perf:
    mean_env_wait_ms: 28.015127230073386
    mean_inference_ms: 13.150606736685548
    mean_processing_ms: 59.79080884136307
  time_since_restore: 7985.247671842575
  time_this_iter_s: 131.92764687538147
  time_total_s: 17111.259485721588
  timestamp: 1637035057
  timesteps_since_restore: 5376000
  timesteps_this_iter: 96000
  timesteps_total: 11136000
  training_iteration: 116
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    116 |          17111.3 | 11136000 |   599.96 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 2.81
    apples_agent-0_min: 0
    apples_agent-1_max: 416
    apples_agent-1_mean: 32.37
    apples_agent-1_min: 0
    apples_agent-2_max: 154
    apples_agent-2_mean: 15.32
    apples_agent-2_min: 0
    apples_agent-3_max: 268
    apples_agent-3_mean: 82.66
    apples_agent-3_min: 7
    apples_agent-4_max: 35
    apples_agent-4_mean: 1.99
    apples_agent-4_min: 0
    apples_agent-5_max: 169
    apples_agent-5_mean: 68.76
    apples_agent-5_min: 14
    cleaning_beam_agent-0_max: 457
    cleaning_beam_agent-0_mean: 295.35
    cleaning_beam_agent-0_min: 128
    cleaning_beam_agent-1_max: 314
    cleaning_beam_agent-1_mean: 184.89
    cleaning_beam_agent-1_min: 89
    cleaning_beam_agent-2_max: 578
    cleaning_beam_agent-2_mean: 362.57
    cleaning_beam_agent-2_min: 37
    cleaning_beam_agent-3_max: 144
    cleaning_beam_agent-3_mean: 50.36
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 495
    cleaning_beam_agent-4_mean: 330.45
    cleaning_beam_agent-4_min: 99
    cleaning_beam_agent-5_max: 253
    cleaning_beam_agent-5_mean: 74.7
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_22-59-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 824.9999999999875
  episode_reward_mean: 570.64
  episode_reward_min: 128.00000000000082
  episodes_this_iter: 96
  episodes_total: 11232
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12505.771
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 1.1315058469772339
        entropy_coeff: 0.0017600000137463212
        kl: 0.011538692750036716
        model: {}
        policy_loss: -0.029728827998042107
        total_loss: -0.02801281213760376
        vf_explained_var: 0.09555388987064362
        vf_loss: 13.997295379638672
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 1.1517760753631592
        entropy_coeff: 0.0017600000137463212
        kl: 0.014535415917634964
        model: {}
        policy_loss: -0.03735019266605377
        total_loss: -0.0349339060485363
        vf_explained_var: 0.008822306990623474
        vf_loss: 15.363265991210938
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 1.0736806392669678
        entropy_coeff: 0.0017600000137463212
        kl: 0.013384014368057251
        model: {}
        policy_loss: -0.0334848016500473
        total_loss: -0.03134430944919586
        vf_explained_var: 0.12528139352798462
        vf_loss: 13.533681869506836
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 0.789811909198761
        entropy_coeff: 0.0017600000137463212
        kl: 0.010441371239721775
        model: {}
        policy_loss: -0.02621430531144142
        total_loss: -0.024345112964510918
        vf_explained_var: 0.24257856607437134
        vf_loss: 11.709907531738281
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 1.115248680114746
        entropy_coeff: 0.0017600000137463212
        kl: 0.014754504896700382
        model: {}
        policy_loss: -0.038853228092193604
        total_loss: -0.036513522267341614
        vf_explained_var: 0.1269468367099762
        vf_loss: 13.516395568847656
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005651136161759496
        entropy: 1.0419697761535645
        entropy_coeff: 0.0017600000137463212
        kl: 0.013565046712756157
        model: {}
        policy_loss: -0.03682408109307289
        total_loss: -0.03466168791055679
        vf_explained_var: 0.17147605121135712
        vf_loss: 12.832504272460938
    load_time_ms: 13644.204
    num_steps_sampled: 11232000
    num_steps_trained: 11232000
    sample_time_ms: 105310.454
    update_time_ms: 37.421
  iterations_since_restore: 57
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.30324324324324
    ram_util_percent: 18.912432432432432
  pid: 30948
  policy_reward_max:
    agent-0: 137.50000000000017
    agent-1: 137.50000000000017
    agent-2: 137.50000000000017
    agent-3: 137.50000000000017
    agent-4: 137.50000000000017
    agent-5: 137.50000000000017
  policy_reward_mean:
    agent-0: 95.10666666666691
    agent-1: 95.10666666666691
    agent-2: 95.10666666666691
    agent-3: 95.10666666666691
    agent-4: 95.10666666666691
    agent-5: 95.10666666666691
  policy_reward_min:
    agent-0: 21.333333333333332
    agent-1: 21.333333333333332
    agent-2: 21.333333333333332
    agent-3: 21.333333333333332
    agent-4: 21.333333333333332
    agent-5: 21.333333333333332
  sampler_perf:
    mean_env_wait_ms: 27.992760798767574
    mean_inference_ms: 13.147223541086548
    mean_processing_ms: 59.75688613947146
  time_since_restore: 8114.542715072632
  time_this_iter_s: 129.29504323005676
  time_total_s: 17240.554528951645
  timestamp: 1637035186
  timesteps_since_restore: 5472000
  timesteps_this_iter: 96000
  timesteps_total: 11232000
  training_iteration: 117
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    117 |          17240.6 | 11232000 |   570.64 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 3.13
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 27.71
    apples_agent-1_min: 0
    apples_agent-2_max: 184
    apples_agent-2_mean: 18.3
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 85.0
    apples_agent-3_min: 28
    apples_agent-4_max: 23
    apples_agent-4_mean: 1.47
    apples_agent-4_min: 0
    apples_agent-5_max: 135
    apples_agent-5_mean: 71.31
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 458
    cleaning_beam_agent-0_mean: 300.61
    cleaning_beam_agent-0_min: 58
    cleaning_beam_agent-1_max: 385
    cleaning_beam_agent-1_mean: 196.43
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 546
    cleaning_beam_agent-2_mean: 356.84
    cleaning_beam_agent-2_min: 85
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 45.38
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 507
    cleaning_beam_agent-4_mean: 343.68
    cleaning_beam_agent-4_min: 186
    cleaning_beam_agent-5_max: 162
    cleaning_beam_agent-5_mean: 57.96
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 5
    fire_beam_agent-0_mean: 0.06
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-02-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 806.9999999999754
  episode_reward_mean: 599.27
  episode_reward_min: 334.0000000000041
  episodes_this_iter: 96
  episodes_total: 11328
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12530.526
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 1.115688681602478
        entropy_coeff: 0.0017600000137463212
        kl: 0.011341029778122902
        model: {}
        policy_loss: -0.030428707599639893
        total_loss: -0.028836697340011597
        vf_explained_var: 0.07543328404426575
        vf_loss: 12.874120712280273
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 1.156733751296997
        entropy_coeff: 0.0017600000137463212
        kl: 0.014877869747579098
        model: {}
        policy_loss: -0.03643210232257843
        total_loss: -0.034091196954250336
        vf_explained_var: -0.007271215319633484
        vf_loss: 14.011802673339844
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 1.0769331455230713
        entropy_coeff: 0.0017600000137463212
        kl: 0.012996378354728222
        model: {}
        policy_loss: -0.032523371279239655
        total_loss: -0.030525151640176773
        vf_explained_var: 0.06926672160625458
        vf_loss: 12.943480491638184
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 0.7593235969543457
        entropy_coeff: 0.0017600000137463212
        kl: 0.009979195892810822
        model: {}
        policy_loss: -0.025683242827653885
        total_loss: -0.02388986013829708
        vf_explained_var: 0.18470978736877441
        vf_loss: 11.33957290649414
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 1.11180579662323
        entropy_coeff: 0.0017600000137463212
        kl: 0.013821318745613098
        model: {}
        policy_loss: -0.036989644169807434
        total_loss: -0.03486262261867523
        vf_explained_var: 0.051487088203430176
        vf_loss: 13.195371627807617
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005591231747530401
        entropy: 1.013126015663147
        entropy_coeff: 0.0017600000137463212
        kl: 0.013403944671154022
        model: {}
        policy_loss: -0.03507519140839577
        total_loss: -0.032940302044153214
        vf_explained_var: 0.10960924625396729
        vf_loss: 12.372057914733887
    load_time_ms: 13656.896
    num_steps_sampled: 11328000
    num_steps_trained: 11328000
    sample_time_ms: 105213.454
    update_time_ms: 37.087
  iterations_since_restore: 58
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.06
    ram_util_percent: 18.92473684210526
  pid: 30948
  policy_reward_max:
    agent-0: 134.5000000000002
    agent-1: 134.5000000000002
    agent-2: 134.5000000000002
    agent-3: 134.5000000000002
    agent-4: 134.5000000000002
    agent-5: 134.5000000000002
  policy_reward_mean:
    agent-0: 99.8783333333336
    agent-1: 99.8783333333336
    agent-2: 99.8783333333336
    agent-3: 99.8783333333336
    agent-4: 99.8783333333336
    agent-5: 99.8783333333336
  policy_reward_min:
    agent-0: 55.666666666666536
    agent-1: 55.666666666666536
    agent-2: 55.666666666666536
    agent-3: 55.666666666666536
    agent-4: 55.666666666666536
    agent-5: 55.666666666666536
  sampler_perf:
    mean_env_wait_ms: 27.97280418643222
    mean_inference_ms: 13.14435153994475
    mean_processing_ms: 59.735702371758116
  time_since_restore: 8247.953099250793
  time_this_iter_s: 133.41038417816162
  time_total_s: 17373.964913129807
  timestamp: 1637035320
  timesteps_since_restore: 5568000
  timesteps_this_iter: 96000
  timesteps_total: 11328000
  training_iteration: 118
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    118 |            17374 | 11328000 |   599.27 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 2.5
    apples_agent-0_min: 0
    apples_agent-1_max: 146
    apples_agent-1_mean: 28.88
    apples_agent-1_min: 0
    apples_agent-2_max: 203
    apples_agent-2_mean: 17.53
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 84.0
    apples_agent-3_min: 20
    apples_agent-4_max: 51
    apples_agent-4_mean: 1.34
    apples_agent-4_min: 0
    apples_agent-5_max: 114
    apples_agent-5_mean: 68.99
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 509
    cleaning_beam_agent-0_mean: 310.89
    cleaning_beam_agent-0_min: 185
    cleaning_beam_agent-1_max: 370
    cleaning_beam_agent-1_mean: 207.76
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 690
    cleaning_beam_agent-2_mean: 373.84
    cleaning_beam_agent-2_min: 107
    cleaning_beam_agent-3_max: 110
    cleaning_beam_agent-3_mean: 45.36
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 464
    cleaning_beam_agent-4_mean: 332.73
    cleaning_beam_agent-4_min: 168
    cleaning_beam_agent-5_max: 241
    cleaning_beam_agent-5_mean: 68.2
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-04-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 819.9999999999727
  episode_reward_mean: 618.3499999999987
  episode_reward_min: 334.0000000000001
  episodes_this_iter: 96
  episodes_total: 11424
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12511.223
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 1.1134262084960938
        entropy_coeff: 0.0017600000137463212
        kl: 0.011650743894279003
        model: {}
        policy_loss: -0.028916476294398308
        total_loss: -0.02729254774749279
        vf_explained_var: 0.12114225327968597
        vf_loss: 12.5341215133667
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 1.1590685844421387
        entropy_coeff: 0.0017600000137463212
        kl: 0.014269684441387653
        model: {}
        policy_loss: -0.03554175794124603
        total_loss: -0.03331278637051582
        vf_explained_var: 0.007658034563064575
        vf_loss: 14.149968147277832
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 1.084971308708191
        entropy_coeff: 0.0017600000137463212
        kl: 0.01371394470334053
        model: {}
        policy_loss: -0.0348573662340641
        total_loss: -0.03272692859172821
        vf_explained_var: 0.08963018655776978
        vf_loss: 12.97199821472168
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 0.7362022399902344
        entropy_coeff: 0.0017600000137463212
        kl: 0.009775789454579353
        model: {}
        policy_loss: -0.025869444012641907
        total_loss: -0.024033749476075172
        vf_explained_var: 0.17473112046718597
        vf_loss: 11.762519836425781
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 1.1090481281280518
        entropy_coeff: 0.0017600000137463212
        kl: 0.014612854458391666
        model: {}
        policy_loss: -0.03765663132071495
        total_loss: -0.03532325476408005
        vf_explained_var: 0.04323837161064148
        vf_loss: 13.627263069152832
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005531327915377915
        entropy: 1.0030533075332642
        entropy_coeff: 0.0017600000137463212
        kl: 0.01314674224704504
        model: {}
        policy_loss: -0.03623192384839058
        total_loss: -0.03411830961704254
        vf_explained_var: 0.12314735352993011
        vf_loss: 12.49642276763916
    load_time_ms: 13675.446
    num_steps_sampled: 11424000
    num_steps_trained: 11424000
    sample_time_ms: 105728.627
    update_time_ms: 36.997
  iterations_since_restore: 59
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.875
    ram_util_percent: 18.913541666666667
  pid: 30948
  policy_reward_max:
    agent-0: 136.66666666666728
    agent-1: 136.66666666666728
    agent-2: 136.66666666666728
    agent-3: 136.66666666666728
    agent-4: 136.66666666666728
    agent-5: 136.66666666666728
  policy_reward_mean:
    agent-0: 103.05833333333364
    agent-1: 103.05833333333364
    agent-2: 103.05833333333364
    agent-3: 103.05833333333364
    agent-4: 103.05833333333364
    agent-5: 103.05833333333364
  policy_reward_min:
    agent-0: 55.666666666666536
    agent-1: 55.666666666666536
    agent-2: 55.666666666666536
    agent-3: 55.666666666666536
    agent-4: 55.666666666666536
    agent-5: 55.666666666666536
  sampler_perf:
    mean_env_wait_ms: 27.9589583947807
    mean_inference_ms: 13.141086142465776
    mean_processing_ms: 59.710537937935186
  time_since_restore: 8383.045627593994
  time_this_iter_s: 135.09252834320068
  time_total_s: 17509.057441473007
  timestamp: 1637035455
  timesteps_since_restore: 5664000
  timesteps_this_iter: 96000
  timesteps_total: 11424000
  training_iteration: 119
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 35.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    119 |          17509.1 | 11424000 |   618.35 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 2.67
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 30.9
    apples_agent-1_min: 0
    apples_agent-2_max: 355
    apples_agent-2_mean: 14.77
    apples_agent-2_min: 0
    apples_agent-3_max: 276
    apples_agent-3_mean: 87.3
    apples_agent-3_min: 16
    apples_agent-4_max: 57
    apples_agent-4_mean: 3.33
    apples_agent-4_min: 0
    apples_agent-5_max: 188
    apples_agent-5_mean: 73.18
    apples_agent-5_min: 27
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 334.08
    cleaning_beam_agent-0_min: 66
    cleaning_beam_agent-1_max: 388
    cleaning_beam_agent-1_mean: 199.32
    cleaning_beam_agent-1_min: 71
    cleaning_beam_agent-2_max: 533
    cleaning_beam_agent-2_mean: 348.26
    cleaning_beam_agent-2_min: 104
    cleaning_beam_agent-3_max: 185
    cleaning_beam_agent-3_mean: 52.03
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 448
    cleaning_beam_agent-4_mean: 323.6
    cleaning_beam_agent-4_min: 166
    cleaning_beam_agent-5_max: 171
    cleaning_beam_agent-5_mean: 62.65
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-06-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 826.999999999979
  episode_reward_mean: 606.4099999999967
  episode_reward_min: 132.00000000000114
  episodes_this_iter: 96
  episodes_total: 11520
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12480.078
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 1.0940030813217163
        entropy_coeff: 0.0017600000137463212
        kl: 0.011287281289696693
        model: {}
        policy_loss: -0.02935977838933468
        total_loss: -0.027507949620485306
        vf_explained_var: 0.09349894523620605
        vf_loss: 15.198174476623535
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 1.1667532920837402
        entropy_coeff: 0.0017600000137463212
        kl: 0.014591516926884651
        model: {}
        policy_loss: -0.03703796863555908
        total_loss: -0.03449852019548416
        vf_explained_var: 0.0033367127180099487
        vf_loss: 16.74634552001953
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 1.1035714149475098
        entropy_coeff: 0.0017600000137463212
        kl: 0.013600630685687065
        model: {}
        policy_loss: -0.03290149196982384
        total_loss: -0.030557716265320778
        vf_explained_var: 0.06561899185180664
        vf_loss: 15.65938663482666
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 0.7570207118988037
        entropy_coeff: 0.0017600000137463212
        kl: 0.010150428861379623
        model: {}
        policy_loss: -0.026846516877412796
        total_loss: -0.02483697235584259
        vf_explained_var: 0.21694327890872955
        vf_loss: 13.118173599243164
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 1.109444260597229
        entropy_coeff: 0.0017600000137463212
        kl: 0.014382950961589813
        model: {}
        policy_loss: -0.03811987489461899
        total_loss: -0.035696759819984436
        vf_explained_var: 0.10562308132648468
        vf_loss: 14.991519927978516
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005471424083225429
        entropy: 1.0015621185302734
        entropy_coeff: 0.0017600000137463212
        kl: 0.012820392847061157
        model: {}
        policy_loss: -0.03435210511088371
        total_loss: -0.03210959956049919
        vf_explained_var: 0.1403745412826538
        vf_loss: 14.411774635314941
    load_time_ms: 13662.28
    num_steps_sampled: 11520000
    num_steps_trained: 11520000
    sample_time_ms: 105782.725
    update_time_ms: 31.658
  iterations_since_restore: 60
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.95631578947369
    ram_util_percent: 18.85578947368421
  pid: 30948
  policy_reward_max:
    agent-0: 137.83333333333385
    agent-1: 137.83333333333385
    agent-2: 137.83333333333385
    agent-3: 137.83333333333385
    agent-4: 137.83333333333385
    agent-5: 137.83333333333385
  policy_reward_mean:
    agent-0: 101.06833333333365
    agent-1: 101.06833333333365
    agent-2: 101.06833333333365
    agent-3: 101.06833333333365
    agent-4: 101.06833333333365
    agent-5: 101.06833333333365
  policy_reward_min:
    agent-0: 22.000000000000018
    agent-1: 22.000000000000018
    agent-2: 22.000000000000018
    agent-3: 22.000000000000018
    agent-4: 22.000000000000018
    agent-5: 22.000000000000018
  sampler_perf:
    mean_env_wait_ms: 27.94267035119065
    mean_inference_ms: 13.138120583959463
    mean_processing_ms: 59.680619540167896
  time_since_restore: 8516.002075433731
  time_this_iter_s: 132.95644783973694
  time_total_s: 17642.013889312744
  timestamp: 1637035588
  timesteps_since_restore: 5760000
  timesteps_this_iter: 96000
  timesteps_total: 11520000
  training_iteration: 120
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    120 |            17642 | 11520000 |   606.41 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 3.41
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 30.38
    apples_agent-1_min: 0
    apples_agent-2_max: 194
    apples_agent-2_mean: 14.01
    apples_agent-2_min: 0
    apples_agent-3_max: 146
    apples_agent-3_mean: 79.19
    apples_agent-3_min: 16
    apples_agent-4_max: 128
    apples_agent-4_mean: 4.0
    apples_agent-4_min: 0
    apples_agent-5_max: 119
    apples_agent-5_mean: 69.13
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 497
    cleaning_beam_agent-0_mean: 346.0
    cleaning_beam_agent-0_min: 66
    cleaning_beam_agent-1_max: 357
    cleaning_beam_agent-1_mean: 202.73
    cleaning_beam_agent-1_min: 48
    cleaning_beam_agent-2_max: 501
    cleaning_beam_agent-2_mean: 336.09
    cleaning_beam_agent-2_min: 67
    cleaning_beam_agent-3_max: 185
    cleaning_beam_agent-3_mean: 49.8
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 462
    cleaning_beam_agent-4_mean: 327.8
    cleaning_beam_agent-4_min: 84
    cleaning_beam_agent-5_max: 199
    cleaning_beam_agent-5_mean: 75.48
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-08-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 879.9999999999849
  episode_reward_mean: 607.9599999999988
  episode_reward_min: 132.00000000000114
  episodes_this_iter: 96
  episodes_total: 11616
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12501.973
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 1.0921578407287598
        entropy_coeff: 0.0017600000137463212
        kl: 0.012056970037519932
        model: {}
        policy_loss: -0.029464032500982285
        total_loss: -0.027620438486337662
        vf_explained_var: 0.11617052555084229
        vf_loss: 13.543981552124023
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 1.166849136352539
        entropy_coeff: 0.0017600000137463212
        kl: 0.014865420758724213
        model: {}
        policy_loss: -0.03669784963130951
        total_loss: -0.03420764580368996
        vf_explained_var: -0.02303898334503174
        vf_loss: 15.707747459411621
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 1.0911333560943604
        entropy_coeff: 0.0017600000137463212
        kl: 0.013867473229765892
        model: {}
        policy_loss: -0.03364685922861099
        total_loss: -0.031363800168037415
        vf_explained_var: 0.06770607829093933
        vf_loss: 14.299604415893555
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 0.7459443807601929
        entropy_coeff: 0.0017600000137463212
        kl: 0.009822625666856766
        model: {}
        policy_loss: -0.026023292914032936
        total_loss: -0.024104801937937737
        vf_explained_var: 0.17439714074134827
        vf_loss: 12.668294906616211
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 1.1140422821044922
        entropy_coeff: 0.0017600000137463212
        kl: 0.015268871560692787
        model: {}
        policy_loss: -0.03631613031029701
        total_loss: -0.03380616381764412
        vf_explained_var: 0.07612976431846619
        vf_loss: 14.169069290161133
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005411520251072943
        entropy: 0.993008017539978
        entropy_coeff: 0.0017600000137463212
        kl: 0.013756129890680313
        model: {}
        policy_loss: -0.03436140716075897
        total_loss: -0.03203976899385452
        vf_explained_var: 0.14008928835391998
        vf_loss: 13.181017875671387
    load_time_ms: 13662.729
    num_steps_sampled: 11616000
    num_steps_trained: 11616000
    sample_time_ms: 106327.488
    update_time_ms: 31.779
  iterations_since_restore: 61
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.787113402061856
    ram_util_percent: 18.898969072164945
  pid: 30948
  policy_reward_max:
    agent-0: 146.66666666666663
    agent-1: 146.66666666666663
    agent-2: 146.66666666666663
    agent-3: 146.66666666666663
    agent-4: 146.66666666666663
    agent-5: 146.66666666666663
  policy_reward_mean:
    agent-0: 101.32666666666694
    agent-1: 101.32666666666694
    agent-2: 101.32666666666694
    agent-3: 101.32666666666694
    agent-4: 101.32666666666694
    agent-5: 101.32666666666694
  policy_reward_min:
    agent-0: 22.000000000000018
    agent-1: 22.000000000000018
    agent-2: 22.000000000000018
    agent-3: 22.000000000000018
    agent-4: 22.000000000000018
    agent-5: 22.000000000000018
  sampler_perf:
    mean_env_wait_ms: 27.93288103414291
    mean_inference_ms: 13.136073584649695
    mean_processing_ms: 59.66040420927665
  time_since_restore: 8652.10783958435
  time_this_iter_s: 136.1057641506195
  time_total_s: 17778.119653463364
  timestamp: 1637035725
  timesteps_since_restore: 5856000
  timesteps_this_iter: 96000
  timesteps_total: 11616000
  training_iteration: 121
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    121 |          17778.1 | 11616000 |   607.96 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 2.38
    apples_agent-0_min: 0
    apples_agent-1_max: 187
    apples_agent-1_mean: 31.0
    apples_agent-1_min: 0
    apples_agent-2_max: 152
    apples_agent-2_mean: 14.34
    apples_agent-2_min: 0
    apples_agent-3_max: 151
    apples_agent-3_mean: 84.7
    apples_agent-3_min: 22
    apples_agent-4_max: 52
    apples_agent-4_mean: 2.24
    apples_agent-4_min: 0
    apples_agent-5_max: 128
    apples_agent-5_mean: 75.29
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 490
    cleaning_beam_agent-0_mean: 352.88
    cleaning_beam_agent-0_min: 138
    cleaning_beam_agent-1_max: 362
    cleaning_beam_agent-1_mean: 203.44
    cleaning_beam_agent-1_min: 48
    cleaning_beam_agent-2_max: 488
    cleaning_beam_agent-2_mean: 321.9
    cleaning_beam_agent-2_min: 29
    cleaning_beam_agent-3_max: 96
    cleaning_beam_agent-3_mean: 44.18
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 495
    cleaning_beam_agent-4_mean: 332.68
    cleaning_beam_agent-4_min: 197
    cleaning_beam_agent-5_max: 169
    cleaning_beam_agent-5_mean: 65.98
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-10-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 897.9999999999816
  episode_reward_mean: 648.5099999999961
  episode_reward_min: 264.99999999999716
  episodes_this_iter: 96
  episodes_total: 11712
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12546.044
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 1.0758382081985474
        entropy_coeff: 0.0017600000137463212
        kl: 0.010499347001314163
        model: {}
        policy_loss: -0.028697911649942398
        total_loss: -0.027060406282544136
        vf_explained_var: 0.06907422840595245
        vf_loss: 14.311083793640137
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 1.1641476154327393
        entropy_coeff: 0.0017600000137463212
        kl: 0.013662387616932392
        model: {}
        policy_loss: -0.036574386060237885
        total_loss: -0.03435855358839035
        vf_explained_var: 0.002995118498802185
        vf_loss: 15.322565078735352
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 1.084652066230774
        entropy_coeff: 0.0017600000137463212
        kl: 0.01387348398566246
        model: {}
        policy_loss: -0.03318701684474945
        total_loss: -0.03089589625597
        vf_explained_var: 0.0722770243883133
        vf_loss: 14.254151344299316
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 0.7186216115951538
        entropy_coeff: 0.0017600000137463212
        kl: 0.010709078051149845
        model: {}
        policy_loss: -0.024824392050504684
        total_loss: -0.022662198171019554
        vf_explained_var: 0.16321267187595367
        vf_loss: 12.851524353027344
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 1.1069166660308838
        entropy_coeff: 0.0017600000137463212
        kl: 0.014042728580534458
        model: {}
        policy_loss: -0.036998309195041656
        total_loss: -0.03466584160923958
        vf_explained_var: 0.04147905111312866
        vf_loss: 14.720934867858887
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005351615836843848
        entropy: 0.9690580368041992
        entropy_coeff: 0.0017600000137463212
        kl: 0.01282763946801424
        model: {}
        policy_loss: -0.03397984057664871
        total_loss: -0.0318121537566185
        vf_explained_var: 0.14852146804332733
        vf_loss: 13.076947212219238
    load_time_ms: 13669.933
    num_steps_sampled: 11712000
    num_steps_trained: 11712000
    sample_time_ms: 106417.882
    update_time_ms: 31.822
  iterations_since_restore: 62
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.93979057591623
    ram_util_percent: 18.91413612565445
  pid: 30948
  policy_reward_max:
    agent-0: 149.66666666666703
    agent-1: 149.66666666666703
    agent-2: 149.66666666666703
    agent-3: 149.66666666666703
    agent-4: 149.66666666666703
    agent-5: 149.66666666666703
  policy_reward_mean:
    agent-0: 108.08500000000035
    agent-1: 108.08500000000035
    agent-2: 108.08500000000035
    agent-3: 108.08500000000035
    agent-4: 108.08500000000035
    agent-5: 108.08500000000035
  policy_reward_min:
    agent-0: 44.16666666666667
    agent-1: 44.16666666666667
    agent-2: 44.16666666666667
    agent-3: 44.16666666666667
    agent-4: 44.16666666666667
    agent-5: 44.16666666666667
  sampler_perf:
    mean_env_wait_ms: 27.91896139900343
    mean_inference_ms: 13.132573669844705
    mean_processing_ms: 59.63548636088246
  time_since_restore: 8785.2395029068
  time_this_iter_s: 133.13166332244873
  time_total_s: 17911.251316785812
  timestamp: 1637035859
  timesteps_since_restore: 5952000
  timesteps_this_iter: 96000
  timesteps_total: 11712000
  training_iteration: 122
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    122 |          17911.3 | 11712000 |   648.51 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 3.02
    apples_agent-0_min: 0
    apples_agent-1_max: 67
    apples_agent-1_mean: 26.92
    apples_agent-1_min: 0
    apples_agent-2_max: 162
    apples_agent-2_mean: 11.71
    apples_agent-2_min: 0
    apples_agent-3_max: 185
    apples_agent-3_mean: 82.93
    apples_agent-3_min: 31
    apples_agent-4_max: 63
    apples_agent-4_mean: 2.24
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 71.32
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 504
    cleaning_beam_agent-0_mean: 344.32
    cleaning_beam_agent-0_min: 154
    cleaning_beam_agent-1_max: 321
    cleaning_beam_agent-1_mean: 199.13
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 584
    cleaning_beam_agent-2_mean: 350.98
    cleaning_beam_agent-2_min: 140
    cleaning_beam_agent-3_max: 145
    cleaning_beam_agent-3_mean: 44.98
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 465
    cleaning_beam_agent-4_mean: 334.73
    cleaning_beam_agent-4_min: 133
    cleaning_beam_agent-5_max: 319
    cleaning_beam_agent-5_mean: 70.64
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-13-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 867.9999999999814
  episode_reward_mean: 627.4199999999965
  episode_reward_min: 242.99999999999594
  episodes_this_iter: 96
  episodes_total: 11808
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12538.542
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 1.093992829322815
        entropy_coeff: 0.0017600000137463212
        kl: 0.011506669223308563
        model: {}
        policy_loss: -0.03082822635769844
        total_loss: -0.029110359027981758
        vf_explained_var: 0.13020265102386475
        vf_loss: 13.419624328613281
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 1.1757845878601074
        entropy_coeff: 0.0017600000137463212
        kl: 0.014971932396292686
        model: {}
        policy_loss: -0.03865998610854149
        total_loss: -0.03615573048591614
        vf_explained_var: -0.0219881534576416
        vf_loss: 15.792496681213379
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 1.0935416221618652
        entropy_coeff: 0.0017600000137463212
        kl: 0.012927331030368805
        model: {}
        policy_loss: -0.032656844705343246
        total_loss: -0.030621599406003952
        vf_explained_var: 0.10956935584545135
        vf_loss: 13.744128227233887
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 0.7144448161125183
        entropy_coeff: 0.0017600000137463212
        kl: 0.009127451106905937
        model: {}
        policy_loss: -0.025063736364245415
        total_loss: -0.02318492904305458
        vf_explained_var: 0.1493639349937439
        vf_loss: 13.107389450073242
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 1.109468936920166
        entropy_coeff: 0.0017600000137463212
        kl: 0.013989320956170559
        model: {}
        policy_loss: -0.036651622503995895
        total_loss: -0.03434304893016815
        vf_explained_var: 0.051112398505210876
        vf_loss: 14.633746147155762
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005291712004691362
        entropy: 0.9794682264328003
        entropy_coeff: 0.0017600000137463212
        kl: 0.01303170621395111
        model: {}
        policy_loss: -0.03472694754600525
        total_loss: -0.03250151872634888
        vf_explained_var: 0.12917384505271912
        vf_loss: 13.429532051086426
    load_time_ms: 13645.679
    num_steps_sampled: 11808000
    num_steps_trained: 11808000
    sample_time_ms: 106744.41
    update_time_ms: 32.004
  iterations_since_restore: 63
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.908376963350786
    ram_util_percent: 18.912565445026175
  pid: 30948
  policy_reward_max:
    agent-0: 144.666666666667
    agent-1: 144.666666666667
    agent-2: 144.666666666667
    agent-3: 144.666666666667
    agent-4: 144.666666666667
    agent-5: 144.666666666667
  policy_reward_mean:
    agent-0: 104.57000000000033
    agent-1: 104.57000000000033
    agent-2: 104.57000000000033
    agent-3: 104.57000000000033
    agent-4: 104.57000000000033
    agent-5: 104.57000000000033
  policy_reward_min:
    agent-0: 40.49999999999998
    agent-1: 40.49999999999998
    agent-2: 40.49999999999998
    agent-3: 40.49999999999998
    agent-4: 40.49999999999998
    agent-5: 40.49999999999998
  sampler_perf:
    mean_env_wait_ms: 27.905542051981808
    mean_inference_ms: 13.129768892322645
    mean_processing_ms: 59.61480414504697
  time_since_restore: 8919.292344808578
  time_this_iter_s: 134.05284190177917
  time_total_s: 18045.30415868759
  timestamp: 1637035993
  timesteps_since_restore: 6048000
  timesteps_this_iter: 96000
  timesteps_total: 11808000
  training_iteration: 123
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    123 |          18045.3 | 11808000 |   627.42 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 1.96
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 28.94
    apples_agent-1_min: 0
    apples_agent-2_max: 101
    apples_agent-2_mean: 12.93
    apples_agent-2_min: 0
    apples_agent-3_max: 148
    apples_agent-3_mean: 83.98
    apples_agent-3_min: 27
    apples_agent-4_max: 63
    apples_agent-4_mean: 2.34
    apples_agent-4_min: 0
    apples_agent-5_max: 148
    apples_agent-5_mean: 71.09
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 519
    cleaning_beam_agent-0_mean: 365.26
    cleaning_beam_agent-0_min: 169
    cleaning_beam_agent-1_max: 356
    cleaning_beam_agent-1_mean: 210.3
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 628
    cleaning_beam_agent-2_mean: 343.57
    cleaning_beam_agent-2_min: 121
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 42.76
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 455
    cleaning_beam_agent-4_mean: 331.84
    cleaning_beam_agent-4_min: 213
    cleaning_beam_agent-5_max: 259
    cleaning_beam_agent-5_mean: 62.43
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 2
    fire_beam_agent-5_mean: 0.05
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-15-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 861.999999999976
  episode_reward_mean: 642.2799999999975
  episode_reward_min: 323.00000000000017
  episodes_this_iter: 96
  episodes_total: 11904
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12516.995
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 1.0689189434051514
        entropy_coeff: 0.0017600000137463212
        kl: 0.011209153570234776
        model: {}
        policy_loss: -0.02763037011027336
        total_loss: -0.02592867985367775
        vf_explained_var: 0.04980155825614929
        vf_loss: 13.411538124084473
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 1.1787714958190918
        entropy_coeff: 0.0017600000137463212
        kl: 0.014883547089993954
        model: {}
        policy_loss: -0.03799568489193916
        total_loss: -0.03571145236492157
        vf_explained_var: 0.021309316158294678
        vf_loss: 13.821633338928223
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 1.099064588546753
        entropy_coeff: 0.0017600000137463212
        kl: 0.013537522405385971
        model: {}
        policy_loss: -0.032611846923828125
        total_loss: -0.03051828220486641
        vf_explained_var: 0.06330011785030365
        vf_loss: 13.204116821289062
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 0.7031950950622559
        entropy_coeff: 0.0017600000137463212
        kl: 0.009152458980679512
        model: {}
        policy_loss: -0.025056252256035805
        total_loss: -0.023234514519572258
        vf_explained_var: 0.12923142313957214
        vf_loss: 12.288719177246094
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 1.1078561544418335
        entropy_coeff: 0.0017600000137463212
        kl: 0.014604676514863968
        model: {}
        policy_loss: -0.036301303654909134
        total_loss: -0.0340009406208992
        vf_explained_var: 0.057550206780433655
        vf_loss: 13.292573928833008
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005231808172538877
        entropy: 0.967320442199707
        entropy_coeff: 0.0017600000137463212
        kl: 0.013220316730439663
        model: {}
        policy_loss: -0.03536919504404068
        total_loss: -0.033181872218847275
        vf_explained_var: 0.11687247455120087
        vf_loss: 12.457417488098145
    load_time_ms: 13681.416
    num_steps_sampled: 11904000
    num_steps_trained: 11904000
    sample_time_ms: 106709.313
    update_time_ms: 15.066
  iterations_since_restore: 64
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.883246073298427
    ram_util_percent: 18.91465968586387
  pid: 30948
  policy_reward_max:
    agent-0: 143.66666666666652
    agent-1: 143.66666666666652
    agent-2: 143.66666666666652
    agent-3: 143.66666666666652
    agent-4: 143.66666666666652
    agent-5: 143.66666666666652
  policy_reward_mean:
    agent-0: 107.04666666666701
    agent-1: 107.04666666666701
    agent-2: 107.04666666666701
    agent-3: 107.04666666666701
    agent-4: 107.04666666666701
    agent-5: 107.04666666666701
  policy_reward_min:
    agent-0: 53.833333333333215
    agent-1: 53.833333333333215
    agent-2: 53.833333333333215
    agent-3: 53.833333333333215
    agent-4: 53.833333333333215
    agent-5: 53.833333333333215
  sampler_perf:
    mean_env_wait_ms: 27.891378568299707
    mean_inference_ms: 13.127244164048147
    mean_processing_ms: 59.58833486016874
  time_since_restore: 9052.464613676071
  time_this_iter_s: 133.17226886749268
  time_total_s: 18178.476427555084
  timestamp: 1637036127
  timesteps_since_restore: 6144000
  timesteps_this_iter: 96000
  timesteps_total: 11904000
  training_iteration: 124
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    124 |          18178.5 | 11904000 |   642.28 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 2.55
    apples_agent-0_min: 0
    apples_agent-1_max: 67
    apples_agent-1_mean: 26.25
    apples_agent-1_min: 0
    apples_agent-2_max: 60
    apples_agent-2_mean: 8.99
    apples_agent-2_min: 0
    apples_agent-3_max: 160
    apples_agent-3_mean: 81.9
    apples_agent-3_min: 13
    apples_agent-4_max: 48
    apples_agent-4_mean: 2.39
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 69.17
    apples_agent-5_min: 28
    cleaning_beam_agent-0_max: 530
    cleaning_beam_agent-0_mean: 355.02
    cleaning_beam_agent-0_min: 177
    cleaning_beam_agent-1_max: 342
    cleaning_beam_agent-1_mean: 218.01
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 585
    cleaning_beam_agent-2_mean: 315.77
    cleaning_beam_agent-2_min: 69
    cleaning_beam_agent-3_max: 106
    cleaning_beam_agent-3_mean: 46.43
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 423
    cleaning_beam_agent-4_mean: 321.94
    cleaning_beam_agent-4_min: 163
    cleaning_beam_agent-5_max: 214
    cleaning_beam_agent-5_mean: 63.65
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-17-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 873.9999999999701
  episode_reward_mean: 632.0199999999968
  episode_reward_min: 293.99999999999926
  episodes_this_iter: 96
  episodes_total: 12000
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12529.712
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 1.0966651439666748
        entropy_coeff: 0.0017600000137463212
        kl: 0.011004693806171417
        model: {}
        policy_loss: -0.028703920543193817
        total_loss: -0.026976699009537697
        vf_explained_var: 0.060133323073387146
        vf_loss: 14.564170837402344
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 1.1511898040771484
        entropy_coeff: 0.0017600000137463212
        kl: 0.014505013823509216
        model: {}
        policy_loss: -0.03825271129608154
        total_loss: -0.03585520759224892
        vf_explained_var: 0.01699519157409668
        vf_loss: 15.225934982299805
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 1.1026631593704224
        entropy_coeff: 0.0017600000137463212
        kl: 0.013037371449172497
        model: {}
        policy_loss: -0.032604604959487915
        total_loss: -0.0305275060236454
        vf_explained_var: 0.0889483094215393
        vf_loss: 14.103103637695312
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 0.7052248120307922
        entropy_coeff: 0.0017600000137463212
        kl: 0.009875791147351265
        model: {}
        policy_loss: -0.025525419041514397
        total_loss: -0.02354675717651844
        vf_explained_var: 0.19587565958499908
        vf_loss: 12.447017669677734
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 1.0911643505096436
        entropy_coeff: 0.0017600000137463212
        kl: 0.013221348635852337
        model: {}
        policy_loss: -0.035433080047369
        total_loss: -0.033292341977357864
        vf_explained_var: 0.08538088202476501
        vf_loss: 14.169159889221191
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005171903758309782
        entropy: 0.9692936539649963
        entropy_coeff: 0.0017600000137463212
        kl: 0.013224653899669647
        model: {}
        policy_loss: -0.033218927681446075
        total_loss: -0.03093153052031994
        vf_explained_var: 0.12987148761749268
        vf_loss: 13.484167098999023
    load_time_ms: 13659.077
    num_steps_sampled: 12000000
    num_steps_trained: 12000000
    sample_time_ms: 106999.065
    update_time_ms: 15.19
  iterations_since_restore: 65
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.994210526315786
    ram_util_percent: 18.89736842105263
  pid: 30948
  policy_reward_max:
    agent-0: 145.66666666666714
    agent-1: 145.66666666666714
    agent-2: 145.66666666666714
    agent-3: 145.66666666666714
    agent-4: 145.66666666666714
    agent-5: 145.66666666666714
  policy_reward_mean:
    agent-0: 105.33666666666697
    agent-1: 105.33666666666697
    agent-2: 105.33666666666697
    agent-3: 105.33666666666697
    agent-4: 105.33666666666697
    agent-5: 105.33666666666697
  policy_reward_min:
    agent-0: 48.99999999999995
    agent-1: 48.99999999999995
    agent-2: 48.99999999999995
    agent-3: 48.99999999999995
    agent-4: 48.99999999999995
    agent-5: 48.99999999999995
  sampler_perf:
    mean_env_wait_ms: 27.878600134746485
    mean_inference_ms: 13.124654065578339
    mean_processing_ms: 59.565918225878924
  time_since_restore: 9186.268165588379
  time_this_iter_s: 133.80355191230774
  time_total_s: 18312.279979467392
  timestamp: 1637036261
  timesteps_since_restore: 6240000
  timesteps_this_iter: 96000
  timesteps_total: 12000000
  training_iteration: 125
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    125 |          18312.3 | 12000000 |   632.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 3.81
    apples_agent-0_min: 0
    apples_agent-1_max: 78
    apples_agent-1_mean: 28.03
    apples_agent-1_min: 0
    apples_agent-2_max: 162
    apples_agent-2_mean: 13.58
    apples_agent-2_min: 0
    apples_agent-3_max: 129
    apples_agent-3_mean: 79.7
    apples_agent-3_min: 15
    apples_agent-4_max: 54
    apples_agent-4_mean: 2.93
    apples_agent-4_min: 0
    apples_agent-5_max: 128
    apples_agent-5_mean: 66.17
    apples_agent-5_min: 22
    cleaning_beam_agent-0_max: 625
    cleaning_beam_agent-0_mean: 330.61
    cleaning_beam_agent-0_min: 121
    cleaning_beam_agent-1_max: 364
    cleaning_beam_agent-1_mean: 211.66
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 500
    cleaning_beam_agent-2_mean: 332.38
    cleaning_beam_agent-2_min: 115
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 42.61
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 428
    cleaning_beam_agent-4_mean: 316.1
    cleaning_beam_agent-4_min: 176
    cleaning_beam_agent-5_max: 228
    cleaning_beam_agent-5_mean: 57.22
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-19-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 853.9999999999983
  episode_reward_mean: 615.5099999999969
  episode_reward_min: 239.99999999999588
  episodes_this_iter: 96
  episodes_total: 12096
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12538.493
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 1.0826575756072998
        entropy_coeff: 0.0017600000137463212
        kl: 0.010784456506371498
        model: {}
        policy_loss: -0.029413465410470963
        total_loss: -0.027776766568422318
        vf_explained_var: 0.11501464247703552
        vf_loss: 13.852845191955566
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 1.1759788990020752
        entropy_coeff: 0.0017600000137463212
        kl: 0.014282435178756714
        model: {}
        policy_loss: -0.03746906295418739
        total_loss: -0.03514229506254196
        vf_explained_var: 0.017222821712493896
        vf_loss: 15.400009155273438
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 1.0941309928894043
        entropy_coeff: 0.0017600000137463212
        kl: 0.013941286131739616
        model: {}
        policy_loss: -0.03278042748570442
        total_loss: -0.03050270304083824
        vf_explained_var: 0.0961185097694397
        vf_loss: 14.151333808898926
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 0.7218345403671265
        entropy_coeff: 0.0017600000137463212
        kl: 0.00977685209363699
        model: {}
        policy_loss: -0.024387918412685394
        total_loss: -0.022432781755924225
        vf_explained_var: 0.18913301825523376
        vf_loss: 12.701948165893555
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 1.1202903985977173
        entropy_coeff: 0.0017600000137463212
        kl: 0.014421291649341583
        model: {}
        policy_loss: -0.03665105625987053
        total_loss: -0.03431330621242523
        vf_explained_var: 0.09016935527324677
        vf_loss: 14.252058029174805
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0005111999926157296
        entropy: 0.9799714088439941
        entropy_coeff: 0.0017600000137463212
        kl: 0.012709240429103374
        model: {}
        policy_loss: -0.03392435982823372
        total_loss: -0.03179125115275383
        vf_explained_var: 0.1598816066980362
        vf_loss: 13.160099983215332
    load_time_ms: 13646.638
    num_steps_sampled: 12096000
    num_steps_trained: 12096000
    sample_time_ms: 107081.735
    update_time_ms: 15.146
  iterations_since_restore: 66
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.046560846560844
    ram_util_percent: 18.909523809523808
  pid: 30948
  policy_reward_max:
    agent-0: 142.33333333333343
    agent-1: 142.33333333333343
    agent-2: 142.33333333333343
    agent-3: 142.33333333333343
    agent-4: 142.33333333333343
    agent-5: 142.33333333333343
  policy_reward_mean:
    agent-0: 102.58500000000029
    agent-1: 102.58500000000029
    agent-2: 102.58500000000029
    agent-3: 102.58500000000029
    agent-4: 102.58500000000029
    agent-5: 102.58500000000029
  policy_reward_min:
    agent-0: 39.99999999999999
    agent-1: 39.99999999999999
    agent-2: 39.99999999999999
    agent-3: 39.99999999999999
    agent-4: 39.99999999999999
    agent-5: 39.99999999999999
  sampler_perf:
    mean_env_wait_ms: 27.866231575879773
    mean_inference_ms: 13.122417110482541
    mean_processing_ms: 59.54188725747097
  time_since_restore: 9319.028438091278
  time_this_iter_s: 132.76027250289917
  time_total_s: 18445.04025197029
  timestamp: 1637036394
  timesteps_since_restore: 6336000
  timesteps_this_iter: 96000
  timesteps_total: 12096000
  training_iteration: 126
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    126 |            18445 | 12096000 |   615.51 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 2.28
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 27.58
    apples_agent-1_min: 0
    apples_agent-2_max: 178
    apples_agent-2_mean: 16.1
    apples_agent-2_min: 0
    apples_agent-3_max: 169
    apples_agent-3_mean: 80.73
    apples_agent-3_min: 14
    apples_agent-4_max: 59
    apples_agent-4_mean: 2.09
    apples_agent-4_min: 0
    apples_agent-5_max: 140
    apples_agent-5_mean: 69.9
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 543
    cleaning_beam_agent-0_mean: 343.08
    cleaning_beam_agent-0_min: 59
    cleaning_beam_agent-1_max: 344
    cleaning_beam_agent-1_mean: 198.98
    cleaning_beam_agent-1_min: 85
    cleaning_beam_agent-2_max: 476
    cleaning_beam_agent-2_mean: 328.59
    cleaning_beam_agent-2_min: 77
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 47.53
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 429
    cleaning_beam_agent-4_mean: 314.22
    cleaning_beam_agent-4_min: 131
    cleaning_beam_agent-5_max: 200
    cleaning_beam_agent-5_mean: 51.79
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-22-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 878.999999999985
  episode_reward_mean: 632.499999999997
  episode_reward_min: 100.0000000000009
  episodes_this_iter: 96
  episodes_total: 12192
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12529.363
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 1.087599277496338
        entropy_coeff: 0.0017600000137463212
        kl: 0.010884727351367474
        model: {}
        policy_loss: -0.028628477826714516
        total_loss: -0.02698267623782158
        vf_explained_var: 0.10051260888576508
        vf_loss: 13.830291748046875
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 1.1642990112304688
        entropy_coeff: 0.0017600000137463212
        kl: 0.014720766805112362
        model: {}
        policy_loss: -0.038045112043619156
        total_loss: -0.03559425100684166
        vf_explained_var: -0.011158570647239685
        vf_loss: 15.558772087097168
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 1.1068967580795288
        entropy_coeff: 0.0017600000137463212
        kl: 0.013177646324038506
        model: {}
        policy_loss: -0.03295247629284859
        total_loss: -0.030818741768598557
        vf_explained_var: 0.059433579444885254
        vf_loss: 14.463471412658691
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 0.7093582153320312
        entropy_coeff: 0.0017600000137463212
        kl: 0.00992013793438673
        model: {}
        policy_loss: -0.025210026651620865
        total_loss: -0.02321207895874977
        vf_explained_var: 0.17820380628108978
        vf_loss: 12.623882293701172
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 1.104064702987671
        entropy_coeff: 0.0017600000137463212
        kl: 0.013730124570429325
        model: {}
        policy_loss: -0.03653816133737564
        total_loss: -0.034319356083869934
        vf_explained_var: 0.078783318400383
        vf_loss: 14.159309387207031
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000505209609400481
        entropy: 0.9584609866142273
        entropy_coeff: 0.0017600000137463212
        kl: 0.012349818833172321
        model: {}
        policy_loss: -0.03436863049864769
        total_loss: -0.032249342650175095
        vf_explained_var: 0.1306142359972
        vf_loss: 13.362178802490234
    load_time_ms: 13639.603
    num_steps_sampled: 12192000
    num_steps_trained: 12192000
    sample_time_ms: 107636.234
    update_time_ms: 15.372
  iterations_since_restore: 67
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.84947916666667
    ram_util_percent: 18.907291666666666
  pid: 30948
  policy_reward_max:
    agent-0: 146.50000000000006
    agent-1: 146.50000000000006
    agent-2: 146.50000000000006
    agent-3: 146.50000000000006
    agent-4: 146.50000000000006
    agent-5: 146.50000000000006
  policy_reward_mean:
    agent-0: 105.41666666666697
    agent-1: 105.41666666666697
    agent-2: 105.41666666666697
    agent-3: 105.41666666666697
    agent-4: 105.41666666666697
    agent-5: 105.41666666666697
  policy_reward_min:
    agent-0: 16.666666666666654
    agent-1: 16.666666666666654
    agent-2: 16.666666666666654
    agent-3: 16.666666666666654
    agent-4: 16.666666666666654
    agent-5: 16.666666666666654
  sampler_perf:
    mean_env_wait_ms: 27.85348209923149
    mean_inference_ms: 13.120489018600955
    mean_processing_ms: 59.52433786380247
  time_since_restore: 9453.665694952011
  time_this_iter_s: 134.63725686073303
  time_total_s: 18579.677508831024
  timestamp: 1637036529
  timesteps_since_restore: 6432000
  timesteps_this_iter: 96000
  timesteps_total: 12192000
  training_iteration: 127
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    127 |          18579.7 | 12192000 |    632.5 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 2.69
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 26.94
    apples_agent-1_min: 0
    apples_agent-2_max: 142
    apples_agent-2_mean: 15.03
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 78.19
    apples_agent-3_min: 30
    apples_agent-4_max: 58
    apples_agent-4_mean: 2.86
    apples_agent-4_min: 0
    apples_agent-5_max: 138
    apples_agent-5_mean: 65.28
    apples_agent-5_min: 12
    cleaning_beam_agent-0_max: 561
    cleaning_beam_agent-0_mean: 359.6
    cleaning_beam_agent-0_min: 179
    cleaning_beam_agent-1_max: 357
    cleaning_beam_agent-1_mean: 194.94
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 616
    cleaning_beam_agent-2_mean: 326.99
    cleaning_beam_agent-2_min: 78
    cleaning_beam_agent-3_max: 125
    cleaning_beam_agent-3_mean: 53.92
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 487
    cleaning_beam_agent-4_mean: 314.84
    cleaning_beam_agent-4_min: 145
    cleaning_beam_agent-5_max: 199
    cleaning_beam_agent-5_mean: 56.04
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-24-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 877.999999999969
  episode_reward_mean: 611.3699999999977
  episode_reward_min: 267.9999999999978
  episodes_this_iter: 96
  episodes_total: 12288
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12496.123
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 1.0756046772003174
        entropy_coeff: 0.0017600000137463212
        kl: 0.010748536325991154
        model: {}
        policy_loss: -0.02862953022122383
        total_loss: -0.026890186592936516
        vf_explained_var: 0.12045449018478394
        vf_loss: 14.827006340026855
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 1.1474413871765137
        entropy_coeff: 0.0017600000137463212
        kl: 0.014316191896796227
        model: {}
        policy_loss: -0.036928143352270126
        total_loss: -0.03438308835029602
        vf_explained_var: -0.007358327507972717
        vf_loss: 17.01313591003418
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 1.083966612815857
        entropy_coeff: 0.0017600000137463212
        kl: 0.013097954913973808
        model: {}
        policy_loss: -0.03185074031352997
        total_loss: -0.029632078483700752
        vf_explained_var: 0.10694879293441772
        vf_loss: 15.068477630615234
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 0.7414032220840454
        entropy_coeff: 0.0017600000137463212
        kl: 0.009635841473937035
        model: {}
        policy_loss: -0.02569742687046528
        total_loss: -0.023735443130135536
        vf_explained_var: 0.20487575232982635
        vf_loss: 13.396873474121094
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 1.112208366394043
        entropy_coeff: 0.0017600000137463212
        kl: 0.013682141900062561
        model: {}
        policy_loss: -0.03786759078502655
        total_loss: -0.03553863987326622
        vf_explained_var: 0.08161500096321106
        vf_loss: 15.500127792358398
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004992192261852324
        entropy: 0.9598506093025208
        entropy_coeff: 0.0017600000137463212
        kl: 0.012634143233299255
        model: {}
        policy_loss: -0.03506409749388695
        total_loss: -0.03281840309500694
        vf_explained_var: 0.1646718829870224
        vf_loss: 14.082008361816406
    load_time_ms: 13659.454
    num_steps_sampled: 12288000
    num_steps_trained: 12288000
    sample_time_ms: 107660.534
    update_time_ms: 15.015
  iterations_since_restore: 68
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.57268041237113
    ram_util_percent: 18.92061855670103
  pid: 30948
  policy_reward_max:
    agent-0: 146.3333333333338
    agent-1: 146.3333333333338
    agent-2: 146.3333333333338
    agent-3: 146.3333333333338
    agent-4: 146.3333333333338
    agent-5: 146.3333333333338
  policy_reward_mean:
    agent-0: 101.89500000000024
    agent-1: 101.89500000000024
    agent-2: 101.89500000000024
    agent-3: 101.89500000000024
    agent-4: 101.89500000000024
    agent-5: 101.89500000000024
  policy_reward_min:
    agent-0: 44.6666666666666
    agent-1: 44.6666666666666
    agent-2: 44.6666666666666
    agent-3: 44.6666666666666
    agent-4: 44.6666666666666
    agent-5: 44.6666666666666
  sampler_perf:
    mean_env_wait_ms: 27.83976887402036
    mean_inference_ms: 13.116896792056632
    mean_processing_ms: 59.50258057867889
  time_since_restore: 9587.176424980164
  time_this_iter_s: 133.51073002815247
  time_total_s: 18713.188238859177
  timestamp: 1637036666
  timesteps_since_restore: 6528000
  timesteps_this_iter: 96000
  timesteps_total: 12288000
  training_iteration: 128
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    128 |          18713.2 | 12288000 |   611.37 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 3.52
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 27.75
    apples_agent-1_min: 0
    apples_agent-2_max: 206
    apples_agent-2_mean: 17.46
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 74.66
    apples_agent-3_min: 34
    apples_agent-4_max: 55
    apples_agent-4_mean: 3.62
    apples_agent-4_min: 0
    apples_agent-5_max: 119
    apples_agent-5_mean: 63.66
    apples_agent-5_min: 5
    cleaning_beam_agent-0_max: 557
    cleaning_beam_agent-0_mean: 347.67
    cleaning_beam_agent-0_min: 112
    cleaning_beam_agent-1_max: 376
    cleaning_beam_agent-1_mean: 203.35
    cleaning_beam_agent-1_min: 85
    cleaning_beam_agent-2_max: 522
    cleaning_beam_agent-2_mean: 317.35
    cleaning_beam_agent-2_min: 101
    cleaning_beam_agent-3_max: 154
    cleaning_beam_agent-3_mean: 59.38
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 412
    cleaning_beam_agent-4_mean: 308.74
    cleaning_beam_agent-4_min: 181
    cleaning_beam_agent-5_max: 296
    cleaning_beam_agent-5_mean: 64.21
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-26-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 818.9999999999807
  episode_reward_mean: 608.159999999999
  episode_reward_min: 236.99999999999628
  episodes_this_iter: 96
  episodes_total: 12384
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12494.968
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 1.0780421495437622
        entropy_coeff: 0.0017600000137463212
        kl: 0.010850680992007256
        model: {}
        policy_loss: -0.02920561283826828
        total_loss: -0.027605347335338593
        vf_explained_var: 0.09541265666484833
        vf_loss: 13.27483081817627
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 1.1497886180877686
        entropy_coeff: 0.0017600000137463212
        kl: 0.014439327642321587
        model: {}
        policy_loss: -0.03683459758758545
        total_loss: -0.034500278532505035
        vf_explained_var: -0.0012566298246383667
        vf_loss: 14.700763702392578
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 1.1206536293029785
        entropy_coeff: 0.0017600000137463212
        kl: 0.014064579270780087
        model: {}
        policy_loss: -0.03239937871694565
        total_loss: -0.03019101172685623
        vf_explained_var: 0.06807734072208405
        vf_loss: 13.677972793579102
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 0.7490592002868652
        entropy_coeff: 0.0017600000137463212
        kl: 0.009626063518226147
        model: {}
        policy_loss: -0.026028046384453773
        total_loss: -0.024184005334973335
        vf_explained_var: 0.1575842797756195
        vf_loss: 12.37172794342041
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 1.1043890714645386
        entropy_coeff: 0.0017600000137463212
        kl: 0.014556385576725006
        model: {}
        policy_loss: -0.0367872379720211
        total_loss: -0.034484829753637314
        vf_explained_var: 0.09038214385509491
        vf_loss: 13.34855842590332
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004932287847623229
        entropy: 0.9696378111839294
        entropy_coeff: 0.0017600000137463212
        kl: 0.012698786333203316
        model: {}
        policy_loss: -0.034256529062986374
        total_loss: -0.03215214982628822
        vf_explained_var: 0.13414086401462555
        vf_loss: 12.71183967590332
    load_time_ms: 13639.899
    num_steps_sampled: 12384000
    num_steps_trained: 12384000
    sample_time_ms: 107332.121
    update_time_ms: 15.071
  iterations_since_restore: 69
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.12606382978724
    ram_util_percent: 18.91595744680851
  pid: 30948
  policy_reward_max:
    agent-0: 136.5000000000001
    agent-1: 136.5000000000001
    agent-2: 136.5000000000001
    agent-3: 136.5000000000001
    agent-4: 136.5000000000001
    agent-5: 136.5000000000001
  policy_reward_mean:
    agent-0: 101.36000000000027
    agent-1: 101.36000000000027
    agent-2: 101.36000000000027
    agent-3: 101.36000000000027
    agent-4: 101.36000000000027
    agent-5: 101.36000000000027
  policy_reward_min:
    agent-0: 39.49999999999996
    agent-1: 39.49999999999996
    agent-2: 39.49999999999996
    agent-3: 39.49999999999996
    agent-4: 39.49999999999996
    agent-5: 39.49999999999996
  sampler_perf:
    mean_env_wait_ms: 27.82620797074982
    mean_inference_ms: 13.113888728769481
    mean_processing_ms: 59.479250303161365
  time_since_restore: 9718.829025268555
  time_this_iter_s: 131.6526002883911
  time_total_s: 18844.840839147568
  timestamp: 1637036798
  timesteps_since_restore: 6624000
  timesteps_this_iter: 96000
  timesteps_total: 12384000
  training_iteration: 129
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    129 |          18844.8 | 12384000 |   608.16 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 1.79
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 27.34
    apples_agent-1_min: 0
    apples_agent-2_max: 137
    apples_agent-2_mean: 15.63
    apples_agent-2_min: 0
    apples_agent-3_max: 150
    apples_agent-3_mean: 79.81
    apples_agent-3_min: 22
    apples_agent-4_max: 59
    apples_agent-4_mean: 4.15
    apples_agent-4_min: 0
    apples_agent-5_max: 128
    apples_agent-5_mean: 64.59
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 527
    cleaning_beam_agent-0_mean: 356.98
    cleaning_beam_agent-0_min: 181
    cleaning_beam_agent-1_max: 380
    cleaning_beam_agent-1_mean: 217.54
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 500
    cleaning_beam_agent-2_mean: 314.49
    cleaning_beam_agent-2_min: 65
    cleaning_beam_agent-3_max: 245
    cleaning_beam_agent-3_mean: 58.17
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 452
    cleaning_beam_agent-4_mean: 310.96
    cleaning_beam_agent-4_min: 194
    cleaning_beam_agent-5_max: 153
    cleaning_beam_agent-5_mean: 57.64
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-28-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 865.9999999999822
  episode_reward_mean: 633.179999999996
  episode_reward_min: 192.99999999999835
  episodes_this_iter: 96
  episodes_total: 12480
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12518.135
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 1.0665180683135986
        entropy_coeff: 0.0017600000137463212
        kl: 0.010773545131087303
        model: {}
        policy_loss: -0.027495570480823517
        total_loss: -0.0256799403578043
        vf_explained_var: 0.07078030705451965
        vf_loss: 15.379949569702148
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 1.1654882431030273
        entropy_coeff: 0.0017600000137463212
        kl: 0.016970397904515266
        model: {}
        policy_loss: -0.03345835208892822
        total_loss: -0.030481211841106415
        vf_explained_var: 0.013629943132400513
        vf_loss: 16.343172073364258
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 1.1026763916015625
        entropy_coeff: 0.0017600000137463212
        kl: 0.013714558444917202
        model: {}
        policy_loss: -0.03293011710047722
        total_loss: -0.0306134931743145
        vf_explained_var: 0.08514882624149323
        vf_loss: 15.144177436828613
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 0.7133457660675049
        entropy_coeff: 0.0017600000137463212
        kl: 0.009189430624246597
        model: {}
        policy_loss: -0.025062520056962967
        total_loss: -0.02309485711157322
        vf_explained_var: 0.16325390338897705
        vf_loss: 13.852667808532715
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 1.1094121932983398
        entropy_coeff: 0.0017600000137463212
        kl: 0.013340350240468979
        model: {}
        policy_loss: -0.03654710575938225
        total_loss: -0.0343550443649292
        vf_explained_var: 0.10814918577671051
        vf_loss: 14.765558242797852
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004872384015470743
        entropy: 0.9462101459503174
        entropy_coeff: 0.0017600000137463212
        kl: 0.01256367564201355
        model: {}
        policy_loss: -0.03314553573727608
        total_loss: -0.03088870272040367
        vf_explained_var: 0.14844481647014618
        vf_loss: 14.09428596496582
    load_time_ms: 13640.419
    num_steps_sampled: 12480000
    num_steps_trained: 12480000
    sample_time_ms: 107601.724
    update_time_ms: 15.194
  iterations_since_restore: 70
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.833160621761653
    ram_util_percent: 18.914507772020723
  pid: 30948
  policy_reward_max:
    agent-0: 144.3333333333333
    agent-1: 144.3333333333333
    agent-2: 144.3333333333333
    agent-3: 144.3333333333333
    agent-4: 144.3333333333333
    agent-5: 144.3333333333333
  policy_reward_mean:
    agent-0: 105.53000000000033
    agent-1: 105.53000000000033
    agent-2: 105.53000000000033
    agent-3: 105.53000000000033
    agent-4: 105.53000000000033
    agent-5: 105.53000000000033
  policy_reward_min:
    agent-0: 32.16666666666672
    agent-1: 32.16666666666672
    agent-2: 32.16666666666672
    agent-3: 32.16666666666672
    agent-4: 32.16666666666672
    agent-5: 32.16666666666672
  sampler_perf:
    mean_env_wait_ms: 27.816391858627416
    mean_inference_ms: 13.112323347689093
    mean_processing_ms: 59.46308877810299
  time_since_restore: 9854.67975640297
  time_this_iter_s: 135.85073113441467
  time_total_s: 18980.691570281982
  timestamp: 1637036934
  timesteps_since_restore: 6720000
  timesteps_this_iter: 96000
  timesteps_total: 12480000
  training_iteration: 130
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    130 |          18980.7 | 12480000 |   633.18 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 2.87
    apples_agent-0_min: 0
    apples_agent-1_max: 144
    apples_agent-1_mean: 28.58
    apples_agent-1_min: 0
    apples_agent-2_max: 113
    apples_agent-2_mean: 12.79
    apples_agent-2_min: 0
    apples_agent-3_max: 186
    apples_agent-3_mean: 77.42
    apples_agent-3_min: 31
    apples_agent-4_max: 55
    apples_agent-4_mean: 3.3
    apples_agent-4_min: 0
    apples_agent-5_max: 110
    apples_agent-5_mean: 66.21
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 478
    cleaning_beam_agent-0_mean: 340.22
    cleaning_beam_agent-0_min: 125
    cleaning_beam_agent-1_max: 331
    cleaning_beam_agent-1_mean: 203.48
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 478
    cleaning_beam_agent-2_mean: 310.11
    cleaning_beam_agent-2_min: 57
    cleaning_beam_agent-3_max: 149
    cleaning_beam_agent-3_mean: 54.92
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 450
    cleaning_beam_agent-4_mean: 309.05
    cleaning_beam_agent-4_min: 149
    cleaning_beam_agent-5_max: 159
    cleaning_beam_agent-5_mean: 58.17
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.04
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-31-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 871.9999999999751
  episode_reward_mean: 628.2199999999984
  episode_reward_min: 294.0000000000004
  episodes_this_iter: 96
  episodes_total: 12576
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12501.745
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 1.077722191810608
        entropy_coeff: 0.0017600000137463212
        kl: 0.010397924110293388
        model: {}
        policy_loss: -0.028396569192409515
        total_loss: -0.026773838326334953
        vf_explained_var: 0.09913608431816101
        vf_loss: 14.399360656738281
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 1.1629122495651245
        entropy_coeff: 0.0017600000137463212
        kl: 0.014131632633507252
        model: {}
        policy_loss: -0.03812478855252266
        total_loss: -0.035719916224479675
        vf_explained_var: -0.014629006385803223
        vf_loss: 16.25273895263672
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 1.1128897666931152
        entropy_coeff: 0.0017600000137463212
        kl: 0.013221729546785355
        model: {}
        policy_loss: -0.032639890909194946
        total_loss: -0.030473949387669563
        vf_explained_var: 0.0739259272813797
        vf_loss: 14.80282974243164
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 0.734424352645874
        entropy_coeff: 0.0017600000137463212
        kl: 0.009324421174824238
        model: {}
        policy_loss: -0.025395290926098824
        total_loss: -0.02352469228208065
        vf_explained_var: 0.1874883621931076
        vf_loss: 12.982988357543945
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 1.1027746200561523
        entropy_coeff: 0.0017600000137463212
        kl: 0.01411021500825882
        model: {}
        policy_loss: -0.03680155798792839
        total_loss: -0.034430161118507385
        vf_explained_var: 0.06720131635665894
        vf_loss: 14.902382850646973
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004812479892279953
        entropy: 0.9630429148674011
        entropy_coeff: 0.0017600000137463212
        kl: 0.012138089165091515
        model: {}
        policy_loss: -0.0332535021007061
        total_loss: -0.031135762110352516
        vf_explained_var: 0.13315561413764954
        vf_loss: 13.850773811340332
    load_time_ms: 13629.222
    num_steps_sampled: 12576000
    num_steps_trained: 12576000
    sample_time_ms: 107214.141
    update_time_ms: 15.102
  iterations_since_restore: 71
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.03386243386243
    ram_util_percent: 18.934391534391537
  pid: 30948
  policy_reward_max:
    agent-0: 145.33333333333357
    agent-1: 145.33333333333357
    agent-2: 145.33333333333357
    agent-3: 145.33333333333357
    agent-4: 145.33333333333357
    agent-5: 145.33333333333357
  policy_reward_mean:
    agent-0: 104.70333333333359
    agent-1: 104.70333333333359
    agent-2: 104.70333333333359
    agent-3: 104.70333333333359
    agent-4: 104.70333333333359
    agent-5: 104.70333333333359
  policy_reward_min:
    agent-0: 48.99999999999984
    agent-1: 48.99999999999984
    agent-2: 48.99999999999984
    agent-3: 48.99999999999984
    agent-4: 48.99999999999984
    agent-5: 48.99999999999984
  sampler_perf:
    mean_env_wait_ms: 27.801946794965048
    mean_inference_ms: 13.109570328587356
    mean_processing_ms: 59.442397000782265
  time_since_restore: 9986.672868967056
  time_this_iter_s: 131.9931125640869
  time_total_s: 19112.68468284607
  timestamp: 1637037066
  timesteps_since_restore: 6816000
  timesteps_this_iter: 96000
  timesteps_total: 12576000
  training_iteration: 131
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    131 |          19112.7 | 12576000 |   628.22 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 2.56
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 25.26
    apples_agent-1_min: 0
    apples_agent-2_max: 130
    apples_agent-2_mean: 15.61
    apples_agent-2_min: 0
    apples_agent-3_max: 162
    apples_agent-3_mean: 80.31
    apples_agent-3_min: 29
    apples_agent-4_max: 69
    apples_agent-4_mean: 3.9
    apples_agent-4_min: 0
    apples_agent-5_max: 131
    apples_agent-5_mean: 68.27
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 559
    cleaning_beam_agent-0_mean: 344.39
    cleaning_beam_agent-0_min: 125
    cleaning_beam_agent-1_max: 368
    cleaning_beam_agent-1_mean: 196.74
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 475
    cleaning_beam_agent-2_mean: 311.39
    cleaning_beam_agent-2_min: 78
    cleaning_beam_agent-3_max: 133
    cleaning_beam_agent-3_mean: 55.08
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 391
    cleaning_beam_agent-4_mean: 295.28
    cleaning_beam_agent-4_min: 141
    cleaning_beam_agent-5_max: 162
    cleaning_beam_agent-5_mean: 60.97
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-33-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 829.9999999999775
  episode_reward_mean: 615.2799999999971
  episode_reward_min: 240.99999999999665
  episodes_this_iter: 96
  episodes_total: 12672
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12482.424
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 1.0854103565216064
        entropy_coeff: 0.0017600000137463212
        kl: 0.010450628586113453
        model: {}
        policy_loss: -0.028413213789463043
        total_loss: -0.02679123729467392
        vf_explained_var: 0.054290756583213806
        vf_loss: 14.421728134155273
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 1.1695425510406494
        entropy_coeff: 0.0017600000137463212
        kl: 0.014201903715729713
        model: {}
        policy_loss: -0.03774726390838623
        total_loss: -0.03548091650009155
        vf_explained_var: 0.026965513825416565
        vf_loss: 14.843639373779297
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 1.1139092445373535
        entropy_coeff: 0.0017600000137463212
        kl: 0.013041437603533268
        model: {}
        policy_loss: -0.032341644167900085
        total_loss: -0.03031117096543312
        vf_explained_var: 0.09257124364376068
        vf_loss: 13.82663631439209
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 0.7260094881057739
        entropy_coeff: 0.0017600000137463212
        kl: 0.009778449311852455
        model: {}
        policy_loss: -0.02488735131919384
        total_loss: -0.0229174867272377
        vf_explained_var: 0.1525140404701233
        vf_loss: 12.919523239135742
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 1.1102733612060547
        entropy_coeff: 0.0017600000137463212
        kl: 0.013761229813098907
        model: {}
        policy_loss: -0.03664768487215042
        total_loss: -0.03440438210964203
        vf_explained_var: 0.05128258466720581
        vf_loss: 14.451387405395508
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004752576060127467
        entropy: 0.9788326621055603
        entropy_coeff: 0.0017600000137463212
        kl: 0.012574885971844196
        model: {}
        policy_loss: -0.033205341547727585
        total_loss: -0.031083978712558746
        vf_explained_var: 0.12721377611160278
        vf_loss: 13.291316986083984
    load_time_ms: 13625.932
    num_steps_sampled: 12672000
    num_steps_trained: 12672000
    sample_time_ms: 107388.137
    update_time_ms: 30.194
  iterations_since_restore: 72
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.78958333333333
    ram_util_percent: 18.92864583333333
  pid: 30948
  policy_reward_max:
    agent-0: 138.3333333333337
    agent-1: 138.3333333333337
    agent-2: 138.3333333333337
    agent-3: 138.3333333333337
    agent-4: 138.3333333333337
    agent-5: 138.3333333333337
  policy_reward_mean:
    agent-0: 102.54666666666695
    agent-1: 102.54666666666695
    agent-2: 102.54666666666695
    agent-3: 102.54666666666695
    agent-4: 102.54666666666695
    agent-5: 102.54666666666695
  policy_reward_min:
    agent-0: 40.16666666666661
    agent-1: 40.16666666666661
    agent-2: 40.16666666666661
    agent-3: 40.16666666666661
    agent-4: 40.16666666666661
    agent-5: 40.16666666666661
  sampler_perf:
    mean_env_wait_ms: 27.78732210338515
    mean_inference_ms: 13.107409461373514
    mean_processing_ms: 59.42243693969568
  time_since_restore: 10121.761171340942
  time_this_iter_s: 135.0883023738861
  time_total_s: 19247.772985219955
  timestamp: 1637037202
  timesteps_since_restore: 6912000
  timesteps_this_iter: 96000
  timesteps_total: 12672000
  training_iteration: 132
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 35.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    132 |          19247.8 | 12672000 |   615.28 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 2.47
    apples_agent-0_min: 0
    apples_agent-1_max: 77
    apples_agent-1_mean: 29.75
    apples_agent-1_min: 0
    apples_agent-2_max: 109
    apples_agent-2_mean: 11.69
    apples_agent-2_min: 0
    apples_agent-3_max: 162
    apples_agent-3_mean: 81.7
    apples_agent-3_min: 22
    apples_agent-4_max: 82
    apples_agent-4_mean: 3.74
    apples_agent-4_min: 0
    apples_agent-5_max: 131
    apples_agent-5_mean: 67.6
    apples_agent-5_min: 19
    cleaning_beam_agent-0_max: 550
    cleaning_beam_agent-0_mean: 350.46
    cleaning_beam_agent-0_min: 156
    cleaning_beam_agent-1_max: 289
    cleaning_beam_agent-1_mean: 192.77
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 490
    cleaning_beam_agent-2_mean: 314.3
    cleaning_beam_agent-2_min: 82
    cleaning_beam_agent-3_max: 153
    cleaning_beam_agent-3_mean: 53.87
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 399
    cleaning_beam_agent-4_mean: 302.4
    cleaning_beam_agent-4_min: 157
    cleaning_beam_agent-5_max: 225
    cleaning_beam_agent-5_mean: 62.31
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-35-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 872.9999999999798
  episode_reward_mean: 624.1899999999976
  episode_reward_min: 258.99999999999574
  episodes_this_iter: 96
  episodes_total: 12768
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12517.173
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 1.076684594154358
        entropy_coeff: 0.0017600000137463212
        kl: 0.010808508843183517
        model: {}
        policy_loss: -0.02813800424337387
        total_loss: -0.026400834321975708
        vf_explained_var: 0.11154833436012268
        vf_loss: 14.70435905456543
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 1.1515756845474243
        entropy_coeff: 0.0017600000137463212
        kl: 0.013896450400352478
        model: {}
        policy_loss: -0.03721589595079422
        total_loss: -0.034806542098522186
        vf_explained_var: 0.0018623173236846924
        vf_loss: 16.568408966064453
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 1.1111278533935547
        entropy_coeff: 0.0017600000137463212
        kl: 0.013573594391345978
        model: {}
        policy_loss: -0.03340242803096771
        total_loss: -0.031085286289453506
        vf_explained_var: 0.058280959725379944
        vf_loss: 15.580057144165039
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 0.7274995446205139
        entropy_coeff: 0.0017600000137463212
        kl: 0.010085868649184704
        model: {}
        policy_loss: -0.02668577991425991
        total_loss: -0.024623103439807892
        vf_explained_var: 0.19974388182163239
        vf_loss: 13.259012222290039
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 1.1051371097564697
        entropy_coeff: 0.0017600000137463212
        kl: 0.01398514024913311
        model: {}
        policy_loss: -0.037054553627967834
        total_loss: -0.03469564765691757
        vf_explained_var: 0.0893738716840744
        vf_loss: 15.06916618347168
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046926719369366765
        entropy: 0.9735344648361206
        entropy_coeff: 0.0017600000137463212
        kl: 0.012786846607923508
        model: {}
        policy_loss: -0.03494616597890854
        total_loss: -0.032736971974372864
        vf_explained_var: 0.17428185045719147
        vf_loss: 13.65241813659668
    load_time_ms: 13652.894
    num_steps_sampled: 12768000
    num_steps_trained: 12768000
    sample_time_ms: 107309.892
    update_time_ms: 30.155
  iterations_since_restore: 73
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.84293193717277
    ram_util_percent: 18.844502617801044
  pid: 30948
  policy_reward_max:
    agent-0: 145.4999999999998
    agent-1: 145.4999999999998
    agent-2: 145.4999999999998
    agent-3: 145.4999999999998
    agent-4: 145.4999999999998
    agent-5: 145.4999999999998
  policy_reward_mean:
    agent-0: 104.03166666666692
    agent-1: 104.03166666666692
    agent-2: 104.03166666666692
    agent-3: 104.03166666666692
    agent-4: 104.03166666666692
    agent-5: 104.03166666666692
  policy_reward_min:
    agent-0: 43.16666666666654
    agent-1: 43.16666666666654
    agent-2: 43.16666666666654
    agent-3: 43.16666666666654
    agent-4: 43.16666666666654
    agent-5: 43.16666666666654
  sampler_perf:
    mean_env_wait_ms: 27.773595317099517
    mean_inference_ms: 13.104983188564809
    mean_processing_ms: 59.40480857721166
  time_since_restore: 10255.689027309418
  time_this_iter_s: 133.92785596847534
  time_total_s: 19381.70084118843
  timestamp: 1637037336
  timesteps_since_restore: 7008000
  timesteps_this_iter: 96000
  timesteps_total: 12768000
  training_iteration: 133
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 34.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    133 |          19381.7 | 12768000 |   624.19 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 2.91
    apples_agent-0_min: 0
    apples_agent-1_max: 75
    apples_agent-1_mean: 24.69
    apples_agent-1_min: 0
    apples_agent-2_max: 235
    apples_agent-2_mean: 12.8
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 81.69
    apples_agent-3_min: 30
    apples_agent-4_max: 73
    apples_agent-4_mean: 2.11
    apples_agent-4_min: 0
    apples_agent-5_max: 112
    apples_agent-5_mean: 68.85
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 549
    cleaning_beam_agent-0_mean: 341.99
    cleaning_beam_agent-0_min: 159
    cleaning_beam_agent-1_max: 395
    cleaning_beam_agent-1_mean: 200.73
    cleaning_beam_agent-1_min: 67
    cleaning_beam_agent-2_max: 530
    cleaning_beam_agent-2_mean: 320.85
    cleaning_beam_agent-2_min: 55
    cleaning_beam_agent-3_max: 143
    cleaning_beam_agent-3_mean: 61.19
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 420
    cleaning_beam_agent-4_mean: 294.1
    cleaning_beam_agent-4_min: 189
    cleaning_beam_agent-5_max: 142
    cleaning_beam_agent-5_mean: 51.63
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-37-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 854.9999999999815
  episode_reward_mean: 620.909999999998
  episode_reward_min: 289.9999999999992
  episodes_this_iter: 96
  episodes_total: 12864
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12547.626
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 1.0942699909210205
        entropy_coeff: 0.0017600000137463212
        kl: 0.011374084278941154
        model: {}
        policy_loss: -0.028506942093372345
        total_loss: -0.02672828547656536
        vf_explained_var: 0.10103295743465424
        vf_loss: 14.29755973815918
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 1.1605212688446045
        entropy_coeff: 0.0017600000137463212
        kl: 0.014724851585924625
        model: {}
        policy_loss: -0.037846699357032776
        total_loss: -0.03533996269106865
        vf_explained_var: -0.008101969957351685
        vf_loss: 16.042877197265625
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 1.0973072052001953
        entropy_coeff: 0.0017600000137463212
        kl: 0.013657568022608757
        model: {}
        policy_loss: -0.03432200103998184
        total_loss: -0.03207387030124664
        vf_explained_var: 0.08879972994327545
        vf_loss: 14.478804588317871
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 0.7453385591506958
        entropy_coeff: 0.0017600000137463212
        kl: 0.009442230686545372
        model: {}
        policy_loss: -0.024676131084561348
        total_loss: -0.02280445769429207
        vf_explained_var: 0.18554893136024475
        vf_loss: 12.950201034545898
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 1.1074661016464233
        entropy_coeff: 0.0017600000137463212
        kl: 0.016292447224259377
        model: {}
        policy_loss: -0.033451542258262634
        total_loss: -0.030623195692896843
        vf_explained_var: 0.044889748096466064
        vf_loss: 15.189988136291504
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00046327681047841907
        entropy: 0.9574611186981201
        entropy_coeff: 0.0017600000137463212
        kl: 0.012384193018078804
        model: {}
        policy_loss: -0.03311514854431152
        total_loss: -0.030984940007328987
        vf_explained_var: 0.1577749103307724
        vf_loss: 13.38502311706543
    load_time_ms: 13630.473
    num_steps_sampled: 12864000
    num_steps_trained: 12864000
    sample_time_ms: 107554.844
    update_time_ms: 30.648
  iterations_since_restore: 74
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 25.529896907216497
    ram_util_percent: 17.98505154639175
  pid: 30948
  policy_reward_max:
    agent-0: 142.50000000000037
    agent-1: 142.50000000000037
    agent-2: 142.50000000000037
    agent-3: 142.50000000000037
    agent-4: 142.50000000000037
    agent-5: 142.50000000000037
  policy_reward_mean:
    agent-0: 103.48500000000031
    agent-1: 103.48500000000031
    agent-2: 103.48500000000031
    agent-3: 103.48500000000031
    agent-4: 103.48500000000031
    agent-5: 103.48500000000031
  policy_reward_min:
    agent-0: 48.333333333333236
    agent-1: 48.333333333333236
    agent-2: 48.333333333333236
    agent-3: 48.333333333333236
    agent-4: 48.333333333333236
    agent-5: 48.333333333333236
  sampler_perf:
    mean_env_wait_ms: 27.763379444001263
    mean_inference_ms: 13.10333347103728
    mean_processing_ms: 59.393840108185756
  time_since_restore: 10391.3225274086
  time_this_iter_s: 135.63350009918213
  time_total_s: 19517.334341287613
  timestamp: 1637037472
  timesteps_since_restore: 7104000
  timesteps_this_iter: 96000
  timesteps_total: 12864000
  training_iteration: 134
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    134 |          19517.3 | 12864000 |   620.91 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 2.78
    apples_agent-0_min: 0
    apples_agent-1_max: 65
    apples_agent-1_mean: 23.8
    apples_agent-1_min: 0
    apples_agent-2_max: 94
    apples_agent-2_mean: 10.48
    apples_agent-2_min: 0
    apples_agent-3_max: 164
    apples_agent-3_mean: 86.47
    apples_agent-3_min: 31
    apples_agent-4_max: 116
    apples_agent-4_mean: 3.52
    apples_agent-4_min: 0
    apples_agent-5_max: 113
    apples_agent-5_mean: 67.98
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 550
    cleaning_beam_agent-0_mean: 338.51
    cleaning_beam_agent-0_min: 69
    cleaning_beam_agent-1_max: 306
    cleaning_beam_agent-1_mean: 201.42
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 546
    cleaning_beam_agent-2_mean: 289.75
    cleaning_beam_agent-2_min: 83
    cleaning_beam_agent-3_max: 188
    cleaning_beam_agent-3_mean: 63.11
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 386
    cleaning_beam_agent-4_mean: 288.95
    cleaning_beam_agent-4_min: 167
    cleaning_beam_agent-5_max: 167
    cleaning_beam_agent-5_mean: 54.57
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-40-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 834.9999999999765
  episode_reward_mean: 627.5699999999965
  episode_reward_min: 222.9999999999974
  episodes_this_iter: 96
  episodes_total: 12960
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12573.425
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 1.092329978942871
        entropy_coeff: 0.0017600000137463212
        kl: 0.010442275553941727
        model: {}
        policy_loss: -0.027980955317616463
        total_loss: -0.026421383023262024
        vf_explained_var: 0.11318224668502808
        vf_loss: 13.936201095581055
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 1.1621308326721191
        entropy_coeff: 0.0017600000137463212
        kl: 0.013718325644731522
        model: {}
        policy_loss: -0.0373200960457325
        total_loss: -0.035040274262428284
        vf_explained_var: -0.0056165605783462524
        vf_loss: 15.815074920654297
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 1.1327356100082397
        entropy_coeff: 0.0017600000137463212
        kl: 0.013019343838095665
        model: {}
        policy_loss: -0.03188344091176987
        total_loss: -0.02983786351978779
        vf_explained_var: 0.08524210751056671
        vf_loss: 14.353241920471191
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 0.7622724771499634
        entropy_coeff: 0.0017600000137463212
        kl: 0.01050660666078329
        model: {}
        policy_loss: -0.024351274594664574
        total_loss: -0.022318081930279732
        vf_explained_var: 0.1893564760684967
        vf_loss: 12.734708786010742
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 1.0982251167297363
        entropy_coeff: 0.0017600000137463212
        kl: 0.013164769858121872
        model: {}
        policy_loss: -0.036472566425800323
        total_loss: -0.03428540751338005
        vf_explained_var: 0.053587883710861206
        vf_loss: 14.870779037475586
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00045728639815934
        entropy: 0.9449024200439453
        entropy_coeff: 0.0017600000137463212
        kl: 0.012370672076940536
        model: {}
        policy_loss: -0.03356702998280525
        total_loss: -0.03142554685473442
        vf_explained_var: 0.15314802527427673
        vf_loss: 13.303712844848633
    load_time_ms: 13647.345
    num_steps_sampled: 12960000
    num_steps_trained: 12960000
    sample_time_ms: 107557.519
    update_time_ms: 30.653
  iterations_since_restore: 75
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.546596858638743
    ram_util_percent: 16.300523560209427
  pid: 30948
  policy_reward_max:
    agent-0: 139.16666666666697
    agent-1: 139.16666666666697
    agent-2: 139.16666666666697
    agent-3: 139.16666666666697
    agent-4: 139.16666666666697
    agent-5: 139.16666666666697
  policy_reward_mean:
    agent-0: 104.59500000000031
    agent-1: 104.59500000000031
    agent-2: 104.59500000000031
    agent-3: 104.59500000000031
    agent-4: 104.59500000000031
    agent-5: 104.59500000000031
  policy_reward_min:
    agent-0: 37.166666666666664
    agent-1: 37.166666666666664
    agent-2: 37.166666666666664
    agent-3: 37.166666666666664
    agent-4: 37.166666666666664
    agent-5: 37.166666666666664
  sampler_perf:
    mean_env_wait_ms: 27.75038338156113
    mean_inference_ms: 13.101103640784233
    mean_processing_ms: 59.37824347868687
  time_since_restore: 10525.654161214828
  time_this_iter_s: 134.33163380622864
  time_total_s: 19651.66597509384
  timestamp: 1637037606
  timesteps_since_restore: 7200000
  timesteps_this_iter: 96000
  timesteps_total: 12960000
  training_iteration: 135
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    135 |          19651.7 | 12960000 |   627.57 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 2.76
    apples_agent-0_min: 0
    apples_agent-1_max: 71
    apples_agent-1_mean: 25.49
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 10.46
    apples_agent-2_min: 0
    apples_agent-3_max: 151
    apples_agent-3_mean: 82.25
    apples_agent-3_min: 18
    apples_agent-4_max: 51
    apples_agent-4_mean: 1.72
    apples_agent-4_min: 0
    apples_agent-5_max: 130
    apples_agent-5_mean: 69.57
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 566
    cleaning_beam_agent-0_mean: 347.62
    cleaning_beam_agent-0_min: 152
    cleaning_beam_agent-1_max: 385
    cleaning_beam_agent-1_mean: 197.01
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 466
    cleaning_beam_agent-2_mean: 298.4
    cleaning_beam_agent-2_min: 89
    cleaning_beam_agent-3_max: 204
    cleaning_beam_agent-3_mean: 65.4
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 397
    cleaning_beam_agent-4_mean: 297.45
    cleaning_beam_agent-4_min: 152
    cleaning_beam_agent-5_max: 285
    cleaning_beam_agent-5_mean: 55.28
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-42-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 882.9999999999675
  episode_reward_mean: 613.8899999999977
  episode_reward_min: 250.99999999999682
  episodes_this_iter: 96
  episodes_total: 13056
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12568.874
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 1.0739942789077759
        entropy_coeff: 0.0017600000137463212
        kl: 0.010263431817293167
        model: {}
        policy_loss: -0.029011230915784836
        total_loss: -0.027424557134509087
        vf_explained_var: 0.08352239429950714
        vf_loss: 14.242195129394531
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 1.1599295139312744
        entropy_coeff: 0.0017600000137463212
        kl: 0.013690524734556675
        model: {}
        policy_loss: -0.03734596446156502
        total_loss: -0.035100292414426804
        vf_explained_var: 0.002535700798034668
        vf_loss: 15.490381240844727
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 1.1503561735153198
        entropy_coeff: 0.0017600000137463212
        kl: 0.01348354946821928
        model: {}
        policy_loss: -0.03287068381905556
        total_loss: -0.030788596719503403
        vf_explained_var: 0.0924006998538971
        vf_loss: 14.100010871887207
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 0.7519844174385071
        entropy_coeff: 0.0017600000137463212
        kl: 0.009985002689063549
        model: {}
        policy_loss: -0.02372882142663002
        total_loss: -0.021759862080216408
        vf_explained_var: 0.16618485748767853
        vf_loss: 12.95454216003418
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 1.1055710315704346
        entropy_coeff: 0.0017600000137463212
        kl: 0.01399738248437643
        model: {}
        policy_loss: -0.036805782467126846
        total_loss: -0.03447427228093147
        vf_explained_var: 0.04967397451400757
        vf_loss: 14.778379440307617
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000451295985840261
        entropy: 0.9521385431289673
        entropy_coeff: 0.0017600000137463212
        kl: 0.011332949623465538
        model: {}
        policy_loss: -0.03210344910621643
        total_loss: -0.03017440438270569
        vf_explained_var: 0.138840451836586
        vf_loss: 13.3821439743042
    load_time_ms: 13648.938
    num_steps_sampled: 13056000
    num_steps_trained: 13056000
    sample_time_ms: 107739.182
    update_time_ms: 30.855
  iterations_since_restore: 76
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.449740932642488
    ram_util_percent: 16.302072538860106
  pid: 30948
  policy_reward_max:
    agent-0: 147.1666666666668
    agent-1: 147.1666666666668
    agent-2: 147.1666666666668
    agent-3: 147.1666666666668
    agent-4: 147.1666666666668
    agent-5: 147.1666666666668
  policy_reward_mean:
    agent-0: 102.31500000000031
    agent-1: 102.31500000000031
    agent-2: 102.31500000000031
    agent-3: 102.31500000000031
    agent-4: 102.31500000000031
    agent-5: 102.31500000000031
  policy_reward_min:
    agent-0: 41.83333333333327
    agent-1: 41.83333333333327
    agent-2: 41.83333333333327
    agent-3: 41.83333333333327
    agent-4: 41.83333333333327
    agent-5: 41.83333333333327
  sampler_perf:
    mean_env_wait_ms: 27.737437124497404
    mean_inference_ms: 13.098676715703396
    mean_processing_ms: 59.36289934638698
  time_since_restore: 10660.15360212326
  time_this_iter_s: 134.499440908432
  time_total_s: 19786.165416002274
  timestamp: 1637037742
  timesteps_since_restore: 7296000
  timesteps_this_iter: 96000
  timesteps_total: 13056000
  training_iteration: 136
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    136 |          19786.2 | 13056000 |   613.89 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 2.22
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 27.39
    apples_agent-1_min: 0
    apples_agent-2_max: 169
    apples_agent-2_mean: 12.64
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 82.03
    apples_agent-3_min: 19
    apples_agent-4_max: 56
    apples_agent-4_mean: 3.02
    apples_agent-4_min: 0
    apples_agent-5_max: 113
    apples_agent-5_mean: 67.82
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 521
    cleaning_beam_agent-0_mean: 341.33
    cleaning_beam_agent-0_min: 118
    cleaning_beam_agent-1_max: 316
    cleaning_beam_agent-1_mean: 190.1
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 497
    cleaning_beam_agent-2_mean: 303.15
    cleaning_beam_agent-2_min: 97
    cleaning_beam_agent-3_max: 177
    cleaning_beam_agent-3_mean: 65.72
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 381
    cleaning_beam_agent-4_mean: 295.45
    cleaning_beam_agent-4_min: 178
    cleaning_beam_agent-5_max: 166
    cleaning_beam_agent-5_mean: 55.08
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-44-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 845.9999999999866
  episode_reward_mean: 623.6999999999982
  episode_reward_min: 246.99999999999682
  episodes_this_iter: 96
  episodes_total: 13152
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12585.341
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 1.0832130908966064
        entropy_coeff: 0.0017600000137463212
        kl: 0.010265648365020752
        model: {}
        policy_loss: -0.027297712862491608
        total_loss: -0.025791658088564873
        vf_explained_var: 0.1031419187784195
        vf_loss: 13.5938081741333
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 1.1618988513946533
        entropy_coeff: 0.0017600000137463212
        kl: 0.013852461241185665
        model: {}
        policy_loss: -0.036986734718084335
        total_loss: -0.03472135215997696
        vf_explained_var: -0.014914467930793762
        vf_loss: 15.39828872680664
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 1.1382795572280884
        entropy_coeff: 0.0017600000137463212
        kl: 0.013131828047335148
        model: {}
        policy_loss: -0.03314793109893799
        total_loss: -0.03111645020544529
        vf_explained_var: 0.07009027898311615
        vf_loss: 14.084918975830078
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 0.7423892021179199
        entropy_coeff: 0.0017600000137463212
        kl: 0.009815659373998642
        model: {}
        policy_loss: -0.02446134388446808
        total_loss: -0.022544555366039276
        vf_explained_var: 0.1691930741071701
        vf_loss: 12.602628707885742
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 1.0988585948944092
        entropy_coeff: 0.0017600000137463212
        kl: 0.01371065154671669
        model: {}
        policy_loss: -0.035912271589040756
        total_loss: -0.03363950923085213
        vf_explained_var: 0.0336897075176239
        vf_loss: 14.646236419677734
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004453056026250124
        entropy: 0.9525059461593628
        entropy_coeff: 0.0017600000137463212
        kl: 0.0120571693405509
        model: {}
        policy_loss: -0.03191576898097992
        total_loss: -0.029849950224161148
        vf_explained_var: 0.12256257236003876
        vf_loss: 13.307952880859375
    load_time_ms: 13646.887
    num_steps_sampled: 13152000
    num_steps_trained: 13152000
    sample_time_ms: 107657.747
    update_time_ms: 30.594
  iterations_since_restore: 77
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.52239583333333
    ram_util_percent: 16.320833333333336
  pid: 30948
  policy_reward_max:
    agent-0: 141.00000000000023
    agent-1: 141.00000000000023
    agent-2: 141.00000000000023
    agent-3: 141.00000000000023
    agent-4: 141.00000000000023
    agent-5: 141.00000000000023
  policy_reward_mean:
    agent-0: 103.95000000000032
    agent-1: 103.95000000000032
    agent-2: 103.95000000000032
    agent-3: 103.95000000000032
    agent-4: 103.95000000000032
    agent-5: 103.95000000000032
  policy_reward_min:
    agent-0: 41.16666666666662
    agent-1: 41.16666666666662
    agent-2: 41.16666666666662
    agent-3: 41.16666666666662
    agent-4: 41.16666666666662
    agent-5: 41.16666666666662
  sampler_perf:
    mean_env_wait_ms: 27.728496939879907
    mean_inference_ms: 13.096903866388026
    mean_processing_ms: 59.350417965738
  time_since_restore: 10794.233781814575
  time_this_iter_s: 134.0801796913147
  time_total_s: 19920.24559569359
  timestamp: 1637037877
  timesteps_since_restore: 7392000
  timesteps_this_iter: 96000
  timesteps_total: 13152000
  training_iteration: 137
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    137 |          19920.2 | 13152000 |    623.7 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 2.52
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 27.89
    apples_agent-1_min: 0
    apples_agent-2_max: 132
    apples_agent-2_mean: 17.29
    apples_agent-2_min: 0
    apples_agent-3_max: 172
    apples_agent-3_mean: 87.29
    apples_agent-3_min: 19
    apples_agent-4_max: 51
    apples_agent-4_mean: 2.65
    apples_agent-4_min: 0
    apples_agent-5_max: 121
    apples_agent-5_mean: 70.18
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 545
    cleaning_beam_agent-0_mean: 343.03
    cleaning_beam_agent-0_min: 139
    cleaning_beam_agent-1_max: 335
    cleaning_beam_agent-1_mean: 192.09
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 504
    cleaning_beam_agent-2_mean: 291.04
    cleaning_beam_agent-2_min: 60
    cleaning_beam_agent-3_max: 163
    cleaning_beam_agent-3_mean: 63.08
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 378
    cleaning_beam_agent-4_mean: 295.71
    cleaning_beam_agent-4_min: 183
    cleaning_beam_agent-5_max: 271
    cleaning_beam_agent-5_mean: 54.46
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-46-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 885.9999999999866
  episode_reward_mean: 632.1499999999969
  episode_reward_min: 226.9999999999968
  episodes_this_iter: 96
  episodes_total: 13248
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12626.196
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 1.0812779664993286
        entropy_coeff: 0.0017600000137463212
        kl: 0.010959439910948277
        model: {}
        policy_loss: -0.028710756450891495
        total_loss: -0.0270762350410223
        vf_explained_var: 0.11813060939311981
        vf_loss: 13.45678997039795
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 1.156879186630249
        entropy_coeff: 0.0017600000137463212
        kl: 0.014562956057488918
        model: {}
        policy_loss: -0.037194859236478806
        total_loss: -0.034792710095644
        vf_explained_var: 0.0005656927824020386
        vf_loss: 15.256610870361328
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 1.1263607740402222
        entropy_coeff: 0.0017600000137463212
        kl: 0.012363726273179054
        model: {}
        policy_loss: -0.03291042149066925
        total_loss: -0.030985098332166672
        vf_explained_var: 0.05971705913543701
        vf_loss: 14.349723815917969
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 0.7443132400512695
        entropy_coeff: 0.0017600000137463212
        kl: 0.009120089933276176
        model: {}
        policy_loss: -0.023788727819919586
        total_loss: -0.02199561521410942
        vf_explained_var: 0.16206076741218567
        vf_loss: 12.79084587097168
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 1.1015892028808594
        entropy_coeff: 0.0017600000137463212
        kl: 0.013212628662586212
        model: {}
        policy_loss: -0.036358729004859924
        total_loss: -0.034206390380859375
        vf_explained_var: 0.050255149602890015
        vf_loss: 14.486115455627441
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043931519030593336
        entropy: 0.9421693086624146
        entropy_coeff: 0.0017600000137463212
        kl: 0.012444505468010902
        model: {}
        policy_loss: -0.03329697996377945
        total_loss: -0.031142480671405792
        vf_explained_var: 0.13258236646652222
        vf_loss: 13.238146781921387
    load_time_ms: 13619.126
    num_steps_sampled: 13248000
    num_steps_trained: 13248000
    sample_time_ms: 107388.042
    update_time_ms: 30.645
  iterations_since_restore: 78
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.70372340425532
    ram_util_percent: 16.31276595744681
  pid: 30948
  policy_reward_max:
    agent-0: 147.66666666666657
    agent-1: 147.66666666666657
    agent-2: 147.66666666666657
    agent-3: 147.66666666666657
    agent-4: 147.66666666666657
    agent-5: 147.66666666666657
  policy_reward_mean:
    agent-0: 105.35833333333366
    agent-1: 105.35833333333366
    agent-2: 105.35833333333366
    agent-3: 105.35833333333366
    agent-4: 105.35833333333366
    agent-5: 105.35833333333366
  policy_reward_min:
    agent-0: 37.83333333333334
    agent-1: 37.83333333333334
    agent-2: 37.83333333333334
    agent-3: 37.83333333333334
    agent-4: 37.83333333333334
    agent-5: 37.83333333333334
  sampler_perf:
    mean_env_wait_ms: 27.71269585105441
    mean_inference_ms: 13.093482204074371
    mean_processing_ms: 59.326648320329284
  time_since_restore: 10925.18536233902
  time_this_iter_s: 130.95158052444458
  time_total_s: 20051.197176218033
  timestamp: 1637038009
  timesteps_since_restore: 7488000
  timesteps_this_iter: 96000
  timesteps_total: 13248000
  training_iteration: 138
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    138 |          20051.2 | 13248000 |   632.15 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 85
    apples_agent-0_mean: 3.6
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 27.33
    apples_agent-1_min: 0
    apples_agent-2_max: 134
    apples_agent-2_mean: 12.22
    apples_agent-2_min: 0
    apples_agent-3_max: 164
    apples_agent-3_mean: 86.64
    apples_agent-3_min: 31
    apples_agent-4_max: 45
    apples_agent-4_mean: 2.15
    apples_agent-4_min: 0
    apples_agent-5_max: 108
    apples_agent-5_mean: 70.32
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 558
    cleaning_beam_agent-0_mean: 342.31
    cleaning_beam_agent-0_min: 113
    cleaning_beam_agent-1_max: 405
    cleaning_beam_agent-1_mean: 200.79
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 502
    cleaning_beam_agent-2_mean: 295.44
    cleaning_beam_agent-2_min: 118
    cleaning_beam_agent-3_max: 151
    cleaning_beam_agent-3_mean: 57.96
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 415
    cleaning_beam_agent-4_mean: 299.22
    cleaning_beam_agent-4_min: 172
    cleaning_beam_agent-5_max: 279
    cleaning_beam_agent-5_mean: 62.88
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-49-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 914.9999999999836
  episode_reward_mean: 639.7999999999957
  episode_reward_min: 211.99999999999744
  episodes_this_iter: 96
  episodes_total: 13344
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12620.199
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 1.0666701793670654
        entropy_coeff: 0.0017600000137463212
        kl: 0.010491068474948406
        model: {}
        policy_loss: -0.028419282287359238
        total_loss: -0.02681393176317215
        vf_explained_var: 0.11456473171710968
        vf_loss: 13.844772338867188
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 1.171352744102478
        entropy_coeff: 0.0017600000137463212
        kl: 0.014395369216799736
        model: {}
        policy_loss: -0.03759028762578964
        total_loss: -0.03519228845834732
        vf_explained_var: -0.008563250303268433
        vf_loss: 15.805036544799805
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 1.147036075592041
        entropy_coeff: 0.0017600000137463212
        kl: 0.013883650302886963
        model: {}
        policy_loss: -0.03421364724636078
        total_loss: -0.03193870931863785
        vf_explained_var: 0.030463293194770813
        vf_loss: 15.169906616210938
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 0.7260545492172241
        entropy_coeff: 0.0017600000137463212
        kl: 0.009223398752510548
        model: {}
        policy_loss: -0.023905709385871887
        total_loss: -0.022033344954252243
        vf_explained_var: 0.1653115451335907
        vf_loss: 13.055437088012695
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 1.1005947589874268
        entropy_coeff: 0.0017600000137463212
        kl: 0.013198480010032654
        model: {}
        policy_loss: -0.035218629986047745
        total_loss: -0.033068325370550156
        vf_explained_var: 0.07393912971019745
        vf_loss: 14.476537704467773
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00043332480709068477
        entropy: 0.9408994317054749
        entropy_coeff: 0.0017600000137463212
        kl: 0.012140990234911442
        model: {}
        policy_loss: -0.032649002969264984
        total_loss: -0.030535312369465828
        vf_explained_var: 0.14264614880084991
        vf_loss: 13.414770126342773
    load_time_ms: 13599.638
    num_steps_sampled: 13344000
    num_steps_trained: 13344000
    sample_time_ms: 107565.89
    update_time_ms: 30.727
  iterations_since_restore: 79
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.5717277486911
    ram_util_percent: 16.319371727748692
  pid: 30948
  policy_reward_max:
    agent-0: 152.50000000000034
    agent-1: 152.50000000000034
    agent-2: 152.50000000000034
    agent-3: 152.50000000000034
    agent-4: 152.50000000000034
    agent-5: 152.50000000000034
  policy_reward_mean:
    agent-0: 106.63333333333365
    agent-1: 106.63333333333365
    agent-2: 106.63333333333365
    agent-3: 106.63333333333365
    agent-4: 106.63333333333365
    agent-5: 106.63333333333365
  policy_reward_min:
    agent-0: 35.33333333333337
    agent-1: 35.33333333333337
    agent-2: 35.33333333333337
    agent-3: 35.33333333333337
    agent-4: 35.33333333333337
    agent-5: 35.33333333333337
  sampler_perf:
    mean_env_wait_ms: 27.69997693351628
    mean_inference_ms: 13.09083063288891
    mean_processing_ms: 59.30583424188073
  time_since_restore: 11058.309098005295
  time_this_iter_s: 133.12373566627502
  time_total_s: 20184.320911884308
  timestamp: 1637038143
  timesteps_since_restore: 7584000
  timesteps_this_iter: 96000
  timesteps_total: 13344000
  training_iteration: 139
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 30.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    139 |          20184.3 | 13344000 |    639.8 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 1.79
    apples_agent-0_min: 0
    apples_agent-1_max: 110
    apples_agent-1_mean: 28.19
    apples_agent-1_min: 0
    apples_agent-2_max: 152
    apples_agent-2_mean: 17.67
    apples_agent-2_min: 0
    apples_agent-3_max: 141
    apples_agent-3_mean: 84.83
    apples_agent-3_min: 9
    apples_agent-4_max: 41
    apples_agent-4_mean: 3.32
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 67.67
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 613
    cleaning_beam_agent-0_mean: 348.94
    cleaning_beam_agent-0_min: 176
    cleaning_beam_agent-1_max: 391
    cleaning_beam_agent-1_mean: 195.42
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 484
    cleaning_beam_agent-2_mean: 286.26
    cleaning_beam_agent-2_min: 95
    cleaning_beam_agent-3_max: 196
    cleaning_beam_agent-3_mean: 61.59
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 374
    cleaning_beam_agent-4_mean: 295.53
    cleaning_beam_agent-4_min: 168
    cleaning_beam_agent-5_max: 237
    cleaning_beam_agent-5_mean: 63.5
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-51-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 883.9999999999728
  episode_reward_mean: 634.0999999999952
  episode_reward_min: 196.999999999999
  episodes_this_iter: 96
  episodes_total: 13440
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12572.167
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 1.0646746158599854
        entropy_coeff: 0.0017600000137463212
        kl: 0.011003047227859497
        model: {}
        policy_loss: -0.0290995966643095
        total_loss: -0.027204744517803192
        vf_explained_var: 0.09075058996677399
        vf_loss: 15.680707931518555
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 1.1602834463119507
        entropy_coeff: 0.0017600000137463212
        kl: 0.013416376896202564
        model: {}
        policy_loss: -0.03702510893344879
        total_loss: -0.0347193218767643
        vf_explained_var: 0.034826844930648804
        vf_loss: 16.64617919921875
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 1.122912049293518
        entropy_coeff: 0.0017600000137463212
        kl: 0.012485538609325886
        model: {}
        policy_loss: -0.032379910349845886
        total_loss: -0.030242323875427246
        vf_explained_var: 0.06199982762336731
        vf_loss: 16.167993545532227
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 0.7225283980369568
        entropy_coeff: 0.0017600000137463212
        kl: 0.008694627322256565
        model: {}
        policy_loss: -0.02422419935464859
        total_loss: -0.022385768592357635
        vf_explained_var: 0.2052663117647171
        vf_loss: 13.711553573608398
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 1.1072850227355957
        entropy_coeff: 0.0017600000137463212
        kl: 0.01388385146856308
        model: {}
        policy_loss: -0.03645789995789528
        total_loss: -0.03404650837182999
        vf_explained_var: 0.08231011033058167
        vf_loss: 15.834413528442383
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042733439477160573
        entropy: 0.9412938356399536
        entropy_coeff: 0.0017600000137463212
        kl: 0.011736848391592503
        model: {}
        policy_loss: -0.03315319865942001
        total_loss: -0.031032655388116837
        vf_explained_var: 0.171096071600914
        vf_loss: 14.29849624633789
    load_time_ms: 13587.882
    num_steps_sampled: 13440000
    num_steps_trained: 13440000
    sample_time_ms: 107406.378
    update_time_ms: 30.645
  iterations_since_restore: 80
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.590000000000003
    ram_util_percent: 16.307368421052633
  pid: 30948
  policy_reward_max:
    agent-0: 147.33333333333323
    agent-1: 147.33333333333323
    agent-2: 147.33333333333323
    agent-3: 147.33333333333323
    agent-4: 147.33333333333323
    agent-5: 147.33333333333323
  policy_reward_mean:
    agent-0: 105.68333333333364
    agent-1: 105.68333333333364
    agent-2: 105.68333333333364
    agent-3: 105.68333333333364
    agent-4: 105.68333333333364
    agent-5: 105.68333333333364
  policy_reward_min:
    agent-0: 32.83333333333339
    agent-1: 32.83333333333339
    agent-2: 32.83333333333339
    agent-3: 32.83333333333339
    agent-4: 32.83333333333339
    agent-5: 32.83333333333339
  sampler_perf:
    mean_env_wait_ms: 27.68754723681889
    mean_inference_ms: 13.089253707076926
    mean_processing_ms: 59.289757386966606
  time_since_restore: 11191.965706825256
  time_this_iter_s: 133.65660881996155
  time_total_s: 20317.97752070427
  timestamp: 1637038276
  timesteps_since_restore: 7680000
  timesteps_this_iter: 96000
  timesteps_total: 13440000
  training_iteration: 140
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    140 |            20318 | 13440000 |    634.1 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 2.03
    apples_agent-0_min: 0
    apples_agent-1_max: 72
    apples_agent-1_mean: 27.88
    apples_agent-1_min: 0
    apples_agent-2_max: 253
    apples_agent-2_mean: 15.7
    apples_agent-2_min: 0
    apples_agent-3_max: 174
    apples_agent-3_mean: 84.65
    apples_agent-3_min: 27
    apples_agent-4_max: 170
    apples_agent-4_mean: 4.51
    apples_agent-4_min: 0
    apples_agent-5_max: 120
    apples_agent-5_mean: 70.76
    apples_agent-5_min: 17
    cleaning_beam_agent-0_max: 555
    cleaning_beam_agent-0_mean: 345.23
    cleaning_beam_agent-0_min: 161
    cleaning_beam_agent-1_max: 409
    cleaning_beam_agent-1_mean: 202.02
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 489
    cleaning_beam_agent-2_mean: 312.91
    cleaning_beam_agent-2_min: 65
    cleaning_beam_agent-3_max: 129
    cleaning_beam_agent-3_mean: 55.65
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 402
    cleaning_beam_agent-4_mean: 294.42
    cleaning_beam_agent-4_min: 131
    cleaning_beam_agent-5_max: 190
    cleaning_beam_agent-5_mean: 57.03
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-53-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 852.9999999999776
  episode_reward_mean: 640.6899999999955
  episode_reward_min: 332.00000000000443
  episodes_this_iter: 96
  episodes_total: 13536
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12536.503
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 1.0623998641967773
        entropy_coeff: 0.0017600000137463212
        kl: 0.010888854041695595
        model: {}
        policy_loss: -0.027308298274874687
        total_loss: -0.025519052520394325
        vf_explained_var: 0.08737513422966003
        vf_loss: 14.81298828125
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 1.1538861989974976
        entropy_coeff: 0.0017600000137463212
        kl: 0.013143223710358143
        model: {}
        policy_loss: -0.035091303288936615
        total_loss: -0.03286847844719887
        vf_explained_var: -0.00033371150493621826
        vf_loss: 16.250179290771484
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 1.1151158809661865
        entropy_coeff: 0.0017600000137463212
        kl: 0.013152236118912697
        model: {}
        policy_loss: -0.03237762674689293
        total_loss: -0.030242685228586197
        vf_explained_var: 0.09615202248096466
        vf_loss: 14.670949935913086
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 0.7246265411376953
        entropy_coeff: 0.0017600000137463212
        kl: 0.008906110189855099
        model: {}
        policy_loss: -0.023753207176923752
        total_loss: -0.02186906524002552
        vf_explained_var: 0.15105107426643372
        vf_loss: 13.782615661621094
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 1.0945289134979248
        entropy_coeff: 0.0017600000137463212
        kl: 0.01297111064195633
        model: {}
        policy_loss: -0.034630246460437775
        total_loss: -0.03243391960859299
        vf_explained_var: 0.05829958617687225
        vf_loss: 15.284717559814453
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00042134401155635715
        entropy: 0.9340596199035645
        entropy_coeff: 0.0017600000137463212
        kl: 0.012442292645573616
        model: {}
        policy_loss: -0.03257324546575546
        total_loss: -0.030358746647834778
        vf_explained_var: 0.15669798851013184
        vf_loss: 13.699836730957031
    load_time_ms: 13615.505
    num_steps_sampled: 13536000
    num_steps_trained: 13536000
    sample_time_ms: 107577.51
    update_time_ms: 30.316
  iterations_since_restore: 81
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.531770833333336
    ram_util_percent: 16.254166666666666
  pid: 30948
  policy_reward_max:
    agent-0: 142.1666666666665
    agent-1: 142.1666666666665
    agent-2: 142.1666666666665
    agent-3: 142.1666666666665
    agent-4: 142.1666666666665
    agent-5: 142.1666666666665
  policy_reward_mean:
    agent-0: 106.78166666666701
    agent-1: 106.78166666666701
    agent-2: 106.78166666666701
    agent-3: 106.78166666666701
    agent-4: 106.78166666666701
    agent-5: 106.78166666666701
  policy_reward_min:
    agent-0: 55.3333333333331
    agent-1: 55.3333333333331
    agent-2: 55.3333333333331
    agent-3: 55.3333333333331
    agent-4: 55.3333333333331
    agent-5: 55.3333333333331
  sampler_perf:
    mean_env_wait_ms: 27.677772353579485
    mean_inference_ms: 13.08712500509786
    mean_processing_ms: 59.27910579249388
  time_since_restore: 11325.581528425217
  time_this_iter_s: 133.61582159996033
  time_total_s: 20451.59334230423
  timestamp: 1637038411
  timesteps_since_restore: 7776000
  timesteps_this_iter: 96000
  timesteps_total: 13536000
  training_iteration: 141
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    141 |          20451.6 | 13536000 |   640.69 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 1.84
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 27.31
    apples_agent-1_min: 0
    apples_agent-2_max: 94
    apples_agent-2_mean: 15.49
    apples_agent-2_min: 0
    apples_agent-3_max: 231
    apples_agent-3_mean: 89.73
    apples_agent-3_min: 22
    apples_agent-4_max: 78
    apples_agent-4_mean: 2.7
    apples_agent-4_min: 0
    apples_agent-5_max: 116
    apples_agent-5_mean: 66.03
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 563
    cleaning_beam_agent-0_mean: 359.07
    cleaning_beam_agent-0_min: 128
    cleaning_beam_agent-1_max: 336
    cleaning_beam_agent-1_mean: 201.15
    cleaning_beam_agent-1_min: 64
    cleaning_beam_agent-2_max: 470
    cleaning_beam_agent-2_mean: 286.87
    cleaning_beam_agent-2_min: 68
    cleaning_beam_agent-3_max: 131
    cleaning_beam_agent-3_mean: 51.47
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 403
    cleaning_beam_agent-4_mean: 293.32
    cleaning_beam_agent-4_min: 189
    cleaning_beam_agent-5_max: 173
    cleaning_beam_agent-5_mean: 66.8
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-55-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 874.9999999999873
  episode_reward_mean: 637.6499999999963
  episode_reward_min: 122.00000000000081
  episodes_this_iter: 96
  episodes_total: 13632
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12500.307
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 1.0548796653747559
        entropy_coeff: 0.0017600000137463212
        kl: 0.010292278602719307
        model: {}
        policy_loss: -0.02699120342731476
        total_loss: -0.02541741542518139
        vf_explained_var: 0.1424119472503662
        vf_loss: 13.719209671020508
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 1.1663074493408203
        entropy_coeff: 0.0017600000137463212
        kl: 0.013389844447374344
        model: {}
        policy_loss: -0.03573758155107498
        total_loss: -0.033516839146614075
        vf_explained_var: 0.005082264542579651
        vf_loss: 15.954765319824219
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 1.1110925674438477
        entropy_coeff: 0.0017600000137463212
        kl: 0.012855615466833115
        model: {}
        policy_loss: -0.03277970850467682
        total_loss: -0.030704133212566376
        vf_explained_var: 0.08841142058372498
        vf_loss: 14.59976577758789
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 0.7195987105369568
        entropy_coeff: 0.0017600000137463212
        kl: 0.010154864750802517
        model: {}
        policy_loss: -0.02361840382218361
        total_loss: -0.021501712501049042
        vf_explained_var: 0.15444837510585785
        vf_loss: 13.522119522094727
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 1.0952385663986206
        entropy_coeff: 0.0017600000137463212
        kl: 0.012888467870652676
        model: {}
        policy_loss: -0.035729777067899704
        total_loss: -0.03354199230670929
        vf_explained_var: 0.038538649678230286
        vf_loss: 15.377098083496094
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004153535992372781
        entropy: 0.9479592442512512
        entropy_coeff: 0.0017600000137463212
        kl: 0.011884991079568863
        model: {}
        policy_loss: -0.0337834469974041
        total_loss: -0.031702183187007904
        vf_explained_var: 0.14192776381969452
        vf_loss: 13.726691246032715
    load_time_ms: 13614.393
    num_steps_sampled: 13632000
    num_steps_trained: 13632000
    sample_time_ms: 107579.063
    update_time_ms: 15.317
  iterations_since_restore: 82
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.547643979057593
    ram_util_percent: 16.313612565445023
  pid: 30948
  policy_reward_max:
    agent-0: 145.8333333333337
    agent-1: 145.8333333333337
    agent-2: 145.8333333333337
    agent-3: 145.8333333333337
    agent-4: 145.8333333333337
    agent-5: 145.8333333333337
  policy_reward_mean:
    agent-0: 106.27500000000035
    agent-1: 106.27500000000035
    agent-2: 106.27500000000035
    agent-3: 106.27500000000035
    agent-4: 106.27500000000035
    agent-5: 106.27500000000035
  policy_reward_min:
    agent-0: 20.333333333333346
    agent-1: 20.333333333333346
    agent-2: 20.333333333333346
    agent-3: 20.333333333333346
    agent-4: 20.333333333333346
    agent-5: 20.333333333333346
  sampler_perf:
    mean_env_wait_ms: 27.66625514421133
    mean_inference_ms: 13.08642277298071
    mean_processing_ms: 59.263360548280396
  time_since_restore: 11459.836265563965
  time_this_iter_s: 134.25473713874817
  time_total_s: 20585.848079442978
  timestamp: 1637038546
  timesteps_since_restore: 7872000
  timesteps_this_iter: 96000
  timesteps_total: 13632000
  training_iteration: 142
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    142 |          20585.8 | 13632000 |   637.65 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 1.98
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 31.33
    apples_agent-1_min: 0
    apples_agent-2_max: 112
    apples_agent-2_mean: 11.84
    apples_agent-2_min: 0
    apples_agent-3_max: 185
    apples_agent-3_mean: 91.71
    apples_agent-3_min: 19
    apples_agent-4_max: 72
    apples_agent-4_mean: 2.19
    apples_agent-4_min: 0
    apples_agent-5_max: 124
    apples_agent-5_mean: 71.96
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 569
    cleaning_beam_agent-0_mean: 371.11
    cleaning_beam_agent-0_min: 103
    cleaning_beam_agent-1_max: 371
    cleaning_beam_agent-1_mean: 196.0
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 458
    cleaning_beam_agent-2_mean: 311.73
    cleaning_beam_agent-2_min: 125
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 56.08
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 394
    cleaning_beam_agent-4_mean: 301.31
    cleaning_beam_agent-4_min: 195
    cleaning_beam_agent-5_max: 177
    cleaning_beam_agent-5_mean: 62.51
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-15_23-58-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 932.9999999999832
  episode_reward_mean: 668.5399999999946
  episode_reward_min: 191.99999999999463
  episodes_this_iter: 96
  episodes_total: 13728
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12475.027
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 1.0551091432571411
        entropy_coeff: 0.0017600000137463212
        kl: 0.010669734328985214
        model: {}
        policy_loss: -0.028038887307047844
        total_loss: -0.02625771053135395
        vf_explained_var: 0.11501117050647736
        vf_loss: 15.042229652404785
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 1.164687991142273
        entropy_coeff: 0.0017600000137463212
        kl: 0.012766994535923004
        model: {}
        policy_loss: -0.03505372256040573
        total_loss: -0.03287737816572189
        vf_explained_var: 0.015993863344192505
        vf_loss: 16.727991104125977
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 1.1237781047821045
        entropy_coeff: 0.0017600000137463212
        kl: 0.012378266081213951
        model: {}
        policy_loss: -0.03166315332055092
        total_loss: -0.029569879174232483
        vf_explained_var: 0.061798691749572754
        vf_loss: 15.954654693603516
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 0.6909757256507874
        entropy_coeff: 0.0017600000137463212
        kl: 0.008941593579947948
        model: {}
        policy_loss: -0.02174466848373413
        total_loss: -0.01973584294319153
        vf_explained_var: 0.1546226590871811
        vf_loss: 14.366260528564453
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 1.1026582717895508
        entropy_coeff: 0.0017600000137463212
        kl: 0.012845748104155064
        model: {}
        policy_loss: -0.03572272136807442
        total_loss: -0.03351033851504326
        vf_explained_var: 0.06895996630191803
        vf_loss: 15.83910083770752
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00040936318691819906
        entropy: 0.9404902458190918
        entropy_coeff: 0.0017600000137463212
        kl: 0.012172733433544636
        model: {}
        policy_loss: -0.033377885818481445
        total_loss: -0.03117811121046543
        vf_explained_var: 0.16464154422283173
        vf_loss: 14.204928398132324
    load_time_ms: 13617.257
    num_steps_sampled: 13728000
    num_steps_trained: 13728000
    sample_time_ms: 107625.295
    update_time_ms: 15.188
  iterations_since_restore: 83
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.474999999999998
    ram_util_percent: 16.301041666666666
  pid: 30948
  policy_reward_max:
    agent-0: 155.49999999999991
    agent-1: 155.49999999999991
    agent-2: 155.49999999999991
    agent-3: 155.49999999999991
    agent-4: 155.49999999999991
    agent-5: 155.49999999999991
  policy_reward_mean:
    agent-0: 111.42333333333363
    agent-1: 111.42333333333363
    agent-2: 111.42333333333363
    agent-3: 111.42333333333363
    agent-4: 111.42333333333363
    agent-5: 111.42333333333363
  policy_reward_min:
    agent-0: 32.00000000000012
    agent-1: 32.00000000000012
    agent-2: 32.00000000000012
    agent-3: 32.00000000000012
    agent-4: 32.00000000000012
    agent-5: 32.00000000000012
  sampler_perf:
    mean_env_wait_ms: 27.65825108185332
    mean_inference_ms: 13.084289795303746
    mean_processing_ms: 59.24781422735197
  time_since_restore: 11593.996946573257
  time_this_iter_s: 134.1606810092926
  time_total_s: 20720.00876045227
  timestamp: 1637038680
  timesteps_since_restore: 7968000
  timesteps_this_iter: 96000
  timesteps_total: 13728000
  training_iteration: 143
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    143 |            20720 | 13728000 |   668.54 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 109
    apples_agent-0_mean: 2.71
    apples_agent-0_min: 0
    apples_agent-1_max: 68
    apples_agent-1_mean: 26.12
    apples_agent-1_min: 0
    apples_agent-2_max: 475
    apples_agent-2_mean: 15.73
    apples_agent-2_min: 0
    apples_agent-3_max: 288
    apples_agent-3_mean: 89.74
    apples_agent-3_min: 31
    apples_agent-4_max: 32
    apples_agent-4_mean: 1.85
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 73.84
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 530
    cleaning_beam_agent-0_mean: 348.47
    cleaning_beam_agent-0_min: 136
    cleaning_beam_agent-1_max: 379
    cleaning_beam_agent-1_mean: 196.47
    cleaning_beam_agent-1_min: 56
    cleaning_beam_agent-2_max: 537
    cleaning_beam_agent-2_mean: 312.72
    cleaning_beam_agent-2_min: 91
    cleaning_beam_agent-3_max: 135
    cleaning_beam_agent-3_mean: 53.93
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 377
    cleaning_beam_agent-4_mean: 293.8
    cleaning_beam_agent-4_min: 140
    cleaning_beam_agent-5_max: 183
    cleaning_beam_agent-5_mean: 65.41
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-00-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 916.9999999999704
  episode_reward_mean: 663.4499999999929
  episode_reward_min: 244.99999999999687
  episodes_this_iter: 96
  episodes_total: 13824
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12491.184
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 1.05326509475708
        entropy_coeff: 0.0017600000137463212
        kl: 0.009969337843358517
        model: {}
        policy_loss: -0.027141526341438293
        total_loss: -0.02557462826371193
        vf_explained_var: 0.13674329221248627
        vf_loss: 14.26778507232666
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 1.158238172531128
        entropy_coeff: 0.0017600000137463212
        kl: 0.013037104159593582
        model: {}
        policy_loss: -0.03555824235081673
        total_loss: -0.033312417566776276
        vf_explained_var: -0.011738568544387817
        vf_loss: 16.769012451171875
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 1.1355903148651123
        entropy_coeff: 0.0017600000137463212
        kl: 0.012490685097873211
        model: {}
        policy_loss: -0.031397927552461624
        total_loss: -0.029410162940621376
        vf_explained_var: 0.09952959418296814
        vf_loss: 14.882637023925781
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 0.6890319585800171
        entropy_coeff: 0.0017600000137463212
        kl: 0.009080092422664165
        model: {}
        policy_loss: -0.023379011079669
        total_loss: -0.021419307217001915
        vf_explained_var: 0.17964403331279755
        vf_loss: 13.563822746276855
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 1.1006813049316406
        entropy_coeff: 0.0017600000137463212
        kl: 0.012986680492758751
        model: {}
        policy_loss: -0.03567216545343399
        total_loss: -0.03346198424696922
        vf_explained_var: 0.06254018843173981
        vf_loss: 15.50048828125
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0004033728037029505
        entropy: 0.9271653890609741
        entropy_coeff: 0.0017600000137463212
        kl: 0.011610078625380993
        model: {}
        policy_loss: -0.03130936250090599
        total_loss: -0.02921547368168831
        vf_explained_var: 0.15096129477024078
        vf_loss: 14.036836624145508
    load_time_ms: 13614.39
    num_steps_sampled: 13824000
    num_steps_trained: 13824000
    sample_time_ms: 107477.864
    update_time_ms: 377.583
  iterations_since_restore: 84
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.376262626262626
    ram_util_percent: 16.331818181818186
  pid: 30948
  policy_reward_max:
    agent-0: 152.83333333333323
    agent-1: 152.83333333333323
    agent-2: 152.83333333333323
    agent-3: 152.83333333333323
    agent-4: 152.83333333333323
    agent-5: 152.83333333333323
  policy_reward_mean:
    agent-0: 110.57500000000036
    agent-1: 110.57500000000036
    agent-2: 110.57500000000036
    agent-3: 110.57500000000036
    agent-4: 110.57500000000036
    agent-5: 110.57500000000036
  policy_reward_min:
    agent-0: 40.83333333333332
    agent-1: 40.83333333333332
    agent-2: 40.83333333333332
    agent-3: 40.83333333333332
    agent-4: 40.83333333333332
    agent-5: 40.83333333333332
  sampler_perf:
    mean_env_wait_ms: 27.647320408733957
    mean_inference_ms: 13.08276049502697
    mean_processing_ms: 59.230264763348984
  time_since_restore: 11731.906518220901
  time_this_iter_s: 137.90957164764404
  time_total_s: 20857.918332099915
  timestamp: 1637038819
  timesteps_since_restore: 8064000
  timesteps_this_iter: 96000
  timesteps_total: 13824000
  training_iteration: 144
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    144 |          20857.9 | 13824000 |   663.45 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 2.26
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 27.23
    apples_agent-1_min: 0
    apples_agent-2_max: 339
    apples_agent-2_mean: 17.78
    apples_agent-2_min: 0
    apples_agent-3_max: 162
    apples_agent-3_mean: 88.91
    apples_agent-3_min: 31
    apples_agent-4_max: 168
    apples_agent-4_mean: 5.0
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 69.05
    apples_agent-5_min: 16
    cleaning_beam_agent-0_max: 518
    cleaning_beam_agent-0_mean: 361.81
    cleaning_beam_agent-0_min: 138
    cleaning_beam_agent-1_max: 341
    cleaning_beam_agent-1_mean: 193.54
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 457
    cleaning_beam_agent-2_mean: 301.77
    cleaning_beam_agent-2_min: 26
    cleaning_beam_agent-3_max: 114
    cleaning_beam_agent-3_mean: 51.98
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 399
    cleaning_beam_agent-4_mean: 290.1
    cleaning_beam_agent-4_min: 114
    cleaning_beam_agent-5_max: 196
    cleaning_beam_agent-5_mean: 69.76
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-02-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 899.9999999999724
  episode_reward_mean: 638.8999999999971
  episode_reward_min: 226.99999999999648
  episodes_this_iter: 96
  episodes_total: 13920
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12506.781
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 1.0577946901321411
        entropy_coeff: 0.0017600000137463212
        kl: 0.01029174868017435
        model: {}
        policy_loss: -0.027828603982925415
        total_loss: -0.02612490765750408
        vf_explained_var: 0.10254231095314026
        vf_loss: 15.07064151763916
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 1.1505286693572998
        entropy_coeff: 0.0017600000137463212
        kl: 0.012797459959983826
        model: {}
        policy_loss: -0.035879433155059814
        total_loss: -0.03366086259484291
        vf_explained_var: -0.0011826008558273315
        vf_loss: 16.840124130249023
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 1.1326279640197754
        entropy_coeff: 0.0017600000137463212
        kl: 0.012166478671133518
        model: {}
        policy_loss: -0.03250332176685333
        total_loss: -0.03048633597791195
        vf_explained_var: 0.06056837737560272
        vf_loss: 15.771183967590332
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 0.7083609700202942
        entropy_coeff: 0.0017600000137463212
        kl: 0.008556085638701916
        model: {}
        policy_loss: -0.023306410759687424
        total_loss: -0.021533019840717316
        vf_explained_var: 0.22019819915294647
        vf_loss: 13.088873863220215
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 1.0970144271850586
        entropy_coeff: 0.0017600000137463212
        kl: 0.012942074798047543
        model: {}
        policy_loss: -0.03562892600893974
        total_loss: -0.03343583270907402
        vf_explained_var: 0.08511555194854736
        vf_loss: 15.354207992553711
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039738239138387144
        entropy: 0.9550577402114868
        entropy_coeff: 0.0017600000137463212
        kl: 0.012167810462415218
        model: {}
        policy_loss: -0.03210197389125824
        total_loss: -0.02998821809887886
        vf_explained_var: 0.18898800015449524
        vf_loss: 13.61094856262207
    load_time_ms: 13610.137
    num_steps_sampled: 13920000
    num_steps_trained: 13920000
    sample_time_ms: 107370.355
    update_time_ms: 377.528
  iterations_since_restore: 85
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.596842105263157
    ram_util_percent: 16.30315789473684
  pid: 30948
  policy_reward_max:
    agent-0: 150.0000000000002
    agent-1: 150.0000000000002
    agent-2: 150.0000000000002
    agent-3: 150.0000000000002
    agent-4: 150.0000000000002
    agent-5: 150.0000000000002
  policy_reward_mean:
    agent-0: 106.48333333333362
    agent-1: 106.48333333333362
    agent-2: 106.48333333333362
    agent-3: 106.48333333333362
    agent-4: 106.48333333333362
    agent-5: 106.48333333333362
  policy_reward_min:
    agent-0: 37.833333333333336
    agent-1: 37.833333333333336
    agent-2: 37.833333333333336
    agent-3: 37.833333333333336
    agent-4: 37.833333333333336
    agent-5: 37.833333333333336
  sampler_perf:
    mean_env_wait_ms: 27.637294077959233
    mean_inference_ms: 13.081016306886067
    mean_processing_ms: 59.2178736351983
  time_since_restore: 11865.234988689423
  time_this_iter_s: 133.32847046852112
  time_total_s: 20991.246802568436
  timestamp: 1637038953
  timesteps_since_restore: 8160000
  timesteps_this_iter: 96000
  timesteps_total: 13920000
  training_iteration: 145
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    145 |          20991.2 | 13920000 |    638.9 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 104
    apples_agent-0_mean: 3.68
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 26.22
    apples_agent-1_min: 0
    apples_agent-2_max: 145
    apples_agent-2_mean: 11.95
    apples_agent-2_min: 0
    apples_agent-3_max: 181
    apples_agent-3_mean: 87.54
    apples_agent-3_min: 0
    apples_agent-4_max: 64
    apples_agent-4_mean: 3.96
    apples_agent-4_min: 0
    apples_agent-5_max: 137
    apples_agent-5_mean: 73.73
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 504
    cleaning_beam_agent-0_mean: 346.95
    cleaning_beam_agent-0_min: 161
    cleaning_beam_agent-1_max: 366
    cleaning_beam_agent-1_mean: 198.6
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 577
    cleaning_beam_agent-2_mean: 305.02
    cleaning_beam_agent-2_min: 127
    cleaning_beam_agent-3_max: 174
    cleaning_beam_agent-3_mean: 60.22
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 380
    cleaning_beam_agent-4_mean: 292.04
    cleaning_beam_agent-4_min: 197
    cleaning_beam_agent-5_max: 197
    cleaning_beam_agent-5_mean: 66.65
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-04-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 854.9999999999744
  episode_reward_mean: 631.8199999999964
  episode_reward_min: 150.9999999999999
  episodes_this_iter: 96
  episodes_total: 14016
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12519.546
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 1.0532946586608887
        entropy_coeff: 0.0017600000137463212
        kl: 0.010394634678959846
        model: {}
        policy_loss: -0.02982841059565544
        total_loss: -0.028089236468076706
        vf_explained_var: 0.11720788478851318
        vf_loss: 15.14046859741211
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 1.1584666967391968
        entropy_coeff: 0.0017600000137463212
        kl: 0.012643472291529179
        model: {}
        policy_loss: -0.03481634706258774
        total_loss: -0.032641444355249405
        vf_explained_var: 0.01815265417098999
        vf_loss: 16.851051330566406
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 1.1347510814666748
        entropy_coeff: 0.0017600000137463212
        kl: 0.013059698976576328
        model: {}
        policy_loss: -0.03231385722756386
        total_loss: -0.030079863965511322
        vf_explained_var: 0.05635644495487213
        vf_loss: 16.19218635559082
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 0.7258972525596619
        entropy_coeff: 0.0017600000137463212
        kl: 0.00887919683009386
        model: {}
        policy_loss: -0.02368873357772827
        total_loss: -0.021810755133628845
        vf_explained_var: 0.19642668962478638
        vf_loss: 13.797174453735352
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 1.0944468975067139
        entropy_coeff: 0.0017600000137463212
        kl: 0.012086791917681694
        model: {}
        policy_loss: -0.035604264587163925
        total_loss: -0.03354796767234802
        vf_explained_var: 0.087883859872818
        vf_loss: 15.651646614074707
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00039139200816862285
        entropy: 0.9481672048568726
        entropy_coeff: 0.0017600000137463212
        kl: 0.011673270724713802
        model: {}
        policy_loss: -0.031939271837472916
        total_loss: -0.02988199144601822
        vf_explained_var: 0.18877990543842316
        vf_loss: 13.914051055908203
    load_time_ms: 13624.791
    num_steps_sampled: 14016000
    num_steps_trained: 14016000
    sample_time_ms: 107267.345
    update_time_ms: 377.19
  iterations_since_restore: 86
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.632460732984296
    ram_util_percent: 16.309424083769635
  pid: 30948
  policy_reward_max:
    agent-0: 142.49999999999986
    agent-1: 142.49999999999986
    agent-2: 142.49999999999986
    agent-3: 142.49999999999986
    agent-4: 142.49999999999986
    agent-5: 142.49999999999986
  policy_reward_mean:
    agent-0: 105.30333333333367
    agent-1: 105.30333333333367
    agent-2: 105.30333333333367
    agent-3: 105.30333333333367
    agent-4: 105.30333333333367
    agent-5: 105.30333333333367
  policy_reward_min:
    agent-0: 25.166666666666696
    agent-1: 25.166666666666696
    agent-2: 25.166666666666696
    agent-3: 25.166666666666696
    agent-4: 25.166666666666696
    agent-5: 25.166666666666696
  sampler_perf:
    mean_env_wait_ms: 27.627783902819548
    mean_inference_ms: 13.079640222089495
    mean_processing_ms: 59.20600739053262
  time_since_restore: 11998.984179735184
  time_this_iter_s: 133.7491910457611
  time_total_s: 21124.995993614197
  timestamp: 1637039087
  timesteps_since_restore: 8256000
  timesteps_this_iter: 96000
  timesteps_total: 14016000
  training_iteration: 146
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    146 |            21125 | 14016000 |   631.82 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 1.93
    apples_agent-0_min: 0
    apples_agent-1_max: 79
    apples_agent-1_mean: 26.65
    apples_agent-1_min: 0
    apples_agent-2_max: 120
    apples_agent-2_mean: 15.59
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 83.81
    apples_agent-3_min: 9
    apples_agent-4_max: 65
    apples_agent-4_mean: 3.05
    apples_agent-4_min: 0
    apples_agent-5_max: 122
    apples_agent-5_mean: 71.61
    apples_agent-5_min: 4
    cleaning_beam_agent-0_max: 496
    cleaning_beam_agent-0_mean: 349.2
    cleaning_beam_agent-0_min: 166
    cleaning_beam_agent-1_max: 385
    cleaning_beam_agent-1_mean: 206.87
    cleaning_beam_agent-1_min: 62
    cleaning_beam_agent-2_max: 515
    cleaning_beam_agent-2_mean: 300.82
    cleaning_beam_agent-2_min: 82
    cleaning_beam_agent-3_max: 149
    cleaning_beam_agent-3_mean: 57.48
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 390
    cleaning_beam_agent-4_mean: 290.74
    cleaning_beam_agent-4_min: 192
    cleaning_beam_agent-5_max: 176
    cleaning_beam_agent-5_mean: 68.12
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-07-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 891.9999999999873
  episode_reward_mean: 637.9099999999968
  episode_reward_min: 94.00000000000043
  episodes_this_iter: 96
  episodes_total: 14112
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12504.028
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 1.0512843132019043
        entropy_coeff: 0.0017600000137463212
        kl: 0.010517624206840992
        model: {}
        policy_loss: -0.028789810836315155
        total_loss: -0.026927754282951355
        vf_explained_var: 0.1028125137090683
        vf_loss: 16.08790397644043
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 1.1638174057006836
        entropy_coeff: 0.0017600000137463212
        kl: 0.012796679511666298
        model: {}
        policy_loss: -0.03583977371454239
        total_loss: -0.03356407955288887
        vf_explained_var: 0.01742134988307953
        vf_loss: 17.646728515625
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 1.104097843170166
        entropy_coeff: 0.0017600000137463212
        kl: 0.012843374162912369
        model: {}
        policy_loss: -0.030915308743715286
        total_loss: -0.028716949746012688
        vf_explained_var: 0.12262322008609772
        vf_loss: 15.728981018066406
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 0.720053493976593
        entropy_coeff: 0.0017600000137463212
        kl: 0.008613510057330132
        model: {}
        policy_loss: -0.023666933178901672
        total_loss: -0.02180936560034752
        vf_explained_var: 0.21827904880046844
        vf_loss: 14.021578788757324
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 1.091381311416626
        entropy_coeff: 0.0017600000137463212
        kl: 0.01255546323955059
        model: {}
        policy_loss: -0.03679342195391655
        total_loss: -0.034567300230264664
        vf_explained_var: 0.08880159258842468
        vf_loss: 16.358654022216797
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003854015958495438
        entropy: 0.9566935300827026
        entropy_coeff: 0.0017600000137463212
        kl: 0.012190435081720352
        model: {}
        policy_loss: -0.0331391766667366
        total_loss: -0.030956801027059555
        vf_explained_var: 0.20399828255176544
        vf_loss: 14.280670166015625
    load_time_ms: 13611.583
    num_steps_sampled: 14112000
    num_steps_trained: 14112000
    sample_time_ms: 107386.068
    update_time_ms: 383.849
  iterations_since_restore: 87
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.467708333333334
    ram_util_percent: 16.29479166666667
  pid: 30948
  policy_reward_max:
    agent-0: 148.66666666666703
    agent-1: 148.66666666666703
    agent-2: 148.66666666666703
    agent-3: 148.66666666666703
    agent-4: 148.66666666666703
    agent-5: 148.66666666666703
  policy_reward_mean:
    agent-0: 106.3183333333336
    agent-1: 106.3183333333336
    agent-2: 106.3183333333336
    agent-3: 106.3183333333336
    agent-4: 106.3183333333336
    agent-5: 106.3183333333336
  policy_reward_min:
    agent-0: 15.666666666666647
    agent-1: 15.666666666666647
    agent-2: 15.666666666666647
    agent-3: 15.666666666666647
    agent-4: 15.666666666666647
    agent-5: 15.666666666666647
  sampler_perf:
    mean_env_wait_ms: 27.61960614617772
    mean_inference_ms: 13.077588613195417
    mean_processing_ms: 59.19191647424139
  time_since_restore: 12133.915480613708
  time_this_iter_s: 134.93130087852478
  time_total_s: 21259.92729449272
  timestamp: 1637039223
  timesteps_since_restore: 8352000
  timesteps_this_iter: 96000
  timesteps_total: 14112000
  training_iteration: 147
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    147 |          21259.9 | 14112000 |   637.91 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 3.22
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 26.71
    apples_agent-1_min: 0
    apples_agent-2_max: 70
    apples_agent-2_mean: 11.27
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 86.17
    apples_agent-3_min: 11
    apples_agent-4_max: 29
    apples_agent-4_mean: 1.83
    apples_agent-4_min: 0
    apples_agent-5_max: 125
    apples_agent-5_mean: 71.38
    apples_agent-5_min: 12
    cleaning_beam_agent-0_max: 516
    cleaning_beam_agent-0_mean: 320.02
    cleaning_beam_agent-0_min: 171
    cleaning_beam_agent-1_max: 334
    cleaning_beam_agent-1_mean: 208.74
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 500
    cleaning_beam_agent-2_mean: 315.11
    cleaning_beam_agent-2_min: 112
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 56.32
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 380
    cleaning_beam_agent-4_mean: 292.4
    cleaning_beam_agent-4_min: 149
    cleaning_beam_agent-5_max: 216
    cleaning_beam_agent-5_mean: 64.86
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-09-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 887.9999999999757
  episode_reward_mean: 644.9499999999949
  episode_reward_min: 199.99999999999898
  episodes_this_iter: 96
  episodes_total: 14208
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12476.209
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 1.0669258832931519
        entropy_coeff: 0.0017600000137463212
        kl: 0.010521785356104374
        model: {}
        policy_loss: -0.02928183600306511
        total_loss: -0.027471160516142845
        vf_explained_var: 0.1423911601305008
        vf_loss: 15.841073036193848
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 1.1724708080291748
        entropy_coeff: 0.0017600000137463212
        kl: 0.012552422471344471
        model: {}
        policy_loss: -0.034726329147815704
        total_loss: -0.0324721485376358
        vf_explained_var: 0.02269729971885681
        vf_loss: 18.07244110107422
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 1.128213882446289
        entropy_coeff: 0.0017600000137463212
        kl: 0.01191416010260582
        model: {}
        policy_loss: -0.03051234968006611
        total_loss: -0.028450634330511093
        vf_explained_var: 0.0989782065153122
        vf_loss: 16.64542007446289
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 0.7098807692527771
        entropy_coeff: 0.0017600000137463212
        kl: 0.008698198944330215
        model: {}
        policy_loss: -0.023305630311369896
        total_loss: -0.021419158205389977
        vf_explained_var: 0.24373352527618408
        vf_loss: 13.962237358093262
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 1.091518759727478
        entropy_coeff: 0.0017600000137463212
        kl: 0.012436898425221443
        model: {}
        policy_loss: -0.03593039885163307
        total_loss: -0.03363783657550812
        vf_explained_var: 0.06449209153652191
        vf_loss: 17.262502670288086
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003794112126342952
        entropy: 0.951520562171936
        entropy_coeff: 0.0017600000137463212
        kl: 0.012204868718981743
        model: {}
        policy_loss: -0.03238334879279137
        total_loss: -0.030129022896289825
        vf_explained_var: 0.1949297934770584
        vf_loss: 14.880231857299805
    load_time_ms: 13598.513
    num_steps_sampled: 14208000
    num_steps_trained: 14208000
    sample_time_ms: 107600.263
    update_time_ms: 396.902
  iterations_since_restore: 88
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.636842105263153
    ram_util_percent: 16.272631578947365
  pid: 30948
  policy_reward_max:
    agent-0: 147.9999999999998
    agent-1: 147.9999999999998
    agent-2: 147.9999999999998
    agent-3: 147.9999999999998
    agent-4: 147.9999999999998
    agent-5: 147.9999999999998
  policy_reward_mean:
    agent-0: 107.49166666666703
    agent-1: 107.49166666666703
    agent-2: 107.49166666666703
    agent-3: 107.49166666666703
    agent-4: 107.49166666666703
    agent-5: 107.49166666666703
  policy_reward_min:
    agent-0: 33.333333333333364
    agent-1: 33.333333333333364
    agent-2: 33.333333333333364
    agent-3: 33.333333333333364
    agent-4: 33.333333333333364
    agent-5: 33.333333333333364
  sampler_perf:
    mean_env_wait_ms: 27.61072472698036
    mean_inference_ms: 13.075662577708222
    mean_processing_ms: 59.1761286417404
  time_since_restore: 12266.771609306335
  time_this_iter_s: 132.85612869262695
  time_total_s: 21392.78342318535
  timestamp: 1637039356
  timesteps_since_restore: 8448000
  timesteps_this_iter: 96000
  timesteps_total: 14208000
  training_iteration: 148
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    148 |          21392.8 | 14208000 |   644.95 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 3.06
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 27.92
    apples_agent-1_min: 0
    apples_agent-2_max: 136
    apples_agent-2_mean: 11.12
    apples_agent-2_min: 0
    apples_agent-3_max: 144
    apples_agent-3_mean: 81.9
    apples_agent-3_min: 29
    apples_agent-4_max: 40
    apples_agent-4_mean: 2.29
    apples_agent-4_min: 0
    apples_agent-5_max: 119
    apples_agent-5_mean: 70.06
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 554
    cleaning_beam_agent-0_mean: 337.86
    cleaning_beam_agent-0_min: 169
    cleaning_beam_agent-1_max: 334
    cleaning_beam_agent-1_mean: 195.1
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 496
    cleaning_beam_agent-2_mean: 305.69
    cleaning_beam_agent-2_min: 56
    cleaning_beam_agent-3_max: 156
    cleaning_beam_agent-3_mean: 60.76
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 421
    cleaning_beam_agent-4_mean: 298.53
    cleaning_beam_agent-4_min: 153
    cleaning_beam_agent-5_max: 237
    cleaning_beam_agent-5_mean: 60.48
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-11-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 874.9999999999769
  episode_reward_mean: 637.159999999996
  episode_reward_min: 234.9999999999962
  episodes_this_iter: 96
  episodes_total: 14304
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12462.381
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 1.06121027469635
        entropy_coeff: 0.0017600000137463212
        kl: 0.010415478609502316
        model: {}
        policy_loss: -0.028354989364743233
        total_loss: -0.026689307764172554
        vf_explained_var: 0.11710458993911743
        vf_loss: 14.503129005432129
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 1.1788341999053955
        entropy_coeff: 0.0017600000137463212
        kl: 0.01294668484479189
        model: {}
        policy_loss: -0.03546983376145363
        total_loss: -0.03331177309155464
        vf_explained_var: -0.0006676465272903442
        vf_loss: 16.434709548950195
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 1.127592921257019
        entropy_coeff: 0.0017600000137463212
        kl: 0.011964650824666023
        model: {}
        policy_loss: -0.030913224443793297
        total_loss: -0.028950583189725876
        vf_explained_var: 0.052465423941612244
        vf_loss: 15.542802810668945
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 0.695814847946167
        entropy_coeff: 0.0017600000137463212
        kl: 0.008439070545136929
        model: {}
        policy_loss: -0.02298252284526825
        total_loss: -0.02116715908050537
        vf_explained_var: 0.17549726366996765
        vf_loss: 13.52188491821289
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 1.0797243118286133
        entropy_coeff: 0.0017600000137463212
        kl: 0.013815555721521378
        model: {}
        policy_loss: -0.0361519493162632
        total_loss: -0.03376874700188637
        vf_explained_var: 0.07362216711044312
        vf_loss: 15.204049110412598
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003734208003152162
        entropy: 0.9504190683364868
        entropy_coeff: 0.0017600000137463212
        kl: 0.011800920590758324
        model: {}
        policy_loss: -0.03168075159192085
        total_loss: -0.029589608311653137
        vf_explained_var: 0.14407646656036377
        vf_loss: 14.036943435668945
    load_time_ms: 13595.105
    num_steps_sampled: 14304000
    num_steps_trained: 14304000
    sample_time_ms: 107524.924
    update_time_ms: 396.899
  iterations_since_restore: 89
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.72446808510638
    ram_util_percent: 16.301063829787235
  pid: 30948
  policy_reward_max:
    agent-0: 145.8333333333335
    agent-1: 145.8333333333335
    agent-2: 145.8333333333335
    agent-3: 145.8333333333335
    agent-4: 145.8333333333335
    agent-5: 145.8333333333335
  policy_reward_mean:
    agent-0: 106.19333333333363
    agent-1: 106.19333333333363
    agent-2: 106.19333333333363
    agent-3: 106.19333333333363
    agent-4: 106.19333333333363
    agent-5: 106.19333333333363
  policy_reward_min:
    agent-0: 39.16666666666663
    agent-1: 39.16666666666663
    agent-2: 39.16666666666663
    agent-3: 39.16666666666663
    agent-4: 39.16666666666663
    agent-5: 39.16666666666663
  sampler_perf:
    mean_env_wait_ms: 27.599094795711206
    mean_inference_ms: 13.073111541299745
    mean_processing_ms: 59.15847013504191
  time_since_restore: 12398.97316622734
  time_this_iter_s: 132.20155692100525
  time_total_s: 21524.984980106354
  timestamp: 1637039488
  timesteps_since_restore: 8544000
  timesteps_this_iter: 96000
  timesteps_total: 14304000
  training_iteration: 149
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    149 |            21525 | 14304000 |   637.16 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 2.06
    apples_agent-0_min: 0
    apples_agent-1_max: 130
    apples_agent-1_mean: 27.89
    apples_agent-1_min: 0
    apples_agent-2_max: 132
    apples_agent-2_mean: 12.06
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 84.5
    apples_agent-3_min: 32
    apples_agent-4_max: 49
    apples_agent-4_mean: 2.5
    apples_agent-4_min: 0
    apples_agent-5_max: 131
    apples_agent-5_mean: 75.97
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 501
    cleaning_beam_agent-0_mean: 336.94
    cleaning_beam_agent-0_min: 164
    cleaning_beam_agent-1_max: 355
    cleaning_beam_agent-1_mean: 200.99
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 473
    cleaning_beam_agent-2_mean: 301.71
    cleaning_beam_agent-2_min: 126
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 52.51
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 394
    cleaning_beam_agent-4_mean: 299.3
    cleaning_beam_agent-4_min: 171
    cleaning_beam_agent-5_max: 321
    cleaning_beam_agent-5_mean: 64.17
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 11
    fire_beam_agent-0_mean: 0.13
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-13-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 901.9999999999845
  episode_reward_mean: 658.7099999999955
  episode_reward_min: 303.99999999999903
  episodes_this_iter: 96
  episodes_total: 14400
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12474.757
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 1.0616816282272339
        entropy_coeff: 0.0017600000137463212
        kl: 0.010219769552350044
        model: {}
        policy_loss: -0.029262565076351166
        total_loss: -0.02765665203332901
        vf_explained_var: 0.10701249539852142
        vf_loss: 14.305234909057617
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 1.176639199256897
        entropy_coeff: 0.0017600000137463212
        kl: 0.0129177775233984
        model: {}
        policy_loss: -0.034444406628608704
        total_loss: -0.032371342182159424
        vf_explained_var: 0.02601824700832367
        vf_loss: 15.603891372680664
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 1.1362080574035645
        entropy_coeff: 0.0017600000137463212
        kl: 0.011635173112154007
        model: {}
        policy_loss: -0.030189141631126404
        total_loss: -0.02835164964199066
        vf_explained_var: 0.05723828077316284
        vf_loss: 15.10183048248291
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 0.6975948214530945
        entropy_coeff: 0.0017600000137463212
        kl: 0.008078273385763168
        model: {}
        policy_loss: -0.021987004205584526
        total_loss: -0.020236734300851822
        vf_explained_var: 0.14971666038036346
        vf_loss: 13.623814582824707
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 1.0835564136505127
        entropy_coeff: 0.0017600000137463212
        kl: 0.01275872252881527
        model: {}
        policy_loss: -0.034766215831041336
        total_loss: -0.03263416141271591
        vf_explained_var: 0.07099080085754395
        vf_loss: 14.873666763305664
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036743038799613714
        entropy: 0.9395776391029358
        entropy_coeff: 0.0017600000137463212
        kl: 0.01168600469827652
        model: {}
        policy_loss: -0.03230106830596924
        total_loss: -0.030232105404138565
        vf_explained_var: 0.134272962808609
        vf_loss: 13.854148864746094
    load_time_ms: 13592.025
    num_steps_sampled: 14400000
    num_steps_trained: 14400000
    sample_time_ms: 107460.418
    update_time_ms: 396.906
  iterations_since_restore: 90
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.645789473684214
    ram_util_percent: 16.32157894736842
  pid: 30948
  policy_reward_max:
    agent-0: 150.3333333333334
    agent-1: 150.3333333333334
    agent-2: 150.3333333333334
    agent-3: 150.3333333333334
    agent-4: 150.3333333333334
    agent-5: 150.3333333333334
  policy_reward_mean:
    agent-0: 109.78500000000032
    agent-1: 109.78500000000032
    agent-2: 109.78500000000032
    agent-3: 109.78500000000032
    agent-4: 109.78500000000032
    agent-5: 109.78500000000032
  policy_reward_min:
    agent-0: 50.66666666666651
    agent-1: 50.66666666666651
    agent-2: 50.66666666666651
    agent-3: 50.66666666666651
    agent-4: 50.66666666666651
    agent-5: 50.66666666666651
  sampler_perf:
    mean_env_wait_ms: 27.58972427546751
    mean_inference_ms: 13.070750328602573
    mean_processing_ms: 59.14449955424542
  time_since_restore: 12532.079690694809
  time_this_iter_s: 133.10652446746826
  time_total_s: 21658.091504573822
  timestamp: 1637039622
  timesteps_since_restore: 8640000
  timesteps_this_iter: 96000
  timesteps_total: 14400000
  training_iteration: 150
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 30.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    150 |          21658.1 | 14400000 |   658.71 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 52
    apples_agent-0_mean: 2.64
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 23.54
    apples_agent-1_min: 0
    apples_agent-2_max: 68
    apples_agent-2_mean: 12.53
    apples_agent-2_min: 0
    apples_agent-3_max: 191
    apples_agent-3_mean: 84.81
    apples_agent-3_min: 37
    apples_agent-4_max: 43
    apples_agent-4_mean: 2.14
    apples_agent-4_min: 0
    apples_agent-5_max: 130
    apples_agent-5_mean: 76.39
    apples_agent-5_min: 2
    cleaning_beam_agent-0_max: 503
    cleaning_beam_agent-0_mean: 349.56
    cleaning_beam_agent-0_min: 175
    cleaning_beam_agent-1_max: 374
    cleaning_beam_agent-1_mean: 204.32
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 500
    cleaning_beam_agent-2_mean: 303.27
    cleaning_beam_agent-2_min: 114
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 51.74
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 386
    cleaning_beam_agent-4_mean: 304.12
    cleaning_beam_agent-4_min: 157
    cleaning_beam_agent-5_max: 258
    cleaning_beam_agent-5_mean: 67.85
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-15-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 886.9999999999858
  episode_reward_mean: 666.2099999999946
  episode_reward_min: 326.0000000000003
  episodes_this_iter: 96
  episodes_total: 14496
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12482.527
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 1.0503263473510742
        entropy_coeff: 0.0017600000137463212
        kl: 0.010405557230114937
        model: {}
        policy_loss: -0.02787381038069725
        total_loss: -0.02625976875424385
        vf_explained_var: 0.0662488043308258
        vf_loss: 13.815042495727539
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 1.1694064140319824
        entropy_coeff: 0.0017600000137463212
        kl: 0.01214826200157404
        model: {}
        policy_loss: -0.033187076449394226
        total_loss: -0.031319472938776016
        vf_explained_var: -0.010125517845153809
        vf_loss: 14.961042404174805
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 1.1260939836502075
        entropy_coeff: 0.0017600000137463212
        kl: 0.011714404448866844
        model: {}
        policy_loss: -0.029246799647808075
        total_loss: -0.027518481016159058
        vf_explained_var: 0.07467232644557953
        vf_loss: 13.673603057861328
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 0.6763476729393005
        entropy_coeff: 0.0017600000137463212
        kl: 0.00752348592504859
        model: {}
        policy_loss: -0.02045483887195587
        total_loss: -0.018880881369113922
        vf_explained_var: 0.14730694890022278
        vf_loss: 12.59628677368164
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 1.0899161100387573
        entropy_coeff: 0.0017600000137463212
        kl: 0.012491729110479355
        model: {}
        policy_loss: -0.034831464290618896
        total_loss: -0.03283468261361122
        vf_explained_var: 0.04166235029697418
        vf_loss: 14.166851997375488
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00036144000478088856
        entropy: 0.9477810859680176
        entropy_coeff: 0.0017600000137463212
        kl: 0.011074828915297985
        model: {}
        policy_loss: -0.03168453276157379
        total_loss: -0.02984636276960373
        vf_explained_var: 0.1265108287334442
        vf_loss: 12.913012504577637
    load_time_ms: 13577.779
    num_steps_sampled: 14496000
    num_steps_trained: 14496000
    sample_time_ms: 107277.071
    update_time_ms: 396.936
  iterations_since_restore: 91
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.72287234042553
    ram_util_percent: 16.305851063829788
  pid: 30948
  policy_reward_max:
    agent-0: 147.8333333333334
    agent-1: 147.8333333333334
    agent-2: 147.8333333333334
    agent-3: 147.8333333333334
    agent-4: 147.8333333333334
    agent-5: 147.8333333333334
  policy_reward_mean:
    agent-0: 111.03500000000037
    agent-1: 111.03500000000037
    agent-2: 111.03500000000037
    agent-3: 111.03500000000037
    agent-4: 111.03500000000037
    agent-5: 111.03500000000037
  policy_reward_min:
    agent-0: 54.3333333333332
    agent-1: 54.3333333333332
    agent-2: 54.3333333333332
    agent-3: 54.3333333333332
    agent-4: 54.3333333333332
    agent-5: 54.3333333333332
  sampler_perf:
    mean_env_wait_ms: 27.580018619497466
    mean_inference_ms: 13.06862475423839
    mean_processing_ms: 59.12598691177854
  time_since_restore: 12663.808353185654
  time_this_iter_s: 131.72866249084473
  time_total_s: 21789.820167064667
  timestamp: 1637039754
  timesteps_since_restore: 8736000
  timesteps_this_iter: 96000
  timesteps_total: 14496000
  training_iteration: 151
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    151 |          21789.8 | 14496000 |   666.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 2.76
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 26.05
    apples_agent-1_min: 0
    apples_agent-2_max: 110
    apples_agent-2_mean: 9.4
    apples_agent-2_min: 0
    apples_agent-3_max: 403
    apples_agent-3_mean: 86.11
    apples_agent-3_min: 25
    apples_agent-4_max: 36
    apples_agent-4_mean: 2.63
    apples_agent-4_min: 0
    apples_agent-5_max: 125
    apples_agent-5_mean: 74.9
    apples_agent-5_min: 3
    cleaning_beam_agent-0_max: 531
    cleaning_beam_agent-0_mean: 331.06
    cleaning_beam_agent-0_min: 182
    cleaning_beam_agent-1_max: 382
    cleaning_beam_agent-1_mean: 204.52
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 482
    cleaning_beam_agent-2_mean: 317.5
    cleaning_beam_agent-2_min: 124
    cleaning_beam_agent-3_max: 120
    cleaning_beam_agent-3_mean: 52.96
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 395
    cleaning_beam_agent-4_mean: 292.06
    cleaning_beam_agent-4_min: 209
    cleaning_beam_agent-5_max: 320
    cleaning_beam_agent-5_mean: 62.4
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 8
    fire_beam_agent-0_mean: 0.09
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-18-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 922.9999999999851
  episode_reward_mean: 648.6499999999943
  episode_reward_min: 193.9999999999989
  episodes_this_iter: 96
  episodes_total: 14592
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12481.18
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 1.0699008703231812
        entropy_coeff: 0.0017600000137463212
        kl: 0.009997108019888401
        model: {}
        policy_loss: -0.028702236711978912
        total_loss: -0.027161112055182457
        vf_explained_var: 0.15171502530574799
        vf_loss: 14.247319221496582
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 1.192631483078003
        entropy_coeff: 0.0017600000137463212
        kl: 0.011688959784805775
        model: {}
        policy_loss: -0.03218946233391762
        total_loss: -0.030285965651273727
        vf_explained_var: 0.008989408612251282
        vf_loss: 16.647361755371094
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 1.155001163482666
        entropy_coeff: 0.0017600000137463212
        kl: 0.012694036588072777
        model: {}
        policy_loss: -0.030168509110808372
        total_loss: -0.028118211776018143
        vf_explained_var: 0.07956978678703308
        vf_loss: 15.44291877746582
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 0.6948444843292236
        entropy_coeff: 0.0017600000137463212
        kl: 0.008046325296163559
        model: {}
        policy_loss: -0.021881399676203728
        total_loss: -0.020088793709874153
        vf_explained_var: 0.16168621182441711
        vf_loss: 14.062684059143066
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 1.0866823196411133
        entropy_coeff: 0.0017600000137463212
        kl: 0.01302347518503666
        model: {}
        policy_loss: -0.03517637029290199
        total_loss: -0.032892920076847076
        vf_explained_var: 0.051685065031051636
        vf_loss: 15.91313648223877
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003554495924618095
        entropy: 0.9494949579238892
        entropy_coeff: 0.0017600000137463212
        kl: 0.010919800028204918
        model: {}
        policy_loss: -0.030922284349799156
        total_loss: -0.029005106538534164
        vf_explained_var: 0.16315224766731262
        vf_loss: 14.04330062866211
    load_time_ms: 13596.004
    num_steps_sampled: 14592000
    num_steps_trained: 14592000
    sample_time_ms: 107276.746
    update_time_ms: 396.907
  iterations_since_restore: 92
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.543229166666666
    ram_util_percent: 16.258854166666666
  pid: 30948
  policy_reward_max:
    agent-0: 153.83333333333331
    agent-1: 153.83333333333331
    agent-2: 153.83333333333331
    agent-3: 153.83333333333331
    agent-4: 153.83333333333331
    agent-5: 153.83333333333331
  policy_reward_mean:
    agent-0: 108.10833333333366
    agent-1: 108.10833333333366
    agent-2: 108.10833333333366
    agent-3: 108.10833333333366
    agent-4: 108.10833333333366
    agent-5: 108.10833333333366
  policy_reward_min:
    agent-0: 32.333333333333385
    agent-1: 32.333333333333385
    agent-2: 32.333333333333385
    agent-3: 32.333333333333385
    agent-4: 32.333333333333385
    agent-5: 32.333333333333385
  sampler_perf:
    mean_env_wait_ms: 27.57220446533284
    mean_inference_ms: 13.067238025430692
    mean_processing_ms: 59.116490466379325
  time_since_restore: 12798.239707231522
  time_this_iter_s: 134.43135404586792
  time_total_s: 21924.251521110535
  timestamp: 1637039889
  timesteps_since_restore: 8832000
  timesteps_this_iter: 96000
  timesteps_total: 14592000
  training_iteration: 152
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.3/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    152 |          21924.3 | 14592000 |   648.65 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 4.04
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 26.36
    apples_agent-1_min: 0
    apples_agent-2_max: 75
    apples_agent-2_mean: 13.67
    apples_agent-2_min: 0
    apples_agent-3_max: 170
    apples_agent-3_mean: 82.57
    apples_agent-3_min: 13
    apples_agent-4_max: 27
    apples_agent-4_mean: 1.56
    apples_agent-4_min: 0
    apples_agent-5_max: 132
    apples_agent-5_mean: 74.04
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 512
    cleaning_beam_agent-0_mean: 320.47
    cleaning_beam_agent-0_min: 128
    cleaning_beam_agent-1_max: 358
    cleaning_beam_agent-1_mean: 210.63
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 585
    cleaning_beam_agent-2_mean: 304.35
    cleaning_beam_agent-2_min: 52
    cleaning_beam_agent-3_max: 128
    cleaning_beam_agent-3_mean: 52.4
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 421
    cleaning_beam_agent-4_mean: 303.99
    cleaning_beam_agent-4_min: 230
    cleaning_beam_agent-5_max: 329
    cleaning_beam_agent-5_mean: 72.11
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-20-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 878.9999999999771
  episode_reward_mean: 661.2899999999947
  episode_reward_min: 334.00000000000375
  episodes_this_iter: 96
  episodes_total: 14688
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12497.126
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 1.0827605724334717
        entropy_coeff: 0.0017600000137463212
        kl: 0.010288357734680176
        model: {}
        policy_loss: -0.029293213039636612
        total_loss: -0.027698468416929245
        vf_explained_var: 0.1073993444442749
        vf_loss: 14.42732048034668
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 1.1954022645950317
        entropy_coeff: 0.0017600000137463212
        kl: 0.012060790322721004
        model: {}
        policy_loss: -0.033220697194337845
        total_loss: -0.0313105471432209
        vf_explained_var: 0.009422644972801208
        vf_loss: 16.018983840942383
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 1.1253342628479004
        entropy_coeff: 0.0017600000137463212
        kl: 0.01180986873805523
        model: {}
        policy_loss: -0.029723167419433594
        total_loss: -0.027781473472714424
        vf_explained_var: 0.03464692831039429
        vf_loss: 15.603076934814453
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 0.6809158325195312
        entropy_coeff: 0.0017600000137463212
        kl: 0.008160066790878773
        model: {}
        policy_loss: -0.019791681319475174
        total_loss: -0.018051376566290855
        vf_explained_var: 0.19170035421848297
        vf_loss: 13.06704330444336
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 1.0810253620147705
        entropy_coeff: 0.0017600000137463212
        kl: 0.011897996068000793
        model: {}
        policy_loss: -0.03380291908979416
        total_loss: -0.031803641468286514
        vf_explained_var: 0.05871342122554779
        vf_loss: 15.22282600402832
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00034945920924656093
        entropy: 0.9565842747688293
        entropy_coeff: 0.0017600000137463212
        kl: 0.01182791218161583
        model: {}
        policy_loss: -0.03195460885763168
        total_loss: -0.029865503311157227
        vf_explained_var: 0.1288074553012848
        vf_loss: 14.071094512939453
    load_time_ms: 13584.666
    num_steps_sampled: 14688000
    num_steps_trained: 14688000
    sample_time_ms: 107271.781
    update_time_ms: 396.858
  iterations_since_restore: 93
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.50531914893617
    ram_util_percent: 16.31648936170213
  pid: 30948
  policy_reward_max:
    agent-0: 146.50000000000006
    agent-1: 146.50000000000006
    agent-2: 146.50000000000006
    agent-3: 146.50000000000006
    agent-4: 146.50000000000006
    agent-5: 146.50000000000006
  policy_reward_mean:
    agent-0: 110.21500000000039
    agent-1: 110.21500000000039
    agent-2: 110.21500000000039
    agent-3: 110.21500000000039
    agent-4: 110.21500000000039
    agent-5: 110.21500000000039
  policy_reward_min:
    agent-0: 55.66666666666654
    agent-1: 55.66666666666654
    agent-2: 55.66666666666654
    agent-3: 55.66666666666654
    agent-4: 55.66666666666654
    agent-5: 55.66666666666654
  sampler_perf:
    mean_env_wait_ms: 27.565436918680906
    mean_inference_ms: 13.066555702655096
    mean_processing_ms: 59.1052918187726
  time_since_restore: 12932.389275312424
  time_this_iter_s: 134.1495680809021
  time_total_s: 22058.401089191437
  timestamp: 1637040023
  timesteps_since_restore: 8928000
  timesteps_this_iter: 96000
  timesteps_total: 14688000
  training_iteration: 153
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    153 |          22058.4 | 14688000 |   661.29 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 4.63
    apples_agent-0_min: 0
    apples_agent-1_max: 125
    apples_agent-1_mean: 27.46
    apples_agent-1_min: 0
    apples_agent-2_max: 291
    apples_agent-2_mean: 15.69
    apples_agent-2_min: 0
    apples_agent-3_max: 130
    apples_agent-3_mean: 78.78
    apples_agent-3_min: 31
    apples_agent-4_max: 52
    apples_agent-4_mean: 2.11
    apples_agent-4_min: 0
    apples_agent-5_max: 110
    apples_agent-5_mean: 71.24
    apples_agent-5_min: 3
    cleaning_beam_agent-0_max: 503
    cleaning_beam_agent-0_mean: 324.8
    cleaning_beam_agent-0_min: 133
    cleaning_beam_agent-1_max: 363
    cleaning_beam_agent-1_mean: 193.92
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 495
    cleaning_beam_agent-2_mean: 309.42
    cleaning_beam_agent-2_min: 30
    cleaning_beam_agent-3_max: 108
    cleaning_beam_agent-3_mean: 59.46
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 425
    cleaning_beam_agent-4_mean: 309.54
    cleaning_beam_agent-4_min: 161
    cleaning_beam_agent-5_max: 229
    cleaning_beam_agent-5_mean: 64.5
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 4
    fire_beam_agent-0_mean: 0.07
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-22-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 927.9999999999812
  episode_reward_mean: 634.0499999999952
  episode_reward_min: 241.99999999999685
  episodes_this_iter: 96
  episodes_total: 14784
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12470.7
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 1.0721330642700195
        entropy_coeff: 0.0017600000137463212
        kl: 0.010264219716191292
        model: {}
        policy_loss: -0.027373410761356354
        total_loss: -0.025693656876683235
        vf_explained_var: 0.16377316415309906
        vf_loss: 15.138615608215332
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 1.1800901889801025
        entropy_coeff: 0.0017600000137463212
        kl: 0.012136371806263924
        model: {}
        policy_loss: -0.03319897502660751
        total_loss: -0.03108634613454342
        vf_explained_var: 0.02838948369026184
        vf_loss: 17.623178482055664
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 1.1172868013381958
        entropy_coeff: 0.0017600000137463212
        kl: 0.011429393664002419
        model: {}
        policy_loss: -0.030051689594984055
        total_loss: -0.028074849396944046
        vf_explained_var: 0.0845213383436203
        vf_loss: 16.573875427246094
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 0.7058923244476318
        entropy_coeff: 0.0017600000137463212
        kl: 0.008738724514842033
        model: {}
        policy_loss: -0.021907519549131393
        total_loss: -0.020023098215460777
        vf_explained_var: 0.2375892996788025
        vf_loss: 13.790473937988281
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 1.0763297080993652
        entropy_coeff: 0.0017600000137463212
        kl: 0.012270196340978146
        model: {}
        policy_loss: -0.03470516949892044
        total_loss: -0.03250046819448471
        vf_explained_var: 0.09108583629131317
        vf_loss: 16.450057983398438
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003434687969274819
        entropy: 0.9631264209747314
        entropy_coeff: 0.0017600000137463212
        kl: 0.011996548622846603
        model: {}
        policy_loss: -0.03161449730396271
        total_loss: -0.02944178134202957
        vf_explained_var: 0.18823248147964478
        vf_loss: 14.685066223144531
    load_time_ms: 13568.881
    num_steps_sampled: 14784000
    num_steps_trained: 14784000
    sample_time_ms: 107253.888
    update_time_ms: 60.264
  iterations_since_restore: 94
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.594210526315788
    ram_util_percent: 16.306315789473683
  pid: 30948
  policy_reward_max:
    agent-0: 154.66666666666686
    agent-1: 154.66666666666686
    agent-2: 154.66666666666686
    agent-3: 154.66666666666686
    agent-4: 154.66666666666686
    agent-5: 154.66666666666686
  policy_reward_mean:
    agent-0: 105.6750000000003
    agent-1: 105.6750000000003
    agent-2: 105.6750000000003
    agent-3: 105.6750000000003
    agent-4: 105.6750000000003
    agent-5: 105.6750000000003
  policy_reward_min:
    agent-0: 40.333333333333314
    agent-1: 40.333333333333314
    agent-2: 40.333333333333314
    agent-3: 40.333333333333314
    agent-4: 40.333333333333314
    agent-5: 40.333333333333314
  sampler_perf:
    mean_env_wait_ms: 27.556389738159766
    mean_inference_ms: 13.064706992734859
    mean_processing_ms: 59.09260908834962
  time_since_restore: 13066.338105201721
  time_this_iter_s: 133.94882988929749
  time_total_s: 22192.349919080734
  timestamp: 1637040157
  timesteps_since_restore: 9024000
  timesteps_this_iter: 96000
  timesteps_total: 14784000
  training_iteration: 154
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    154 |          22192.3 | 14784000 |   634.05 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 2.75
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 25.68
    apples_agent-1_min: 0
    apples_agent-2_max: 98
    apples_agent-2_mean: 15.44
    apples_agent-2_min: 0
    apples_agent-3_max: 161
    apples_agent-3_mean: 86.25
    apples_agent-3_min: 28
    apples_agent-4_max: 193
    apples_agent-4_mean: 4.47
    apples_agent-4_min: 0
    apples_agent-5_max: 123
    apples_agent-5_mean: 76.13
    apples_agent-5_min: 27
    cleaning_beam_agent-0_max: 485
    cleaning_beam_agent-0_mean: 333.22
    cleaning_beam_agent-0_min: 186
    cleaning_beam_agent-1_max: 382
    cleaning_beam_agent-1_mean: 203.69
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 487
    cleaning_beam_agent-2_mean: 313.14
    cleaning_beam_agent-2_min: 41
    cleaning_beam_agent-3_max: 121
    cleaning_beam_agent-3_mean: 57.69
    cleaning_beam_agent-3_min: 25
    cleaning_beam_agent-4_max: 428
    cleaning_beam_agent-4_mean: 314.18
    cleaning_beam_agent-4_min: 188
    cleaning_beam_agent-5_max: 318
    cleaning_beam_agent-5_mean: 63.61
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-24-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 862.9999999999744
  episode_reward_mean: 659.1099999999947
  episode_reward_min: 356.00000000000057
  episodes_this_iter: 96
  episodes_total: 14880
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12447.858
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 1.0740201473236084
        entropy_coeff: 0.0017600000137463212
        kl: 0.009810304269194603
        model: {}
        policy_loss: -0.027044419199228287
        total_loss: -0.025560669600963593
        vf_explained_var: 0.11631383001804352
        vf_loss: 14.119627952575684
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 1.1755454540252686
        entropy_coeff: 0.0017600000137463212
        kl: 0.011463334783911705
        model: {}
        policy_loss: -0.031786881387233734
        total_loss: -0.029979249462485313
        vf_explained_var: 0.00833180546760559
        vf_loss: 15.839210510253906
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 1.1169703006744385
        entropy_coeff: 0.0017600000137463212
        kl: 0.01121322251856327
        model: {}
        policy_loss: -0.029888689517974854
        total_loss: -0.028137939050793648
        vf_explained_var: 0.07668404281139374
        vf_loss: 14.739778518676758
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 0.6909124255180359
        entropy_coeff: 0.0017600000137463212
        kl: 0.007750446908175945
        model: {}
        policy_loss: -0.021331366151571274
        total_loss: -0.01968955062329769
        vf_explained_var: 0.18115074932575226
        vf_loss: 13.077274322509766
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 1.0881892442703247
        entropy_coeff: 0.0017600000137463212
        kl: 0.0125524140894413
        model: {}
        policy_loss: -0.03337632119655609
        total_loss: -0.03127991035580635
        vf_explained_var: 0.05970361828804016
        vf_loss: 15.01133918762207
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003374784137122333
        entropy: 0.9725379347801208
        entropy_coeff: 0.0017600000137463212
        kl: 0.011663337238132954
        model: {}
        policy_loss: -0.03199039772152901
        total_loss: -0.029967226088047028
        vf_explained_var: 0.12154212594032288
        vf_loss: 14.021709442138672
    load_time_ms: 13580.782
    num_steps_sampled: 14880000
    num_steps_trained: 14880000
    sample_time_ms: 107194.239
    update_time_ms: 60.205
  iterations_since_restore: 95
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.593684210526316
    ram_util_percent: 16.323157894736845
  pid: 30948
  policy_reward_max:
    agent-0: 143.83333333333326
    agent-1: 143.83333333333326
    agent-2: 143.83333333333326
    agent-3: 143.83333333333326
    agent-4: 143.83333333333326
    agent-5: 143.83333333333326
  policy_reward_mean:
    agent-0: 109.85166666666697
    agent-1: 109.85166666666697
    agent-2: 109.85166666666697
    agent-3: 109.85166666666697
    agent-4: 109.85166666666697
    agent-5: 109.85166666666697
  policy_reward_min:
    agent-0: 59.33333333333317
    agent-1: 59.33333333333317
    agent-2: 59.33333333333317
    agent-3: 59.33333333333317
    agent-4: 59.33333333333317
    agent-5: 59.33333333333317
  sampler_perf:
    mean_env_wait_ms: 27.548224275099866
    mean_inference_ms: 13.062626137452975
    mean_processing_ms: 59.07701868883532
  time_since_restore: 13198.955701828003
  time_this_iter_s: 132.61759662628174
  time_total_s: 22324.967515707016
  timestamp: 1637040290
  timesteps_since_restore: 9120000
  timesteps_this_iter: 96000
  timesteps_total: 14880000
  training_iteration: 155
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    155 |            22325 | 14880000 |   659.11 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 3.15
    apples_agent-0_min: 0
    apples_agent-1_max: 143
    apples_agent-1_mean: 26.17
    apples_agent-1_min: 0
    apples_agent-2_max: 98
    apples_agent-2_mean: 10.42
    apples_agent-2_min: 0
    apples_agent-3_max: 143
    apples_agent-3_mean: 74.72
    apples_agent-3_min: 22
    apples_agent-4_max: 49
    apples_agent-4_mean: 3.12
    apples_agent-4_min: 0
    apples_agent-5_max: 169
    apples_agent-5_mean: 76.61
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 515
    cleaning_beam_agent-0_mean: 336.31
    cleaning_beam_agent-0_min: 198
    cleaning_beam_agent-1_max: 350
    cleaning_beam_agent-1_mean: 187.49
    cleaning_beam_agent-1_min: 58
    cleaning_beam_agent-2_max: 502
    cleaning_beam_agent-2_mean: 321.09
    cleaning_beam_agent-2_min: 130
    cleaning_beam_agent-3_max: 101
    cleaning_beam_agent-3_mean: 51.07
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 412
    cleaning_beam_agent-4_mean: 305.96
    cleaning_beam_agent-4_min: 197
    cleaning_beam_agent-5_max: 169
    cleaning_beam_agent-5_mean: 62.06
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-27-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 859.9999999999794
  episode_reward_mean: 652.6199999999948
  episode_reward_min: 373.000000000006
  episodes_this_iter: 96
  episodes_total: 14976
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12427.632
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 1.0745773315429688
        entropy_coeff: 0.0017600000137463212
        kl: 0.009602099657058716
        model: {}
        policy_loss: -0.02659423090517521
        total_loss: -0.025091078132390976
        vf_explained_var: 0.11365556716918945
        vf_loss: 14.739924430847168
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 1.1755244731903076
        entropy_coeff: 0.0017600000137463212
        kl: 0.011255832388997078
        model: {}
        policy_loss: -0.03154207393527031
        total_loss: -0.029703784734010696
        vf_explained_var: 0.004948645830154419
        vf_loss: 16.560449600219727
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 1.1399602890014648
        entropy_coeff: 0.0017600000137463212
        kl: 0.011356604285538197
        model: {}
        policy_loss: -0.028586015105247498
        total_loss: -0.02678399533033371
        vf_explained_var: 0.07483716309070587
        vf_loss: 15.370274543762207
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 0.6694505214691162
        entropy_coeff: 0.0017600000137463212
        kl: 0.007566043175756931
        model: {}
        policy_loss: -0.020854279398918152
        total_loss: -0.019177744165062904
        vf_explained_var: 0.19204583764076233
        vf_loss: 13.415605545043945
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 1.0832953453063965
        entropy_coeff: 0.0017600000137463212
        kl: 0.011694665998220444
        model: {}
        policy_loss: -0.03387834504246712
        total_loss: -0.03188124671578407
        vf_explained_var: 0.0594208687543869
        vf_loss: 15.64761734008789
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00033148800139315426
        entropy: 0.9607762098312378
        entropy_coeff: 0.0017600000137463212
        kl: 0.010573240928351879
        model: {}
        policy_loss: -0.029688263311982155
        total_loss: -0.027903025969862938
        vf_explained_var: 0.18027472496032715
        vf_loss: 13.615527153015137
    load_time_ms: 13573.456
    num_steps_sampled: 14976000
    num_steps_trained: 14976000
    sample_time_ms: 107125.742
    update_time_ms: 60.376
  iterations_since_restore: 96
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.712698412698412
    ram_util_percent: 16.32116402116402
  pid: 30948
  policy_reward_max:
    agent-0: 143.33333333333331
    agent-1: 143.33333333333331
    agent-2: 143.33333333333331
    agent-3: 143.33333333333331
    agent-4: 143.33333333333331
    agent-5: 143.33333333333331
  policy_reward_mean:
    agent-0: 108.7700000000003
    agent-1: 108.7700000000003
    agent-2: 108.7700000000003
    agent-3: 108.7700000000003
    agent-4: 108.7700000000003
    agent-5: 108.7700000000003
  policy_reward_min:
    agent-0: 62.16666666666637
    agent-1: 62.16666666666637
    agent-2: 62.16666666666637
    agent-3: 62.16666666666637
    agent-4: 62.16666666666637
    agent-5: 62.16666666666637
  sampler_perf:
    mean_env_wait_ms: 27.540502094962154
    mean_inference_ms: 13.060714194264046
    mean_processing_ms: 59.06407084519893
  time_since_restore: 13331.755836248398
  time_this_iter_s: 132.8001344203949
  time_total_s: 22457.76765012741
  timestamp: 1637040423
  timesteps_since_restore: 9216000
  timesteps_this_iter: 96000
  timesteps_total: 14976000
  training_iteration: 156
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    156 |          22457.8 | 14976000 |   652.62 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 2.73
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 27.96
    apples_agent-1_min: 0
    apples_agent-2_max: 98
    apples_agent-2_mean: 12.46
    apples_agent-2_min: 0
    apples_agent-3_max: 168
    apples_agent-3_mean: 77.73
    apples_agent-3_min: 35
    apples_agent-4_max: 44
    apples_agent-4_mean: 2.87
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 72.7
    apples_agent-5_min: 4
    cleaning_beam_agent-0_max: 482
    cleaning_beam_agent-0_mean: 340.51
    cleaning_beam_agent-0_min: 206
    cleaning_beam_agent-1_max: 478
    cleaning_beam_agent-1_mean: 187.4
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 467
    cleaning_beam_agent-2_mean: 324.65
    cleaning_beam_agent-2_min: 93
    cleaning_beam_agent-3_max: 126
    cleaning_beam_agent-3_mean: 53.31
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 404
    cleaning_beam_agent-4_mean: 307.69
    cleaning_beam_agent-4_min: 210
    cleaning_beam_agent-5_max: 207
    cleaning_beam_agent-5_mean: 58.38
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-29-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 888.9999999999723
  episode_reward_mean: 664.8199999999935
  episode_reward_min: 208.99999999999773
  episodes_this_iter: 96
  episodes_total: 15072
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12441.893
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 1.0591353178024292
        entropy_coeff: 0.0017600000137463212
        kl: 0.00952752586454153
        model: {}
        policy_loss: -0.02695426717400551
        total_loss: -0.025454824790358543
        vf_explained_var: 0.11324992775917053
        vf_loss: 14.580184936523438
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 1.1712054014205933
        entropy_coeff: 0.0017600000137463212
        kl: 0.011528344824910164
        model: {}
        policy_loss: -0.03214217722415924
        total_loss: -0.03029494918882847
        vf_explained_var: 0.026485785841941833
        vf_loss: 16.028770446777344
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 1.110931396484375
        entropy_coeff: 0.0017600000137463212
        kl: 0.010929090902209282
        model: {}
        policy_loss: -0.028412630781531334
        total_loss: -0.0266091451048851
        vf_explained_var: 0.044337451457977295
        vf_loss: 15.729040145874023
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 0.6613029837608337
        entropy_coeff: 0.0017600000137463212
        kl: 0.007723386399447918
        model: {}
        policy_loss: -0.020990023389458656
        total_loss: -0.019256092607975006
        vf_explained_var: 0.17772962152957916
        vf_loss: 13.531469345092773
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 1.086951494216919
        entropy_coeff: 0.0017600000137463212
        kl: 0.012509745545685291
        model: {}
        policy_loss: -0.03440852090716362
        total_loss: -0.032284095883369446
        vf_explained_var: 0.06729169189929962
        vf_loss: 15.355056762695312
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003254975890740752
        entropy: 0.9504431486129761
        entropy_coeff: 0.0017600000137463212
        kl: 0.0107314707711339
        model: {}
        policy_loss: -0.028836634010076523
        total_loss: -0.02696879766881466
        vf_explained_var: 0.15187495946884155
        vf_loss: 13.943229675292969
    load_time_ms: 13585.642
    num_steps_sampled: 15072000
    num_steps_trained: 15072000
    sample_time_ms: 106886.228
    update_time_ms: 53.981
  iterations_since_restore: 97
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.695789473684204
    ram_util_percent: 16.309473684210527
  pid: 30948
  policy_reward_max:
    agent-0: 148.16666666666677
    agent-1: 148.16666666666677
    agent-2: 148.16666666666677
    agent-3: 148.16666666666677
    agent-4: 148.16666666666677
    agent-5: 148.16666666666677
  policy_reward_mean:
    agent-0: 110.80333333333368
    agent-1: 110.80333333333368
    agent-2: 110.80333333333368
    agent-3: 110.80333333333368
    agent-4: 110.80333333333368
    agent-5: 110.80333333333368
  policy_reward_min:
    agent-0: 34.833333333333364
    agent-1: 34.833333333333364
    agent-2: 34.833333333333364
    agent-3: 34.833333333333364
    agent-4: 34.833333333333364
    agent-5: 34.833333333333364
  sampler_perf:
    mean_env_wait_ms: 27.532990362120795
    mean_inference_ms: 13.059727089611517
    mean_processing_ms: 59.05294261156425
  time_since_restore: 13464.489612579346
  time_this_iter_s: 132.73377633094788
  time_total_s: 22590.50142645836
  timestamp: 1637040556
  timesteps_since_restore: 9312000
  timesteps_this_iter: 96000
  timesteps_total: 15072000
  training_iteration: 157
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    157 |          22590.5 | 15072000 |   664.82 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 3.45
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 26.1
    apples_agent-1_min: 0
    apples_agent-2_max: 110
    apples_agent-2_mean: 15.08
    apples_agent-2_min: 0
    apples_agent-3_max: 128
    apples_agent-3_mean: 75.6
    apples_agent-3_min: 20
    apples_agent-4_max: 57
    apples_agent-4_mean: 2.18
    apples_agent-4_min: 0
    apples_agent-5_max: 126
    apples_agent-5_mean: 73.57
    apples_agent-5_min: 13
    cleaning_beam_agent-0_max: 490
    cleaning_beam_agent-0_mean: 334.87
    cleaning_beam_agent-0_min: 145
    cleaning_beam_agent-1_max: 369
    cleaning_beam_agent-1_mean: 205.88
    cleaning_beam_agent-1_min: 101
    cleaning_beam_agent-2_max: 488
    cleaning_beam_agent-2_mean: 299.31
    cleaning_beam_agent-2_min: 65
    cleaning_beam_agent-3_max: 125
    cleaning_beam_agent-3_mean: 59.8
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 400
    cleaning_beam_agent-4_mean: 317.69
    cleaning_beam_agent-4_min: 210
    cleaning_beam_agent-5_max: 194
    cleaning_beam_agent-5_mean: 62.51
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-31-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 856.9999999999735
  episode_reward_mean: 651.6599999999949
  episode_reward_min: 225.999999999997
  episodes_this_iter: 96
  episodes_total: 15168
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12459.647
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 1.0797648429870605
        entropy_coeff: 0.0017600000137463212
        kl: 0.008967923000454903
        model: {}
        policy_loss: -0.026950808241963387
        total_loss: -0.025556504726409912
        vf_explained_var: 0.09346480667591095
        vf_loss: 15.011027336120605
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 1.1751400232315063
        entropy_coeff: 0.0017600000137463212
        kl: 0.011218765750527382
        model: {}
        policy_loss: -0.03161662817001343
        total_loss: -0.029832761734724045
        vf_explained_var: 0.028190433979034424
        vf_loss: 16.0836124420166
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 1.114823341369629
        entropy_coeff: 0.0017600000137463212
        kl: 0.012146839872002602
        model: {}
        policy_loss: -0.029374979436397552
        total_loss: -0.027416696771979332
        vf_explained_var: 0.09755370020866394
        vf_loss: 14.91003131866455
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 0.673210859298706
        entropy_coeff: 0.0017600000137463212
        kl: 0.0076604681089520454
        model: {}
        policy_loss: -0.020112978294491768
        total_loss: -0.018415017053484917
        vf_explained_var: 0.1833256483078003
        vf_loss: 13.50716781616211
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 1.0727653503417969
        entropy_coeff: 0.0017600000137463212
        kl: 0.011795694008469582
        model: {}
        policy_loss: -0.033256933093070984
        total_loss: -0.031208358705043793
        vf_explained_var: 0.04563465714454651
        vf_loss: 15.775018692016602
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00031950720585882664
        entropy: 0.9577901363372803
        entropy_coeff: 0.0017600000137463212
        kl: 0.011320230551064014
        model: {}
        policy_loss: -0.029735108837485313
        total_loss: -0.027778074145317078
        vf_explained_var: 0.16472330689430237
        vf_loss: 13.78701400756836
    load_time_ms: 13607.557
    num_steps_sampled: 15168000
    num_steps_trained: 15168000
    sample_time_ms: 106958.258
    update_time_ms: 40.941
  iterations_since_restore: 98
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.970157068062825
    ram_util_percent: 16.368586387434554
  pid: 30948
  policy_reward_max:
    agent-0: 142.83333333333346
    agent-1: 142.83333333333346
    agent-2: 142.83333333333346
    agent-3: 142.83333333333346
    agent-4: 142.83333333333346
    agent-5: 142.83333333333346
  policy_reward_mean:
    agent-0: 108.61000000000033
    agent-1: 108.61000000000033
    agent-2: 108.61000000000033
    agent-3: 108.61000000000033
    agent-4: 108.61000000000033
    agent-5: 108.61000000000033
  policy_reward_min:
    agent-0: 37.66666666666664
    agent-1: 37.66666666666664
    agent-2: 37.66666666666664
    agent-3: 37.66666666666664
    agent-4: 37.66666666666664
    agent-5: 37.66666666666664
  sampler_perf:
    mean_env_wait_ms: 27.527345483371686
    mean_inference_ms: 13.058470246843918
    mean_processing_ms: 59.045481429523605
  time_since_restore: 13598.331669330597
  time_this_iter_s: 133.84205675125122
  time_total_s: 22724.34348320961
  timestamp: 1637040690
  timesteps_since_restore: 9408000
  timesteps_this_iter: 96000
  timesteps_total: 15168000
  training_iteration: 158
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    158 |          22724.3 | 15168000 |   651.66 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 3.68
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 27.87
    apples_agent-1_min: 0
    apples_agent-2_max: 117
    apples_agent-2_mean: 12.53
    apples_agent-2_min: 0
    apples_agent-3_max: 121
    apples_agent-3_mean: 76.42
    apples_agent-3_min: 35
    apples_agent-4_max: 70
    apples_agent-4_mean: 3.48
    apples_agent-4_min: 0
    apples_agent-5_max: 124
    apples_agent-5_mean: 74.89
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 539
    cleaning_beam_agent-0_mean: 327.9
    cleaning_beam_agent-0_min: 136
    cleaning_beam_agent-1_max: 391
    cleaning_beam_agent-1_mean: 207.74
    cleaning_beam_agent-1_min: 89
    cleaning_beam_agent-2_max: 572
    cleaning_beam_agent-2_mean: 304.05
    cleaning_beam_agent-2_min: 119
    cleaning_beam_agent-3_max: 167
    cleaning_beam_agent-3_mean: 58.13
    cleaning_beam_agent-3_min: 22
    cleaning_beam_agent-4_max: 413
    cleaning_beam_agent-4_mean: 318.18
    cleaning_beam_agent-4_min: 219
    cleaning_beam_agent-5_max: 186
    cleaning_beam_agent-5_mean: 66.0
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.03
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-33-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 883.999999999942
  episode_reward_mean: 664.9099999999946
  episode_reward_min: 367.0000000000054
  episodes_this_iter: 96
  episodes_total: 15264
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12508.226
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 1.0749859809875488
        entropy_coeff: 0.0017600000137463212
        kl: 0.009189631789922714
        model: {}
        policy_loss: -0.026800738647580147
        total_loss: -0.025324609130620956
        vf_explained_var: 0.06103752553462982
        vf_loss: 15.301803588867188
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 1.1600314378738403
        entropy_coeff: 0.0017600000137463212
        kl: 0.0108338613063097
        model: {}
        policy_loss: -0.030645012855529785
        total_loss: -0.028869986534118652
        vf_explained_var: -0.01170782744884491
        vf_loss: 16.49912452697754
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 1.1391348838806152
        entropy_coeff: 0.0017600000137463212
        kl: 0.0112254424020648
        model: {}
        policy_loss: -0.028843771666288376
        total_loss: -0.027086490765213966
        vf_explained_var: 0.06888556480407715
        vf_loss: 15.170734405517578
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 0.6659035682678223
        entropy_coeff: 0.0017600000137463212
        kl: 0.007804923690855503
        model: {}
        policy_loss: -0.02077733352780342
        total_loss: -0.019071772694587708
        vf_explained_var: 0.19206106662750244
        vf_loss: 13.165702819824219
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 1.0908359289169312
        entropy_coeff: 0.0017600000137463212
        kl: 0.011510027572512627
        model: {}
        policy_loss: -0.03301402926445007
        total_loss: -0.03108775056898594
        vf_explained_var: 0.05243931710720062
        vf_loss: 15.441425323486328
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0003135167935397476
        entropy: 0.9542701244354248
        entropy_coeff: 0.0017600000137463212
        kl: 0.010768887586891651
        model: {}
        policy_loss: -0.030136283487081528
        total_loss: -0.028295932337641716
        vf_explained_var: 0.1613059937953949
        vf_loss: 13.660893440246582
    load_time_ms: 13603.185
    num_steps_sampled: 15264000
    num_steps_trained: 15264000
    sample_time_ms: 106926.556
    update_time_ms: 40.921
  iterations_since_restore: 99
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.8031914893617
    ram_util_percent: 16.313829787234045
  pid: 30948
  policy_reward_max:
    agent-0: 147.33333333333374
    agent-1: 147.33333333333374
    agent-2: 147.33333333333374
    agent-3: 147.33333333333374
    agent-4: 147.33333333333374
    agent-5: 147.33333333333374
  policy_reward_mean:
    agent-0: 110.81833333333367
    agent-1: 110.81833333333367
    agent-2: 110.81833333333367
    agent-3: 110.81833333333367
    agent-4: 110.81833333333367
    agent-5: 110.81833333333367
  policy_reward_min:
    agent-0: 61.1666666666664
    agent-1: 61.1666666666664
    agent-2: 61.1666666666664
    agent-3: 61.1666666666664
    agent-4: 61.1666666666664
    agent-5: 61.1666666666664
  sampler_perf:
    mean_env_wait_ms: 27.519532877316724
    mean_inference_ms: 13.056770263444735
    mean_processing_ms: 59.030926123427164
  time_since_restore: 13730.65931391716
  time_this_iter_s: 132.3276445865631
  time_total_s: 22856.671127796173
  timestamp: 1637040823
  timesteps_since_restore: 9504000
  timesteps_this_iter: 96000
  timesteps_total: 15264000
  training_iteration: 159
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    159 |          22856.7 | 15264000 |   664.91 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 2.6
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 26.67
    apples_agent-1_min: 0
    apples_agent-2_max: 102
    apples_agent-2_mean: 13.05
    apples_agent-2_min: 0
    apples_agent-3_max: 140
    apples_agent-3_mean: 78.42
    apples_agent-3_min: 27
    apples_agent-4_max: 51
    apples_agent-4_mean: 2.29
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 76.38
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 442
    cleaning_beam_agent-0_mean: 326.52
    cleaning_beam_agent-0_min: 190
    cleaning_beam_agent-1_max: 591
    cleaning_beam_agent-1_mean: 192.73
    cleaning_beam_agent-1_min: 89
    cleaning_beam_agent-2_max: 557
    cleaning_beam_agent-2_mean: 313.67
    cleaning_beam_agent-2_min: 113
    cleaning_beam_agent-3_max: 140
    cleaning_beam_agent-3_mean: 57.86
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 405
    cleaning_beam_agent-4_mean: 322.98
    cleaning_beam_agent-4_min: 223
    cleaning_beam_agent-5_max: 189
    cleaning_beam_agent-5_mean: 60.03
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-35-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 855.9999999999902
  episode_reward_mean: 673.6799999999934
  episode_reward_min: 188.99999999999767
  episodes_this_iter: 96
  episodes_total: 15360
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12515.504
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 1.0827124118804932
        entropy_coeff: 0.0017600000137463212
        kl: 0.009245522320270538
        model: {}
        policy_loss: -0.027169421315193176
        total_loss: -0.02582232654094696
        vf_explained_var: 0.09394502639770508
        vf_loss: 14.035630226135254
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 1.1507322788238525
        entropy_coeff: 0.0017600000137463212
        kl: 0.010519265197217464
        model: {}
        policy_loss: -0.030773254111409187
        total_loss: -0.029188191518187523
        vf_explained_var: 0.02737194299697876
        vf_loss: 15.065008163452148
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 1.1175551414489746
        entropy_coeff: 0.0017600000137463212
        kl: 0.010465269908308983
        model: {}
        policy_loss: -0.02779304049909115
        total_loss: -0.026229236274957657
        vf_explained_var: 0.072258859872818
        vf_loss: 14.376479148864746
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 0.6725469827651978
        entropy_coeff: 0.0017600000137463212
        kl: 0.0076371654868125916
        model: {}
        policy_loss: -0.021552342921495438
        total_loss: -0.019890196621418
        vf_explained_var: 0.1484789252281189
        vf_loss: 13.18394660949707
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 1.0778112411499023
        entropy_coeff: 0.0017600000137463212
        kl: 0.011437324807047844
        model: {}
        policy_loss: -0.03158899024128914
        total_loss: -0.029689474031329155
        vf_explained_var: 0.02665993571281433
        vf_loss: 15.089972496032715
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000307526410324499
        entropy: 0.9567775130271912
        entropy_coeff: 0.0017600000137463212
        kl: 0.01089746318757534
        model: {}
        policy_loss: -0.02927505224943161
        total_loss: -0.027449099346995354
        vf_explained_var: 0.1404227316379547
        vf_loss: 13.303882598876953
    load_time_ms: 13625.121
    num_steps_sampled: 15360000
    num_steps_trained: 15360000
    sample_time_ms: 106848.512
    update_time_ms: 41.352
  iterations_since_restore: 100
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.676719576719574
    ram_util_percent: 16.332275132275132
  pid: 30948
  policy_reward_max:
    agent-0: 142.6666666666669
    agent-1: 142.6666666666669
    agent-2: 142.6666666666669
    agent-3: 142.6666666666669
    agent-4: 142.6666666666669
    agent-5: 142.6666666666669
  policy_reward_mean:
    agent-0: 112.28000000000037
    agent-1: 112.28000000000037
    agent-2: 112.28000000000037
    agent-3: 112.28000000000037
    agent-4: 112.28000000000037
    agent-5: 112.28000000000037
  policy_reward_min:
    agent-0: 31.500000000000092
    agent-1: 31.500000000000092
    agent-2: 31.500000000000092
    agent-3: 31.500000000000092
    agent-4: 31.500000000000092
    agent-5: 31.500000000000092
  sampler_perf:
    mean_env_wait_ms: 27.50923275913426
    mean_inference_ms: 13.0547858228846
    mean_processing_ms: 59.015784864404786
  time_since_restore: 13863.28644490242
  time_this_iter_s: 132.62713098526
  time_total_s: 22989.298258781433
  timestamp: 1637040955
  timesteps_since_restore: 9600000
  timesteps_this_iter: 96000
  timesteps_total: 15360000
  training_iteration: 160
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.1/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    160 |          22989.3 | 15360000 |   673.68 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 5.19
    apples_agent-0_min: 0
    apples_agent-1_max: 237
    apples_agent-1_mean: 27.5
    apples_agent-1_min: 0
    apples_agent-2_max: 110
    apples_agent-2_mean: 10.74
    apples_agent-2_min: 0
    apples_agent-3_max: 162
    apples_agent-3_mean: 76.48
    apples_agent-3_min: 29
    apples_agent-4_max: 66
    apples_agent-4_mean: 4.04
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 77.52
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 455
    cleaning_beam_agent-0_mean: 314.87
    cleaning_beam_agent-0_min: 168
    cleaning_beam_agent-1_max: 368
    cleaning_beam_agent-1_mean: 199.53
    cleaning_beam_agent-1_min: 115
    cleaning_beam_agent-2_max: 495
    cleaning_beam_agent-2_mean: 306.48
    cleaning_beam_agent-2_min: 93
    cleaning_beam_agent-3_max: 197
    cleaning_beam_agent-3_mean: 58.26
    cleaning_beam_agent-3_min: 24
    cleaning_beam_agent-4_max: 430
    cleaning_beam_agent-4_mean: 321.67
    cleaning_beam_agent-4_min: 160
    cleaning_beam_agent-5_max: 231
    cleaning_beam_agent-5_mean: 61.51
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 3
    fire_beam_agent-0_mean: 0.05
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-38-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 935.9999999999698
  episode_reward_mean: 647.7399999999944
  episode_reward_min: 285.99999999999966
  episodes_this_iter: 96
  episodes_total: 15456
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12568.014
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 1.088584542274475
        entropy_coeff: 0.0017600000137463212
        kl: 0.009565336629748344
        model: {}
        policy_loss: -0.02832718938589096
        total_loss: -0.026721548289060593
        vf_explained_var: 0.12326125800609589
        vf_loss: 16.084789276123047
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 1.1691032648086548
        entropy_coeff: 0.0017600000137463212
        kl: 0.01062627974897623
        model: {}
        policy_loss: -0.02985653653740883
        total_loss: -0.027965333312749863
        vf_explained_var: 0.005884632468223572
        vf_loss: 18.23564910888672
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 1.1441454887390137
        entropy_coeff: 0.0017600000137463212
        kl: 0.011173506267368793
        model: {}
        policy_loss: -0.02802710421383381
        total_loss: -0.02606954425573349
        vf_explained_var: 0.05369630455970764
        vf_loss: 17.365570068359375
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.6858177185058594
        entropy_coeff: 0.0017600000137463212
        kl: 0.007344430312514305
        model: {}
        policy_loss: -0.021388418972492218
        total_loss: -0.019699860364198685
        vf_explained_var: 0.22150275111198425
        vf_loss: 14.267086029052734
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 1.0811083316802979
        entropy_coeff: 0.0017600000137463212
        kl: 0.011455168947577477
        model: {}
        policy_loss: -0.032686732709407806
        total_loss: -0.030602436512708664
        vf_explained_var: 0.07553185522556305
        vf_loss: 16.960063934326172
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00030153599800541997
        entropy: 0.9576541781425476
        entropy_coeff: 0.0017600000137463212
        kl: 0.010534029453992844
        model: {}
        policy_loss: -0.02905953675508499
        total_loss: -0.02713649719953537
        vf_explained_var: 0.18170493841171265
        vf_loss: 15.01710319519043
    load_time_ms: 13637.411
    num_steps_sampled: 15456000
    num_steps_trained: 15456000
    sample_time_ms: 106900.696
    update_time_ms: 41.377
  iterations_since_restore: 101
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.643684210526317
    ram_util_percent: 16.31157894736842
  pid: 30948
  policy_reward_max:
    agent-0: 156.0000000000001
    agent-1: 156.0000000000001
    agent-2: 156.0000000000001
    agent-3: 156.0000000000001
    agent-4: 156.0000000000001
    agent-5: 156.0000000000001
  policy_reward_mean:
    agent-0: 107.95666666666695
    agent-1: 107.95666666666695
    agent-2: 107.95666666666695
    agent-3: 107.95666666666695
    agent-4: 107.95666666666695
    agent-5: 107.95666666666695
  policy_reward_min:
    agent-0: 47.66666666666658
    agent-1: 47.66666666666658
    agent-2: 47.66666666666658
    agent-3: 47.66666666666658
    agent-4: 47.66666666666658
    agent-5: 47.66666666666658
  sampler_perf:
    mean_env_wait_ms: 27.501962817563054
    mean_inference_ms: 13.05338298493059
    mean_processing_ms: 59.00504037821461
  time_since_restore: 13996.176983356476
  time_this_iter_s: 132.8905384540558
  time_total_s: 23122.18879723549
  timestamp: 1637041089
  timesteps_since_restore: 9696000
  timesteps_this_iter: 96000
  timesteps_total: 15456000
  training_iteration: 161
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.2/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    161 |          23122.2 | 15456000 |   647.74 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 3.29
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 22.7
    apples_agent-1_min: 0
    apples_agent-2_max: 123
    apples_agent-2_mean: 13.48
    apples_agent-2_min: 0
    apples_agent-3_max: 157
    apples_agent-3_mean: 79.92
    apples_agent-3_min: 31
    apples_agent-4_max: 56
    apples_agent-4_mean: 3.76
    apples_agent-4_min: 0
    apples_agent-5_max: 129
    apples_agent-5_mean: 76.3
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 462
    cleaning_beam_agent-0_mean: 322.57
    cleaning_beam_agent-0_min: 175
    cleaning_beam_agent-1_max: 496
    cleaning_beam_agent-1_mean: 213.68
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 547
    cleaning_beam_agent-2_mean: 308.93
    cleaning_beam_agent-2_min: 124
    cleaning_beam_agent-3_max: 122
    cleaning_beam_agent-3_mean: 62.11
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 430
    cleaning_beam_agent-4_mean: 322.4
    cleaning_beam_agent-4_min: 160
    cleaning_beam_agent-5_max: 308
    cleaning_beam_agent-5_mean: 61.57
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-40-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 881.9999999999739
  episode_reward_mean: 661.5199999999948
  episode_reward_min: 381.0000000000008
  episodes_this_iter: 96
  episodes_total: 15552
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12594.37
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 1.0756984949111938
        entropy_coeff: 0.0017600000137463212
        kl: 0.009819859638810158
        model: {}
        policy_loss: -0.027097884565591812
        total_loss: -0.02550622634589672
        vf_explained_var: 0.11821664869785309
        vf_loss: 15.20918083190918
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 1.1604480743408203
        entropy_coeff: 0.0017600000137463212
        kl: 0.011368639767169952
        model: {}
        policy_loss: -0.03138350322842598
        total_loss: -0.029460955411195755
        vf_explained_var: 0.01900617778301239
        vf_loss: 16.9121036529541
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 1.1369404792785645
        entropy_coeff: 0.0017600000137463212
        kl: 0.010271307080984116
        model: {}
        policy_loss: -0.028442036360502243
        total_loss: -0.026774873957037926
        vf_explained_var: 0.06360949575901031
        vf_loss: 16.139156341552734
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.6738734245300293
        entropy_coeff: 0.0017600000137463212
        kl: 0.00741090252995491
        model: {}
        policy_loss: -0.021133247762918472
        total_loss: -0.0194413922727108
        vf_explained_var: 0.19058825075626373
        vf_loss: 13.956929206848145
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 1.073070764541626
        entropy_coeff: 0.0017600000137463212
        kl: 0.011176818050444126
        model: {}
        policy_loss: -0.03298225253820419
        total_loss: -0.030967378988862038
        vf_explained_var: 0.032706961035728455
        vf_loss: 16.681140899658203
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00029554558568634093
        entropy: 0.9350305199623108
        entropy_coeff: 0.0017600000137463212
        kl: 0.010229947045445442
        model: {}
        policy_loss: -0.028215711936354637
        total_loss: -0.026423942297697067
        vf_explained_var: 0.19314491748809814
        vf_loss: 13.91433048248291
    load_time_ms: 13625.585
    num_steps_sampled: 15552000
    num_steps_trained: 15552000
    sample_time_ms: 106729.604
    update_time_ms: 41.202
  iterations_since_restore: 102
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.762962962962963
    ram_util_percent: 16.315873015873017
  pid: 30948
  policy_reward_max:
    agent-0: 147.00000000000017
    agent-1: 147.00000000000017
    agent-2: 147.00000000000017
    agent-3: 147.00000000000017
    agent-4: 147.00000000000017
    agent-5: 147.00000000000017
  policy_reward_mean:
    agent-0: 110.25333333333364
    agent-1: 110.25333333333364
    agent-2: 110.25333333333364
    agent-3: 110.25333333333364
    agent-4: 110.25333333333364
    agent-5: 110.25333333333364
  policy_reward_min:
    agent-0: 63.49999999999977
    agent-1: 63.49999999999977
    agent-2: 63.49999999999977
    agent-3: 63.49999999999977
    agent-4: 63.49999999999977
    agent-5: 63.49999999999977
  sampler_perf:
    mean_env_wait_ms: 27.495360932594426
    mean_inference_ms: 13.05179721701369
    mean_processing_ms: 58.99339677930867
  time_since_restore: 14129.033596992493
  time_this_iter_s: 132.85661363601685
  time_total_s: 23255.045410871506
  timestamp: 1637041222
  timesteps_since_restore: 9792000
  timesteps_this_iter: 96000
  timesteps_total: 15552000
  training_iteration: 162
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    162 |            23255 | 15552000 |   661.52 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 3.63
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 26.85
    apples_agent-1_min: 0
    apples_agent-2_max: 119
    apples_agent-2_mean: 9.74
    apples_agent-2_min: 0
    apples_agent-3_max: 120
    apples_agent-3_mean: 71.26
    apples_agent-3_min: 22
    apples_agent-4_max: 96
    apples_agent-4_mean: 3.92
    apples_agent-4_min: 0
    apples_agent-5_max: 135
    apples_agent-5_mean: 77.47
    apples_agent-5_min: 4
    cleaning_beam_agent-0_max: 445
    cleaning_beam_agent-0_mean: 314.32
    cleaning_beam_agent-0_min: 141
    cleaning_beam_agent-1_max: 385
    cleaning_beam_agent-1_mean: 201.79
    cleaning_beam_agent-1_min: 71
    cleaning_beam_agent-2_max: 482
    cleaning_beam_agent-2_mean: 302.24
    cleaning_beam_agent-2_min: 142
    cleaning_beam_agent-3_max: 138
    cleaning_beam_agent-3_mean: 66.68
    cleaning_beam_agent-3_min: 34
    cleaning_beam_agent-4_max: 400
    cleaning_beam_agent-4_mean: 314.91
    cleaning_beam_agent-4_min: 145
    cleaning_beam_agent-5_max: 356
    cleaning_beam_agent-5_mean: 55.81
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-42-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 861.9999999999745
  episode_reward_mean: 629.1999999999965
  episode_reward_min: 237.999999999998
  episodes_this_iter: 96
  episodes_total: 15648
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12583.939
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 1.0825092792510986
        entropy_coeff: 0.0017600000137463212
        kl: 0.009817756712436676
        model: {}
        policy_loss: -0.028265856206417084
        total_loss: -0.02671070769429207
        vf_explained_var: 0.10196691751480103
        vf_loss: 14.96815013885498
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 1.16117525100708
        entropy_coeff: 0.0017600000137463212
        kl: 0.01047133095562458
        model: {}
        policy_loss: -0.0297110415995121
        total_loss: -0.028023356571793556
        vf_explained_var: 0.01676037907600403
        vf_loss: 16.370925903320312
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 1.1436303853988647
        entropy_coeff: 0.0017600000137463212
        kl: 0.010486479848623276
        model: {}
        policy_loss: -0.028408389538526535
        total_loss: -0.026761092245578766
        vf_explained_var: 0.060815706849098206
        vf_loss: 15.627894401550293
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.7043670415878296
        entropy_coeff: 0.0017600000137463212
        kl: 0.007606361526995897
        model: {}
        policy_loss: -0.02154339849948883
        total_loss: -0.019946156069636345
        vf_explained_var: 0.20946641266345978
        vf_loss: 13.156562805175781
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 1.0740199089050293
        entropy_coeff: 0.0017600000137463212
        kl: 0.010997776873409748
        model: {}
        policy_loss: -0.032173868268728256
        total_loss: -0.03031894750893116
        vf_explained_var: 0.07065938413143158
        vf_loss: 15.456371307373047
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00028955520247109234
        entropy: 0.9751137495040894
        entropy_coeff: 0.0017600000137463212
        kl: 0.010207219049334526
        model: {}
        policy_loss: -0.02903025969862938
        total_loss: -0.027368515729904175
        vf_explained_var: 0.1973581165075302
        vf_loss: 13.365006446838379
    load_time_ms: 13617.245
    num_steps_sampled: 15648000
    num_steps_trained: 15648000
    sample_time_ms: 106674.283
    update_time_ms: 41.378
  iterations_since_restore: 103
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.67315789473684
    ram_util_percent: 16.307894736842105
  pid: 30948
  policy_reward_max:
    agent-0: 143.66666666666728
    agent-1: 143.66666666666728
    agent-2: 143.66666666666728
    agent-3: 143.66666666666728
    agent-4: 143.66666666666728
    agent-5: 143.66666666666728
  policy_reward_mean:
    agent-0: 104.86666666666697
    agent-1: 104.86666666666697
    agent-2: 104.86666666666697
    agent-3: 104.86666666666697
    agent-4: 104.86666666666697
    agent-5: 104.86666666666697
  policy_reward_min:
    agent-0: 39.66666666666671
    agent-1: 39.66666666666671
    agent-2: 39.66666666666671
    agent-3: 39.66666666666671
    agent-4: 39.66666666666671
    agent-5: 39.66666666666671
  sampler_perf:
    mean_env_wait_ms: 27.488902126642913
    mean_inference_ms: 13.050596067439894
    mean_processing_ms: 58.985425666546014
  time_since_restore: 14262.485784053802
  time_this_iter_s: 133.45218706130981
  time_total_s: 23388.497597932816
  timestamp: 1637041355
  timesteps_since_restore: 9888000
  timesteps_this_iter: 96000
  timesteps_total: 15648000
  training_iteration: 163
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    163 |          23388.5 | 15648000 |    629.2 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 2.56
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 28.35
    apples_agent-1_min: 0
    apples_agent-2_max: 114
    apples_agent-2_mean: 12.86
    apples_agent-2_min: 0
    apples_agent-3_max: 127
    apples_agent-3_mean: 76.64
    apples_agent-3_min: 23
    apples_agent-4_max: 65
    apples_agent-4_mean: 2.63
    apples_agent-4_min: 0
    apples_agent-5_max: 138
    apples_agent-5_mean: 78.41
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 469
    cleaning_beam_agent-0_mean: 322.35
    cleaning_beam_agent-0_min: 197
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 204.48
    cleaning_beam_agent-1_min: 76
    cleaning_beam_agent-2_max: 468
    cleaning_beam_agent-2_mean: 289.55
    cleaning_beam_agent-2_min: 97
    cleaning_beam_agent-3_max: 147
    cleaning_beam_agent-3_mean: 65.26
    cleaning_beam_agent-3_min: 27
    cleaning_beam_agent-4_max: 501
    cleaning_beam_agent-4_mean: 330.15
    cleaning_beam_agent-4_min: 238
    cleaning_beam_agent-5_max: 211
    cleaning_beam_agent-5_mean: 55.72
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-44-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 889.9999999999692
  episode_reward_mean: 648.7299999999955
  episode_reward_min: 258.99999999999585
  episodes_this_iter: 96
  episodes_total: 15744
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12594.34
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 1.0914294719696045
        entropy_coeff: 0.0017600000137463212
        kl: 0.008828727528452873
        model: {}
        policy_loss: -0.02590182051062584
        total_loss: -0.024549003690481186
        vf_explained_var: 0.10709609091281891
        vf_loss: 15.079853057861328
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 1.157345175743103
        entropy_coeff: 0.0017600000137463212
        kl: 0.01073408592492342
        model: {}
        policy_loss: -0.030739925801753998
        total_loss: -0.028950106352567673
        vf_explained_var: 0.004769712686538696
        vf_loss: 16.79932975769043
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 1.1437692642211914
        entropy_coeff: 0.0017600000137463212
        kl: 0.010770321823656559
        model: {}
        policy_loss: -0.02946500852704048
        total_loss: -0.02776765637099743
        vf_explained_var: 0.0777929276227951
        vf_loss: 15.56319808959961
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.7039767503738403
        entropy_coeff: 0.0017600000137463212
        kl: 0.007836949080228806
        model: {}
        policy_loss: -0.02170748822391033
        total_loss: -0.02004900760948658
        vf_explained_var: 0.21230252087116241
        vf_loss: 13.300907135009766
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 1.0707203149795532
        entropy_coeff: 0.0017600000137463212
        kl: 0.011097826063632965
        model: {}
        policy_loss: -0.031847573816776276
        total_loss: -0.029878072440624237
        vf_explained_var: 0.0319109708070755
        vf_loss: 16.344083786010742
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002835647901520133
        entropy: 0.9581656455993652
        entropy_coeff: 0.0017600000137463212
        kl: 0.010148577392101288
        model: {}
        policy_loss: -0.028284022584557533
        total_loss: -0.026548298075795174
        vf_explained_var: 0.17486195266246796
        vf_loss: 13.923789978027344
    load_time_ms: 13614.031
    num_steps_sampled: 15744000
    num_steps_trained: 15744000
    sample_time_ms: 106689.201
    update_time_ms: 15.762
  iterations_since_restore: 104
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.61269841269841
    ram_util_percent: 16.23121693121693
  pid: 30948
  policy_reward_max:
    agent-0: 148.3333333333333
    agent-1: 148.3333333333333
    agent-2: 148.3333333333333
    agent-3: 148.3333333333333
    agent-4: 148.3333333333333
    agent-5: 148.3333333333333
  policy_reward_mean:
    agent-0: 108.12166666666704
    agent-1: 108.12166666666704
    agent-2: 108.12166666666704
    agent-3: 108.12166666666704
    agent-4: 108.12166666666704
    agent-5: 108.12166666666704
  policy_reward_min:
    agent-0: 43.166666666666615
    agent-1: 43.166666666666615
    agent-2: 43.166666666666615
    agent-3: 43.166666666666615
    agent-4: 43.166666666666615
    agent-5: 43.166666666666615
  sampler_perf:
    mean_env_wait_ms: 27.48116043837121
    mean_inference_ms: 13.049854647411392
    mean_processing_ms: 58.97435074663888
  time_since_restore: 14396.394308805466
  time_this_iter_s: 133.9085247516632
  time_total_s: 23522.40612268448
  timestamp: 1637041490
  timesteps_since_restore: 9984000
  timesteps_this_iter: 96000
  timesteps_total: 15744000
  training_iteration: 164
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 30.0/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    164 |          23522.4 | 15744000 |   648.73 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 2.0
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 24.15
    apples_agent-1_min: 0
    apples_agent-2_max: 109
    apples_agent-2_mean: 11.98
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 77.82
    apples_agent-3_min: 34
    apples_agent-4_max: 83
    apples_agent-4_mean: 4.51
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 77.87
    apples_agent-5_min: 20
    cleaning_beam_agent-0_max: 479
    cleaning_beam_agent-0_mean: 318.34
    cleaning_beam_agent-0_min: 170
    cleaning_beam_agent-1_max: 338
    cleaning_beam_agent-1_mean: 211.21
    cleaning_beam_agent-1_min: 101
    cleaning_beam_agent-2_max: 497
    cleaning_beam_agent-2_mean: 295.33
    cleaning_beam_agent-2_min: 96
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 59.7
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 442
    cleaning_beam_agent-4_mean: 321.46
    cleaning_beam_agent-4_min: 179
    cleaning_beam_agent-5_max: 220
    cleaning_beam_agent-5_mean: 60.89
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 12
    fire_beam_agent-0_mean: 0.16
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-47-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 907.9999999999817
  episode_reward_mean: 661.0599999999941
  episode_reward_min: 236.9999999999963
  episodes_this_iter: 96
  episodes_total: 15840
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12583.436
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 1.0906281471252441
        entropy_coeff: 0.0017600000137463212
        kl: 0.009245763532817364
        model: {}
        policy_loss: -0.026554249227046967
        total_loss: -0.025100570172071457
        vf_explained_var: 0.10071983933448792
        vf_loss: 15.240339279174805
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 1.167150855064392
        entropy_coeff: 0.0017600000137463212
        kl: 0.010158681310713291
        model: {}
        policy_loss: -0.028956644237041473
        total_loss: -0.027339942753314972
        vf_explained_var: 0.032749325037002563
        vf_loss: 16.391498565673828
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 1.1347206830978394
        entropy_coeff: 0.0017600000137463212
        kl: 0.011010374873876572
        model: {}
        policy_loss: -0.028395650908350945
        total_loss: -0.026590779423713684
        vf_explained_var: 0.05649951100349426
        vf_loss: 15.999038696289062
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.6873236894607544
        entropy_coeff: 0.0017600000137463212
        kl: 0.007307686377316713
        model: {}
        policy_loss: -0.02026567980647087
        total_loss: -0.018610065802931786
        vf_explained_var: 0.17141775786876678
        vf_loss: 14.037663459777832
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 1.0698745250701904
        entropy_coeff: 0.0017600000137463212
        kl: 0.01026397105306387
        model: {}
        policy_loss: -0.031124554574489594
        total_loss: -0.02932756394147873
        vf_explained_var: 0.040056586265563965
        vf_loss: 16.271751403808594
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002775744069367647
        entropy: 0.9538853168487549
        entropy_coeff: 0.0017600000137463212
        kl: 0.00939026940613985
        model: {}
        policy_loss: -0.026909004896879196
        total_loss: -0.025363702327013016
        vf_explained_var: 0.2056145817041397
        vf_loss: 13.460865020751953
    load_time_ms: 13655.009
    num_steps_sampled: 15840000
    num_steps_trained: 15840000
    sample_time_ms: 106930.541
    update_time_ms: 15.93
  iterations_since_restore: 105
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 21.376683937823834
    ram_util_percent: 16.26632124352332
  pid: 30948
  policy_reward_max:
    agent-0: 151.33333333333323
    agent-1: 151.33333333333323
    agent-2: 151.33333333333323
    agent-3: 151.33333333333323
    agent-4: 151.33333333333323
    agent-5: 151.33333333333323
  policy_reward_mean:
    agent-0: 110.176666666667
    agent-1: 110.176666666667
    agent-2: 110.176666666667
    agent-3: 110.176666666667
    agent-4: 110.176666666667
    agent-5: 110.176666666667
  policy_reward_min:
    agent-0: 39.49999999999988
    agent-1: 39.49999999999988
    agent-2: 39.49999999999988
    agent-3: 39.49999999999988
    agent-4: 39.49999999999988
    agent-5: 39.49999999999988
  sampler_perf:
    mean_env_wait_ms: 27.475250838757503
    mean_inference_ms: 13.049364219641198
    mean_processing_ms: 58.968540056993994
  time_since_restore: 14531.698196411133
  time_this_iter_s: 135.30388760566711
  time_total_s: 23657.710010290146
  timestamp: 1637041625
  timesteps_since_restore: 10080000
  timesteps_this_iter: 96000
  timesteps_total: 15840000
  training_iteration: 165
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    165 |          23657.7 | 15840000 |   661.06 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 60
    apples_agent-0_mean: 4.52
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 23.17
    apples_agent-1_min: 0
    apples_agent-2_max: 93
    apples_agent-2_mean: 13.31
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 76.3
    apples_agent-3_min: 31
    apples_agent-4_max: 38
    apples_agent-4_mean: 2.55
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 76.53
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 490
    cleaning_beam_agent-0_mean: 304.4
    cleaning_beam_agent-0_min: 127
    cleaning_beam_agent-1_max: 448
    cleaning_beam_agent-1_mean: 220.32
    cleaning_beam_agent-1_min: 97
    cleaning_beam_agent-2_max: 463
    cleaning_beam_agent-2_mean: 303.72
    cleaning_beam_agent-2_min: 112
    cleaning_beam_agent-3_max: 130
    cleaning_beam_agent-3_mean: 59.55
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 421
    cleaning_beam_agent-4_mean: 322.09
    cleaning_beam_agent-4_min: 208
    cleaning_beam_agent-5_max: 268
    cleaning_beam_agent-5_mean: 70.11
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 4
    fire_beam_agent-0_mean: 0.04
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-49-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 865.9999999999756
  episode_reward_mean: 658.8699999999949
  episode_reward_min: 236.99999999999704
  episodes_this_iter: 96
  episodes_total: 15936
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12527.243
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 1.1190824508666992
        entropy_coeff: 0.0017600000137463212
        kl: 0.009427739307284355
        model: {}
        policy_loss: -0.026835385710000992
        total_loss: -0.025436416268348694
        vf_explained_var: 0.14346843957901
        vf_loss: 14.830053329467773
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 1.1539349555969238
        entropy_coeff: 0.0017600000137463212
        kl: 0.009650826454162598
        model: {}
        policy_loss: -0.029116583988070488
        total_loss: -0.027512002736330032
        vf_explained_var: 0.015048936009407043
        vf_loss: 17.053421020507812
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 1.1311123371124268
        entropy_coeff: 0.0017600000137463212
        kl: 0.00987077783793211
        model: {}
        policy_loss: -0.027354121208190918
        total_loss: -0.02570822834968567
        vf_explained_var: 0.040412530303001404
        vf_loss: 16.624958038330078
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.6899905800819397
        entropy_coeff: 0.0017600000137463212
        kl: 0.007150943856686354
        model: {}
        policy_loss: -0.020689865574240685
        total_loss: -0.019128641113638878
        vf_explained_var: 0.22179725766181946
        vf_loss: 13.454180717468262
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 1.0648939609527588
        entropy_coeff: 0.0017600000137463212
        kl: 0.011209730990231037
        model: {}
        policy_loss: -0.031627293676137924
        total_loss: -0.029579147696495056
        vf_explained_var: 0.029508337378501892
        vf_loss: 16.804126739501953
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002715839946176857
        entropy: 0.9641180038452148
        entropy_coeff: 0.0017600000137463212
        kl: 0.010251430794596672
        model: {}
        policy_loss: -0.027723651379346848
        total_loss: -0.025909604504704475
        vf_explained_var: 0.15636655688285828
        vf_loss: 14.60609245300293
    load_time_ms: 13659.773
    num_steps_sampled: 15936000
    num_steps_trained: 15936000
    sample_time_ms: 106399.401
    update_time_ms: 15.81
  iterations_since_restore: 106
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.82486187845304
    ram_util_percent: 13.516574585635357
  pid: 30948
  policy_reward_max:
    agent-0: 144.33333333333354
    agent-1: 144.33333333333354
    agent-2: 144.33333333333354
    agent-3: 144.33333333333354
    agent-4: 144.33333333333354
    agent-5: 144.33333333333354
  policy_reward_mean:
    agent-0: 109.811666666667
    agent-1: 109.811666666667
    agent-2: 109.811666666667
    agent-3: 109.811666666667
    agent-4: 109.811666666667
    agent-5: 109.811666666667
  policy_reward_min:
    agent-0: 39.50000000000001
    agent-1: 39.50000000000001
    agent-2: 39.50000000000001
    agent-3: 39.50000000000001
    agent-4: 39.50000000000001
    agent-5: 39.50000000000001
  sampler_perf:
    mean_env_wait_ms: 27.461081271300905
    mean_inference_ms: 13.04529081034814
    mean_processing_ms: 58.94184476228351
  time_since_restore: 14658.69347500801
  time_this_iter_s: 126.99527859687805
  time_total_s: 23784.705288887024
  timestamp: 1637041752
  timesteps_since_restore: 10176000
  timesteps_this_iter: 96000
  timesteps_total: 15936000
  training_iteration: 166
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    166 |          23784.7 | 15936000 |   658.87 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 73
    apples_agent-0_mean: 4.39
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 26.75
    apples_agent-1_min: 0
    apples_agent-2_max: 96
    apples_agent-2_mean: 9.97
    apples_agent-2_min: 0
    apples_agent-3_max: 120
    apples_agent-3_mean: 75.94
    apples_agent-3_min: 17
    apples_agent-4_max: 57
    apples_agent-4_mean: 4.39
    apples_agent-4_min: 0
    apples_agent-5_max: 218
    apples_agent-5_mean: 76.62
    apples_agent-5_min: 15
    cleaning_beam_agent-0_max: 464
    cleaning_beam_agent-0_mean: 310.35
    cleaning_beam_agent-0_min: 160
    cleaning_beam_agent-1_max: 413
    cleaning_beam_agent-1_mean: 205.46
    cleaning_beam_agent-1_min: 70
    cleaning_beam_agent-2_max: 447
    cleaning_beam_agent-2_mean: 305.12
    cleaning_beam_agent-2_min: 163
    cleaning_beam_agent-3_max: 127
    cleaning_beam_agent-3_mean: 64.23
    cleaning_beam_agent-3_min: 26
    cleaning_beam_agent-4_max: 454
    cleaning_beam_agent-4_mean: 322.26
    cleaning_beam_agent-4_min: 164
    cleaning_beam_agent-5_max: 269
    cleaning_beam_agent-5_mean: 67.61
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-51-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 896.9999999999744
  episode_reward_mean: 644.2599999999951
  episode_reward_min: 214.99999999999721
  episodes_this_iter: 96
  episodes_total: 16032
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12445.343
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 1.1179243326187134
        entropy_coeff: 0.0017600000137463212
        kl: 0.009227713569998741
        model: {}
        policy_loss: -0.02627970650792122
        total_loss: -0.024851132184267044
        vf_explained_var: 0.10530301928520203
        vf_loss: 15.505802154541016
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 1.148292899131775
        entropy_coeff: 0.0017600000137463212
        kl: 0.01126179564744234
        model: {}
        policy_loss: -0.026855193078517914
        total_loss: -0.02488170750439167
        vf_explained_var: -0.004339545965194702
        vf_loss: 17.421253204345703
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 1.1442935466766357
        entropy_coeff: 0.0017600000137463212
        kl: 0.009903350844979286
        model: {}
        policy_loss: -0.027489375323057175
        total_loss: -0.02585945650935173
        vf_explained_var: 0.03984591364860535
        vf_loss: 16.63205909729004
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.700879693031311
        entropy_coeff: 0.0017600000137463212
        kl: 0.007623528130352497
        model: {}
        policy_loss: -0.02029363624751568
        total_loss: -0.01865392178297043
        vf_explained_var: 0.2211485207080841
        vf_loss: 13.485612869262695
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 1.0596508979797363
        entropy_coeff: 0.0017600000137463212
        kl: 0.010662583634257317
        model: {}
        policy_loss: -0.03095538355410099
        total_loss: -0.02906169183552265
        vf_explained_var: 0.06178338825702667
        vf_loss: 16.261587142944336
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002655936114024371
        entropy: 0.9581049680709839
        entropy_coeff: 0.0017600000137463212
        kl: 0.009625689126551151
        model: {}
        policy_loss: -0.02595176361501217
        total_loss: -0.024276157841086388
        vf_explained_var: 0.17033737897872925
        vf_loss: 14.367292404174805
    load_time_ms: 13635.874
    num_steps_sampled: 16032000
    num_steps_trained: 16032000
    sample_time_ms: 106041.535
    update_time_ms: 15.671
  iterations_since_restore: 107
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.741208791208788
    ram_util_percent: 13.570329670329668
  pid: 30948
  policy_reward_max:
    agent-0: 149.5000000000002
    agent-1: 149.5000000000002
    agent-2: 149.5000000000002
    agent-3: 149.5000000000002
    agent-4: 149.5000000000002
    agent-5: 149.5000000000002
  policy_reward_mean:
    agent-0: 107.37666666666696
    agent-1: 107.37666666666696
    agent-2: 107.37666666666696
    agent-3: 107.37666666666696
    agent-4: 107.37666666666696
    agent-5: 107.37666666666696
  policy_reward_min:
    agent-0: 35.83333333333335
    agent-1: 35.83333333333335
    agent-2: 35.83333333333335
    agent-3: 35.83333333333335
    agent-4: 35.83333333333335
    agent-5: 35.83333333333335
  sampler_perf:
    mean_env_wait_ms: 27.447765635767254
    mean_inference_ms: 13.041539571617909
    mean_processing_ms: 58.91741138015946
  time_since_restore: 14786.794184446335
  time_this_iter_s: 128.10070943832397
  time_total_s: 23912.805998325348
  timestamp: 1637041880
  timesteps_since_restore: 10272000
  timesteps_this_iter: 96000
  timesteps_total: 16032000
  training_iteration: 167
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 24.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    167 |          23912.8 | 16032000 |   644.26 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 2.21
    apples_agent-0_min: 0
    apples_agent-1_max: 143
    apples_agent-1_mean: 25.25
    apples_agent-1_min: 0
    apples_agent-2_max: 136
    apples_agent-2_mean: 8.95
    apples_agent-2_min: 0
    apples_agent-3_max: 143
    apples_agent-3_mean: 82.57
    apples_agent-3_min: 45
    apples_agent-4_max: 57
    apples_agent-4_mean: 3.13
    apples_agent-4_min: 0
    apples_agent-5_max: 133
    apples_agent-5_mean: 79.89
    apples_agent-5_min: 8
    cleaning_beam_agent-0_max: 446
    cleaning_beam_agent-0_mean: 311.6
    cleaning_beam_agent-0_min: 205
    cleaning_beam_agent-1_max: 364
    cleaning_beam_agent-1_mean: 202.24
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 561
    cleaning_beam_agent-2_mean: 310.38
    cleaning_beam_agent-2_min: 96
    cleaning_beam_agent-3_max: 210
    cleaning_beam_agent-3_mean: 64.17
    cleaning_beam_agent-3_min: 20
    cleaning_beam_agent-4_max: 447
    cleaning_beam_agent-4_mean: 343.51
    cleaning_beam_agent-4_min: 195
    cleaning_beam_agent-5_max: 152
    cleaning_beam_agent-5_mean: 60.16
    cleaning_beam_agent-5_min: 19
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-53-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 882.9999999999819
  episode_reward_mean: 674.1299999999932
  episode_reward_min: 233.99999999999747
  episodes_this_iter: 96
  episodes_total: 16128
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12393.957
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 1.100405216217041
        entropy_coeff: 0.0017600000137463212
        kl: 0.009838051162660122
        model: {}
        policy_loss: -0.02574118785560131
        total_loss: -0.024186858907341957
        vf_explained_var: 0.1265731304883957
        vf_loss: 15.234322547912598
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 1.150274634361267
        entropy_coeff: 0.0017600000137463212
        kl: 0.009876200929284096
        model: {}
        policy_loss: -0.027571331709623337
        total_loss: -0.025892212986946106
        vf_explained_var: 0.011771485209465027
        vf_loss: 17.283607482910156
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 1.1292169094085693
        entropy_coeff: 0.0017600000137463212
        kl: 0.009623769670724869
        model: {}
        policy_loss: -0.026463955640792847
        total_loss: -0.02489359676837921
        vf_explained_var: 0.06412942707538605
        vf_loss: 16.33028793334961
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.6879215836524963
        entropy_coeff: 0.0017600000137463212
        kl: 0.007290318142622709
        model: {}
        policy_loss: -0.01970488205552101
        total_loss: -0.018053501844406128
        vf_explained_var: 0.19495926797389984
        vf_loss: 14.040595054626465
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 1.0465471744537354
        entropy_coeff: 0.0017600000137463212
        kl: 0.01014765165746212
        model: {}
        policy_loss: -0.03034016117453575
        total_loss: -0.028507214039564133
        vf_explained_var: 0.057150185108184814
        vf_loss: 16.453439712524414
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00025960319908335805
        entropy: 0.9540286064147949
        entropy_coeff: 0.0017600000137463212
        kl: 0.009889533743262291
        model: {}
        policy_loss: -0.02718706615269184
        total_loss: -0.02548895962536335
        vf_explained_var: 0.1974613517522812
        vf_loss: 13.992899894714355
    load_time_ms: 13606.796
    num_steps_sampled: 16128000
    num_steps_trained: 16128000
    sample_time_ms: 105508.432
    update_time_ms: 15.722
  iterations_since_restore: 108
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.795604395604393
    ram_util_percent: 13.436263736263735
  pid: 30948
  policy_reward_max:
    agent-0: 147.1666666666665
    agent-1: 147.1666666666665
    agent-2: 147.1666666666665
    agent-3: 147.1666666666665
    agent-4: 147.1666666666665
    agent-5: 147.1666666666665
  policy_reward_mean:
    agent-0: 112.35500000000036
    agent-1: 112.35500000000036
    agent-2: 112.35500000000036
    agent-3: 112.35500000000036
    agent-4: 112.35500000000036
    agent-5: 112.35500000000036
  policy_reward_min:
    agent-0: 39.00000000000002
    agent-1: 39.00000000000002
    agent-2: 39.00000000000002
    agent-3: 39.00000000000002
    agent-4: 39.00000000000002
    agent-5: 39.00000000000002
  sampler_perf:
    mean_env_wait_ms: 27.435803232342092
    mean_inference_ms: 13.037780758663535
    mean_processing_ms: 58.8922774713903
  time_since_restore: 14914.494131803513
  time_this_iter_s: 127.69994735717773
  time_total_s: 24040.505945682526
  timestamp: 1637042008
  timesteps_since_restore: 10368000
  timesteps_this_iter: 96000
  timesteps_total: 16128000
  training_iteration: 168
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    168 |          24040.5 | 16128000 |   674.13 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 65
    apples_agent-0_mean: 2.52
    apples_agent-0_min: 0
    apples_agent-1_max: 83
    apples_agent-1_mean: 21.92
    apples_agent-1_min: 0
    apples_agent-2_max: 204
    apples_agent-2_mean: 13.72
    apples_agent-2_min: 0
    apples_agent-3_max: 146
    apples_agent-3_mean: 78.7
    apples_agent-3_min: 26
    apples_agent-4_max: 39
    apples_agent-4_mean: 1.9
    apples_agent-4_min: 0
    apples_agent-5_max: 129
    apples_agent-5_mean: 74.69
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 452
    cleaning_beam_agent-0_mean: 308.97
    cleaning_beam_agent-0_min: 162
    cleaning_beam_agent-1_max: 342
    cleaning_beam_agent-1_mean: 214.08
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 561
    cleaning_beam_agent-2_mean: 306.67
    cleaning_beam_agent-2_min: 24
    cleaning_beam_agent-3_max: 210
    cleaning_beam_agent-3_mean: 62.41
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 464
    cleaning_beam_agent-4_mean: 341.29
    cleaning_beam_agent-4_min: 243
    cleaning_beam_agent-5_max: 193
    cleaning_beam_agent-5_mean: 65.76
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-55-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 895.9999999999754
  episode_reward_mean: 657.0999999999963
  episode_reward_min: 322.00000000000057
  episodes_this_iter: 96
  episodes_total: 16224
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12343.49
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 1.1184567213058472
        entropy_coeff: 0.0017600000137463212
        kl: 0.008929630741477013
        model: {}
        policy_loss: -0.02570333145558834
        total_loss: -0.024342764168977737
        vf_explained_var: 0.09344899654388428
        vf_loss: 15.431230545043945
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 1.150807499885559
        entropy_coeff: 0.0017600000137463212
        kl: 0.009421093389391899
        model: {}
        policy_loss: -0.027973530814051628
        total_loss: -0.02643626555800438
        vf_explained_var: 0.015424579381942749
        vf_loss: 16.784685134887695
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 1.1307355165481567
        entropy_coeff: 0.0017600000137463212
        kl: 0.009805139154195786
        model: {}
        policy_loss: -0.026780860498547554
        total_loss: -0.0252151470631361
        vf_explained_var: 0.0622362345457077
        vf_loss: 15.94778823852539
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.6958457827568054
        entropy_coeff: 0.0017600000137463212
        kl: 0.007068511098623276
        model: {}
        policy_loss: -0.01956806518137455
        total_loss: -0.018029438331723213
        vf_explained_var: 0.20700190961360931
        vf_loss: 13.49616527557373
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 1.0409847497940063
        entropy_coeff: 0.0017600000137463212
        kl: 0.010204361751675606
        model: {}
        policy_loss: -0.03039507567882538
        total_loss: -0.02857191115617752
        vf_explained_var: 0.05087338387966156
        vf_loss: 16.144222259521484
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000253612786764279
        entropy: 0.9547966718673706
        entropy_coeff: 0.0017600000137463212
        kl: 0.00952500943094492
        model: {}
        policy_loss: -0.027046557515859604
        total_loss: -0.025406338274478912
        vf_explained_var: 0.16828520596027374
        vf_loss: 14.156603813171387
    load_time_ms: 13607.662
    num_steps_sampled: 16224000
    num_steps_trained: 16224000
    sample_time_ms: 105071.955
    update_time_ms: 15.858
  iterations_since_restore: 109
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.686813186813186
    ram_util_percent: 13.596153846153847
  pid: 30948
  policy_reward_max:
    agent-0: 149.3333333333336
    agent-1: 149.3333333333336
    agent-2: 149.3333333333336
    agent-3: 149.3333333333336
    agent-4: 149.3333333333336
    agent-5: 149.3333333333336
  policy_reward_mean:
    agent-0: 109.51666666666695
    agent-1: 109.51666666666695
    agent-2: 109.51666666666695
    agent-3: 109.51666666666695
    agent-4: 109.51666666666695
    agent-5: 109.51666666666695
  policy_reward_min:
    agent-0: 53.66666666666667
    agent-1: 53.66666666666667
    agent-2: 53.66666666666667
    agent-3: 53.66666666666667
    agent-4: 53.66666666666667
    agent-5: 53.66666666666667
  sampler_perf:
    mean_env_wait_ms: 27.422027639044288
    mean_inference_ms: 13.033595235145906
    mean_processing_ms: 58.86349017491272
  time_since_restore: 15041.954620838165
  time_this_iter_s: 127.46048903465271
  time_total_s: 24167.96643471718
  timestamp: 1637042136
  timesteps_since_restore: 10464000
  timesteps_this_iter: 96000
  timesteps_total: 16224000
  training_iteration: 169
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 24.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    169 |            24168 | 16224000 |    657.1 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 4.67
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 25.38
    apples_agent-1_min: 0
    apples_agent-2_max: 140
    apples_agent-2_mean: 13.47
    apples_agent-2_min: 0
    apples_agent-3_max: 135
    apples_agent-3_mean: 74.44
    apples_agent-3_min: 21
    apples_agent-4_max: 39
    apples_agent-4_mean: 2.21
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 80.81
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 474
    cleaning_beam_agent-0_mean: 313.03
    cleaning_beam_agent-0_min: 163
    cleaning_beam_agent-1_max: 405
    cleaning_beam_agent-1_mean: 220.87
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 463
    cleaning_beam_agent-2_mean: 308.19
    cleaning_beam_agent-2_min: 117
    cleaning_beam_agent-3_max: 171
    cleaning_beam_agent-3_mean: 60.45
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 481
    cleaning_beam_agent-4_mean: 340.3
    cleaning_beam_agent-4_min: 261
    cleaning_beam_agent-5_max: 240
    cleaning_beam_agent-5_mean: 56.02
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 8
    fire_beam_agent-0_mean: 0.09
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-57-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 968.9999999999806
  episode_reward_mean: 672.5299999999945
  episode_reward_min: 291.0000000000008
  episodes_this_iter: 96
  episodes_total: 16320
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12313.778
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 1.1221129894256592
        entropy_coeff: 0.0017600000137463212
        kl: 0.008609472773969173
        model: {}
        policy_loss: -0.02463538758456707
        total_loss: -0.023372171446681023
        vf_explained_var: 0.11446544528007507
        vf_loss: 15.162429809570312
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 1.153039813041687
        entropy_coeff: 0.0017600000137463212
        kl: 0.009911010973155499
        model: {}
        policy_loss: -0.028630942106246948
        total_loss: -0.02699609100818634
        vf_explained_var: 0.018044903874397278
        vf_loss: 16.819974899291992
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 1.1409485340118408
        entropy_coeff: 0.0017600000137463212
        kl: 0.009875059127807617
        model: {}
        policy_loss: -0.02659808285534382
        total_loss: -0.025055140256881714
        vf_explained_var: 0.0785275548696518
        vf_loss: 15.760009765625
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 0.6913625597953796
        entropy_coeff: 0.0017600000137463212
        kl: 0.007180642336606979
        model: {}
        policy_loss: -0.018314335495233536
        total_loss: -0.01673128455877304
        vf_explained_var: 0.20284315943717957
        vf_loss: 13.63723087310791
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 1.0511436462402344
        entropy_coeff: 0.0017600000137463212
        kl: 0.00989313144236803
        model: {}
        policy_loss: -0.029724635183811188
        total_loss: -0.027937721461057663
        vf_explained_var: 0.03147153556346893
        vf_loss: 16.583009719848633
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002476224035490304
        entropy: 0.9494608640670776
        entropy_coeff: 0.0017600000137463212
        kl: 0.009251824580132961
        model: {}
        policy_loss: -0.026503274217247963
        total_loss: -0.02495182678103447
        vf_explained_var: 0.19706875085830688
        vf_loss: 13.721373558044434
    load_time_ms: 13689.885
    num_steps_sampled: 16320000
    num_steps_trained: 16320000
    sample_time_ms: 104704.726
    update_time_ms: 15.338
  iterations_since_restore: 110
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.65815217391304
    ram_util_percent: 13.625543478260871
  pid: 30948
  policy_reward_max:
    agent-0: 161.50000000000009
    agent-1: 161.50000000000009
    agent-2: 161.50000000000009
    agent-3: 161.50000000000009
    agent-4: 161.50000000000009
    agent-5: 161.50000000000009
  policy_reward_mean:
    agent-0: 112.08833333333367
    agent-1: 112.08833333333367
    agent-2: 112.08833333333367
    agent-3: 112.08833333333367
    agent-4: 112.08833333333367
    agent-5: 112.08833333333367
  policy_reward_min:
    agent-0: 48.49999999999991
    agent-1: 48.49999999999991
    agent-2: 48.49999999999991
    agent-3: 48.49999999999991
    agent-4: 48.49999999999991
    agent-5: 48.49999999999991
  sampler_perf:
    mean_env_wait_ms: 27.411500400922936
    mean_inference_ms: 13.029973125255465
    mean_processing_ms: 58.84068293788514
  time_since_restore: 15171.41969537735
  time_this_iter_s: 129.46507453918457
  time_total_s: 24297.431509256363
  timestamp: 1637042265
  timesteps_since_restore: 10560000
  timesteps_this_iter: 96000
  timesteps_total: 16320000
  training_iteration: 170
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    170 |          24297.4 | 16320000 |   672.53 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 2.22
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 26.62
    apples_agent-1_min: 0
    apples_agent-2_max: 100
    apples_agent-2_mean: 13.93
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 78.18
    apples_agent-3_min: 28
    apples_agent-4_max: 59
    apples_agent-4_mean: 3.0
    apples_agent-4_min: 0
    apples_agent-5_max: 133
    apples_agent-5_mean: 78.69
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 505
    cleaning_beam_agent-0_mean: 323.36
    cleaning_beam_agent-0_min: 215
    cleaning_beam_agent-1_max: 477
    cleaning_beam_agent-1_mean: 204.56
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 444
    cleaning_beam_agent-2_mean: 294.27
    cleaning_beam_agent-2_min: 148
    cleaning_beam_agent-3_max: 143
    cleaning_beam_agent-3_mean: 55.48
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 465
    cleaning_beam_agent-4_mean: 342.07
    cleaning_beam_agent-4_min: 165
    cleaning_beam_agent-5_max: 174
    cleaning_beam_agent-5_mean: 52.35
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_00-59-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 865.9999999999794
  episode_reward_mean: 673.5799999999936
  episode_reward_min: 359.00000000000483
  episodes_this_iter: 96
  episodes_total: 16416
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12252.223
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 1.1146652698516846
        entropy_coeff: 0.0017600000137463212
        kl: 0.0083776181563735
        model: {}
        policy_loss: -0.023936638608574867
        total_loss: -0.02282060496509075
        vf_explained_var: 0.11236129701137543
        vf_loss: 14.023181915283203
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 1.1658629179000854
        entropy_coeff: 0.0017600000137463212
        kl: 0.009654722176492214
        model: {}
        policy_loss: -0.027149584144353867
        total_loss: -0.025667740032076836
        vf_explained_var: -0.013567328453063965
        vf_loss: 16.028167724609375
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 1.1483891010284424
        entropy_coeff: 0.0017600000137463212
        kl: 0.010096874088048935
        model: {}
        policy_loss: -0.027197696268558502
        total_loss: -0.025714801624417305
        vf_explained_var: 0.05921293795108795
        vf_loss: 14.846826553344727
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 0.7001991271972656
        entropy_coeff: 0.0017600000137463212
        kl: 0.006588330492377281
        model: {}
        policy_loss: -0.019215060397982597
        total_loss: -0.017806200310587883
        vf_explained_var: 0.16095241904258728
        vf_loss: 13.23542308807373
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 1.0536110401153564
        entropy_coeff: 0.0017600000137463212
        kl: 0.009770212695002556
        model: {}
        policy_loss: -0.02919141761958599
        total_loss: -0.027565307915210724
        vf_explained_var: 0.03306230902671814
        vf_loss: 15.264243125915527
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002416320057818666
        entropy: 0.9602826833724976
        entropy_coeff: 0.0017600000137463212
        kl: 0.009249810129404068
        model: {}
        policy_loss: -0.026380879804491997
        total_loss: -0.024886995553970337
        vf_explained_var: 0.15481191873550415
        vf_loss: 13.340262413024902
    load_time_ms: 13652.675
    num_steps_sampled: 16416000
    num_steps_trained: 16416000
    sample_time_ms: 104144.694
    update_time_ms: 15.279
  iterations_since_restore: 111
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.822222222222223
    ram_util_percent: 13.520555555555555
  pid: 30948
  policy_reward_max:
    agent-0: 144.33333333333334
    agent-1: 144.33333333333334
    agent-2: 144.33333333333334
    agent-3: 144.33333333333334
    agent-4: 144.33333333333334
    agent-5: 144.33333333333334
  policy_reward_mean:
    agent-0: 112.26333333333365
    agent-1: 112.26333333333365
    agent-2: 112.26333333333365
    agent-3: 112.26333333333365
    agent-4: 112.26333333333365
    agent-5: 112.26333333333365
  policy_reward_min:
    agent-0: 59.83333333333309
    agent-1: 59.83333333333309
    agent-2: 59.83333333333309
    agent-3: 59.83333333333309
    agent-4: 59.83333333333309
    agent-5: 59.83333333333309
  sampler_perf:
    mean_env_wait_ms: 27.397128053384513
    mean_inference_ms: 13.02601028319246
    mean_processing_ms: 58.81403593151671
  time_since_restore: 15297.72379732132
  time_this_iter_s: 126.30410194396973
  time_total_s: 24423.735611200333
  timestamp: 1637042392
  timesteps_since_restore: 10656000
  timesteps_this_iter: 96000
  timesteps_total: 16416000
  training_iteration: 171
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 24.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    171 |          24423.7 | 16416000 |   673.58 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 3.7
    apples_agent-0_min: 0
    apples_agent-1_max: 97
    apples_agent-1_mean: 19.41
    apples_agent-1_min: 0
    apples_agent-2_max: 190
    apples_agent-2_mean: 13.49
    apples_agent-2_min: 0
    apples_agent-3_max: 153
    apples_agent-3_mean: 78.31
    apples_agent-3_min: 23
    apples_agent-4_max: 34
    apples_agent-4_mean: 2.71
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 80.16
    apples_agent-5_min: 6
    cleaning_beam_agent-0_max: 531
    cleaning_beam_agent-0_mean: 329.67
    cleaning_beam_agent-0_min: 192
    cleaning_beam_agent-1_max: 445
    cleaning_beam_agent-1_mean: 229.65
    cleaning_beam_agent-1_min: 85
    cleaning_beam_agent-2_max: 486
    cleaning_beam_agent-2_mean: 319.29
    cleaning_beam_agent-2_min: 57
    cleaning_beam_agent-3_max: 158
    cleaning_beam_agent-3_mean: 52.0
    cleaning_beam_agent-3_min: 21
    cleaning_beam_agent-4_max: 471
    cleaning_beam_agent-4_mean: 336.55
    cleaning_beam_agent-4_min: 190
    cleaning_beam_agent-5_max: 304
    cleaning_beam_agent-5_mean: 67.51
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.05
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-02-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 902.9999999999817
  episode_reward_mean: 687.9399999999928
  episode_reward_min: 280.9999999999987
  episodes_this_iter: 96
  episodes_total: 16512
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12209.528
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 1.1039211750030518
        entropy_coeff: 0.0017600000137463212
        kl: 0.008191012777388096
        model: {}
        policy_loss: -0.024222491309046745
        total_loss: -0.022894181311130524
        vf_explained_var: 0.0982017070055008
        vf_loss: 16.330081939697266
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 1.1541436910629272
        entropy_coeff: 0.0017600000137463212
        kl: 0.009343136101961136
        model: {}
        policy_loss: -0.027821669355034828
        total_loss: -0.026188449934124947
        vf_explained_var: 0.00877949595451355
        vf_loss: 17.958866119384766
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 1.1321184635162354
        entropy_coeff: 0.0017600000137463212
        kl: 0.009057139046490192
        model: {}
        policy_loss: -0.025049839168787003
        total_loss: -0.023587724193930626
        vf_explained_var: 0.09201608598232269
        vf_loss: 16.432161331176758
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 0.6569455862045288
        entropy_coeff: 0.0017600000137463212
        kl: 0.0064485645852983
        model: {}
        policy_loss: -0.018881777301430702
        total_loss: -0.01733575388789177
        vf_explained_var: 0.21942377090454102
        vf_loss: 14.125388145446777
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 1.0458420515060425
        entropy_coeff: 0.0017600000137463212
        kl: 0.010182610712945461
        model: {}
        policy_loss: -0.02842416614294052
        total_loss: -0.026512591168284416
        vf_explained_var: 0.0525701642036438
        vf_loss: 17.157333374023438
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00023564159346278757
        entropy: 0.9415265321731567
        entropy_coeff: 0.0017600000137463212
        kl: 0.00877704843878746
        model: {}
        policy_loss: -0.026111261919140816
        total_loss: -0.024514805525541306
        vf_explained_var: 0.1721554547548294
        vf_loss: 14.981330871582031
    load_time_ms: 13623.065
    num_steps_sampled: 16512000
    num_steps_trained: 16512000
    sample_time_ms: 103740.54
    update_time_ms: 15.227
  iterations_since_restore: 112
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 18.525274725274723
    ram_util_percent: 13.570329670329667
  pid: 30948
  policy_reward_max:
    agent-0: 150.50000000000028
    agent-1: 150.50000000000028
    agent-2: 150.50000000000028
    agent-3: 150.50000000000028
    agent-4: 150.50000000000028
    agent-5: 150.50000000000028
  policy_reward_mean:
    agent-0: 114.65666666666701
    agent-1: 114.65666666666701
    agent-2: 114.65666666666701
    agent-3: 114.65666666666701
    agent-4: 114.65666666666701
    agent-5: 114.65666666666701
  policy_reward_min:
    agent-0: 46.83333333333333
    agent-1: 46.83333333333333
    agent-2: 46.83333333333333
    agent-3: 46.83333333333333
    agent-4: 46.83333333333333
    agent-5: 46.83333333333333
  sampler_perf:
    mean_env_wait_ms: 27.38714641822479
    mean_inference_ms: 13.022767136539033
    mean_processing_ms: 58.792360303000095
  time_since_restore: 15425.805391073227
  time_this_iter_s: 128.08159375190735
  time_total_s: 24551.81720495224
  timestamp: 1637042520
  timesteps_since_restore: 10752000
  timesteps_this_iter: 96000
  timesteps_total: 16512000
  training_iteration: 172
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    172 |          24551.8 | 16512000 |   687.94 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 3.45
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 28.5
    apples_agent-1_min: 0
    apples_agent-2_max: 73
    apples_agent-2_mean: 11.15
    apples_agent-2_min: 0
    apples_agent-3_max: 169
    apples_agent-3_mean: 79.09
    apples_agent-3_min: 27
    apples_agent-4_max: 68
    apples_agent-4_mean: 2.96
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 83.36
    apples_agent-5_min: 18
    cleaning_beam_agent-0_max: 523
    cleaning_beam_agent-0_mean: 329.81
    cleaning_beam_agent-0_min: 206
    cleaning_beam_agent-1_max: 446
    cleaning_beam_agent-1_mean: 208.97
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 525
    cleaning_beam_agent-2_mean: 321.79
    cleaning_beam_agent-2_min: 136
    cleaning_beam_agent-3_max: 158
    cleaning_beam_agent-3_mean: 50.73
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 460
    cleaning_beam_agent-4_mean: 342.1
    cleaning_beam_agent-4_min: 195
    cleaning_beam_agent-5_max: 246
    cleaning_beam_agent-5_mean: 56.17
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-04-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 929.999999999979
  episode_reward_mean: 698.5199999999915
  episode_reward_min: 165.9999999999989
  episodes_this_iter: 96
  episodes_total: 16608
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12218.989
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 1.1213926076889038
        entropy_coeff: 0.0017600000137463212
        kl: 0.008569697849452496
        model: {}
        policy_loss: -0.023892607539892197
        total_loss: -0.02257974073290825
        vf_explained_var: 0.1138719916343689
        vf_loss: 15.725801467895508
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 1.1660559177398682
        entropy_coeff: 0.0017600000137463212
        kl: 0.009407520294189453
        model: {}
        policy_loss: -0.027521010488271713
        total_loss: -0.025894450023770332
        vf_explained_var: -0.010721161961555481
        vf_loss: 17.97317123413086
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 1.1352821588516235
        entropy_coeff: 0.0017600000137463212
        kl: 0.009330956265330315
        model: {}
        policy_loss: -0.02613730914890766
        total_loss: -0.024612966924905777
        vf_explained_var: 0.06625251471996307
        vf_loss: 16.562454223632812
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 0.6578770875930786
        entropy_coeff: 0.0017600000137463212
        kl: 0.006326432339847088
        model: {}
        policy_loss: -0.01867329329252243
        total_loss: -0.017103135585784912
        vf_explained_var: 0.17536894977092743
        vf_loss: 14.62738037109375
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 1.051377773284912
        entropy_coeff: 0.0017600000137463212
        kl: 0.009142570197582245
        model: {}
        policy_loss: -0.027880821377038956
        total_loss: -0.02622196450829506
        vf_explained_var: 0.05235365033149719
        vf_loss: 16.807641983032227
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022965119569562376
        entropy: 0.9555326104164124
        entropy_coeff: 0.0017600000137463212
        kl: 0.009069282561540604
        model: {}
        policy_loss: -0.0260603204369545
        total_loss: -0.024440012872219086
        vf_explained_var: 0.16094185411930084
        vf_loss: 14.881855010986328
    load_time_ms: 13600.32
    num_steps_sampled: 16608000
    num_steps_trained: 16608000
    sample_time_ms: 103263.921
    update_time_ms: 15.152
  iterations_since_restore: 113
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.68688524590164
    ram_util_percent: 13.581420765027321
  pid: 30948
  policy_reward_max:
    agent-0: 155.0000000000001
    agent-1: 155.0000000000001
    agent-2: 155.0000000000001
    agent-3: 155.0000000000001
    agent-4: 155.0000000000001
    agent-5: 155.0000000000001
  policy_reward_mean:
    agent-0: 116.42000000000036
    agent-1: 116.42000000000036
    agent-2: 116.42000000000036
    agent-3: 116.42000000000036
    agent-4: 116.42000000000036
    agent-5: 116.42000000000036
  policy_reward_min:
    agent-0: 27.66666666666672
    agent-1: 27.66666666666672
    agent-2: 27.66666666666672
    agent-3: 27.66666666666672
    agent-4: 27.66666666666672
    agent-5: 27.66666666666672
  sampler_perf:
    mean_env_wait_ms: 27.377428256757593
    mean_inference_ms: 13.01915036854248
    mean_processing_ms: 58.76971145464236
  time_since_restore: 15554.30785536766
  time_this_iter_s: 128.5024642944336
  time_total_s: 24680.319669246674
  timestamp: 1637042649
  timesteps_since_restore: 10848000
  timesteps_this_iter: 96000
  timesteps_total: 16608000
  training_iteration: 173
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    173 |          24680.3 | 16608000 |   698.52 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 3.34
    apples_agent-0_min: 0
    apples_agent-1_max: 185
    apples_agent-1_mean: 31.13
    apples_agent-1_min: 0
    apples_agent-2_max: 77
    apples_agent-2_mean: 9.53
    apples_agent-2_min: 0
    apples_agent-3_max: 198
    apples_agent-3_mean: 79.9
    apples_agent-3_min: 22
    apples_agent-4_max: 52
    apples_agent-4_mean: 2.19
    apples_agent-4_min: 0
    apples_agent-5_max: 185
    apples_agent-5_mean: 80.55
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 447
    cleaning_beam_agent-0_mean: 330.97
    cleaning_beam_agent-0_min: 180
    cleaning_beam_agent-1_max: 394
    cleaning_beam_agent-1_mean: 212.52
    cleaning_beam_agent-1_min: 84
    cleaning_beam_agent-2_max: 538
    cleaning_beam_agent-2_mean: 312.42
    cleaning_beam_agent-2_min: 125
    cleaning_beam_agent-3_max: 171
    cleaning_beam_agent-3_mean: 53.75
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 430
    cleaning_beam_agent-4_mean: 338.15
    cleaning_beam_agent-4_min: 149
    cleaning_beam_agent-5_max: 194
    cleaning_beam_agent-5_mean: 56.41
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-06-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 934.9999999999831
  episode_reward_mean: 698.2399999999911
  episode_reward_min: 240.9999999999965
  episodes_this_iter: 96
  episodes_total: 16704
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12196.745
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 1.1139349937438965
        entropy_coeff: 0.0017600000137463212
        kl: 0.0078058307990431786
        model: {}
        policy_loss: -0.023276932537555695
        total_loss: -0.02197718434035778
        vf_explained_var: 0.09868890047073364
        vf_loss: 16.99107551574707
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 1.162971019744873
        entropy_coeff: 0.0017600000137463212
        kl: 0.009152278304100037
        model: {}
        policy_loss: -0.02772017940878868
        total_loss: -0.026084087789058685
        vf_explained_var: 0.018859639763832092
        vf_loss: 18.524642944335938
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 1.1405370235443115
        entropy_coeff: 0.0017600000137463212
        kl: 0.009198088198900223
        model: {}
        policy_loss: -0.02629486657679081
        total_loss: -0.024669285863637924
        vf_explained_var: 0.04929398000240326
        vf_loss: 17.93309783935547
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 0.6748637557029724
        entropy_coeff: 0.0017600000137463212
        kl: 0.006830084137618542
        model: {}
        policy_loss: -0.02014179155230522
        total_loss: -0.018512137234210968
        vf_explained_var: 0.23032715916633606
        vf_loss: 14.51400089263916
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 1.0512902736663818
        entropy_coeff: 0.0017600000137463212
        kl: 0.009089731611311436
        model: {}
        policy_loss: -0.027746859937906265
        total_loss: -0.026038404554128647
        vf_explained_var: 0.07648567855358124
        vf_loss: 17.40778350830078
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00022366079792845994
        entropy: 0.9384667873382568
        entropy_coeff: 0.0017600000137463212
        kl: 0.00883821863681078
        model: {}
        policy_loss: -0.025885453447699547
        total_loss: -0.024190090596675873
        vf_explained_var: 0.16241012513637543
        vf_loss: 15.794163703918457
    load_time_ms: 13584.82
    num_steps_sampled: 16704000
    num_steps_trained: 16704000
    sample_time_ms: 102821.66
    update_time_ms: 14.793
  iterations_since_restore: 114
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 18.003278688524592
    ram_util_percent: 13.586885245901637
  pid: 30948
  policy_reward_max:
    agent-0: 155.83333333333331
    agent-1: 155.83333333333331
    agent-2: 155.83333333333331
    agent-3: 155.83333333333331
    agent-4: 155.83333333333331
    agent-5: 155.83333333333331
  policy_reward_mean:
    agent-0: 116.37333333333365
    agent-1: 116.37333333333365
    agent-2: 116.37333333333365
    agent-3: 116.37333333333365
    agent-4: 116.37333333333365
    agent-5: 116.37333333333365
  policy_reward_min:
    agent-0: 40.16666666666661
    agent-1: 40.16666666666661
    agent-2: 40.16666666666661
    agent-3: 40.16666666666661
    agent-4: 40.16666666666661
    agent-5: 40.16666666666661
  sampler_perf:
    mean_env_wait_ms: 27.367146181670496
    mean_inference_ms: 13.015573940400557
    mean_processing_ms: 58.747100501912776
  time_since_restore: 15683.406814575195
  time_this_iter_s: 129.0989592075348
  time_total_s: 24809.41862845421
  timestamp: 1637042778
  timesteps_since_restore: 10944000
  timesteps_this_iter: 96000
  timesteps_total: 16704000
  training_iteration: 174
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    174 |          24809.4 | 16704000 |   698.24 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 58
    apples_agent-0_mean: 3.67
    apples_agent-0_min: 0
    apples_agent-1_max: 240
    apples_agent-1_mean: 27.23
    apples_agent-1_min: 0
    apples_agent-2_max: 116
    apples_agent-2_mean: 8.24
    apples_agent-2_min: 0
    apples_agent-3_max: 191
    apples_agent-3_mean: 81.37
    apples_agent-3_min: 27
    apples_agent-4_max: 41
    apples_agent-4_mean: 2.18
    apples_agent-4_min: 0
    apples_agent-5_max: 123
    apples_agent-5_mean: 78.33
    apples_agent-5_min: 29
    cleaning_beam_agent-0_max: 474
    cleaning_beam_agent-0_mean: 324.79
    cleaning_beam_agent-0_min: 228
    cleaning_beam_agent-1_max: 368
    cleaning_beam_agent-1_mean: 212.93
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 493
    cleaning_beam_agent-2_mean: 331.65
    cleaning_beam_agent-2_min: 138
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 46.58
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 420
    cleaning_beam_agent-4_mean: 334.34
    cleaning_beam_agent-4_min: 145
    cleaning_beam_agent-5_max: 269
    cleaning_beam_agent-5_mean: 59.35
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-08-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 923.9999999999819
  episode_reward_mean: 710.6099999999904
  episode_reward_min: 294.99999999999886
  episodes_this_iter: 96
  episodes_total: 16800
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12163.574
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 1.1057233810424805
        entropy_coeff: 0.0017600000137463212
        kl: 0.007209004368633032
        model: {}
        policy_loss: -0.02214508317410946
        total_loss: -0.021036213263869286
        vf_explained_var: 0.07207594811916351
        vf_loss: 16.13143539428711
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 1.1582977771759033
        entropy_coeff: 0.0017600000137463212
        kl: 0.008657081052660942
        model: {}
        policy_loss: -0.026110654696822166
        total_loss: -0.02469666860997677
        vf_explained_var: 0.010234728455543518
        vf_loss: 17.21170425415039
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 1.1405370235443115
        entropy_coeff: 0.0017600000137463212
        kl: 0.009181209839880466
        model: {}
        policy_loss: -0.024941319599747658
        total_loss: -0.023486413061618805
        vf_explained_var: 0.06445415318012238
        vf_loss: 16.26007080078125
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 0.6366495490074158
        entropy_coeff: 0.0017600000137463212
        kl: 0.0057932003401219845
        model: {}
        policy_loss: -0.016967272385954857
        total_loss: -0.015513332560658455
        vf_explained_var: 0.18634392321109772
        vf_loss: 14.158038139343262
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 1.0522499084472656
        entropy_coeff: 0.0017600000137463212
        kl: 0.009351765736937523
        model: {}
        policy_loss: -0.027537111192941666
        total_loss: -0.025935903191566467
        vf_explained_var: 0.0905371904373169
        vf_loss: 15.828128814697266
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021767040016129613
        entropy: 0.9497926831245422
        entropy_coeff: 0.0017600000137463212
        kl: 0.008433131501078606
        model: {}
        policy_loss: -0.025627873837947845
        total_loss: -0.024121303111314774
        vf_explained_var: 0.14184612035751343
        vf_loss: 14.91580867767334
    load_time_ms: 13517.107
    num_steps_sampled: 16800000
    num_steps_trained: 16800000
    sample_time_ms: 102171.901
    update_time_ms: 14.773
  iterations_since_restore: 115
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.76263736263736
    ram_util_percent: 13.552197802197803
  pid: 30948
  policy_reward_max:
    agent-0: 153.9999999999998
    agent-1: 153.9999999999998
    agent-2: 153.9999999999998
    agent-3: 153.9999999999998
    agent-4: 153.9999999999998
    agent-5: 153.9999999999998
  policy_reward_mean:
    agent-0: 118.43500000000039
    agent-1: 118.43500000000039
    agent-2: 118.43500000000039
    agent-3: 118.43500000000039
    agent-4: 118.43500000000039
    agent-5: 118.43500000000039
  policy_reward_min:
    agent-0: 49.166666666666586
    agent-1: 49.166666666666586
    agent-2: 49.166666666666586
    agent-3: 49.166666666666586
    agent-4: 49.166666666666586
    agent-5: 49.166666666666586
  sampler_perf:
    mean_env_wait_ms: 27.355957631630385
    mean_inference_ms: 13.01232308644125
    mean_processing_ms: 58.724053378678136
  time_since_restore: 15811.23679971695
  time_this_iter_s: 127.82998514175415
  time_total_s: 24937.248613595963
  timestamp: 1637042906
  timesteps_since_restore: 11040000
  timesteps_this_iter: 96000
  timesteps_total: 16800000
  training_iteration: 175
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    175 |          24937.2 | 16800000 |   710.61 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 2.6
    apples_agent-0_min: 0
    apples_agent-1_max: 122
    apples_agent-1_mean: 22.89
    apples_agent-1_min: 0
    apples_agent-2_max: 142
    apples_agent-2_mean: 9.3
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 78.68
    apples_agent-3_min: 36
    apples_agent-4_max: 82
    apples_agent-4_mean: 2.68
    apples_agent-4_min: 0
    apples_agent-5_max: 125
    apples_agent-5_mean: 81.63
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 459
    cleaning_beam_agent-0_mean: 326.23
    cleaning_beam_agent-0_min: 207
    cleaning_beam_agent-1_max: 439
    cleaning_beam_agent-1_mean: 221.51
    cleaning_beam_agent-1_min: 53
    cleaning_beam_agent-2_max: 520
    cleaning_beam_agent-2_mean: 325.55
    cleaning_beam_agent-2_min: 169
    cleaning_beam_agent-3_max: 119
    cleaning_beam_agent-3_mean: 43.9
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 408
    cleaning_beam_agent-4_mean: 332.52
    cleaning_beam_agent-4_min: 101
    cleaning_beam_agent-5_max: 158
    cleaning_beam_agent-5_mean: 56.33
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-10-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 911.9999999999915
  episode_reward_mean: 705.1999999999914
  episode_reward_min: 344.00000000000216
  episodes_this_iter: 96
  episodes_total: 16896
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12228.025
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 1.1256842613220215
        entropy_coeff: 0.0017600000137463212
        kl: 0.008040983229875565
        model: {}
        policy_loss: -0.02302529104053974
        total_loss: -0.02182953804731369
        vf_explained_var: 0.10972820222377777
        vf_loss: 15.687616348266602
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 1.1508021354675293
        entropy_coeff: 0.0017600000137463212
        kl: 0.008632877841591835
        model: {}
        policy_loss: -0.02556409314274788
        total_loss: -0.024136919528245926
        vf_explained_var: 0.02062903344631195
        vf_loss: 17.26009750366211
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 1.1344547271728516
        entropy_coeff: 0.0017600000137463212
        kl: 0.008811051025986671
        model: {}
        policy_loss: -0.02478717267513275
        total_loss: -0.023358160629868507
        vf_explained_var: 0.05524943768978119
        vf_loss: 16.634445190429688
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 0.6374199986457825
        entropy_coeff: 0.0017600000137463212
        kl: 0.006010702811181545
        model: {}
        policy_loss: -0.017622020095586777
        total_loss: -0.016139160841703415
        vf_explained_var: 0.203046977519989
        vf_loss: 14.025834083557129
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 1.0557334423065186
        entropy_coeff: 0.0017600000137463212
        kl: 0.009152047336101532
        model: {}
        policy_loss: -0.027254918590188026
        total_loss: -0.025602877140045166
        vf_explained_var: 0.04555913805961609
        vf_loss: 16.797279357910156
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00021168000239413232
        entropy: 0.9456102252006531
        entropy_coeff: 0.0017600000137463212
        kl: 0.008116030134260654
        model: {}
        policy_loss: -0.02460458129644394
        total_loss: -0.02316954731941223
        vf_explained_var: 0.16126926243305206
        vf_loss: 14.760972023010254
    load_time_ms: 13481.975
    num_steps_sampled: 16896000
    num_steps_trained: 16896000
    sample_time_ms: 102236.34
    update_time_ms: 15.172
  iterations_since_restore: 116
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.70274725274725
    ram_util_percent: 13.521428571428569
  pid: 30948
  policy_reward_max:
    agent-0: 152.00000000000009
    agent-1: 152.00000000000009
    agent-2: 152.00000000000009
    agent-3: 152.00000000000009
    agent-4: 152.00000000000009
    agent-5: 152.00000000000009
  policy_reward_mean:
    agent-0: 117.53333333333367
    agent-1: 117.53333333333367
    agent-2: 117.53333333333367
    agent-3: 117.53333333333367
    agent-4: 117.53333333333367
    agent-5: 117.53333333333367
  policy_reward_min:
    agent-0: 57.33333333333315
    agent-1: 57.33333333333315
    agent-2: 57.33333333333315
    agent-3: 57.33333333333315
    agent-4: 57.33333333333315
    agent-5: 57.33333333333315
  sampler_perf:
    mean_env_wait_ms: 27.34502359838031
    mean_inference_ms: 13.009744060874123
    mean_processing_ms: 58.700489195921776
  time_since_restore: 15939.132326364517
  time_this_iter_s: 127.89552664756775
  time_total_s: 25065.14414024353
  timestamp: 1637043034
  timesteps_since_restore: 11136000
  timesteps_this_iter: 96000
  timesteps_total: 16896000
  training_iteration: 176
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    176 |          25065.1 | 16896000 |    705.2 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 2.68
    apples_agent-0_min: 0
    apples_agent-1_max: 82
    apples_agent-1_mean: 23.1
    apples_agent-1_min: 0
    apples_agent-2_max: 64
    apples_agent-2_mean: 7.96
    apples_agent-2_min: 0
    apples_agent-3_max: 129
    apples_agent-3_mean: 75.5
    apples_agent-3_min: 31
    apples_agent-4_max: 74
    apples_agent-4_mean: 2.79
    apples_agent-4_min: 0
    apples_agent-5_max: 128
    apples_agent-5_mean: 77.88
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 447
    cleaning_beam_agent-0_mean: 333.57
    cleaning_beam_agent-0_min: 198
    cleaning_beam_agent-1_max: 431
    cleaning_beam_agent-1_mean: 235.72
    cleaning_beam_agent-1_min: 85
    cleaning_beam_agent-2_max: 556
    cleaning_beam_agent-2_mean: 324.6
    cleaning_beam_agent-2_min: 154
    cleaning_beam_agent-3_max: 164
    cleaning_beam_agent-3_mean: 48.15
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 461
    cleaning_beam_agent-4_mean: 344.08
    cleaning_beam_agent-4_min: 215
    cleaning_beam_agent-5_max: 272
    cleaning_beam_agent-5_mean: 62.93
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-12-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 936.9999999999851
  episode_reward_mean: 715.7499999999911
  episode_reward_min: 321.0000000000019
  episodes_this_iter: 96
  episodes_total: 16992
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12268.502
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 1.1125860214233398
        entropy_coeff: 0.0017600000137463212
        kl: 0.007679685950279236
        model: {}
        policy_loss: -0.02257334068417549
        total_loss: -0.021293170750141144
        vf_explained_var: 0.07665060460567474
        vf_loss: 17.023893356323242
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 1.1608011722564697
        entropy_coeff: 0.0017600000137463212
        kl: 0.008928044699132442
        model: {}
        policy_loss: -0.02631504274904728
        total_loss: -0.024758590385317802
        vf_explained_var: 0.017383426427841187
        vf_loss: 18.13855743408203
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 1.1514933109283447
        entropy_coeff: 0.0017600000137463212
        kl: 0.008733772672712803
        model: {}
        policy_loss: -0.023585733026266098
        total_loss: -0.022145092487335205
        vf_explained_var: 0.067339226603508
        vf_loss: 17.20513343811035
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 0.6371433138847351
        entropy_coeff: 0.0017600000137463212
        kl: 0.006114500109106302
        model: {}
        policy_loss: -0.01729869842529297
        total_loss: -0.015643976628780365
        vf_explained_var: 0.15756291151046753
        vf_loss: 15.531922340393066
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 1.0458812713623047
        entropy_coeff: 0.0017600000137463212
        kl: 0.00891074351966381
        model: {}
        policy_loss: -0.0269794762134552
        total_loss: -0.025287587195634842
        vf_explained_var: 0.05117875337600708
        vf_loss: 17.5048828125
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0002056896046269685
        entropy: 0.9279173612594604
        entropy_coeff: 0.0017600000137463212
        kl: 0.007998691871762276
        model: {}
        policy_loss: -0.02486250549554825
        total_loss: -0.023348497226834297
        vf_explained_var: 0.16065089404582977
        vf_loss: 15.474061965942383
    load_time_ms: 13487.756
    num_steps_sampled: 16992000
    num_steps_trained: 16992000
    sample_time_ms: 102033.006
    update_time_ms: 15.265
  iterations_since_restore: 117
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.88950276243094
    ram_util_percent: 13.527624309392262
  pid: 30948
  policy_reward_max:
    agent-0: 156.16666666666657
    agent-1: 156.16666666666657
    agent-2: 156.16666666666657
    agent-3: 156.16666666666657
    agent-4: 156.16666666666657
    agent-5: 156.16666666666657
  policy_reward_mean:
    agent-0: 119.29166666666697
    agent-1: 119.29166666666697
    agent-2: 119.29166666666697
    agent-3: 119.29166666666697
    agent-4: 119.29166666666697
    agent-5: 119.29166666666697
  policy_reward_min:
    agent-0: 53.499999999999865
    agent-1: 53.499999999999865
    agent-2: 53.499999999999865
    agent-3: 53.499999999999865
    agent-4: 53.499999999999865
    agent-5: 53.499999999999865
  sampler_perf:
    mean_env_wait_ms: 27.33627395460853
    mean_inference_ms: 13.006314080537015
    mean_processing_ms: 58.67445877926004
  time_since_restore: 16065.735960006714
  time_this_iter_s: 126.60363364219666
  time_total_s: 25191.747773885727
  timestamp: 1637043160
  timesteps_since_restore: 11232000
  timesteps_this_iter: 96000
  timesteps_total: 16992000
  training_iteration: 177
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    177 |          25191.7 | 16992000 |   715.75 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 3.34
    apples_agent-0_min: 0
    apples_agent-1_max: 76
    apples_agent-1_mean: 26.53
    apples_agent-1_min: 0
    apples_agent-2_max: 256
    apples_agent-2_mean: 10.03
    apples_agent-2_min: 0
    apples_agent-3_max: 323
    apples_agent-3_mean: 76.82
    apples_agent-3_min: 38
    apples_agent-4_max: 85
    apples_agent-4_mean: 1.91
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 79.7
    apples_agent-5_min: 12
    cleaning_beam_agent-0_max: 420
    cleaning_beam_agent-0_mean: 322.33
    cleaning_beam_agent-0_min: 207
    cleaning_beam_agent-1_max: 405
    cleaning_beam_agent-1_mean: 221.16
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 491
    cleaning_beam_agent-2_mean: 311.87
    cleaning_beam_agent-2_min: 144
    cleaning_beam_agent-3_max: 116
    cleaning_beam_agent-3_mean: 45.05
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 460
    cleaning_beam_agent-4_mean: 340.52
    cleaning_beam_agent-4_min: 163
    cleaning_beam_agent-5_max: 202
    cleaning_beam_agent-5_mean: 65.63
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-14-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 915.9999999999822
  episode_reward_mean: 711.0099999999907
  episode_reward_min: 320.00000000000085
  episodes_this_iter: 96
  episodes_total: 17088
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12246.728
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 1.1160447597503662
        entropy_coeff: 0.0017600000137463212
        kl: 0.008051419630646706
        model: {}
        policy_loss: -0.023041700944304466
        total_loss: -0.021939676254987717
        vf_explained_var: 0.13630330562591553
        vf_loss: 14.559788703918457
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 1.151347041130066
        entropy_coeff: 0.0017600000137463212
        kl: 0.008281543850898743
        model: {}
        policy_loss: -0.024721087887883186
        total_loss: -0.023333977907896042
        vf_explained_var: -0.03871068358421326
        vf_loss: 17.57172203063965
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 1.1520534753799438
        entropy_coeff: 0.0017600000137463212
        kl: 0.008672237396240234
        model: {}
        policy_loss: -0.02389291301369667
        total_loss: -0.022631146013736725
        vf_explained_var: 0.07786500453948975
        vf_loss: 15.549342155456543
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 0.6365157961845398
        entropy_coeff: 0.0017600000137463212
        kl: 0.0056572360917925835
        model: {}
        policy_loss: -0.016390744596719742
        total_loss: -0.014975449070334435
        vf_explained_var: 0.16706301271915436
        vf_loss: 14.041176795959473
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 1.042902946472168
        entropy_coeff: 0.0017600000137463212
        kl: 0.009038352407515049
        model: {}
        policy_loss: -0.026940923184156418
        total_loss: -0.02533409371972084
        vf_explained_var: 0.03044562041759491
        vf_loss: 16.34670066833496
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001996992068598047
        entropy: 0.9408587217330933
        entropy_coeff: 0.0017600000137463212
        kl: 0.007814884185791016
        model: {}
        policy_loss: -0.02406192757189274
        total_loss: -0.022699741646647453
        vf_explained_var: 0.1373010277748108
        vf_loss: 14.551223754882812
    load_time_ms: 13471.255
    num_steps_sampled: 17088000
    num_steps_trained: 17088000
    sample_time_ms: 101892.138
    update_time_ms: 15.443
  iterations_since_restore: 118
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.889325842696632
    ram_util_percent: 13.590449438202247
  pid: 30948
  policy_reward_max:
    agent-0: 152.66666666666646
    agent-1: 152.66666666666646
    agent-2: 152.66666666666646
    agent-3: 152.66666666666646
    agent-4: 152.66666666666646
    agent-5: 152.66666666666646
  policy_reward_mean:
    agent-0: 118.50166666666705
    agent-1: 118.50166666666705
    agent-2: 118.50166666666705
    agent-3: 118.50166666666705
    agent-4: 118.50166666666705
    agent-5: 118.50166666666705
  policy_reward_min:
    agent-0: 53.33333333333317
    agent-1: 53.33333333333317
    agent-2: 53.33333333333317
    agent-3: 53.33333333333317
    agent-4: 53.33333333333317
    agent-5: 53.33333333333317
  sampler_perf:
    mean_env_wait_ms: 27.32467662644879
    mean_inference_ms: 13.002454794120743
    mean_processing_ms: 58.65044645628398
  time_since_restore: 16191.602096319199
  time_this_iter_s: 125.86613631248474
  time_total_s: 25317.61391019821
  timestamp: 1637043286
  timesteps_since_restore: 11328000
  timesteps_this_iter: 96000
  timesteps_total: 17088000
  training_iteration: 178
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    178 |          25317.6 | 17088000 |   711.01 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 51
    apples_agent-0_mean: 3.87
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 24.06
    apples_agent-1_min: 0
    apples_agent-2_max: 157
    apples_agent-2_mean: 11.22
    apples_agent-2_min: 0
    apples_agent-3_max: 201
    apples_agent-3_mean: 78.9
    apples_agent-3_min: 46
    apples_agent-4_max: 36
    apples_agent-4_mean: 1.07
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 81.76
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 422
    cleaning_beam_agent-0_mean: 309.71
    cleaning_beam_agent-0_min: 191
    cleaning_beam_agent-1_max: 409
    cleaning_beam_agent-1_mean: 231.52
    cleaning_beam_agent-1_min: 82
    cleaning_beam_agent-2_max: 487
    cleaning_beam_agent-2_mean: 316.92
    cleaning_beam_agent-2_min: 112
    cleaning_beam_agent-3_max: 78
    cleaning_beam_agent-3_mean: 40.87
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 476
    cleaning_beam_agent-4_mean: 341.79
    cleaning_beam_agent-4_min: 232
    cleaning_beam_agent-5_max: 184
    cleaning_beam_agent-5_mean: 60.7
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-16-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 899.9999999999878
  episode_reward_mean: 721.0899999999888
  episode_reward_min: 349.0000000000053
  episodes_this_iter: 96
  episodes_total: 17184
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12481.223
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 1.1232761144638062
        entropy_coeff: 0.0017600000137463212
        kl: 0.009129082784056664
        model: {}
        policy_loss: -0.02081349305808544
        total_loss: -0.019555632025003433
        vf_explained_var: 0.09238626062870026
        vf_loss: 14.090105056762695
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 1.1572911739349365
        entropy_coeff: 0.0017600000137463212
        kl: 0.008254341781139374
        model: {}
        policy_loss: -0.02379203587770462
        total_loss: -0.02262410894036293
        vf_explained_var: 0.0008253604173660278
        vf_loss: 15.538899421691895
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 1.1461329460144043
        entropy_coeff: 0.0017600000137463212
        kl: 0.008956890553236008
        model: {}
        policy_loss: -0.024013083428144455
        total_loss: -0.022745463997125626
        vf_explained_var: 0.036865755915641785
        vf_loss: 14.934362411499023
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 0.6110923886299133
        entropy_coeff: 0.0017600000137463212
        kl: 0.005464657675474882
        model: {}
        policy_loss: -0.015556938014924526
        total_loss: -0.014181451871991158
        vf_explained_var: 0.12499162554740906
        vf_loss: 13.580808639526367
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 1.0462112426757812
        entropy_coeff: 0.0017600000137463212
        kl: 0.008355352096259594
        model: {}
        policy_loss: -0.025920622050762177
        total_loss: -0.02456212230026722
        vf_explained_var: 0.013901174068450928
        vf_loss: 15.287618637084961
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00019370879454072565
        entropy: 0.9560147523880005
        entropy_coeff: 0.0017600000137463212
        kl: 0.008502709679305553
        model: {}
        policy_loss: -0.024407237768173218
        total_loss: -0.02304580621421337
        vf_explained_var: 0.13369174301624298
        vf_loss: 13.434759140014648
    load_time_ms: 13468.354
    num_steps_sampled: 17184000
    num_steps_trained: 17184000
    sample_time_ms: 101966.81
    update_time_ms: 15.162
  iterations_since_restore: 119
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.486559139784948
    ram_util_percent: 13.552150537634407
  pid: 30948
  policy_reward_max:
    agent-0: 149.99999999999997
    agent-1: 149.99999999999997
    agent-2: 149.99999999999997
    agent-3: 149.99999999999997
    agent-4: 149.99999999999997
    agent-5: 149.99999999999997
  policy_reward_mean:
    agent-0: 120.18166666666704
    agent-1: 120.18166666666704
    agent-2: 120.18166666666704
    agent-3: 120.18166666666704
    agent-4: 120.18166666666704
    agent-5: 120.18166666666704
  policy_reward_min:
    agent-0: 58.16666666666641
    agent-1: 58.16666666666641
    agent-2: 58.16666666666641
    agent-3: 58.16666666666641
    agent-4: 58.16666666666641
    agent-5: 58.16666666666641
  sampler_perf:
    mean_env_wait_ms: 27.31333866525868
    mean_inference_ms: 12.998583508307968
    mean_processing_ms: 58.62946261363988
  time_since_restore: 16322.12304520607
  time_this_iter_s: 130.52094888687134
  time_total_s: 25448.134859085083
  timestamp: 1637043417
  timesteps_since_restore: 11424000
  timesteps_this_iter: 96000
  timesteps_total: 17184000
  training_iteration: 179
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    179 |          25448.1 | 17184000 |   721.09 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 2.96
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 22.27
    apples_agent-1_min: 0
    apples_agent-2_max: 157
    apples_agent-2_mean: 11.85
    apples_agent-2_min: 0
    apples_agent-3_max: 129
    apples_agent-3_mean: 76.53
    apples_agent-3_min: 32
    apples_agent-4_max: 58
    apples_agent-4_mean: 2.46
    apples_agent-4_min: 0
    apples_agent-5_max: 152
    apples_agent-5_mean: 81.42
    apples_agent-5_min: 1
    cleaning_beam_agent-0_max: 454
    cleaning_beam_agent-0_mean: 317.79
    cleaning_beam_agent-0_min: 166
    cleaning_beam_agent-1_max: 485
    cleaning_beam_agent-1_mean: 224.06
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 512
    cleaning_beam_agent-2_mean: 311.78
    cleaning_beam_agent-2_min: 80
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 40.26
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 449
    cleaning_beam_agent-4_mean: 342.28
    cleaning_beam_agent-4_min: 153
    cleaning_beam_agent-5_max: 363
    cleaning_beam_agent-5_mean: 63.15
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 7
    fire_beam_agent-4_mean: 0.08
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-19-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 878.9999999999607
  episode_reward_mean: 705.0899999999917
  episode_reward_min: 349.0000000000053
  episodes_this_iter: 96
  episodes_total: 17280
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12472.228
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 1.113592267036438
        entropy_coeff: 0.0017600000137463212
        kl: 0.0077222189866006374
        model: {}
        policy_loss: -0.021675683557987213
        total_loss: -0.020484352484345436
        vf_explained_var: 0.06307339668273926
        vf_loss: 16.068132400512695
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 1.1681715250015259
        entropy_coeff: 0.0017600000137463212
        kl: 0.008839860558509827
        model: {}
        policy_loss: -0.025202535092830658
        total_loss: -0.023762039840221405
        vf_explained_var: -0.00795409083366394
        vf_loss: 17.285072326660156
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 1.1393004655838013
        entropy_coeff: 0.0017600000137463212
        kl: 0.008244588971138
        model: {}
        policy_loss: -0.02340572327375412
        total_loss: -0.022142410278320312
        vf_explained_var: 0.05464375019073486
        vf_loss: 16.19564437866211
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 0.6395977139472961
        entropy_coeff: 0.0017600000137463212
        kl: 0.006079645827412605
        model: {}
        policy_loss: -0.016739174723625183
        total_loss: -0.015204469673335552
        vf_explained_var: 0.15714667737483978
        vf_loss: 14.444679260253906
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 1.0376254320144653
        entropy_coeff: 0.0017600000137463212
        kl: 0.00795181468129158
        model: {}
        policy_loss: -0.024975227192044258
        total_loss: -0.02356601320207119
        vf_explained_var: 0.04040174186229706
        vf_loss: 16.450702667236328
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018771839677356184
        entropy: 0.9358536601066589
        entropy_coeff: 0.0017600000137463212
        kl: 0.008328319527208805
        model: {}
        policy_loss: -0.02309398725628853
        total_loss: -0.021657053381204605
        vf_explained_var: 0.1717575341463089
        vf_loss: 14.183701515197754
    load_time_ms: 13356.937
    num_steps_sampled: 17280000
    num_steps_trained: 17280000
    sample_time_ms: 101799.706
    update_time_ms: 15.074
  iterations_since_restore: 120
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.741436464088398
    ram_util_percent: 13.601104972375689
  pid: 30948
  policy_reward_max:
    agent-0: 146.49999999999935
    agent-1: 146.49999999999935
    agent-2: 146.49999999999935
    agent-3: 146.49999999999935
    agent-4: 146.49999999999935
    agent-5: 146.49999999999935
  policy_reward_mean:
    agent-0: 117.51500000000034
    agent-1: 117.51500000000034
    agent-2: 117.51500000000034
    agent-3: 117.51500000000034
    agent-4: 117.51500000000034
    agent-5: 117.51500000000034
  policy_reward_min:
    agent-0: 58.16666666666641
    agent-1: 58.16666666666641
    agent-2: 58.16666666666641
    agent-3: 58.16666666666641
    agent-4: 58.16666666666641
    agent-5: 58.16666666666641
  sampler_perf:
    mean_env_wait_ms: 27.302800387109937
    mean_inference_ms: 12.994908238464381
    mean_processing_ms: 58.60753443829403
  time_since_restore: 16448.7461643219
  time_this_iter_s: 126.62311911582947
  time_total_s: 25574.757978200912
  timestamp: 1637043544
  timesteps_since_restore: 11520000
  timesteps_this_iter: 96000
  timesteps_total: 17280000
  training_iteration: 180
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    180 |          25574.8 | 17280000 |   705.09 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 2.96
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 24.75
    apples_agent-1_min: 0
    apples_agent-2_max: 94
    apples_agent-2_mean: 10.68
    apples_agent-2_min: 0
    apples_agent-3_max: 121
    apples_agent-3_mean: 74.83
    apples_agent-3_min: 31
    apples_agent-4_max: 46
    apples_agent-4_mean: 2.37
    apples_agent-4_min: 0
    apples_agent-5_max: 135
    apples_agent-5_mean: 84.34
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 477
    cleaning_beam_agent-0_mean: 323.19
    cleaning_beam_agent-0_min: 192
    cleaning_beam_agent-1_max: 485
    cleaning_beam_agent-1_mean: 236.62
    cleaning_beam_agent-1_min: 64
    cleaning_beam_agent-2_max: 544
    cleaning_beam_agent-2_mean: 333.62
    cleaning_beam_agent-2_min: 168
    cleaning_beam_agent-3_max: 83
    cleaning_beam_agent-3_mean: 42.29
    cleaning_beam_agent-3_min: 19
    cleaning_beam_agent-4_max: 484
    cleaning_beam_agent-4_mean: 346.64
    cleaning_beam_agent-4_min: 252
    cleaning_beam_agent-5_max: 162
    cleaning_beam_agent-5_mean: 66.3
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-21-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 951.9999999999765
  episode_reward_mean: 729.9799999999898
  episode_reward_min: 383.0000000000008
  episodes_this_iter: 96
  episodes_total: 17376
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12456.821
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 1.107696533203125
        entropy_coeff: 0.0017600000137463212
        kl: 0.007446145638823509
        model: {}
        policy_loss: -0.021166253834962845
        total_loss: -0.02004408650100231
        vf_explained_var: 0.09509988129138947
        vf_loss: 15.824843406677246
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 1.1589754819869995
        entropy_coeff: 0.0017600000137463212
        kl: 0.008038743399083614
        model: {}
        policy_loss: -0.02384169027209282
        total_loss: -0.02256045490503311
        vf_explained_var: 0.019961968064308167
        vf_loss: 17.132822036743164
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 1.1500468254089355
        entropy_coeff: 0.0017600000137463212
        kl: 0.008337784558534622
        model: {}
        policy_loss: -0.023154236376285553
        total_loss: -0.021812457591295242
        vf_explained_var: 0.026807889342308044
        vf_loss: 16.98304557800293
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 0.6249307990074158
        entropy_coeff: 0.0017600000137463212
        kl: 0.005691200494766235
        model: {}
        policy_loss: -0.015242748893797398
        total_loss: -0.0137322461232543
        vf_explained_var: 0.15687528252601624
        vf_loss: 14.721405029296875
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 1.0414822101593018
        entropy_coeff: 0.0017600000137463212
        kl: 0.008501332253217697
        model: {}
        policy_loss: -0.025334041565656662
        total_loss: -0.023773659020662308
        vf_explained_var: 0.031293243169784546
        vf_loss: 16.931278228759766
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00018172799900639802
        entropy: 0.9463699460029602
        entropy_coeff: 0.0017600000137463212
        kl: 0.00798138789832592
        model: {}
        policy_loss: -0.023851128295063972
        total_loss: -0.022466367110610008
        vf_explained_var: 0.16658881306648254
        vf_loss: 14.540948867797852
    load_time_ms: 13362.569
    num_steps_sampled: 17376000
    num_steps_trained: 17376000
    sample_time_ms: 101924.072
    update_time_ms: 15.122
  iterations_since_restore: 121
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.793406593406594
    ram_util_percent: 13.519780219780216
  pid: 30948
  policy_reward_max:
    agent-0: 158.6666666666666
    agent-1: 158.6666666666666
    agent-2: 158.6666666666666
    agent-3: 158.6666666666666
    agent-4: 158.6666666666666
    agent-5: 158.6666666666666
  policy_reward_mean:
    agent-0: 121.66333333333365
    agent-1: 121.66333333333365
    agent-2: 121.66333333333365
    agent-3: 121.66333333333365
    agent-4: 121.66333333333365
    agent-5: 121.66333333333365
  policy_reward_min:
    agent-0: 63.83333333333315
    agent-1: 63.83333333333315
    agent-2: 63.83333333333315
    agent-3: 63.83333333333315
    agent-4: 63.83333333333315
    agent-5: 63.83333333333315
  sampler_perf:
    mean_env_wait_ms: 27.29302789244628
    mean_inference_ms: 12.991501931299286
    mean_processing_ms: 58.58501803858698
  time_since_restore: 16576.153978824615
  time_this_iter_s: 127.40781450271606
  time_total_s: 25702.16579270363
  timestamp: 1637043672
  timesteps_since_restore: 11616000
  timesteps_this_iter: 96000
  timesteps_total: 17376000
  training_iteration: 181
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    181 |          25702.2 | 17376000 |   729.98 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 2.99
    apples_agent-0_min: 0
    apples_agent-1_max: 134
    apples_agent-1_mean: 21.97
    apples_agent-1_min: 0
    apples_agent-2_max: 96
    apples_agent-2_mean: 6.99
    apples_agent-2_min: 0
    apples_agent-3_max: 128
    apples_agent-3_mean: 75.51
    apples_agent-3_min: 0
    apples_agent-4_max: 62
    apples_agent-4_mean: 2.55
    apples_agent-4_min: 0
    apples_agent-5_max: 136
    apples_agent-5_mean: 83.84
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 472
    cleaning_beam_agent-0_mean: 316.06
    cleaning_beam_agent-0_min: 196
    cleaning_beam_agent-1_max: 400
    cleaning_beam_agent-1_mean: 237.58
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 552
    cleaning_beam_agent-2_mean: 319.57
    cleaning_beam_agent-2_min: 168
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 40.93
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 455
    cleaning_beam_agent-4_mean: 350.33
    cleaning_beam_agent-4_min: 231
    cleaning_beam_agent-5_max: 456
    cleaning_beam_agent-5_mean: 69.65
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-23-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 904.9999999999866
  episode_reward_mean: 717.9499999999892
  episode_reward_min: 208.99999999999852
  episodes_this_iter: 96
  episodes_total: 17472
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12474.998
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 1.1061437129974365
        entropy_coeff: 0.0017600000137463212
        kl: 0.006454112008213997
        model: {}
        policy_loss: -0.02034466713666916
        total_loss: -0.01956215128302574
        vf_explained_var: 0.11964385211467743
        vf_loss: 14.385087966918945
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 1.162668228149414
        entropy_coeff: 0.0017600000137463212
        kl: 0.008118953555822372
        model: {}
        policy_loss: -0.02410958707332611
        total_loss: -0.022865168750286102
        vf_explained_var: -0.02034926414489746
        vf_loss: 16.669208526611328
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 1.1651378870010376
        entropy_coeff: 0.0017600000137463212
        kl: 0.008024360984563828
        model: {}
        policy_loss: -0.022409712895751
        total_loss: -0.021329551935195923
        vf_explained_var: 0.06582950055599213
        vf_loss: 15.259308815002441
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 0.616424560546875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0053858766332268715
        model: {}
        policy_loss: -0.015587514266371727
        total_loss: -0.014256110414862633
        vf_explained_var: 0.1816776543855667
        vf_loss: 13.391368865966797
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 1.0340943336486816
        entropy_coeff: 0.0017600000137463212
        kl: 0.008136427029967308
        model: {}
        policy_loss: -0.024815550073981285
        total_loss: -0.023390254005789757
        vf_explained_var: 0.010117024183273315
        vf_loss: 16.18014907836914
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001757376012392342
        entropy: 0.9605140686035156
        entropy_coeff: 0.0017600000137463212
        kl: 0.007742813788354397
        model: {}
        policy_loss: -0.02253870666027069
        total_loss: -0.021246185526251793
        vf_explained_var: 0.12240637838840485
        vf_loss: 14.344654083251953
    load_time_ms: 13372.868
    num_steps_sampled: 17472000
    num_steps_trained: 17472000
    sample_time_ms: 101725.984
    update_time_ms: 15.111
  iterations_since_restore: 122
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.814444444444447
    ram_util_percent: 13.528888888888888
  pid: 30948
  policy_reward_max:
    agent-0: 150.83333333333348
    agent-1: 150.83333333333348
    agent-2: 150.83333333333348
    agent-3: 150.83333333333348
    agent-4: 150.83333333333348
    agent-5: 150.83333333333348
  policy_reward_mean:
    agent-0: 119.6583333333337
    agent-1: 119.6583333333337
    agent-2: 119.6583333333337
    agent-3: 119.6583333333337
    agent-4: 119.6583333333337
    agent-5: 119.6583333333337
  policy_reward_min:
    agent-0: 34.83333333333337
    agent-1: 34.83333333333337
    agent-2: 34.83333333333337
    agent-3: 34.83333333333337
    agent-4: 34.83333333333337
    agent-5: 34.83333333333337
  sampler_perf:
    mean_env_wait_ms: 27.282729680815205
    mean_inference_ms: 12.988282334099345
    mean_processing_ms: 58.56289075756881
  time_since_restore: 16702.592709064484
  time_this_iter_s: 126.43873023986816
  time_total_s: 25828.604522943497
  timestamp: 1637043798
  timesteps_since_restore: 11712000
  timesteps_this_iter: 96000
  timesteps_total: 17472000
  training_iteration: 182
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    182 |          25828.6 | 17472000 |   717.95 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 2.79
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 23.23
    apples_agent-1_min: 0
    apples_agent-2_max: 95
    apples_agent-2_mean: 10.96
    apples_agent-2_min: 0
    apples_agent-3_max: 123
    apples_agent-3_mean: 74.88
    apples_agent-3_min: 28
    apples_agent-4_max: 77
    apples_agent-4_mean: 3.89
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 81.47
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 306.7
    cleaning_beam_agent-0_min: 196
    cleaning_beam_agent-1_max: 513
    cleaning_beam_agent-1_mean: 237.97
    cleaning_beam_agent-1_min: 113
    cleaning_beam_agent-2_max: 483
    cleaning_beam_agent-2_mean: 304.95
    cleaning_beam_agent-2_min: 168
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 40.47
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 453
    cleaning_beam_agent-4_mean: 339.55
    cleaning_beam_agent-4_min: 164
    cleaning_beam_agent-5_max: 153
    cleaning_beam_agent-5_mean: 66.88
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-25-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 900.9999999999768
  episode_reward_mean: 709.7499999999902
  episode_reward_min: 208.99999999999852
  episodes_this_iter: 96
  episodes_total: 17568
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12503.841
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 1.095211386680603
        entropy_coeff: 0.0017600000137463212
        kl: 0.0071134562604129314
        model: {}
        policy_loss: -0.020914124324917793
        total_loss: -0.019779633730649948
        vf_explained_var: 0.052218958735466
        vf_loss: 16.39373779296875
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 1.1592929363250732
        entropy_coeff: 0.0017600000137463212
        kl: 0.007299093063920736
        model: {}
        policy_loss: -0.02267405018210411
        total_loss: -0.02151331678032875
        vf_explained_var: -0.0061444491147994995
        vf_loss: 17.412670135498047
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 1.1663581132888794
        entropy_coeff: 0.0017600000137463212
        kl: 0.007722197100520134
        model: {}
        policy_loss: -0.02190910093486309
        total_loss: -0.020754672586917877
        vf_explained_var: 0.03789666295051575
        vf_loss: 16.62784194946289
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 0.6187059879302979
        entropy_coeff: 0.0017600000137463212
        kl: 0.005612093489617109
        model: {}
        policy_loss: -0.015613713301718235
        total_loss: -0.014138169586658478
        vf_explained_var: 0.16458411514759064
        vf_loss: 14.420504570007324
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 1.0407116413116455
        entropy_coeff: 0.0017600000137463212
        kl: 0.00797008816152811
        model: {}
        policy_loss: -0.024456899613142014
        total_loss: -0.023031026124954224
        vf_explained_var: 0.03854338824748993
        vf_loss: 16.635055541992188
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001697472034720704
        entropy: 0.954619288444519
        entropy_coeff: 0.0017600000137463212
        kl: 0.00752976443618536
        model: {}
        policy_loss: -0.022922346368432045
        total_loss: -0.021635010838508606
        vf_explained_var: 0.15453046560287476
        vf_loss: 14.61509895324707
    load_time_ms: 13393.69
    num_steps_sampled: 17568000
    num_steps_trained: 17568000
    sample_time_ms: 101579.457
    update_time_ms: 15.303
  iterations_since_restore: 123
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.63736263736264
    ram_util_percent: 13.545604395604391
  pid: 30948
  policy_reward_max:
    agent-0: 150.16666666666623
    agent-1: 150.16666666666623
    agent-2: 150.16666666666623
    agent-3: 150.16666666666623
    agent-4: 150.16666666666623
    agent-5: 150.16666666666623
  policy_reward_mean:
    agent-0: 118.29166666666704
    agent-1: 118.29166666666704
    agent-2: 118.29166666666704
    agent-3: 118.29166666666704
    agent-4: 118.29166666666704
    agent-5: 118.29166666666704
  policy_reward_min:
    agent-0: 34.83333333333337
    agent-1: 34.83333333333337
    agent-2: 34.83333333333337
    agent-3: 34.83333333333337
    agent-4: 34.83333333333337
    agent-5: 34.83333333333337
  sampler_perf:
    mean_env_wait_ms: 27.27245805606497
    mean_inference_ms: 12.984690666428872
    mean_processing_ms: 58.539254645936325
  time_since_restore: 16830.09459042549
  time_this_iter_s: 127.50188136100769
  time_total_s: 25956.106404304504
  timestamp: 1637043926
  timesteps_since_restore: 11808000
  timesteps_this_iter: 96000
  timesteps_total: 17568000
  training_iteration: 183
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    183 |          25956.1 | 17568000 |   709.75 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 70
    apples_agent-0_mean: 5.37
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 25.73
    apples_agent-1_min: 0
    apples_agent-2_max: 112
    apples_agent-2_mean: 7.99
    apples_agent-2_min: 0
    apples_agent-3_max: 153
    apples_agent-3_mean: 77.55
    apples_agent-3_min: 32
    apples_agent-4_max: 56
    apples_agent-4_mean: 2.89
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 84.8
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 466
    cleaning_beam_agent-0_mean: 305.14
    cleaning_beam_agent-0_min: 195
    cleaning_beam_agent-1_max: 413
    cleaning_beam_agent-1_mean: 240.97
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 454
    cleaning_beam_agent-2_mean: 312.64
    cleaning_beam_agent-2_min: 134
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 38.81
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 433
    cleaning_beam_agent-4_mean: 346.89
    cleaning_beam_agent-4_min: 204
    cleaning_beam_agent-5_max: 354
    cleaning_beam_agent-5_mean: 64.6
    cleaning_beam_agent-5_min: 24
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-27-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 923.9999999999775
  episode_reward_mean: 739.4099999999896
  episode_reward_min: 473.0000000000052
  episodes_this_iter: 96
  episodes_total: 17664
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12479.812
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 1.1002302169799805
        entropy_coeff: 0.0017600000137463212
        kl: 0.007066155318170786
        model: {}
        policy_loss: -0.02017756551504135
        total_loss: -0.019131187349557877
        vf_explained_var: 0.04055000841617584
        vf_loss: 15.69552993774414
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 1.1600635051727295
        entropy_coeff: 0.0017600000137463212
        kl: 0.007717828266322613
        model: {}
        policy_loss: -0.022500839084386826
        total_loss: -0.021359452977776527
        vf_explained_var: -0.0004025697708129883
        vf_loss: 16.39536476135254
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 1.1556127071380615
        entropy_coeff: 0.0017600000137463212
        kl: 0.007610808592289686
        model: {}
        policy_loss: -0.021125508472323418
        total_loss: -0.020068731158971786
        vf_explained_var: 0.03932926058769226
        vf_loss: 15.6849365234375
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 0.6041544079780579
        entropy_coeff: 0.0017600000137463212
        kl: 0.005053284578025341
        model: {}
        policy_loss: -0.015329400077462196
        total_loss: -0.013932515867054462
        vf_explained_var: 0.11141255497932434
        vf_loss: 14.495377540588379
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 1.0403387546539307
        entropy_coeff: 0.0017600000137463212
        kl: 0.007871396839618683
        model: {}
        policy_loss: -0.023379188030958176
        total_loss: -0.02202095463871956
        vf_explained_var: 0.012787014245986938
        vf_loss: 16.14946746826172
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00016375680570490658
        entropy: 0.9346081018447876
        entropy_coeff: 0.0017600000137463212
        kl: 0.007487396243959665
        model: {}
        policy_loss: -0.022089140489697456
        total_loss: -0.02080441638827324
        vf_explained_var: 0.12232880294322968
        vf_loss: 14.32157039642334
    load_time_ms: 13407.144
    num_steps_sampled: 17664000
    num_steps_trained: 17664000
    sample_time_ms: 101506.565
    update_time_ms: 15.011
  iterations_since_restore: 124
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.691256830601095
    ram_util_percent: 13.511475409836063
  pid: 30948
  policy_reward_max:
    agent-0: 153.99999999999997
    agent-1: 153.99999999999997
    agent-2: 153.99999999999997
    agent-3: 153.99999999999997
    agent-4: 153.99999999999997
    agent-5: 153.99999999999997
  policy_reward_mean:
    agent-0: 123.23500000000037
    agent-1: 123.23500000000037
    agent-2: 123.23500000000037
    agent-3: 123.23500000000037
    agent-4: 123.23500000000037
    agent-5: 123.23500000000037
  policy_reward_min:
    agent-0: 78.83333333333341
    agent-1: 78.83333333333341
    agent-2: 78.83333333333341
    agent-3: 78.83333333333341
    agent-4: 78.83333333333341
    agent-5: 78.83333333333341
  sampler_perf:
    mean_env_wait_ms: 27.264820283932703
    mean_inference_ms: 12.981956187304206
    mean_processing_ms: 58.51982897166826
  time_since_restore: 16958.392897844315
  time_this_iter_s: 128.29830741882324
  time_total_s: 26084.404711723328
  timestamp: 1637044055
  timesteps_since_restore: 11904000
  timesteps_this_iter: 96000
  timesteps_total: 17664000
  training_iteration: 184
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    184 |          26084.4 | 17664000 |   739.41 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 3.35
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 23.8
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 10.33
    apples_agent-2_min: 0
    apples_agent-3_max: 139
    apples_agent-3_mean: 78.37
    apples_agent-3_min: 37
    apples_agent-4_max: 56
    apples_agent-4_mean: 2.23
    apples_agent-4_min: 0
    apples_agent-5_max: 150
    apples_agent-5_mean: 84.12
    apples_agent-5_min: 10
    cleaning_beam_agent-0_max: 418
    cleaning_beam_agent-0_mean: 313.34
    cleaning_beam_agent-0_min: 201
    cleaning_beam_agent-1_max: 426
    cleaning_beam_agent-1_mean: 233.59
    cleaning_beam_agent-1_min: 87
    cleaning_beam_agent-2_max: 542
    cleaning_beam_agent-2_mean: 297.74
    cleaning_beam_agent-2_min: 164
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 38.48
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 437
    cleaning_beam_agent-4_mean: 341.47
    cleaning_beam_agent-4_min: 214
    cleaning_beam_agent-5_max: 417
    cleaning_beam_agent-5_mean: 66.12
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-29-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 998.9999999999758
  episode_reward_mean: 728.8399999999896
  episode_reward_min: 310.9999999999992
  episodes_this_iter: 96
  episodes_total: 17760
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12495.239
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 1.0971314907073975
        entropy_coeff: 0.0017600000137463212
        kl: 0.006744328886270523
        model: {}
        policy_loss: -0.020495334640145302
        total_loss: -0.019373618066310883
        vf_explained_var: 0.055831849575042725
        vf_loss: 17.03798484802246
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 1.1559697389602661
        entropy_coeff: 0.0017600000137463212
        kl: 0.007092170882970095
        model: {}
        policy_loss: -0.02193392440676689
        total_loss: -0.020723406225442886
        vf_explained_var: -0.01260286569595337
        vf_loss: 18.26590919494629
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 1.1681132316589355
        entropy_coeff: 0.0017600000137463212
        kl: 0.007561874110251665
        model: {}
        policy_loss: -0.022358570247888565
        total_loss: -0.02124517783522606
        vf_explained_var: 0.08096246421337128
        vf_loss: 16.56900405883789
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 0.6199535727500916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0051692938432097435
        model: {}
        policy_loss: -0.01573851890861988
        total_loss: -0.01433885283768177
        vf_explained_var: 0.19189320504665375
        vf_loss: 14.569253921508789
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 1.047715425491333
        entropy_coeff: 0.0017600000137463212
        kl: 0.007608981803059578
        model: {}
        policy_loss: -0.02392115630209446
        total_loss: -0.02246875688433647
        vf_explained_var: 0.015765681862831116
        vf_loss: 17.745803833007812
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015776639338582754
        entropy: 0.9545043706893921
        entropy_coeff: 0.0017600000137463212
        kl: 0.007478613872081041
        model: {}
        policy_loss: -0.022226572036743164
        total_loss: -0.02090553008019924
        vf_explained_var: 0.1650114357471466
        vf_loss: 15.052495956420898
    load_time_ms: 13397.426
    num_steps_sampled: 17760000
    num_steps_trained: 17760000
    sample_time_ms: 101452.156
    update_time_ms: 14.885
  iterations_since_restore: 125
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.779558011049723
    ram_util_percent: 13.523204419889503
  pid: 30948
  policy_reward_max:
    agent-0: 166.49999999999986
    agent-1: 166.49999999999986
    agent-2: 166.49999999999986
    agent-3: 166.49999999999986
    agent-4: 166.49999999999986
    agent-5: 166.49999999999986
  policy_reward_mean:
    agent-0: 121.47333333333368
    agent-1: 121.47333333333368
    agent-2: 121.47333333333368
    agent-3: 121.47333333333368
    agent-4: 121.47333333333368
    agent-5: 121.47333333333368
  policy_reward_min:
    agent-0: 51.83333333333325
    agent-1: 51.83333333333325
    agent-2: 51.83333333333325
    agent-3: 51.83333333333325
    agent-4: 51.83333333333325
    agent-5: 51.83333333333325
  sampler_perf:
    mean_env_wait_ms: 27.25374442377582
    mean_inference_ms: 12.977973873650692
    mean_processing_ms: 58.4997980104812
  time_since_restore: 17085.69602417946
  time_this_iter_s: 127.30312633514404
  time_total_s: 26211.70783805847
  timestamp: 1637044182
  timesteps_since_restore: 12000000
  timesteps_this_iter: 96000
  timesteps_total: 17760000
  training_iteration: 185
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    185 |          26211.7 | 17760000 |   728.84 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 2.48
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 25.23
    apples_agent-1_min: 0
    apples_agent-2_max: 275
    apples_agent-2_mean: 16.12
    apples_agent-2_min: 0
    apples_agent-3_max: 216
    apples_agent-3_mean: 77.15
    apples_agent-3_min: 29
    apples_agent-4_max: 54
    apples_agent-4_mean: 2.02
    apples_agent-4_min: 0
    apples_agent-5_max: 199
    apples_agent-5_mean: 88.75
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 460
    cleaning_beam_agent-0_mean: 323.52
    cleaning_beam_agent-0_min: 231
    cleaning_beam_agent-1_max: 384
    cleaning_beam_agent-1_mean: 230.31
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 466
    cleaning_beam_agent-2_mean: 295.07
    cleaning_beam_agent-2_min: 103
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 40.51
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 422
    cleaning_beam_agent-4_mean: 334.98
    cleaning_beam_agent-4_min: 246
    cleaning_beam_agent-5_max: 145
    cleaning_beam_agent-5_mean: 58.56
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-31-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 920.9999999999828
  episode_reward_mean: 735.4799999999891
  episode_reward_min: 301.9999999999986
  episodes_this_iter: 96
  episodes_total: 17856
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12477.627
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 1.084216833114624
        entropy_coeff: 0.0017600000137463212
        kl: 0.006577285006642342
        model: {}
        policy_loss: -0.01938590407371521
        total_loss: -0.01843610778450966
        vf_explained_var: 0.06454576551914215
        vf_loss: 15.425607681274414
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 1.158442735671997
        entropy_coeff: 0.0017600000137463212
        kl: 0.0068578701466321945
        model: {}
        policy_loss: -0.021306496113538742
        total_loss: -0.02033246122300625
        vf_explained_var: 0.005834057927131653
        vf_loss: 16.413169860839844
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 1.162192940711975
        entropy_coeff: 0.0017600000137463212
        kl: 0.007311013527214527
        model: {}
        policy_loss: -0.02051946148276329
        total_loss: -0.01954391971230507
        vf_explained_var: 0.05515080690383911
        vf_loss: 15.587989807128906
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 0.6051201820373535
        entropy_coeff: 0.0017600000137463212
        kl: 0.005347561091184616
        model: {}
        policy_loss: -0.01457013376057148
        total_loss: -0.013096068985760212
        vf_explained_var: 0.10950757563114166
        vf_loss: 14.695619583129883
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 1.0459518432617188
        entropy_coeff: 0.0017600000137463212
        kl: 0.007589305751025677
        model: {}
        policy_loss: -0.02278164215385914
        total_loss: -0.02148456498980522
        vf_explained_var: 0.017702504992485046
        vf_loss: 16.200912475585938
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00015177599561866373
        entropy: 0.9376856088638306
        entropy_coeff: 0.0017600000137463212
        kl: 0.006852833554148674
        model: {}
        policy_loss: -0.02130756713449955
        total_loss: -0.020173143595457077
        vf_explained_var: 0.14115281403064728
        vf_loss: 14.141838073730469
    load_time_ms: 13398.962
    num_steps_sampled: 17856000
    num_steps_trained: 17856000
    sample_time_ms: 101513.822
    update_time_ms: 14.306
  iterations_since_restore: 126
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 18.178688524590164
    ram_util_percent: 13.579234972677595
  pid: 30948
  policy_reward_max:
    agent-0: 153.49999999999997
    agent-1: 153.49999999999997
    agent-2: 153.49999999999997
    agent-3: 153.49999999999997
    agent-4: 153.49999999999997
    agent-5: 153.49999999999997
  policy_reward_mean:
    agent-0: 122.58000000000038
    agent-1: 122.58000000000038
    agent-2: 122.58000000000038
    agent-3: 122.58000000000038
    agent-4: 122.58000000000038
    agent-5: 122.58000000000038
  policy_reward_min:
    agent-0: 50.333333333333265
    agent-1: 50.333333333333265
    agent-2: 50.333333333333265
    agent-3: 50.333333333333265
    agent-4: 50.333333333333265
    agent-5: 50.333333333333265
  sampler_perf:
    mean_env_wait_ms: 27.24487144505486
    mean_inference_ms: 12.975146502661847
    mean_processing_ms: 58.48165582872151
  time_since_restore: 17214.039795398712
  time_this_iter_s: 128.34377121925354
  time_total_s: 26340.051609277725
  timestamp: 1637044311
  timesteps_since_restore: 12096000
  timesteps_this_iter: 96000
  timesteps_total: 17856000
  training_iteration: 186
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    186 |          26340.1 | 17856000 |   735.48 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 75
    apples_agent-0_mean: 2.36
    apples_agent-0_min: 0
    apples_agent-1_max: 139
    apples_agent-1_mean: 26.18
    apples_agent-1_min: 0
    apples_agent-2_max: 70
    apples_agent-2_mean: 9.83
    apples_agent-2_min: 0
    apples_agent-3_max: 149
    apples_agent-3_mean: 77.57
    apples_agent-3_min: 41
    apples_agent-4_max: 112
    apples_agent-4_mean: 4.64
    apples_agent-4_min: 0
    apples_agent-5_max: 152
    apples_agent-5_mean: 90.48
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 412
    cleaning_beam_agent-0_mean: 319.19
    cleaning_beam_agent-0_min: 180
    cleaning_beam_agent-1_max: 385
    cleaning_beam_agent-1_mean: 216.19
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 449
    cleaning_beam_agent-2_mean: 301.2
    cleaning_beam_agent-2_min: 143
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 42.88
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 439
    cleaning_beam_agent-4_mean: 331.2
    cleaning_beam_agent-4_min: 189
    cleaning_beam_agent-5_max: 195
    cleaning_beam_agent-5_mean: 71.58
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-34-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 912.999999999965
  episode_reward_mean: 723.9699999999896
  episode_reward_min: 249.99999999999616
  episodes_this_iter: 96
  episodes_total: 17952
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12510.834
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 1.0768868923187256
        entropy_coeff: 0.0017600000137463212
        kl: 0.006790349259972572
        model: {}
        policy_loss: -0.019681721925735474
        total_loss: -0.01863769441843033
        vf_explained_var: 0.10383342206478119
        vf_loss: 15.81273078918457
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 1.1440255641937256
        entropy_coeff: 0.0017600000137463212
        kl: 0.007091787178069353
        model: {}
        policy_loss: -0.02175859920680523
        total_loss: -0.020510002970695496
        vf_explained_var: -0.04275774955749512
        vf_loss: 18.437238693237305
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 1.155514121055603
        entropy_coeff: 0.0017600000137463212
        kl: 0.007172172423452139
        model: {}
        policy_loss: -0.021293845027685165
        total_loss: -0.02019082009792328
        vf_explained_var: 0.03485213220119476
        vf_loss: 17.022930145263672
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 0.6235425472259521
        entropy_coeff: 0.0017600000137463212
        kl: 0.005256212316453457
        model: {}
        policy_loss: -0.015531953424215317
        total_loss: -0.01410485990345478
        vf_explained_var: 0.16554692387580872
        vf_loss: 14.73287582397461
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 1.0611090660095215
        entropy_coeff: 0.0017600000137463212
        kl: 0.007230663672089577
        model: {}
        policy_loss: -0.022539187222719193
        total_loss: -0.021324248984456062
        vf_explained_var: 0.07242617011070251
        vf_loss: 16.363534927368164
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00014578559785149992
        entropy: 0.956118106842041
        entropy_coeff: 0.0017600000137463212
        kl: 0.007239151746034622
        model: {}
        policy_loss: -0.021374501287937164
        total_loss: -0.02009269967675209
        vf_explained_var: 0.1401713788509369
        vf_loss: 15.167424201965332
    load_time_ms: 13412.387
    num_steps_sampled: 17952000
    num_steps_trained: 17952000
    sample_time_ms: 101696.657
    update_time_ms: 14.088
  iterations_since_restore: 127
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.691256830601095
    ram_util_percent: 13.591256830601091
  pid: 30948
  policy_reward_max:
    agent-0: 152.16666666666612
    agent-1: 152.16666666666612
    agent-2: 152.16666666666612
    agent-3: 152.16666666666612
    agent-4: 152.16666666666612
    agent-5: 152.16666666666612
  policy_reward_mean:
    agent-0: 120.66166666666703
    agent-1: 120.66166666666703
    agent-2: 120.66166666666703
    agent-3: 120.66166666666703
    agent-4: 120.66166666666703
    agent-5: 120.66166666666703
  policy_reward_min:
    agent-0: 41.666666666666615
    agent-1: 41.666666666666615
    agent-2: 41.666666666666615
    agent-3: 41.666666666666615
    agent-4: 41.666666666666615
    agent-5: 41.666666666666615
  sampler_perf:
    mean_env_wait_ms: 27.23747885983177
    mean_inference_ms: 12.973114237072501
    mean_processing_ms: 58.463476500865816
  time_since_restore: 17342.88915014267
  time_this_iter_s: 128.84935474395752
  time_total_s: 26468.900964021683
  timestamp: 1637044440
  timesteps_since_restore: 12192000
  timesteps_this_iter: 96000
  timesteps_total: 17952000
  training_iteration: 187
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    187 |          26468.9 | 17952000 |   723.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 75
    apples_agent-0_mean: 4.24
    apples_agent-0_min: 0
    apples_agent-1_max: 144
    apples_agent-1_mean: 23.15
    apples_agent-1_min: 0
    apples_agent-2_max: 68
    apples_agent-2_mean: 8.13
    apples_agent-2_min: 0
    apples_agent-3_max: 138
    apples_agent-3_mean: 76.28
    apples_agent-3_min: 28
    apples_agent-4_max: 112
    apples_agent-4_mean: 3.37
    apples_agent-4_min: 0
    apples_agent-5_max: 152
    apples_agent-5_mean: 85.64
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 438
    cleaning_beam_agent-0_mean: 318.82
    cleaning_beam_agent-0_min: 180
    cleaning_beam_agent-1_max: 432
    cleaning_beam_agent-1_mean: 222.18
    cleaning_beam_agent-1_min: 52
    cleaning_beam_agent-2_max: 457
    cleaning_beam_agent-2_mean: 318.56
    cleaning_beam_agent-2_min: 176
    cleaning_beam_agent-3_max: 94
    cleaning_beam_agent-3_mean: 45.35
    cleaning_beam_agent-3_min: 23
    cleaning_beam_agent-4_max: 430
    cleaning_beam_agent-4_mean: 332.68
    cleaning_beam_agent-4_min: 197
    cleaning_beam_agent-5_max: 280
    cleaning_beam_agent-5_mean: 72.66
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-36-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 924.9999999999773
  episode_reward_mean: 719.3599999999907
  episode_reward_min: 300.9999999999966
  episodes_this_iter: 96
  episodes_total: 18048
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12567.234
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001397952000843361
        entropy: 1.0803605318069458
        entropy_coeff: 0.0017600000137463212
        kl: 0.006779419258236885
        model: {}
        policy_loss: -0.019592881202697754
        total_loss: -0.018593546003103256
        vf_explained_var: 0.09756481647491455
        vf_loss: 15.448898315429688
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001397952000843361
        entropy: 1.1541469097137451
        entropy_coeff: 0.0017600000137463212
        kl: 0.007590210065245628
        model: {}
        policy_loss: -0.022013090550899506
        total_loss: -0.020806152373552322
        vf_explained_var: -0.0044403523206710815
        vf_loss: 17.201955795288086
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001397952000843361
        entropy: 1.169575810432434
        entropy_coeff: 0.0017600000137463212
        kl: 0.007051406893879175
        model: {}
        policy_loss: -0.01934150606393814
        total_loss: -0.018376898020505905
        vf_explained_var: 0.05591095983982086
        vf_loss: 16.1278076171875
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001397952000843361
        entropy: 0.6235224604606628
        entropy_coeff: 0.0017600000137463212
        kl: 0.00498173339292407
        model: {}
        policy_loss: -0.014633131213486195
        total_loss: -0.013253627344965935
        vf_explained_var: 0.13292574882507324
        vf_loss: 14.805562973022461
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001397952000843361
        entropy: 1.0517492294311523
        entropy_coeff: 0.0017600000137463212
        kl: 0.007187020033597946
        model: {}
        policy_loss: -0.02196776121854782
        total_loss: -0.02074819803237915
        vf_explained_var: 0.04470530152320862
        vf_loss: 16.332380294799805
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001397952000843361
        entropy: 0.952994704246521
        entropy_coeff: 0.0017600000137463212
        kl: 0.006599048618227243
        model: {}
        policy_loss: -0.020702645182609558
        total_loss: -0.019581926986575127
        vf_explained_var: 0.13552220165729523
        vf_loss: 14.781778335571289
    load_time_ms: 13412.696
    num_steps_sampled: 18048000
    num_steps_trained: 18048000
    sample_time_ms: 102020.744
    update_time_ms: 13.948
  iterations_since_restore: 128
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.516216216216215
    ram_util_percent: 13.523783783783783
  pid: 30948
  policy_reward_max:
    agent-0: 154.16666666666666
    agent-1: 154.16666666666666
    agent-2: 154.16666666666666
    agent-3: 154.16666666666666
    agent-4: 154.16666666666666
    agent-5: 154.16666666666666
  policy_reward_mean:
    agent-0: 119.89333333333364
    agent-1: 119.89333333333364
    agent-2: 119.89333333333364
    agent-3: 119.89333333333364
    agent-4: 119.89333333333364
    agent-5: 119.89333333333364
  policy_reward_min:
    agent-0: 50.16666666666661
    agent-1: 50.16666666666661
    agent-2: 50.16666666666661
    agent-3: 50.16666666666661
    agent-4: 50.16666666666661
    agent-5: 50.16666666666661
  sampler_perf:
    mean_env_wait_ms: 27.23053409007921
    mean_inference_ms: 12.9709610490028
    mean_processing_ms: 58.44365518782209
  time_since_restore: 17472.561635255814
  time_this_iter_s: 129.67248511314392
  time_total_s: 26598.573449134827
  timestamp: 1637044569
  timesteps_since_restore: 12288000
  timesteps_this_iter: 96000
  timesteps_total: 18048000
  training_iteration: 188
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    188 |          26598.6 | 18048000 |   719.36 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 2.73
    apples_agent-0_min: 0
    apples_agent-1_max: 131
    apples_agent-1_mean: 30.22
    apples_agent-1_min: 0
    apples_agent-2_max: 112
    apples_agent-2_mean: 10.6
    apples_agent-2_min: 0
    apples_agent-3_max: 145
    apples_agent-3_mean: 72.84
    apples_agent-3_min: 33
    apples_agent-4_max: 64
    apples_agent-4_mean: 2.88
    apples_agent-4_min: 0
    apples_agent-5_max: 144
    apples_agent-5_mean: 85.66
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 482
    cleaning_beam_agent-0_mean: 324.86
    cleaning_beam_agent-0_min: 206
    cleaning_beam_agent-1_max: 369
    cleaning_beam_agent-1_mean: 201.74
    cleaning_beam_agent-1_min: 52
    cleaning_beam_agent-2_max: 495
    cleaning_beam_agent-2_mean: 303.69
    cleaning_beam_agent-2_min: 159
    cleaning_beam_agent-3_max: 114
    cleaning_beam_agent-3_mean: 44.51
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 430
    cleaning_beam_agent-4_mean: 337.51
    cleaning_beam_agent-4_min: 197
    cleaning_beam_agent-5_max: 228
    cleaning_beam_agent-5_mean: 76.26
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-38-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 913.9999999999778
  episode_reward_mean: 715.7499999999902
  episode_reward_min: 395.0000000000041
  episodes_this_iter: 96
  episodes_total: 18144
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12359.04
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001338048023171723
        entropy: 1.0729446411132812
        entropy_coeff: 0.0017600000137463212
        kl: 0.006278106477111578
        model: {}
        policy_loss: -0.01940462552011013
        total_loss: -0.01852767914533615
        vf_explained_var: 0.08044126629829407
        vf_loss: 15.097113609313965
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001338048023171723
        entropy: 1.1655657291412354
        entropy_coeff: 0.0017600000137463212
        kl: 0.00648995116353035
        model: {}
        policy_loss: -0.020145010203123093
        total_loss: -0.019245807081460953
        vf_explained_var: -0.006009101867675781
        vf_loss: 16.526107788085938
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001338048023171723
        entropy: 1.1784520149230957
        entropy_coeff: 0.0017600000137463212
        kl: 0.007307892665266991
        model: {}
        policy_loss: -0.020104799419641495
        total_loss: -0.01919517107307911
        vf_explained_var: 0.0721273124217987
        vf_loss: 15.22120475769043
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0001338048023171723
        entropy: 0.6302894949913025
        entropy_coeff: 0.0017600000137463212
        kl: 0.005359489005059004
        model: {}
        policy_loss: -0.014441926032304764
        total_loss: -0.013579954393208027
        vf_explained_var: 0.12636645138263702
        vf_loss: 14.353343963623047
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001338048023171723
        entropy: 1.0440016984939575
        entropy_coeff: 0.0017600000137463212
        kl: 0.0072697801515460014
        model: {}
        policy_loss: -0.020994342863559723
        total_loss: -0.019773881882429123
        vf_explained_var: 0.022386878728866577
        vf_loss: 16.03949737548828
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0001338048023171723
        entropy: 0.9533330202102661
        entropy_coeff: 0.0017600000137463212
        kl: 0.006836216896772385
        model: {}
        policy_loss: -0.020428843796253204
        total_loss: -0.019313205033540726
        vf_explained_var: 0.13029713928699493
        vf_loss: 14.262580871582031
    load_time_ms: 13435.765
    num_steps_sampled: 18144000
    num_steps_trained: 18144000
    sample_time_ms: 102095.112
    update_time_ms: 14.052
  iterations_since_restore: 129
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.47945945945946
    ram_util_percent: 13.521621621621621
  pid: 30948
  policy_reward_max:
    agent-0: 152.33333333333317
    agent-1: 152.33333333333317
    agent-2: 152.33333333333317
    agent-3: 152.33333333333317
    agent-4: 152.33333333333317
    agent-5: 152.33333333333317
  policy_reward_mean:
    agent-0: 119.29166666666706
    agent-1: 119.29166666666706
    agent-2: 119.29166666666706
    agent-3: 119.29166666666706
    agent-4: 119.29166666666706
    agent-5: 119.29166666666706
  policy_reward_min:
    agent-0: 65.8333333333332
    agent-1: 65.8333333333332
    agent-2: 65.8333333333332
    agent-3: 65.8333333333332
    agent-4: 65.8333333333332
    agent-5: 65.8333333333332
  sampler_perf:
    mean_env_wait_ms: 27.222958690803992
    mean_inference_ms: 12.968911325321084
    mean_processing_ms: 58.42463284818183
  time_since_restore: 17602.025525569916
  time_this_iter_s: 129.46389031410217
  time_total_s: 26728.03733944893
  timestamp: 1637044699
  timesteps_since_restore: 12384000
  timesteps_this_iter: 96000
  timesteps_total: 18144000
  training_iteration: 189
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    189 |            26728 | 18144000 |   715.75 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 2.66
    apples_agent-0_min: 0
    apples_agent-1_max: 131
    apples_agent-1_mean: 27.74
    apples_agent-1_min: 0
    apples_agent-2_max: 119
    apples_agent-2_mean: 10.42
    apples_agent-2_min: 0
    apples_agent-3_max: 177
    apples_agent-3_mean: 76.48
    apples_agent-3_min: 21
    apples_agent-4_max: 62
    apples_agent-4_mean: 3.1
    apples_agent-4_min: 0
    apples_agent-5_max: 166
    apples_agent-5_mean: 84.75
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 480
    cleaning_beam_agent-0_mean: 325.63
    cleaning_beam_agent-0_min: 178
    cleaning_beam_agent-1_max: 441
    cleaning_beam_agent-1_mean: 221.14
    cleaning_beam_agent-1_min: 98
    cleaning_beam_agent-2_max: 458
    cleaning_beam_agent-2_mean: 314.26
    cleaning_beam_agent-2_min: 144
    cleaning_beam_agent-3_max: 209
    cleaning_beam_agent-3_mean: 44.52
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 431
    cleaning_beam_agent-4_mean: 336.89
    cleaning_beam_agent-4_min: 228
    cleaning_beam_agent-5_max: 246
    cleaning_beam_agent-5_mean: 72.74
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-40-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 928.9999999999817
  episode_reward_mean: 724.3899999999894
  episode_reward_min: 394.00000000000364
  episodes_this_iter: 96
  episodes_total: 18240
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12332.89
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012781440455000848
        entropy: 1.0570776462554932
        entropy_coeff: 0.0017600000137463212
        kl: 0.006170295178890228
        model: {}
        policy_loss: -0.01851591467857361
        total_loss: -0.017543721944093704
        vf_explained_var: 0.03300260007381439
        vf_loss: 15.985904693603516
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012781440455000848
        entropy: 1.1508510112762451
        entropy_coeff: 0.0017600000137463212
        kl: 0.006688805762678385
        model: {}
        policy_loss: -0.020001841709017754
        total_loss: -0.01897638477385044
        vf_explained_var: -0.03546890616416931
        vf_loss: 17.13193130493164
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012781440455000848
        entropy: 1.1570448875427246
        entropy_coeff: 0.0017600000137463212
        kl: 0.007039082236588001
        model: {}
        policy_loss: -0.019431903958320618
        total_loss: -0.018498972058296204
        vf_explained_var: 0.05468189716339111
        vf_loss: 15.615135192871094
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012781440455000848
        entropy: 0.6135237216949463
        entropy_coeff: 0.0017600000137463212
        kl: 0.0053971088491380215
        model: {}
        policy_loss: -0.014320757240056992
        total_loss: -0.01340576820075512
        vf_explained_var: 0.1194099634885788
        vf_loss: 14.550787925720215
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012781440455000848
        entropy: 1.0509458780288696
        entropy_coeff: 0.0017600000137463212
        kl: 0.006885120645165443
        model: {}
        policy_loss: -0.02144535258412361
        total_loss: -0.020278846845030785
        vf_explained_var: 0.008482962846755981
        vf_loss: 16.39148712158203
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012781440455000848
        entropy: 0.9533953666687012
        entropy_coeff: 0.0017600000137463212
        kl: 0.006446033716201782
        model: {}
        policy_loss: -0.01956907846033573
        total_loss: -0.018456855788826942
        vf_explained_var: 0.09060400724411011
        vf_loss: 15.009965896606445
    load_time_ms: 13437.896
    num_steps_sampled: 18240000
    num_steps_trained: 18240000
    sample_time_ms: 102324.917
    update_time_ms: 14.273
  iterations_since_restore: 130
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.619672131147542
    ram_util_percent: 13.584699453551913
  pid: 30948
  policy_reward_max:
    agent-0: 154.83333333333348
    agent-1: 154.83333333333348
    agent-2: 154.83333333333348
    agent-3: 154.83333333333348
    agent-4: 154.83333333333348
    agent-5: 154.83333333333348
  policy_reward_mean:
    agent-0: 120.73166666666704
    agent-1: 120.73166666666704
    agent-2: 120.73166666666704
    agent-3: 120.73166666666704
    agent-4: 120.73166666666704
    agent-5: 120.73166666666704
  policy_reward_min:
    agent-0: 65.66666666666652
    agent-1: 65.66666666666652
    agent-2: 65.66666666666652
    agent-3: 65.66666666666652
    agent-4: 65.66666666666652
    agent-5: 65.66666666666652
  sampler_perf:
    mean_env_wait_ms: 27.215392429985215
    mean_inference_ms: 12.966774060171593
    mean_processing_ms: 58.405437563045986
  time_since_restore: 17730.672293424606
  time_this_iter_s: 128.64676785469055
  time_total_s: 26856.68410730362
  timestamp: 1637044828
  timesteps_since_restore: 12480000
  timesteps_this_iter: 96000
  timesteps_total: 18240000
  training_iteration: 190
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    190 |          26856.7 | 18240000 |   724.39 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 118
    apples_agent-0_mean: 4.39
    apples_agent-0_min: 0
    apples_agent-1_max: 146
    apples_agent-1_mean: 27.06
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 10.03
    apples_agent-2_min: 0
    apples_agent-3_max: 152
    apples_agent-3_mean: 79.82
    apples_agent-3_min: 38
    apples_agent-4_max: 54
    apples_agent-4_mean: 2.27
    apples_agent-4_min: 0
    apples_agent-5_max: 160
    apples_agent-5_mean: 82.61
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 454
    cleaning_beam_agent-0_mean: 325.91
    cleaning_beam_agent-0_min: 190
    cleaning_beam_agent-1_max: 491
    cleaning_beam_agent-1_mean: 223.84
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 454
    cleaning_beam_agent-2_mean: 314.15
    cleaning_beam_agent-2_min: 141
    cleaning_beam_agent-3_max: 120
    cleaning_beam_agent-3_mean: 44.64
    cleaning_beam_agent-3_min: 17
    cleaning_beam_agent-4_max: 439
    cleaning_beam_agent-4_mean: 336.15
    cleaning_beam_agent-4_min: 241
    cleaning_beam_agent-5_max: 230
    cleaning_beam_agent-5_mean: 69.19
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-42-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 921.9999999999751
  episode_reward_mean: 735.8899999999899
  episode_reward_min: 387.00000000000574
  episodes_this_iter: 96
  episodes_total: 18336
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12368.591
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012182399950688705
        entropy: 1.0559041500091553
        entropy_coeff: 0.0017600000137463212
        kl: 0.006270408630371094
        model: {}
        policy_loss: -0.017880000174045563
        total_loss: -0.016882607713341713
        vf_explained_var: 0.04442150890827179
        vf_loss: 16.017059326171875
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012182399950688705
        entropy: 1.157143473625183
        entropy_coeff: 0.0017600000137463212
        kl: 0.006354048382490873
        model: {}
        policy_loss: -0.02001223713159561
        total_loss: -0.01910986565053463
        vf_explained_var: 0.004170522093772888
        vf_loss: 16.681365966796875
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012182399950688705
        entropy: 1.1632254123687744
        entropy_coeff: 0.0017600000137463212
        kl: 0.006852398626506329
        model: {}
        policy_loss: -0.019396036863327026
        total_loss: -0.01853790134191513
        vf_explained_var: 0.0824052095413208
        vf_loss: 15.349357604980469
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00012182399950688705
        entropy: 0.611670196056366
        entropy_coeff: 0.0017600000137463212
        kl: 0.005141919478774071
        model: {}
        policy_loss: -0.013269475661218166
        total_loss: -0.012400164268910885
        vf_explained_var: 0.14403490722179413
        vf_loss: 14.316603660583496
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012182399950688705
        entropy: 1.044419527053833
        entropy_coeff: 0.0017600000137463212
        kl: 0.006516129709780216
        model: {}
        policy_loss: -0.01995887979865074
        total_loss: -0.01885412447154522
        vf_explained_var: 0.020870044827461243
        vf_loss: 16.397064208984375
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00012182399950688705
        entropy: 0.9453967809677124
        entropy_coeff: 0.0017600000137463212
        kl: 0.0063289497047662735
        model: {}
        policy_loss: -0.019032582640647888
        total_loss: -0.01796591654419899
        vf_explained_var: 0.12609094381332397
        vf_loss: 14.6477632522583
    load_time_ms: 13441.424
    num_steps_sampled: 18336000
    num_steps_trained: 18336000
    sample_time_ms: 102465.153
    update_time_ms: 14.323
  iterations_since_restore: 131
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.55054347826087
    ram_util_percent: 13.597826086956522
  pid: 30948
  policy_reward_max:
    agent-0: 153.6666666666666
    agent-1: 153.6666666666666
    agent-2: 153.6666666666666
    agent-3: 153.6666666666666
    agent-4: 153.6666666666666
    agent-5: 153.6666666666666
  policy_reward_mean:
    agent-0: 122.64833333333362
    agent-1: 122.64833333333362
    agent-2: 122.64833333333362
    agent-3: 122.64833333333362
    agent-4: 122.64833333333362
    agent-5: 122.64833333333362
  policy_reward_min:
    agent-0: 64.49999999999977
    agent-1: 64.49999999999977
    agent-2: 64.49999999999977
    agent-3: 64.49999999999977
    agent-4: 64.49999999999977
    agent-5: 64.49999999999977
  sampler_perf:
    mean_env_wait_ms: 27.208588572646114
    mean_inference_ms: 12.964891465836109
    mean_processing_ms: 58.38670773551907
  time_since_restore: 17859.907642364502
  time_this_iter_s: 129.23534893989563
  time_total_s: 26985.919456243515
  timestamp: 1637044957
  timesteps_since_restore: 12576000
  timesteps_this_iter: 96000
  timesteps_total: 18336000
  training_iteration: 191
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    191 |          26985.9 | 18336000 |   735.89 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 75
    apples_agent-0_mean: 4.68
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 23.76
    apples_agent-1_min: 0
    apples_agent-2_max: 192
    apples_agent-2_mean: 11.23
    apples_agent-2_min: 0
    apples_agent-3_max: 119
    apples_agent-3_mean: 77.18
    apples_agent-3_min: 27
    apples_agent-4_max: 54
    apples_agent-4_mean: 4.26
    apples_agent-4_min: 0
    apples_agent-5_max: 166
    apples_agent-5_mean: 86.79
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 461
    cleaning_beam_agent-0_mean: 330.96
    cleaning_beam_agent-0_min: 200
    cleaning_beam_agent-1_max: 491
    cleaning_beam_agent-1_mean: 234.51
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 561
    cleaning_beam_agent-2_mean: 314.84
    cleaning_beam_agent-2_min: 164
    cleaning_beam_agent-3_max: 115
    cleaning_beam_agent-3_mean: 43.55
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 427
    cleaning_beam_agent-4_mean: 330.07
    cleaning_beam_agent-4_min: 209
    cleaning_beam_agent-5_max: 321
    cleaning_beam_agent-5_mean: 70.95
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-44-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 931.9999999999827
  episode_reward_mean: 727.87999999999
  episode_reward_min: 388.00000000001097
  episodes_this_iter: 96
  episodes_total: 18432
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12356.777
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00011583360173972324
        entropy: 1.049298644065857
        entropy_coeff: 0.0017600000137463212
        kl: 0.0054978798143565655
        model: {}
        policy_loss: -0.017358597368001938
        total_loss: -0.016523858532309532
        vf_explained_var: 0.0908215194940567
        vf_loss: 15.819302558898926
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00011583360173972324
        entropy: 1.1482889652252197
        entropy_coeff: 0.0017600000137463212
        kl: 0.005681807175278664
        model: {}
        policy_loss: -0.017923306673765182
        total_loss: -0.017056429758667946
        vf_explained_var: -0.006628885865211487
        vf_loss: 17.515047073364258
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00011583360173972324
        entropy: 1.1603704690933228
        entropy_coeff: 0.0017600000137463212
        kl: 0.006781494244933128
        model: {}
        policy_loss: -0.018702395260334015
        total_loss: -0.01777893677353859
        vf_explained_var: 0.07522615790367126
        vf_loss: 16.094085693359375
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00011583360173972324
        entropy: 0.6136397123336792
        entropy_coeff: 0.0017600000137463212
        kl: 0.005092039238661528
        model: {}
        policy_loss: -0.013105833902955055
        total_loss: -0.012186565436422825
        vf_explained_var: 0.14397035539150238
        vf_loss: 14.900711059570312
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00011583360173972324
        entropy: 1.0506916046142578
        entropy_coeff: 0.0017600000137463212
        kl: 0.006492258980870247
        model: {}
        policy_loss: -0.02022661082446575
        total_loss: -0.01910877786576748
        vf_explained_var: 0.04156644642353058
        vf_loss: 16.685989379882812
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00011583360173972324
        entropy: 0.955777108669281
        entropy_coeff: 0.0017600000137463212
        kl: 0.006261291913688183
        model: {}
        policy_loss: -0.01914067193865776
        total_loss: -0.018107334151864052
        vf_explained_var: 0.1588888317346573
        vf_loss: 14.632454872131348
    load_time_ms: 13442.103
    num_steps_sampled: 18432000
    num_steps_trained: 18432000
    sample_time_ms: 102672.739
    update_time_ms: 14.659
  iterations_since_restore: 132
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.660655737704918
    ram_util_percent: 13.528961748633876
  pid: 30948
  policy_reward_max:
    agent-0: 155.3333333333333
    agent-1: 155.3333333333333
    agent-2: 155.3333333333333
    agent-3: 155.3333333333333
    agent-4: 155.3333333333333
    agent-5: 155.3333333333333
  policy_reward_mean:
    agent-0: 121.31333333333367
    agent-1: 121.31333333333367
    agent-2: 121.31333333333367
    agent-3: 121.31333333333367
    agent-4: 121.31333333333367
    agent-5: 121.31333333333367
  policy_reward_min:
    agent-0: 64.66666666666636
    agent-1: 64.66666666666636
    agent-2: 64.66666666666636
    agent-3: 64.66666666666636
    agent-4: 64.66666666666636
    agent-5: 64.66666666666636
  sampler_perf:
    mean_env_wait_ms: 27.20236379522529
    mean_inference_ms: 12.962519072774112
    mean_processing_ms: 58.36857527893498
  time_since_restore: 17988.259790420532
  time_this_iter_s: 128.35214805603027
  time_total_s: 27114.271604299545
  timestamp: 1637045085
  timesteps_since_restore: 12672000
  timesteps_this_iter: 96000
  timesteps_total: 18432000
  training_iteration: 192
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    192 |          27114.3 | 18432000 |   727.88 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 100
    apples_agent-0_mean: 7.97
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 22.18
    apples_agent-1_min: 0
    apples_agent-2_max: 82
    apples_agent-2_mean: 9.05
    apples_agent-2_min: 0
    apples_agent-3_max: 151
    apples_agent-3_mean: 80.22
    apples_agent-3_min: 43
    apples_agent-4_max: 67
    apples_agent-4_mean: 1.22
    apples_agent-4_min: 0
    apples_agent-5_max: 166
    apples_agent-5_mean: 89.26
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 451
    cleaning_beam_agent-0_mean: 328.22
    cleaning_beam_agent-0_min: 208
    cleaning_beam_agent-1_max: 420
    cleaning_beam_agent-1_mean: 225.23
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 487
    cleaning_beam_agent-2_mean: 324.65
    cleaning_beam_agent-2_min: 139
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 36.53
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 416
    cleaning_beam_agent-4_mean: 339.9
    cleaning_beam_agent-4_min: 218
    cleaning_beam_agent-5_max: 196
    cleaning_beam_agent-5_mean: 68.0
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-46-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 953.9999999999789
  episode_reward_mean: 764.269999999987
  episode_reward_min: 510.0000000000203
  episodes_this_iter: 96
  episodes_total: 18528
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12295.579
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00010984319669660181
        entropy: 1.0539413690567017
        entropy_coeff: 0.0017600000137463212
        kl: 0.00535334600135684
        model: {}
        policy_loss: -0.016084402799606323
        total_loss: -0.015352829359471798
        vf_explained_var: 0.06602519750595093
        vf_loss: 15.158411026000977
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00010984319669660181
        entropy: 1.1498075723648071
        entropy_coeff: 0.0017600000137463212
        kl: 0.005934048909693956
        model: {}
        policy_loss: -0.017780164256691933
        total_loss: -0.016979532316327095
        vf_explained_var: -0.0028884857892990112
        vf_loss: 16.374828338623047
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00010984319669660181
        entropy: 1.1541666984558105
        entropy_coeff: 0.0017600000137463212
        kl: 0.005863341037184
        model: {}
        policy_loss: -0.017393499612808228
        total_loss: -0.01672450080513954
        vf_explained_var: 0.05579085648059845
        vf_loss: 15.27665901184082
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.00010984319669660181
        entropy: 0.582226037979126
        entropy_coeff: 0.0017600000137463212
        kl: 0.0045592100359499454
        model: {}
        policy_loss: -0.01209981832653284
        total_loss: -0.011229829862713814
        vf_explained_var: 0.1081080436706543
        vf_loss: 14.387840270996094
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00010984319669660181
        entropy: 1.0340865850448608
        entropy_coeff: 0.0017600000137463212
        kl: 0.006040065083652735
        model: {}
        policy_loss: -0.01892736181616783
        total_loss: -0.01798192225396633
        vf_explained_var: 0.03752397000789642
        vf_loss: 15.574163436889648
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.00010984319669660181
        entropy: 0.9520271420478821
        entropy_coeff: 0.0017600000137463212
        kl: 0.005770825780928135
        model: {}
        policy_loss: -0.01789810135960579
        total_loss: -0.01700686477124691
        vf_explained_var: 0.12763118743896484
        vf_loss: 14.12636661529541
    load_time_ms: 13454.797
    num_steps_sampled: 18528000
    num_steps_trained: 18528000
    sample_time_ms: 102788.924
    update_time_ms: 14.307
  iterations_since_restore: 133
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.678021978021977
    ram_util_percent: 13.524725274725272
  pid: 30948
  policy_reward_max:
    agent-0: 158.99999999999977
    agent-1: 158.99999999999977
    agent-2: 158.99999999999977
    agent-3: 158.99999999999977
    agent-4: 158.99999999999977
    agent-5: 158.99999999999977
  policy_reward_mean:
    agent-0: 127.37833333333373
    agent-1: 127.37833333333373
    agent-2: 127.37833333333373
    agent-3: 127.37833333333373
    agent-4: 127.37833333333373
    agent-5: 127.37833333333373
  policy_reward_min:
    agent-0: 85.00000000000036
    agent-1: 85.00000000000036
    agent-2: 85.00000000000036
    agent-3: 85.00000000000036
    agent-4: 85.00000000000036
    agent-5: 85.00000000000036
  sampler_perf:
    mean_env_wait_ms: 27.19521615816706
    mean_inference_ms: 12.960463671405234
    mean_processing_ms: 58.349417512850394
  time_since_restore: 18116.4719479084
  time_this_iter_s: 128.21215748786926
  time_total_s: 27242.483761787415
  timestamp: 1637045214
  timesteps_since_restore: 12768000
  timesteps_this_iter: 96000
  timesteps_total: 18528000
  training_iteration: 193
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    193 |          27242.5 | 18528000 |   764.27 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 2.36
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 26.36
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 7.61
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 77.17
    apples_agent-3_min: 40
    apples_agent-4_max: 57
    apples_agent-4_mean: 3.04
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 90.63
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 483
    cleaning_beam_agent-0_mean: 342.84
    cleaning_beam_agent-0_min: 220
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 218.06
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 511
    cleaning_beam_agent-2_mean: 328.17
    cleaning_beam_agent-2_min: 177
    cleaning_beam_agent-3_max: 84
    cleaning_beam_agent-3_mean: 38.65
    cleaning_beam_agent-3_min: 18
    cleaning_beam_agent-4_max: 439
    cleaning_beam_agent-4_mean: 338.62
    cleaning_beam_agent-4_min: 170
    cleaning_beam_agent-5_max: 274
    cleaning_beam_agent-5_mean: 66.12
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-49-02
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 956.9999999999745
  episode_reward_mean: 755.9699999999882
  episode_reward_min: 369.00000000000233
  episodes_this_iter: 96
  episodes_total: 18624
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12273.782
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000103852798929438
        entropy: 1.058188557624817
        entropy_coeff: 0.0017600000137463212
        kl: 0.005416907835751772
        model: {}
        policy_loss: -0.016226282343268394
        total_loss: -0.01548094768077135
        vf_explained_var: 0.06564085185527802
        vf_loss: 15.243663787841797
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000103852798929438
        entropy: 1.1522682905197144
        entropy_coeff: 0.0017600000137463212
        kl: 0.005460814572870731
        model: {}
        policy_loss: -0.017512323334813118
        total_loss: -0.016828719526529312
        vf_explained_var: 0.010239794850349426
        vf_loss: 16.194368362426758
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000103852798929438
        entropy: 1.156622052192688
        entropy_coeff: 0.0017600000137463212
        kl: 0.006019902415573597
        model: {}
        policy_loss: -0.018150070682168007
        total_loss: -0.017393633723258972
        vf_explained_var: 0.027163222432136536
        vf_loss: 15.88113784790039
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.000103852798929438
        entropy: 0.5806982517242432
        entropy_coeff: 0.0017600000137463212
        kl: 0.004941741935908794
        model: {}
        policy_loss: -0.011733213439583778
        total_loss: -0.011093677952885628
        vf_explained_var: 0.13263146579265594
        vf_loss: 14.144789695739746
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000103852798929438
        entropy: 1.0419577360153198
        entropy_coeff: 0.0017600000137463212
        kl: 0.005539068020880222
        model: {}
        policy_loss: -0.018155504018068314
        total_loss: -0.017334338277578354
        vf_explained_var: 0.053804829716682434
        vf_loss: 15.472017288208008
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.000103852798929438
        entropy: 0.9610053300857544
        entropy_coeff: 0.0017600000137463212
        kl: 0.00581669807434082
        model: {}
        policy_loss: -0.018117377534508705
        total_loss: -0.01719718426465988
        vf_explained_var: 0.11339853703975677
        vf_loss: 14.482219696044922
    load_time_ms: 13441.283
    num_steps_sampled: 18624000
    num_steps_trained: 18624000
    sample_time_ms: 102842.043
    update_time_ms: 14.377
  iterations_since_restore: 134
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.627173913043478
    ram_util_percent: 13.50869565217391
  pid: 30948
  policy_reward_max:
    agent-0: 159.49999999999986
    agent-1: 159.49999999999986
    agent-2: 159.49999999999986
    agent-3: 159.49999999999986
    agent-4: 159.49999999999986
    agent-5: 159.49999999999986
  policy_reward_mean:
    agent-0: 125.9950000000003
    agent-1: 125.9950000000003
    agent-2: 125.9950000000003
    agent-3: 125.9950000000003
    agent-4: 125.9950000000003
    agent-5: 125.9950000000003
  policy_reward_min:
    agent-0: 61.49999999999982
    agent-1: 61.49999999999982
    agent-2: 61.49999999999982
    agent-3: 61.49999999999982
    agent-4: 61.49999999999982
    agent-5: 61.49999999999982
  sampler_perf:
    mean_env_wait_ms: 27.189484119228133
    mean_inference_ms: 12.958272630167464
    mean_processing_ms: 58.33139600192738
  time_since_restore: 18244.913480997086
  time_this_iter_s: 128.44153308868408
  time_total_s: 27370.9252948761
  timestamp: 1637045342
  timesteps_since_restore: 12864000
  timesteps_this_iter: 96000
  timesteps_total: 18624000
  training_iteration: 194
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    194 |          27370.9 | 18624000 |   755.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 2.33
    apples_agent-0_min: 0
    apples_agent-1_max: 117
    apples_agent-1_mean: 29.4
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 7.92
    apples_agent-2_min: 0
    apples_agent-3_max: 130
    apples_agent-3_mean: 80.08
    apples_agent-3_min: 30
    apples_agent-4_max: 58
    apples_agent-4_mean: 1.87
    apples_agent-4_min: 0
    apples_agent-5_max: 200
    apples_agent-5_mean: 88.16
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 477
    cleaning_beam_agent-0_mean: 350.54
    cleaning_beam_agent-0_min: 202
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 200.17
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 453
    cleaning_beam_agent-2_mean: 323.3
    cleaning_beam_agent-2_min: 126
    cleaning_beam_agent-3_max: 112
    cleaning_beam_agent-3_mean: 41.27
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 442
    cleaning_beam_agent-4_mean: 344.28
    cleaning_beam_agent-4_min: 246
    cleaning_beam_agent-5_max: 175
    cleaning_beam_agent-5_mean: 62.65
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-51-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 956.9999999999745
  episode_reward_mean: 755.0599999999871
  episode_reward_min: 202.99999999999804
  episodes_this_iter: 96
  episodes_total: 18720
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12258.662
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.786240116227418e-05
        entropy: 1.0375080108642578
        entropy_coeff: 0.0017600000137463212
        kl: 0.00485374266281724
        model: {}
        policy_loss: -0.015033677220344543
        total_loss: -0.014325114898383617
        vf_explained_var: 0.09241674840450287
        vf_loss: 15.638269424438477
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.786240116227418e-05
        entropy: 1.1661465167999268
        entropy_coeff: 0.0017600000137463212
        kl: 0.005921357776969671
        model: {}
        policy_loss: -0.017936309799551964
        total_loss: -0.017053453251719475
        vf_explained_var: -0.015167564153671265
        vf_loss: 17.5100040435791
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.786240116227418e-05
        entropy: 1.1633074283599854
        entropy_coeff: 0.0017600000137463212
        kl: 0.006333144381642342
        model: {}
        policy_loss: -0.017446298152208328
        total_loss: -0.01659795641899109
        vf_explained_var: 0.056021034717559814
        vf_loss: 16.29132080078125
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 9.786240116227418e-05
        entropy: 0.592964768409729
        entropy_coeff: 0.0017600000137463212
        kl: 0.00547392014414072
        model: {}
        policy_loss: -0.011432424187660217
        total_loss: -0.010877640917897224
        vf_explained_var: 0.15269266068935394
        vf_loss: 14.615533828735352
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.786240116227418e-05
        entropy: 1.0430152416229248
        entropy_coeff: 0.0017600000137463212
        kl: 0.005342863965779543
        model: {}
        policy_loss: -0.017130956053733826
        total_loss: -0.016214942559599876
        vf_explained_var: 0.023009106516838074
        vf_loss: 16.831497192382812
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.786240116227418e-05
        entropy: 0.9446566700935364
        entropy_coeff: 0.0017600000137463212
        kl: 0.005744381342083216
        model: {}
        policy_loss: -0.0175556018948555
        total_loss: -0.016557680442929268
        vf_explained_var: 0.12227658927440643
        vf_loss: 15.116406440734863
    load_time_ms: 13463.205
    num_steps_sampled: 18720000
    num_steps_trained: 18720000
    sample_time_ms: 102861.344
    update_time_ms: 14.302
  iterations_since_restore: 135
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.51954022988506
    ram_util_percent: 13.539655172413791
  pid: 30948
  policy_reward_max:
    agent-0: 159.49999999999986
    agent-1: 159.49999999999986
    agent-2: 159.49999999999986
    agent-3: 159.49999999999986
    agent-4: 159.49999999999986
    agent-5: 159.49999999999986
  policy_reward_mean:
    agent-0: 125.8433333333337
    agent-1: 125.8433333333337
    agent-2: 125.8433333333337
    agent-3: 125.8433333333337
    agent-4: 125.8433333333337
    agent-5: 125.8433333333337
  policy_reward_min:
    agent-0: 33.833333333333364
    agent-1: 33.833333333333364
    agent-2: 33.833333333333364
    agent-3: 33.833333333333364
    agent-4: 33.833333333333364
    agent-5: 33.833333333333364
  sampler_perf:
    mean_env_wait_ms: 27.181514371613016
    mean_inference_ms: 12.956021771099884
    mean_processing_ms: 58.31251828397941
  time_since_restore: 18372.513245105743
  time_this_iter_s: 127.59976410865784
  time_total_s: 27498.525058984756
  timestamp: 1637045470
  timesteps_since_restore: 12960000
  timesteps_this_iter: 96000
  timesteps_total: 18720000
  training_iteration: 195
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    195 |          27498.5 | 18720000 |   755.06 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 2.34
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 26.34
    apples_agent-1_min: 0
    apples_agent-2_max: 109
    apples_agent-2_mean: 8.93
    apples_agent-2_min: 0
    apples_agent-3_max: 118
    apples_agent-3_mean: 71.08
    apples_agent-3_min: 30
    apples_agent-4_max: 56
    apples_agent-4_mean: 3.84
    apples_agent-4_min: 0
    apples_agent-5_max: 200
    apples_agent-5_mean: 90.02
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 457
    cleaning_beam_agent-0_mean: 351.03
    cleaning_beam_agent-0_min: 277
    cleaning_beam_agent-1_max: 354
    cleaning_beam_agent-1_mean: 201.18
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 477
    cleaning_beam_agent-2_mean: 320.34
    cleaning_beam_agent-2_min: 131
    cleaning_beam_agent-3_max: 123
    cleaning_beam_agent-3_mean: 42.62
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 427
    cleaning_beam_agent-4_mean: 338.63
    cleaning_beam_agent-4_min: 197
    cleaning_beam_agent-5_max: 283
    cleaning_beam_agent-5_mean: 71.56
    cleaning_beam_agent-5_min: 23
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-53-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 950.999999999969
  episode_reward_mean: 743.5499999999889
  episode_reward_min: 404.00000000000125
  episodes_this_iter: 96
  episodes_total: 18816
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12238.572
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 9.187200339511037e-05
        entropy: 1.055016040802002
        entropy_coeff: 0.0017600000137463212
        kl: 0.005411789286881685
        model: {}
        policy_loss: -0.015022143721580505
        total_loss: -0.014656526036560535
        vf_explained_var: 0.07125312089920044
        vf_loss: 16.81268310546875
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.187200339511037e-05
        entropy: 1.1666312217712402
        entropy_coeff: 0.0017600000137463212
        kl: 0.005124709568917751
        model: {}
        policy_loss: -0.016540324315428734
        total_loss: -0.015769651159644127
        vf_explained_var: 0.005922272801399231
        vf_loss: 17.990015029907227
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.187200339511037e-05
        entropy: 1.1676732301712036
        entropy_coeff: 0.0017600000137463212
        kl: 0.006210106424987316
        model: {}
        policy_loss: -0.016383647918701172
        total_loss: -0.01547729130834341
        vf_explained_var: 0.05053745210170746
        vf_loss: 17.19443130493164
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 9.187200339511037e-05
        entropy: 0.5936670899391174
        entropy_coeff: 0.0017600000137463212
        kl: 0.005019368603825569
        model: {}
        policy_loss: -0.01167280599474907
        total_loss: -0.01104380376636982
        vf_explained_var: 0.14441193640232086
        vf_loss: 15.483720779418945
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.187200339511037e-05
        entropy: 1.0363258123397827
        entropy_coeff: 0.0017600000137463212
        kl: 0.00566756771877408
        model: {}
        policy_loss: -0.017060192301869392
        total_loss: -0.016031499952077866
        vf_explained_var: 0.051172420382499695
        vf_loss: 17.191110610961914
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 9.187200339511037e-05
        entropy: 0.9612888097763062
        entropy_coeff: 0.0017600000137463212
        kl: 0.005506039131432772
        model: {}
        policy_loss: -0.017017990350723267
        total_loss: -0.016072172671556473
        vf_explained_var: 0.15140970051288605
        vf_loss: 15.364806175231934
    load_time_ms: 13470.878
    num_steps_sampled: 18816000
    num_steps_trained: 18816000
    sample_time_ms: 102885.2
    update_time_ms: 14.658
  iterations_since_restore: 136
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.59781420765027
    ram_util_percent: 13.53606557377049
  pid: 30948
  policy_reward_max:
    agent-0: 158.49999999999991
    agent-1: 158.49999999999991
    agent-2: 158.49999999999991
    agent-3: 158.49999999999991
    agent-4: 158.49999999999991
    agent-5: 158.49999999999991
  policy_reward_mean:
    agent-0: 123.92500000000032
    agent-1: 123.92500000000032
    agent-2: 123.92500000000032
    agent-3: 123.92500000000032
    agent-4: 123.92500000000032
    agent-5: 123.92500000000032
  policy_reward_min:
    agent-0: 67.33333333333321
    agent-1: 67.33333333333321
    agent-2: 67.33333333333321
    agent-3: 67.33333333333321
    agent-4: 67.33333333333321
    agent-5: 67.33333333333321
  sampler_perf:
    mean_env_wait_ms: 27.174347331026393
    mean_inference_ms: 12.953613423830808
    mean_processing_ms: 58.293458395820274
  time_since_restore: 18500.977554798126
  time_this_iter_s: 128.4643096923828
  time_total_s: 27626.98936867714
  timestamp: 1637045599
  timesteps_since_restore: 13056000
  timesteps_this_iter: 96000
  timesteps_total: 18816000
  training_iteration: 196
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    196 |            27627 | 18816000 |   743.55 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 3.03
    apples_agent-0_min: 0
    apples_agent-1_max: 81
    apples_agent-1_mean: 30.09
    apples_agent-1_min: 0
    apples_agent-2_max: 138
    apples_agent-2_mean: 7.74
    apples_agent-2_min: 0
    apples_agent-3_max: 116
    apples_agent-3_mean: 71.73
    apples_agent-3_min: 20
    apples_agent-4_max: 54
    apples_agent-4_mean: 2.32
    apples_agent-4_min: 0
    apples_agent-5_max: 144
    apples_agent-5_mean: 84.79
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 457
    cleaning_beam_agent-0_mean: 340.21
    cleaning_beam_agent-0_min: 235
    cleaning_beam_agent-1_max: 362
    cleaning_beam_agent-1_mean: 203.56
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 562
    cleaning_beam_agent-2_mean: 322.06
    cleaning_beam_agent-2_min: 163
    cleaning_beam_agent-3_max: 99
    cleaning_beam_agent-3_mean: 41.48
    cleaning_beam_agent-3_min: 15
    cleaning_beam_agent-4_max: 440
    cleaning_beam_agent-4_mean: 338.34
    cleaning_beam_agent-4_min: 217
    cleaning_beam_agent-5_max: 241
    cleaning_beam_agent-5_mean: 65.73
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-55-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 933.9999999999784
  episode_reward_mean: 751.3899999999885
  episode_reward_min: 500.0000000000062
  episodes_this_iter: 96
  episodes_total: 18912
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12208.433
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 8.588159835198894e-05
        entropy: 1.0405261516571045
        entropy_coeff: 0.0017600000137463212
        kl: 0.005303476471453905
        model: {}
        policy_loss: -0.014435410499572754
        total_loss: -0.014283750206232071
        vf_explained_var: 0.08715985715389252
        vf_loss: 14.526418685913086
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 8.588159835198894e-05
        entropy: 1.1608033180236816
        entropy_coeff: 0.0017600000137463212
        kl: 0.005508915521204472
        model: {}
        policy_loss: -0.016501829028129578
        total_loss: -0.015828296542167664
        vf_explained_var: -0.012195691466331482
        vf_loss: 16.147640228271484
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 8.588159835198894e-05
        entropy: 1.1622430086135864
        entropy_coeff: 0.0017600000137463212
        kl: 0.005382485222071409
        model: {}
        policy_loss: -0.015876326709985733
        total_loss: -0.015328167006373405
        vf_explained_var: 0.04516215622425079
        vf_loss: 15.172131538391113
      agent-3:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 8.588159835198894e-05
        entropy: 0.5868549942970276
        entropy_coeff: 0.0017600000137463212
        kl: 0.004357263445854187
        model: {}
        policy_loss: -0.010297372937202454
        total_loss: -0.009871695190668106
        vf_explained_var: 0.15123318135738373
        vf_loss: 13.496131896972656
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 8.588159835198894e-05
        entropy: 1.045694351196289
        entropy_coeff: 0.0017600000137463212
        kl: 0.005128261633217335
        model: {}
        policy_loss: -0.015883101150393486
        total_loss: -0.015148937702178955
        vf_explained_var: 0.026649966835975647
        vf_loss: 15.489334106445312
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 8.588159835198894e-05
        entropy: 0.94642573595047
        entropy_coeff: 0.0017600000137463212
        kl: 0.004576041363179684
        model: {}
        policy_loss: -0.01522496435791254
        total_loss: -0.014514656737446785
        vf_explained_var: 0.08202095329761505
        vf_loss: 14.608088493347168
    load_time_ms: 13471.298
    num_steps_sampled: 18912000
    num_steps_trained: 18912000
    sample_time_ms: 102802.735
    update_time_ms: 14.586
  iterations_since_restore: 137
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 17.795580110497237
    ram_util_percent: 13.585082872928174
  pid: 30948
  policy_reward_max:
    agent-0: 155.66666666666654
    agent-1: 155.66666666666654
    agent-2: 155.66666666666654
    agent-3: 155.66666666666654
    agent-4: 155.66666666666654
    agent-5: 155.66666666666654
  policy_reward_mean:
    agent-0: 125.23166666666702
    agent-1: 125.23166666666702
    agent-2: 125.23166666666702
    agent-3: 125.23166666666702
    agent-4: 125.23166666666702
    agent-5: 125.23166666666702
  policy_reward_min:
    agent-0: 83.3333333333335
    agent-1: 83.3333333333335
    agent-2: 83.3333333333335
    agent-3: 83.3333333333335
    agent-4: 83.3333333333335
    agent-5: 83.3333333333335
  sampler_perf:
    mean_env_wait_ms: 27.16773070349484
    mean_inference_ms: 12.95178493140874
    mean_processing_ms: 58.276108369708446
  time_since_restore: 18628.704236745834
  time_this_iter_s: 127.72668194770813
  time_total_s: 27754.716050624847
  timestamp: 1637045727
  timesteps_since_restore: 13152000
  timesteps_this_iter: 96000
  timesteps_total: 18912000
  training_iteration: 197
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 24.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    197 |          27754.7 | 18912000 |   751.39 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 2.23
    apples_agent-0_min: 0
    apples_agent-1_max: 200
    apples_agent-1_mean: 31.53
    apples_agent-1_min: 0
    apples_agent-2_max: 102
    apples_agent-2_mean: 8.43
    apples_agent-2_min: 0
    apples_agent-3_max: 145
    apples_agent-3_mean: 76.66
    apples_agent-3_min: 30
    apples_agent-4_max: 85
    apples_agent-4_mean: 2.39
    apples_agent-4_min: 0
    apples_agent-5_max: 140
    apples_agent-5_mean: 88.27
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 457
    cleaning_beam_agent-0_mean: 346.63
    cleaning_beam_agent-0_min: 257
    cleaning_beam_agent-1_max: 359
    cleaning_beam_agent-1_mean: 178.71
    cleaning_beam_agent-1_min: 72
    cleaning_beam_agent-2_max: 470
    cleaning_beam_agent-2_mean: 305.44
    cleaning_beam_agent-2_min: 175
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 41.26
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 443
    cleaning_beam_agent-4_mean: 346.0
    cleaning_beam_agent-4_min: 207
    cleaning_beam_agent-5_max: 164
    cleaning_beam_agent-5_mean: 58.53
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-57-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 924.9999999999798
  episode_reward_mean: 737.39999999999
  episode_reward_min: 250.99999999999685
  episodes_this_iter: 96
  episodes_total: 19008
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12168.647
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.989120058482513e-05
        entropy: 1.0324965715408325
        entropy_coeff: 0.0017600000137463212
        kl: 0.005757050588726997
        model: {}
        policy_loss: -0.014238082803785801
        total_loss: -0.013839242048561573
        vf_explained_var: 0.12154155969619751
        vf_loss: 16.40330696105957
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 7.989120058482513e-05
        entropy: 1.1533278226852417
        entropy_coeff: 0.0017600000137463212
        kl: 0.005222642794251442
        model: {}
        policy_loss: -0.0151975704357028
        total_loss: -0.014284984208643436
        vf_explained_var: -0.015880778431892395
        vf_loss: 18.97913360595703
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 7.989120058482513e-05
        entropy: 1.1867902278900146
        entropy_coeff: 0.0017600000137463212
        kl: 0.005071534775197506
        model: {}
        policy_loss: -0.015032459981739521
        total_loss: -0.01438713725656271
        vf_explained_var: 0.07825063169002533
        vf_loss: 17.19768714904785
      agent-3:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 7.989120058482513e-05
        entropy: 0.6035841703414917
        entropy_coeff: 0.0017600000137463212
        kl: 0.0044779423624277115
        model: {}
        policy_loss: -0.010964020155370235
        total_loss: -0.010432729497551918
        vf_explained_var: 0.1763489544391632
        vf_loss: 15.376276969909668
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 7.989120058482513e-05
        entropy: 1.0355746746063232
        entropy_coeff: 0.0017600000137463212
        kl: 0.004926925990730524
        model: {}
        policy_loss: -0.015758533030748367
        total_loss: -0.014849497005343437
        vf_explained_var: 0.06429746747016907
        vf_loss: 17.462615966796875
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.989120058482513e-05
        entropy: 0.9583308100700378
        entropy_coeff: 0.0017600000137463212
        kl: 0.005864514969289303
        model: {}
        policy_loss: -0.015952371060848236
        total_loss: -0.015469830483198166
        vf_explained_var: 0.15241435170173645
        vf_loss: 15.827533721923828
    load_time_ms: 13474.136
    num_steps_sampled: 19008000
    num_steps_trained: 19008000
    sample_time_ms: 102760.488
    update_time_ms: 14.538
  iterations_since_restore: 138
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 20.5125
    ram_util_percent: 14.204347826086956
  pid: 30948
  policy_reward_max:
    agent-0: 154.16666666666666
    agent-1: 154.16666666666666
    agent-2: 154.16666666666666
    agent-3: 154.16666666666666
    agent-4: 154.16666666666666
    agent-5: 154.16666666666666
  policy_reward_mean:
    agent-0: 122.90000000000035
    agent-1: 122.90000000000035
    agent-2: 122.90000000000035
    agent-3: 122.90000000000035
    agent-4: 122.90000000000035
    agent-5: 122.90000000000035
  policy_reward_min:
    agent-0: 41.833333333333314
    agent-1: 41.833333333333314
    agent-2: 41.833333333333314
    agent-3: 41.833333333333314
    agent-4: 41.833333333333314
    agent-5: 41.833333333333314
  sampler_perf:
    mean_env_wait_ms: 27.160143828233448
    mean_inference_ms: 12.949863771904418
    mean_processing_ms: 58.26120131908774
  time_since_restore: 18757.588389873505
  time_this_iter_s: 128.8841531276703
  time_total_s: 27883.600203752518
  timestamp: 1637045856
  timesteps_since_restore: 13248000
  timesteps_this_iter: 96000
  timesteps_total: 19008000
  training_iteration: 198
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    198 |          27883.6 | 19008000 |    737.4 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 2.37
    apples_agent-0_min: 0
    apples_agent-1_max: 129
    apples_agent-1_mean: 25.28
    apples_agent-1_min: 0
    apples_agent-2_max: 102
    apples_agent-2_mean: 11.73
    apples_agent-2_min: 0
    apples_agent-3_max: 122
    apples_agent-3_mean: 74.64
    apples_agent-3_min: 36
    apples_agent-4_max: 65
    apples_agent-4_mean: 3.54
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 88.89
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 422
    cleaning_beam_agent-0_mean: 334.44
    cleaning_beam_agent-0_min: 173
    cleaning_beam_agent-1_max: 401
    cleaning_beam_agent-1_mean: 204.9
    cleaning_beam_agent-1_min: 87
    cleaning_beam_agent-2_max: 488
    cleaning_beam_agent-2_mean: 288.33
    cleaning_beam_agent-2_min: 119
    cleaning_beam_agent-3_max: 95
    cleaning_beam_agent-3_mean: 45.55
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 419
    cleaning_beam_agent-4_mean: 322.25
    cleaning_beam_agent-4_min: 144
    cleaning_beam_agent-5_max: 179
    cleaning_beam_agent-5_mean: 64.55
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_01-59-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 943.9999999999827
  episode_reward_mean: 738.25999999999
  episode_reward_min: 250.99999999999685
  episodes_this_iter: 96
  episodes_total: 19104
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12066.244
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.390080281766132e-05
        entropy: 1.0282450914382935
        entropy_coeff: 0.0017600000137463212
        kl: 0.004920467268675566
        model: {}
        policy_loss: -0.013096777722239494
        total_loss: -0.012827792204916477
        vf_explained_var: 0.03570106625556946
        vf_loss: 15.866508483886719
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 7.390080281766132e-05
        entropy: 1.1700259447097778
        entropy_coeff: 0.0017600000137463212
        kl: 0.004682491533458233
        model: {}
        policy_loss: -0.014720384031534195
        total_loss: -0.014173830859363079
        vf_explained_var: -0.011866778135299683
        vf_loss: 16.69304847717285
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 7.390080281766132e-05
        entropy: 1.1841431856155396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0046742805279791355
        model: {}
        policy_loss: -0.014274489134550095
        total_loss: -0.01386222429573536
        vf_explained_var: 0.05153505504131317
        vf_loss: 15.615023612976074
      agent-3:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 7.390080281766132e-05
        entropy: 0.6022477149963379
        entropy_coeff: 0.0017600000137463212
        kl: 0.004576148930937052
        model: {}
        policy_loss: -0.010508231818675995
        total_loss: -0.010128330439329147
        vf_explained_var: 0.1419590413570404
        vf_loss: 14.112582206726074
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.390080281766132e-05
        entropy: 1.0456801652908325
        entropy_coeff: 0.0017600000137463212
        kl: 0.00546584976837039
        model: {}
        policy_loss: -0.015236820094287395
        total_loss: -0.014963755384087563
        vf_explained_var: 0.04863850772380829
        vf_loss: 15.668760299682617
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 7.390080281766132e-05
        entropy: 0.9556366801261902
        entropy_coeff: 0.0017600000137463212
        kl: 0.005498579237610102
        model: {}
        policy_loss: -0.015501145273447037
        total_loss: -0.015190647915005684
        vf_explained_var: 0.12328755855560303
        vf_loss: 14.425577163696289
    load_time_ms: 13465.303
    num_steps_sampled: 19104000
    num_steps_trained: 19104000
    sample_time_ms: 102462.68
    update_time_ms: 14.448
  iterations_since_restore: 139
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.59943820224719
    ram_util_percent: 16.19157303370787
  pid: 30948
  policy_reward_max:
    agent-0: 157.33333333333314
    agent-1: 157.33333333333314
    agent-2: 157.33333333333314
    agent-3: 157.33333333333314
    agent-4: 157.33333333333314
    agent-5: 157.33333333333314
  policy_reward_mean:
    agent-0: 123.04333333333369
    agent-1: 123.04333333333369
    agent-2: 123.04333333333369
    agent-3: 123.04333333333369
    agent-4: 123.04333333333369
    agent-5: 123.04333333333369
  policy_reward_min:
    agent-0: 41.833333333333314
    agent-1: 41.833333333333314
    agent-2: 41.833333333333314
    agent-3: 41.833333333333314
    agent-4: 41.833333333333314
    agent-5: 41.833333333333314
  sampler_perf:
    mean_env_wait_ms: 27.1533114201466
    mean_inference_ms: 12.95047794794862
    mean_processing_ms: 58.25253577032019
  time_since_restore: 18882.94487977028
  time_this_iter_s: 125.35648989677429
  time_total_s: 28008.956693649292
  timestamp: 1637045981
  timesteps_since_restore: 13344000
  timesteps_this_iter: 96000
  timesteps_total: 19104000
  training_iteration: 199
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    199 |            28009 | 19104000 |   738.26 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 3.36
    apples_agent-0_min: 0
    apples_agent-1_max: 163
    apples_agent-1_mean: 27.02
    apples_agent-1_min: 0
    apples_agent-2_max: 63
    apples_agent-2_mean: 7.77
    apples_agent-2_min: 0
    apples_agent-3_max: 141
    apples_agent-3_mean: 77.3
    apples_agent-3_min: 36
    apples_agent-4_max: 60
    apples_agent-4_mean: 2.16
    apples_agent-4_min: 0
    apples_agent-5_max: 147
    apples_agent-5_mean: 92.0
    apples_agent-5_min: 46
    cleaning_beam_agent-0_max: 491
    cleaning_beam_agent-0_mean: 347.8
    cleaning_beam_agent-0_min: 226
    cleaning_beam_agent-1_max: 541
    cleaning_beam_agent-1_mean: 216.7
    cleaning_beam_agent-1_min: 61
    cleaning_beam_agent-2_max: 447
    cleaning_beam_agent-2_mean: 317.93
    cleaning_beam_agent-2_min: 166
    cleaning_beam_agent-3_max: 89
    cleaning_beam_agent-3_mean: 39.45
    cleaning_beam_agent-3_min: 16
    cleaning_beam_agent-4_max: 424
    cleaning_beam_agent-4_mean: 335.49
    cleaning_beam_agent-4_min: 85
    cleaning_beam_agent-5_max: 264
    cleaning_beam_agent-5_mean: 69.85
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-01-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 951.9999999999948
  episode_reward_mean: 778.019999999988
  episode_reward_min: 368.00000000000347
  episodes_this_iter: 96
  episodes_total: 19200
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12025.691
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.791039777453989e-05
        entropy: 1.0158344507217407
        entropy_coeff: 0.0017600000137463212
        kl: 0.005663526244461536
        model: {}
        policy_loss: -0.01223855558782816
        total_loss: -0.012189719825983047
        vf_explained_var: 0.06267520785331726
        vf_loss: 15.535318374633789
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.791039777453989e-05
        entropy: 1.1557016372680664
        entropy_coeff: 0.0017600000137463212
        kl: 0.004887949675321579
        model: {}
        policy_loss: -0.013671532273292542
        total_loss: -0.0135188028216362
        vf_explained_var: -0.017192095518112183
        vf_loss: 16.979732513427734
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.791039777453989e-05
        entropy: 1.1760311126708984
        entropy_coeff: 0.0017600000137463212
        kl: 0.005332889501005411
        model: {}
        policy_loss: -0.013687056489288807
        total_loss: -0.013673258014023304
        vf_explained_var: 0.05815716087818146
        vf_loss: 15.503273010253906
      agent-3:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 6.791039777453989e-05
        entropy: 0.5625737309455872
        entropy_coeff: 0.0017600000137463212
        kl: 0.0040601203218102455
        model: {}
        policy_loss: -0.009253621101379395
        total_loss: -0.008783834055066109
        vf_explained_var: 0.12134896218776703
        vf_loss: 14.472338676452637
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.791039777453989e-05
        entropy: 1.0375138521194458
        entropy_coeff: 0.0017600000137463212
        kl: 0.005509576760232449
        model: {}
        policy_loss: -0.01423550583422184
        total_loss: -0.013927161693572998
        vf_explained_var: 0.043663814663887024
        vf_loss: 15.834115982055664
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.791039777453989e-05
        entropy: 0.9400836825370789
        entropy_coeff: 0.0017600000137463212
        kl: 0.005141866393387318
        model: {}
        policy_loss: -0.013674935325980186
        total_loss: -0.013345402665436268
        vf_explained_var: 0.10933148860931396
        vf_loss: 14.69892692565918
    load_time_ms: 13466.011
    num_steps_sampled: 19200000
    num_steps_trained: 19200000
    sample_time_ms: 102477.278
    update_time_ms: 14.389
  iterations_since_restore: 140
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.148633879781425
    ram_util_percent: 16.207650273224044
  pid: 30948
  policy_reward_max:
    agent-0: 158.6666666666664
    agent-1: 158.6666666666664
    agent-2: 158.6666666666664
    agent-3: 158.6666666666664
    agent-4: 158.6666666666664
    agent-5: 158.6666666666664
  policy_reward_mean:
    agent-0: 129.6700000000003
    agent-1: 129.6700000000003
    agent-2: 129.6700000000003
    agent-3: 129.6700000000003
    agent-4: 129.6700000000003
    agent-5: 129.6700000000003
  policy_reward_min:
    agent-0: 61.333333333333194
    agent-1: 61.333333333333194
    agent-2: 61.333333333333194
    agent-3: 61.333333333333194
    agent-4: 61.333333333333194
    agent-5: 61.333333333333194
  sampler_perf:
    mean_env_wait_ms: 27.15025885805016
    mean_inference_ms: 12.95103458027371
    mean_processing_ms: 58.24886957743277
  time_since_restore: 19011.33862042427
  time_this_iter_s: 128.3937406539917
  time_total_s: 28137.350434303284
  timestamp: 1637046110
  timesteps_since_restore: 13440000
  timesteps_this_iter: 96000
  timesteps_total: 19200000
  training_iteration: 200
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    200 |          28137.4 | 19200000 |   778.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 1.74
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 28.19
    apples_agent-1_min: 0
    apples_agent-2_max: 115
    apples_agent-2_mean: 10.87
    apples_agent-2_min: 0
    apples_agent-3_max: 141
    apples_agent-3_mean: 79.97
    apples_agent-3_min: 25
    apples_agent-4_max: 161
    apples_agent-4_mean: 3.79
    apples_agent-4_min: 0
    apples_agent-5_max: 143
    apples_agent-5_mean: 88.58
    apples_agent-5_min: 28
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 346.76
    cleaning_beam_agent-0_min: 211
    cleaning_beam_agent-1_max: 490
    cleaning_beam_agent-1_mean: 214.32
    cleaning_beam_agent-1_min: 61
    cleaning_beam_agent-2_max: 453
    cleaning_beam_agent-2_mean: 307.35
    cleaning_beam_agent-2_min: 142
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 37.14
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 440
    cleaning_beam_agent-4_mean: 339.46
    cleaning_beam_agent-4_min: 155
    cleaning_beam_agent-5_max: 328
    cleaning_beam_agent-5_mean: 68.82
    cleaning_beam_agent-5_min: 21
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-03-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 922.9999999999816
  episode_reward_mean: 767.7699999999879
  episode_reward_min: 344.0000000000053
  episodes_this_iter: 96
  episodes_total: 19296
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12056.659
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.192000000737607e-05
        entropy: 1.0340845584869385
        entropy_coeff: 0.0017600000137463212
        kl: 0.004909220151603222
        model: {}
        policy_loss: -0.011690565384924412
        total_loss: -0.011626013554632664
        vf_explained_var: 0.049428701400756836
        vf_loss: 16.390789031982422
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 6.192000000737607e-05
        entropy: 1.1569137573242188
        entropy_coeff: 0.0017600000137463212
        kl: 0.005511037074029446
        model: {}
        policy_loss: -0.013569424860179424
        total_loss: -0.013574565760791302
        vf_explained_var: -0.014157503843307495
        vf_loss: 17.554765701293945
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.192000000737607e-05
        entropy: 1.1639723777770996
        entropy_coeff: 0.0017600000137463212
        kl: 0.0055761793628335
        model: {}
        policy_loss: -0.013480912894010544
        total_loss: -0.013328921049833298
        vf_explained_var: 0.04745699465274811
        vf_loss: 16.429649353027344
      agent-3:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 6.192000000737607e-05
        entropy: 0.5819082856178284
        entropy_coeff: 0.0017600000137463212
        kl: 0.0037085218355059624
        model: {}
        policy_loss: -0.00879990216344595
        total_loss: -0.008291786536574364
        vf_explained_var: 0.1139802634716034
        vf_loss: 15.264806747436523
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.192000000737607e-05
        entropy: 1.033411979675293
        entropy_coeff: 0.0017600000137463212
        kl: 0.004941005725413561
        model: {}
        policy_loss: -0.013430817052721977
        total_loss: -0.013082459568977356
        vf_explained_var: 0.02969580888748169
        vf_loss: 16.730606079101562
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 6.192000000737607e-05
        entropy: 0.94034343957901
        entropy_coeff: 0.0017600000137463212
        kl: 0.004911980591714382
        model: {}
        policy_loss: -0.013136280700564384
        total_loss: -0.012760363519191742
        vf_explained_var: 0.10826072096824646
        vf_loss: 15.397253036499023
    load_time_ms: 13441.03
    num_steps_sampled: 19296000
    num_steps_trained: 19296000
    sample_time_ms: 102296.936
    update_time_ms: 14.311
  iterations_since_restore: 141
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.921428571428564
    ram_util_percent: 16.201098901098906
  pid: 30948
  policy_reward_max:
    agent-0: 153.8333333333335
    agent-1: 153.8333333333335
    agent-2: 153.8333333333335
    agent-3: 153.8333333333335
    agent-4: 153.8333333333335
    agent-5: 153.8333333333335
  policy_reward_mean:
    agent-0: 127.961666666667
    agent-1: 127.961666666667
    agent-2: 127.961666666667
    agent-3: 127.961666666667
    agent-4: 127.961666666667
    agent-5: 127.961666666667
  policy_reward_min:
    agent-0: 57.33333333333309
    agent-1: 57.33333333333309
    agent-2: 57.33333333333309
    agent-3: 57.33333333333309
    agent-4: 57.33333333333309
    agent-5: 57.33333333333309
  sampler_perf:
    mean_env_wait_ms: 27.145228459082578
    mean_inference_ms: 12.950881488096881
    mean_processing_ms: 58.2392462674089
  time_since_restore: 19138.793683052063
  time_this_iter_s: 127.45506262779236
  time_total_s: 28264.805496931076
  timestamp: 1637046238
  timesteps_since_restore: 13536000
  timesteps_this_iter: 96000
  timesteps_total: 19296000
  training_iteration: 201
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    201 |          28264.8 | 19296000 |   767.77 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 4.58
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 23.74
    apples_agent-1_min: 0
    apples_agent-2_max: 136
    apples_agent-2_mean: 8.92
    apples_agent-2_min: 0
    apples_agent-3_max: 122
    apples_agent-3_mean: 73.53
    apples_agent-3_min: 25
    apples_agent-4_max: 34
    apples_agent-4_mean: 1.99
    apples_agent-4_min: 0
    apples_agent-5_max: 190
    apples_agent-5_mean: 93.13
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 522
    cleaning_beam_agent-0_mean: 359.48
    cleaning_beam_agent-0_min: 201
    cleaning_beam_agent-1_max: 433
    cleaning_beam_agent-1_mean: 218.77
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 466
    cleaning_beam_agent-2_mean: 315.99
    cleaning_beam_agent-2_min: 173
    cleaning_beam_agent-3_max: 86
    cleaning_beam_agent-3_mean: 37.95
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 429
    cleaning_beam_agent-4_mean: 342.67
    cleaning_beam_agent-4_min: 251
    cleaning_beam_agent-5_max: 248
    cleaning_beam_agent-5_mean: 65.47
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-06-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 943.999999999977
  episode_reward_mean: 761.6599999999864
  episode_reward_min: 389.00000000001154
  episodes_this_iter: 96
  episodes_total: 19392
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 12003.67
    learner:
      agent-0:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 5.5929598602233455e-05
        entropy: 1.0236730575561523
        entropy_coeff: 0.0017600000137463212
        kl: 0.004758893512189388
        model: {}
        policy_loss: -0.010546164587140083
        total_loss: -0.010552605614066124
        vf_explained_var: 0.06013503670692444
        vf_loss: 16.76245880126953
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 5.5929598602233455e-05
        entropy: 1.1669104099273682
        entropy_coeff: 0.0017600000137463212
        kl: 0.00508754001930356
        model: {}
        policy_loss: -0.012797977775335312
        total_loss: -0.012831686064600945
        vf_explained_var: 0.008871987462043762
        vf_loss: 17.65677833557129
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 5.5929598602233455e-05
        entropy: 1.154953956604004
        entropy_coeff: 0.0017600000137463212
        kl: 0.004882540553808212
        model: {}
        policy_loss: -0.012124085798859596
        total_loss: -0.011881228536367416
        vf_explained_var: -0.0036241114139556885
        vf_loss: 17.873245239257812
      agent-3:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 5.5929598602233455e-05
        entropy: 0.5768109560012817
        entropy_coeff: 0.0017600000137463212
        kl: 0.0036155490670353174
        model: {}
        policy_loss: -0.008471881039440632
        total_loss: -0.007968042977154255
        vf_explained_var: 0.148092120885849
        vf_loss: 15.162029266357422
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 5.5929598602233455e-05
        entropy: 1.0285670757293701
        entropy_coeff: 0.0017600000137463212
        kl: 0.004965074826031923
        model: {}
        policy_loss: -0.012861877679824829
        total_loss: -0.012655326165258884
        vf_explained_var: 0.007573261857032776
        vf_loss: 17.68574333190918
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 5.5929598602233455e-05
        entropy: 0.9467921257019043
        entropy_coeff: 0.0017600000137463212
        kl: 0.004501486662775278
        model: {}
        policy_loss: -0.012457692995667458
        total_loss: -0.012348592281341553
        vf_explained_var: 0.13006910681724548
        vf_loss: 15.503811836242676
    load_time_ms: 13653.895
    num_steps_sampled: 19392000
    num_steps_trained: 19392000
    sample_time_ms: 102169.031
    update_time_ms: 13.956
  iterations_since_restore: 142
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.900000000000006
    ram_util_percent: 16.163387978142076
  pid: 30948
  policy_reward_max:
    agent-0: 157.33333333333334
    agent-1: 157.33333333333334
    agent-2: 157.33333333333334
    agent-3: 157.33333333333334
    agent-4: 157.33333333333334
    agent-5: 157.33333333333334
  policy_reward_mean:
    agent-0: 126.94333333333367
    agent-1: 126.94333333333367
    agent-2: 126.94333333333367
    agent-3: 126.94333333333367
    agent-4: 126.94333333333367
    agent-5: 126.94333333333367
  policy_reward_min:
    agent-0: 64.83333333333304
    agent-1: 64.83333333333304
    agent-2: 64.83333333333304
    agent-3: 64.83333333333304
    agent-4: 64.83333333333304
    agent-5: 64.83333333333304
  sampler_perf:
    mean_env_wait_ms: 27.142814288829996
    mean_inference_ms: 12.951470136350096
    mean_processing_ms: 58.233929399577754
  time_since_restore: 19267.495074272156
  time_this_iter_s: 128.70139122009277
  time_total_s: 28393.50688815117
  timestamp: 1637046366
  timesteps_since_restore: 13632000
  timesteps_this_iter: 96000
  timesteps_total: 19392000
  training_iteration: 202
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    202 |          28393.5 | 19392000 |   761.66 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 92
    apples_agent-0_mean: 2.96
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 27.47
    apples_agent-1_min: 0
    apples_agent-2_max: 108
    apples_agent-2_mean: 7.01
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 79.33
    apples_agent-3_min: 34
    apples_agent-4_max: 61
    apples_agent-4_mean: 4.61
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 91.37
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 500
    cleaning_beam_agent-0_mean: 361.14
    cleaning_beam_agent-0_min: 201
    cleaning_beam_agent-1_max: 433
    cleaning_beam_agent-1_mean: 230.01
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 501
    cleaning_beam_agent-2_mean: 343.26
    cleaning_beam_agent-2_min: 187
    cleaning_beam_agent-3_max: 70
    cleaning_beam_agent-3_mean: 37.67
    cleaning_beam_agent-3_min: 14
    cleaning_beam_agent-4_max: 450
    cleaning_beam_agent-4_mean: 349.29
    cleaning_beam_agent-4_min: 217
    cleaning_beam_agent-5_max: 115
    cleaning_beam_agent-5_mean: 57.16
    cleaning_beam_agent-5_min: 20
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-08-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 989.999999999979
  episode_reward_mean: 789.0999999999863
  episode_reward_min: 389.00000000001154
  episodes_this_iter: 96
  episodes_total: 19488
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11920.875
    learner:
      agent-0:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 4.993920083506964e-05
        entropy: 1.0064268112182617
        entropy_coeff: 0.0017600000137463212
        kl: 0.004436729941517115
        model: {}
        policy_loss: -0.009859553538262844
        total_loss: -0.009981345385313034
        vf_explained_var: 0.03733186423778534
        vf_loss: 15.940577507019043
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 4.993920083506964e-05
        entropy: 1.1766839027404785
        entropy_coeff: 0.0017600000137463212
        kl: 0.004794309381395578
        model: {}
        policy_loss: -0.012128131464123726
        total_loss: -0.012276113964617252
        vf_explained_var: -0.006524369120597839
        vf_loss: 16.832645416259766
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 4.993920083506964e-05
        entropy: 1.1475207805633545
        entropy_coeff: 0.0017600000137463212
        kl: 0.005232003051787615
        model: {}
        policy_loss: -0.011283937841653824
        total_loss: -0.01144471112638712
        vf_explained_var: 0.03942900896072388
        vf_loss: 15.972638130187988
      agent-3:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 4.993920083506964e-05
        entropy: 0.5730083584785461
        entropy_coeff: 0.0017600000137463212
        kl: 0.003238200442865491
        model: {}
        policy_loss: -0.007621681317687035
        total_loss: -0.007176514714956284
        vf_explained_var: 0.1214621514081955
        vf_loss: 14.52395248413086
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 4.993920083506964e-05
        entropy: 1.0156306028366089
        entropy_coeff: 0.0017600000137463212
        kl: 0.005417205393314362
        model: {}
        policy_loss: -0.011923464946448803
        total_loss: -0.011970022693276405
        vf_explained_var: 0.03753510117530823
        vf_loss: 16.055234909057617
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 4.993920083506964e-05
        entropy: 0.9500157833099365
        entropy_coeff: 0.0017600000137463212
        kl: 0.005142430309206247
        model: {}
        policy_loss: -0.011957420967519283
        total_loss: -0.012042587623000145
        vf_explained_var: 0.12114918231964111
        vf_loss: 14.583025932312012
    load_time_ms: 13622.287
    num_steps_sampled: 19488000
    num_steps_trained: 19488000
    sample_time_ms: 102065.732
    update_time_ms: 13.997
  iterations_since_restore: 143
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.219444444444438
    ram_util_percent: 16.19277777777778
  pid: 30948
  policy_reward_max:
    agent-0: 164.99999999999983
    agent-1: 164.99999999999983
    agent-2: 164.99999999999983
    agent-3: 164.99999999999983
    agent-4: 164.99999999999983
    agent-5: 164.99999999999983
  policy_reward_mean:
    agent-0: 131.51666666666696
    agent-1: 131.51666666666696
    agent-2: 131.51666666666696
    agent-3: 131.51666666666696
    agent-4: 131.51666666666696
    agent-5: 131.51666666666696
  policy_reward_min:
    agent-0: 64.83333333333304
    agent-1: 64.83333333333304
    agent-2: 64.83333333333304
    agent-3: 64.83333333333304
    agent-4: 64.83333333333304
    agent-5: 64.83333333333304
  sampler_perf:
    mean_env_wait_ms: 27.140243980594445
    mean_inference_ms: 12.95205274417046
    mean_processing_ms: 58.225373128611494
  time_since_restore: 19393.494070529938
  time_this_iter_s: 125.99899625778198
  time_total_s: 28519.50588440895
  timestamp: 1637046493
  timesteps_since_restore: 13728000
  timesteps_this_iter: 96000
  timesteps_total: 19488000
  training_iteration: 203
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    203 |          28519.5 | 19488000 |    789.1 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 92
    apples_agent-0_mean: 3.82
    apples_agent-0_min: 0
    apples_agent-1_max: 85
    apples_agent-1_mean: 25.13
    apples_agent-1_min: 0
    apples_agent-2_max: 93
    apples_agent-2_mean: 11.15
    apples_agent-2_min: 0
    apples_agent-3_max: 152
    apples_agent-3_mean: 78.11
    apples_agent-3_min: 41
    apples_agent-4_max: 60
    apples_agent-4_mean: 3.5
    apples_agent-4_min: 0
    apples_agent-5_max: 127
    apples_agent-5_mean: 87.21
    apples_agent-5_min: 28
    cleaning_beam_agent-0_max: 480
    cleaning_beam_agent-0_mean: 361.93
    cleaning_beam_agent-0_min: 256
    cleaning_beam_agent-1_max: 386
    cleaning_beam_agent-1_mean: 223.16
    cleaning_beam_agent-1_min: 77
    cleaning_beam_agent-2_max: 502
    cleaning_beam_agent-2_mean: 322.32
    cleaning_beam_agent-2_min: 79
    cleaning_beam_agent-3_max: 90
    cleaning_beam_agent-3_mean: 38.87
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 457
    cleaning_beam_agent-4_mean: 353.69
    cleaning_beam_agent-4_min: 200
    cleaning_beam_agent-5_max: 156
    cleaning_beam_agent-5_mean: 56.82
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-10-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 963.9999999999822
  episode_reward_mean: 765.1899999999882
  episode_reward_min: 361.00000000000466
  episodes_this_iter: 96
  episodes_total: 19584
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11872.678
    learner:
      agent-0:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 4.394879942992702e-05
        entropy: 1.017269492149353
        entropy_coeff: 0.0017600000137463212
        kl: 0.0042242989875376225
        model: {}
        policy_loss: -0.009639110416173935
        total_loss: -0.009731892496347427
        vf_explained_var: 0.06718054413795471
        vf_loss: 16.712068557739258
      agent-1:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 4.394879942992702e-05
        entropy: 1.1629678010940552
        entropy_coeff: 0.0017600000137463212
        kl: 0.004408941138535738
        model: {}
        policy_loss: -0.011052289046347141
        total_loss: -0.01115480437874794
        vf_explained_var: -0.023688137531280518
        vf_loss: 18.340839385986328
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 4.394879942992702e-05
        entropy: 1.1607372760772705
        entropy_coeff: 0.0017600000137463212
        kl: 0.003852597437798977
        model: {}
        policy_loss: -0.009999717585742474
        total_loss: -0.010126760229468346
        vf_explained_var: 0.03776054084300995
        vf_loss: 17.232254028320312
      agent-3:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 4.394879942992702e-05
        entropy: 0.5979596972465515
        entropy_coeff: 0.0017600000137463212
        kl: 0.003177480772137642
        model: {}
        policy_loss: -0.007701267022639513
        total_loss: -0.0071948496624827385
        vf_explained_var: 0.13098739087581635
        vf_loss: 15.582091331481934
      agent-4:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 4.394879942992702e-05
        entropy: 1.0195728540420532
        entropy_coeff: 0.0017600000137463212
        kl: 0.004659163765609264
        model: {}
        policy_loss: -0.011186047457158566
        total_loss: -0.011137125082314014
        vf_explained_var: 0.036030739545822144
        vf_loss: 17.2689266204834
      agent-5:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 4.394879942992702e-05
        entropy: 0.9576579928398132
        entropy_coeff: 0.0017600000137463212
        kl: 0.004365936387330294
        model: {}
        policy_loss: -0.010610079392790794
        total_loss: -0.010614143684506416
        vf_explained_var: 0.12295852601528168
        vf_loss: 15.722635269165039
    load_time_ms: 13669.615
    num_steps_sampled: 19584000
    num_steps_trained: 19584000
    sample_time_ms: 101867.841
    update_time_ms: 14.091
  iterations_since_restore: 144
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.176666666666662
    ram_util_percent: 16.12611111111111
  pid: 30948
  policy_reward_max:
    agent-0: 160.6666666666663
    agent-1: 160.6666666666663
    agent-2: 160.6666666666663
    agent-3: 160.6666666666663
    agent-4: 160.6666666666663
    agent-5: 160.6666666666663
  policy_reward_mean:
    agent-0: 127.53166666666696
    agent-1: 127.53166666666696
    agent-2: 127.53166666666696
    agent-3: 127.53166666666696
    agent-4: 127.53166666666696
    agent-5: 127.53166666666696
  policy_reward_min:
    agent-0: 60.1666666666665
    agent-1: 60.1666666666665
    agent-2: 60.1666666666665
    agent-3: 60.1666666666665
    agent-4: 60.1666666666665
    agent-5: 60.1666666666665
  sampler_perf:
    mean_env_wait_ms: 27.13707330525482
    mean_inference_ms: 12.952273225094679
    mean_processing_ms: 58.21761931553676
  time_since_restore: 19519.982570409775
  time_this_iter_s: 126.48849987983704
  time_total_s: 28645.994384288788
  timestamp: 1637046619
  timesteps_since_restore: 13824000
  timesteps_this_iter: 96000
  timesteps_total: 19584000
  training_iteration: 204
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    204 |            28646 | 19584000 |   765.19 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 3.6
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 27.47
    apples_agent-1_min: 0
    apples_agent-2_max: 80
    apples_agent-2_mean: 7.95
    apples_agent-2_min: 0
    apples_agent-3_max: 176
    apples_agent-3_mean: 77.88
    apples_agent-3_min: 35
    apples_agent-4_max: 72
    apples_agent-4_mean: 2.62
    apples_agent-4_min: 0
    apples_agent-5_max: 166
    apples_agent-5_mean: 92.18
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 494
    cleaning_beam_agent-0_mean: 359.56
    cleaning_beam_agent-0_min: 224
    cleaning_beam_agent-1_max: 346
    cleaning_beam_agent-1_mean: 215.15
    cleaning_beam_agent-1_min: 77
    cleaning_beam_agent-2_max: 484
    cleaning_beam_agent-2_mean: 320.94
    cleaning_beam_agent-2_min: 129
    cleaning_beam_agent-3_max: 73
    cleaning_beam_agent-3_mean: 33.78
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 444
    cleaning_beam_agent-4_mean: 358.51
    cleaning_beam_agent-4_min: 230
    cleaning_beam_agent-5_max: 165
    cleaning_beam_agent-5_mean: 54.67
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-12-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 964.9999999999814
  episode_reward_mean: 773.5699999999881
  episode_reward_min: 364.99999999999926
  episodes_this_iter: 96
  episodes_total: 19680
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11825.646
    learner:
      agent-0:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 3.795840166276321e-05
        entropy: 1.0283453464508057
        entropy_coeff: 0.0017600000137463212
        kl: 0.003951389342546463
        model: {}
        policy_loss: -0.008941024541854858
        total_loss: -0.009080525487661362
        vf_explained_var: 0.06765425205230713
        vf_loss: 16.58038330078125
      agent-1:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 3.795840166276321e-05
        entropy: 1.1608786582946777
        entropy_coeff: 0.0017600000137463212
        kl: 0.0048660654574632645
        model: {}
        policy_loss: -0.010305467993021011
        total_loss: -0.010439488105475903
        vf_explained_var: -0.03627088665962219
        vf_loss: 18.483007431030273
      agent-2:
        cur_kl_coeff: 0.02500000037252903
        cur_lr: 3.795840166276321e-05
        entropy: 1.1666574478149414
        entropy_coeff: 0.0017600000137463212
        kl: 0.004296989645808935
        model: {}
        policy_loss: -0.010187924839556217
        total_loss: -0.010389627888798714
        vf_explained_var: 0.018780305981636047
        vf_loss: 17.441883087158203
      agent-3:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 3.795840166276321e-05
        entropy: 0.5895899534225464
        entropy_coeff: 0.0017600000137463212
        kl: 0.00309598445892334
        model: {}
        policy_loss: -0.006435195449739695
        total_loss: -0.005983852781355381
        vf_explained_var: 0.16280749440193176
        vf_loss: 14.887198448181152
      agent-4:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 3.795840166276321e-05
        entropy: 1.007989525794983
        entropy_coeff: 0.0017600000137463212
        kl: 0.00418630987405777
        model: {}
        policy_loss: -0.009558804333209991
        total_loss: -0.009560761041939259
        vf_explained_var: 0.03354845941066742
        vf_loss: 17.197734832763672
      agent-5:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 3.795840166276321e-05
        entropy: 0.9591330289840698
        entropy_coeff: 0.0017600000137463212
        kl: 0.004199684597551823
        model: {}
        policy_loss: -0.009994124993681908
        total_loss: -0.010030727833509445
        vf_explained_var: 0.10104788839817047
        vf_loss: 15.989748001098633
    load_time_ms: 13647.04
    num_steps_sampled: 19680000
    num_steps_trained: 19680000
    sample_time_ms: 101865.328
    update_time_ms: 14.37
  iterations_since_restore: 145
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.14917127071823
    ram_util_percent: 16.187292817679563
  pid: 30948
  policy_reward_max:
    agent-0: 160.83333333333374
    agent-1: 160.83333333333374
    agent-2: 160.83333333333374
    agent-3: 160.83333333333374
    agent-4: 160.83333333333374
    agent-5: 160.83333333333374
  policy_reward_mean:
    agent-0: 128.92833333333363
    agent-1: 128.92833333333363
    agent-2: 128.92833333333363
    agent-3: 128.92833333333363
    agent-4: 128.92833333333363
    agent-5: 128.92833333333363
  policy_reward_min:
    agent-0: 60.833333333333286
    agent-1: 60.833333333333286
    agent-2: 60.833333333333286
    agent-3: 60.833333333333286
    agent-4: 60.833333333333286
    agent-5: 60.833333333333286
  sampler_perf:
    mean_env_wait_ms: 27.134896663572057
    mean_inference_ms: 12.953014981195329
    mean_processing_ms: 58.212566845715834
  time_since_restore: 19646.828423261642
  time_this_iter_s: 126.84585285186768
  time_total_s: 28772.840237140656
  timestamp: 1637046746
  timesteps_since_restore: 13920000
  timesteps_this_iter: 96000
  timesteps_total: 19680000
  training_iteration: 205
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    205 |          28772.8 | 19680000 |   773.57 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 1.35
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 26.91
    apples_agent-1_min: 0
    apples_agent-2_max: 84
    apples_agent-2_mean: 10.08
    apples_agent-2_min: 0
    apples_agent-3_max: 142
    apples_agent-3_mean: 82.82
    apples_agent-3_min: 43
    apples_agent-4_max: 72
    apples_agent-4_mean: 4.55
    apples_agent-4_min: 0
    apples_agent-5_max: 200
    apples_agent-5_mean: 90.93
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 368.43
    cleaning_beam_agent-0_min: 257
    cleaning_beam_agent-1_max: 433
    cleaning_beam_agent-1_mean: 221.89
    cleaning_beam_agent-1_min: 75
    cleaning_beam_agent-2_max: 472
    cleaning_beam_agent-2_mean: 293.41
    cleaning_beam_agent-2_min: 84
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 30.7
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 463
    cleaning_beam_agent-4_mean: 365.46
    cleaning_beam_agent-4_min: 207
    cleaning_beam_agent-5_max: 159
    cleaning_beam_agent-5_mean: 52.42
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-14-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 934.9999999999783
  episode_reward_mean: 787.0199999999865
  episode_reward_min: 408.00000000001074
  episodes_this_iter: 96
  episodes_total: 19776
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11802.084
    learner:
      agent-0:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 3.196800025762059e-05
        entropy: 1.0200717449188232
        entropy_coeff: 0.0017600000137463212
        kl: 0.00336780515499413
        model: {}
        policy_loss: -0.007374342530965805
        total_loss: -0.007634117268025875
        vf_explained_var: 0.07237543165683746
        vf_loss: 15.302898406982422
      agent-1:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 3.196800025762059e-05
        entropy: 1.173736572265625
        entropy_coeff: 0.0017600000137463212
        kl: 0.003604161087423563
        model: {}
        policy_loss: -0.008600312285125256
        total_loss: -0.008936459198594093
        vf_explained_var: -0.026419848203659058
        vf_loss: 17.071020126342773
      agent-2:
        cur_kl_coeff: 0.012500000186264515
        cur_lr: 3.196800025762059e-05
        entropy: 1.1745514869689941
        entropy_coeff: 0.0017600000137463212
        kl: 0.0035247448831796646
        model: {}
        policy_loss: -0.008014276623725891
        total_loss: -0.008419438265264034
        vf_explained_var: 0.028590217232704163
        vf_loss: 16.179887771606445
      agent-3:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 3.196800025762059e-05
        entropy: 0.579293966293335
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024705599062144756
        model: {}
        policy_loss: -0.005786451976746321
        total_loss: -0.00533853005617857
        vf_explained_var: 0.11128051578998566
        vf_loss: 14.673635482788086
      agent-4:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 3.196800025762059e-05
        entropy: 1.0180034637451172
        entropy_coeff: 0.0017600000137463212
        kl: 0.0038757415022701025
        model: {}
        policy_loss: -0.00862165354192257
        total_loss: -0.00881026592105627
        vf_explained_var: 0.04741153120994568
        vf_loss: 15.788493156433105
      agent-5:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 3.196800025762059e-05
        entropy: 0.9512186646461487
        entropy_coeff: 0.0017600000137463212
        kl: 0.0035093757323920727
        model: {}
        policy_loss: -0.008894609287381172
        total_loss: -0.00907632801681757
        vf_explained_var: 0.11198203265666962
        vf_loss: 14.704900741577148
    load_time_ms: 13666.181
    num_steps_sampled: 19776000
    num_steps_trained: 19776000
    sample_time_ms: 101845.586
    update_time_ms: 14.371
  iterations_since_restore: 146
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.944808743169396
    ram_util_percent: 16.122404371584704
  pid: 30948
  policy_reward_max:
    agent-0: 155.83333333333363
    agent-1: 155.83333333333363
    agent-2: 155.83333333333363
    agent-3: 155.83333333333363
    agent-4: 155.83333333333363
    agent-5: 155.83333333333363
  policy_reward_mean:
    agent-0: 131.1700000000003
    agent-1: 131.1700000000003
    agent-2: 131.1700000000003
    agent-3: 131.1700000000003
    agent-4: 131.1700000000003
    agent-5: 131.1700000000003
  policy_reward_min:
    agent-0: 67.99999999999983
    agent-1: 67.99999999999983
    agent-2: 67.99999999999983
    agent-3: 67.99999999999983
    agent-4: 67.99999999999983
    agent-5: 67.99999999999983
  sampler_perf:
    mean_env_wait_ms: 27.13317636567038
    mean_inference_ms: 12.953556779039046
    mean_processing_ms: 58.208022806424886
  time_since_restore: 19775.085874080658
  time_this_iter_s: 128.2574508190155
  time_total_s: 28901.09768795967
  timestamp: 1637046875
  timesteps_since_restore: 14016000
  timesteps_this_iter: 96000
  timesteps_total: 19776000
  training_iteration: 206
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    206 |          28901.1 | 19776000 |   787.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 2.49
    apples_agent-0_min: 0
    apples_agent-1_max: 183
    apples_agent-1_mean: 27.86
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 9.27
    apples_agent-2_min: 0
    apples_agent-3_max: 135
    apples_agent-3_mean: 79.52
    apples_agent-3_min: 40
    apples_agent-4_max: 47
    apples_agent-4_mean: 2.95
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 88.18
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 501
    cleaning_beam_agent-0_mean: 364.11
    cleaning_beam_agent-0_min: 238
    cleaning_beam_agent-1_max: 380
    cleaning_beam_agent-1_mean: 214.33
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 522
    cleaning_beam_agent-2_mean: 295.24
    cleaning_beam_agent-2_min: 145
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 29.77
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 438
    cleaning_beam_agent-4_mean: 359.85
    cleaning_beam_agent-4_min: 278
    cleaning_beam_agent-5_max: 267
    cleaning_beam_agent-5_mean: 55.73
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-16-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 979.9999999999857
  episode_reward_mean: 776.2899999999873
  episode_reward_min: 442.00000000001137
  episodes_this_iter: 96
  episodes_total: 19872
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11771.589
    learner:
      agent-0:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 2.597760067146737e-05
        entropy: 1.0242366790771484
        entropy_coeff: 0.0017600000137463212
        kl: 0.003005899954587221
        model: {}
        policy_loss: -0.006625606212764978
        total_loss: -0.0067286742851138115
        vf_explained_var: 0.04413361847400665
        vf_loss: 16.97240447998047
      agent-1:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 2.597760067146737e-05
        entropy: 1.1739647388458252
        entropy_coeff: 0.0017600000137463212
        kl: 0.0030846104491502047
        model: {}
        policy_loss: -0.007670623250305653
        total_loss: -0.00788054522126913
        vf_explained_var: -0.03606840968132019
        vf_loss: 18.46613311767578
      agent-2:
        cur_kl_coeff: 0.0062500000931322575
        cur_lr: 2.597760067146737e-05
        entropy: 1.1618930101394653
        entropy_coeff: 0.0017600000137463212
        kl: 0.0034758192487061024
        model: {}
        policy_loss: -0.007737352512776852
        total_loss: -0.008035995066165924
        vf_explained_var: 0.029705479741096497
        vf_loss: 17.245664596557617
      agent-3:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 2.597760067146737e-05
        entropy: 0.6031844615936279
        entropy_coeff: 0.0017600000137463212
        kl: 0.0028598832432180643
        model: {}
        policy_loss: -0.005521656014025211
        total_loss: -0.0050704400055110455
        vf_explained_var: 0.14828230440616608
        vf_loss: 15.127540588378906
      agent-4:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 2.597760067146737e-05
        entropy: 1.026473045349121
        entropy_coeff: 0.0017600000137463212
        kl: 0.003677493892610073
        model: {}
        policy_loss: -0.007934760302305222
        total_loss: -0.008015894331037998
        vf_explained_var: 0.03444060683250427
        vf_loss: 17.139644622802734
      agent-5:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 2.597760067146737e-05
        entropy: 0.9553688168525696
        entropy_coeff: 0.0017600000137463212
        kl: 0.0034736786037683487
        model: {}
        policy_loss: -0.00770995207130909
        total_loss: -0.00782798882573843
        vf_explained_var: 0.12472757697105408
        vf_loss: 15.5255708694458
    load_time_ms: 13651.419
    num_steps_sampled: 19872000
    num_steps_trained: 19872000
    sample_time_ms: 101746.549
    update_time_ms: 14.4
  iterations_since_restore: 147
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.4195530726257
    ram_util_percent: 16.195530726256987
  pid: 30948
  policy_reward_max:
    agent-0: 163.33333333333303
    agent-1: 163.33333333333303
    agent-2: 163.33333333333303
    agent-3: 163.33333333333303
    agent-4: 163.33333333333303
    agent-5: 163.33333333333303
  policy_reward_mean:
    agent-0: 129.38166666666694
    agent-1: 129.38166666666694
    agent-2: 129.38166666666694
    agent-3: 129.38166666666694
    agent-4: 129.38166666666694
    agent-5: 129.38166666666694
  policy_reward_min:
    agent-0: 73.66666666666659
    agent-1: 73.66666666666659
    agent-2: 73.66666666666659
    agent-3: 73.66666666666659
    agent-4: 73.66666666666659
    agent-5: 73.66666666666659
  sampler_perf:
    mean_env_wait_ms: 27.130494300652103
    mean_inference_ms: 12.9539031973512
    mean_processing_ms: 58.201960865685486
  time_since_restore: 19901.333005428314
  time_this_iter_s: 126.24713134765625
  time_total_s: 29027.344819307327
  timestamp: 1637047001
  timesteps_since_restore: 14112000
  timesteps_this_iter: 96000
  timesteps_total: 19872000
  training_iteration: 207
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    207 |          29027.3 | 19872000 |   776.29 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 68
    apples_agent-0_mean: 3.48
    apples_agent-0_min: 0
    apples_agent-1_max: 89
    apples_agent-1_mean: 24.63
    apples_agent-1_min: 0
    apples_agent-2_max: 43
    apples_agent-2_mean: 7.39
    apples_agent-2_min: 0
    apples_agent-3_max: 132
    apples_agent-3_mean: 82.22
    apples_agent-3_min: 26
    apples_agent-4_max: 50
    apples_agent-4_mean: 1.39
    apples_agent-4_min: 0
    apples_agent-5_max: 143
    apples_agent-5_mean: 85.13
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 503
    cleaning_beam_agent-0_mean: 378.0
    cleaning_beam_agent-0_min: 235
    cleaning_beam_agent-1_max: 419
    cleaning_beam_agent-1_mean: 220.38
    cleaning_beam_agent-1_min: 94
    cleaning_beam_agent-2_max: 522
    cleaning_beam_agent-2_mean: 285.51
    cleaning_beam_agent-2_min: 137
    cleaning_beam_agent-3_max: 65
    cleaning_beam_agent-3_mean: 29.94
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 458
    cleaning_beam_agent-4_mean: 365.07
    cleaning_beam_agent-4_min: 197
    cleaning_beam_agent-5_max: 330
    cleaning_beam_agent-5_mean: 58.83
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-18-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 971.9999999999866
  episode_reward_mean: 784.0199999999866
  episode_reward_min: 387.0000000000056
  episodes_this_iter: 96
  episodes_total: 19968
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11703.51
    learner:
      agent-0:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.998719926632475e-05
        entropy: 1.0108343362808228
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023681647144258022
        model: {}
        policy_loss: -0.0053264182060956955
        total_loss: -0.005481102503836155
        vf_explained_var: 0.08302800357341766
        vf_loss: 16.23461151123047
      agent-1:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.998719926632475e-05
        entropy: 1.180873155593872
        entropy_coeff: 0.0017600000137463212
        kl: 0.002186180790886283
        model: {}
        policy_loss: -0.006192480679601431
        total_loss: -0.006461598444730043
        vf_explained_var: -0.020349472761154175
        vf_loss: 18.058040618896484
      agent-2:
        cur_kl_coeff: 0.0031250000465661287
        cur_lr: 1.998719926632475e-05
        entropy: 1.1704264879226685
        entropy_coeff: 0.0017600000137463212
        kl: 0.002438613213598728
        model: {}
        policy_loss: -0.005917291156947613
        total_loss: -0.00621459586545825
        vf_explained_var: 0.005217641592025757
        vf_loss: 17.550230026245117
      agent-3:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.998719926632475e-05
        entropy: 0.5733781456947327
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014974137302488089
        model: {}
        policy_loss: -0.003858942072838545
        total_loss: -0.0033238264732062817
        vf_explained_var: 0.12201832234859467
        vf_loss: 15.442440032958984
      agent-4:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.998719926632475e-05
        entropy: 1.0128602981567383
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026625553146004677
        model: {}
        policy_loss: -0.0064145708456635475
        total_loss: -0.006475622765719891
        vf_explained_var: 0.026098474860191345
        vf_loss: 17.17420196533203
      agent-5:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.998719926632475e-05
        entropy: 0.9413400888442993
        entropy_coeff: 0.0017600000137463212
        kl: 0.002095610834658146
        model: {}
        policy_loss: -0.00603704247623682
        total_loss: -0.0061525446362793446
        vf_explained_var: 0.1274842917919159
        vf_loss: 15.379802703857422
    load_time_ms: 13656.666
    num_steps_sampled: 19968000
    num_steps_trained: 19968000
    sample_time_ms: 101599.349
    update_time_ms: 14.367
  iterations_since_restore: 148
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.159116022099454
    ram_util_percent: 16.186740331491716
  pid: 30948
  policy_reward_max:
    agent-0: 161.99999999999977
    agent-1: 161.99999999999977
    agent-2: 161.99999999999977
    agent-3: 161.99999999999977
    agent-4: 161.99999999999977
    agent-5: 161.99999999999977
  policy_reward_mean:
    agent-0: 130.67000000000027
    agent-1: 130.67000000000027
    agent-2: 130.67000000000027
    agent-3: 130.67000000000027
    agent-4: 130.67000000000027
    agent-5: 130.67000000000027
  policy_reward_min:
    agent-0: 64.49999999999979
    agent-1: 64.49999999999979
    agent-2: 64.49999999999979
    agent-3: 64.49999999999979
    agent-4: 64.49999999999979
    agent-5: 64.49999999999979
  sampler_perf:
    mean_env_wait_ms: 27.12741736531574
    mean_inference_ms: 12.953931231593522
    mean_processing_ms: 58.19504425502338
  time_since_restore: 20028.117291212082
  time_this_iter_s: 126.7842857837677
  time_total_s: 29154.129105091095
  timestamp: 1637047128
  timesteps_since_restore: 14208000
  timesteps_this_iter: 96000
  timesteps_total: 19968000
  training_iteration: 208
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    208 |          29154.1 | 19968000 |   784.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 3.25
    apples_agent-0_min: 0
    apples_agent-1_max: 100
    apples_agent-1_mean: 29.68
    apples_agent-1_min: 0
    apples_agent-2_max: 80
    apples_agent-2_mean: 7.54
    apples_agent-2_min: 0
    apples_agent-3_max: 156
    apples_agent-3_mean: 79.37
    apples_agent-3_min: 24
    apples_agent-4_max: 76
    apples_agent-4_mean: 2.66
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 89.87
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 516
    cleaning_beam_agent-0_mean: 376.58
    cleaning_beam_agent-0_min: 234
    cleaning_beam_agent-1_max: 368
    cleaning_beam_agent-1_mean: 213.36
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 494
    cleaning_beam_agent-2_mean: 308.35
    cleaning_beam_agent-2_min: 143
    cleaning_beam_agent-3_max: 57
    cleaning_beam_agent-3_mean: 29.99
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 473
    cleaning_beam_agent-4_mean: 372.97
    cleaning_beam_agent-4_min: 268
    cleaning_beam_agent-5_max: 303
    cleaning_beam_agent-5_mean: 53.29
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-20-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 983.9999999999749
  episode_reward_mean: 770.8399999999865
  episode_reward_min: 293.9999999999987
  episodes_this_iter: 96
  episodes_total: 20064
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11689.253
    learner:
      agent-0:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.3996799680171534e-05
        entropy: 1.0080657005310059
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016566561535000801
        model: {}
        policy_loss: -0.004276800900697708
        total_loss: -0.004240504465997219
        vf_explained_var: 0.0639142096042633
        vf_loss: 18.10171890258789
      agent-1:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.3996799680171534e-05
        entropy: 1.1660470962524414
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013254353543743491
        model: {}
        policy_loss: -0.004632379859685898
        total_loss: -0.004719752352684736
        vf_explained_var: -0.014525741338729858
        vf_loss: 19.638355255126953
      agent-2:
        cur_kl_coeff: 0.0015625000232830644
        cur_lr: 1.3996799680171534e-05
        entropy: 1.1604335308074951
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023533408530056477
        model: {}
        policy_loss: -0.00467910198494792
        total_loss: -0.004909856710582972
        vf_explained_var: 0.06463155150413513
        vf_loss: 18.079320907592773
      agent-3:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.3996799680171534e-05
        entropy: 0.5991983413696289
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011652375105768442
        model: {}
        policy_loss: -0.0034576067700982094
        total_loss: -0.002903836779296398
        vf_explained_var: 0.1683754324913025
        vf_loss: 16.083602905273438
      agent-4:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.3996799680171534e-05
        entropy: 1.010225772857666
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025409541558474302
        model: {}
        policy_loss: -0.00520216254517436
        total_loss: -0.005100916605442762
        vf_explained_var: 0.028394967317581177
        vf_loss: 18.77257537841797
      agent-5:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.3996799680171534e-05
        entropy: 0.9503783583641052
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015672853915020823
        model: {}
        policy_loss: -0.004749487154185772
        total_loss: -0.004727993160486221
        vf_explained_var: 0.12430249154567719
        vf_loss: 16.929357528686523
    load_time_ms: 13634.24
    num_steps_sampled: 20064000
    num_steps_trained: 20064000
    sample_time_ms: 101843.88
    update_time_ms: 14.603
  iterations_since_restore: 149
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.1585635359116
    ram_util_percent: 16.12154696132597
  pid: 30948
  policy_reward_max:
    agent-0: 164.00000000000023
    agent-1: 164.00000000000023
    agent-2: 164.00000000000023
    agent-3: 164.00000000000023
    agent-4: 164.00000000000023
    agent-5: 164.00000000000023
  policy_reward_mean:
    agent-0: 128.47333333333364
    agent-1: 128.47333333333364
    agent-2: 128.47333333333364
    agent-3: 128.47333333333364
    agent-4: 128.47333333333364
    agent-5: 128.47333333333364
  policy_reward_min:
    agent-0: 48.99999999999995
    agent-1: 48.99999999999995
    agent-2: 48.99999999999995
    agent-3: 48.99999999999995
    agent-4: 48.99999999999995
    agent-5: 48.99999999999995
  sampler_perf:
    mean_env_wait_ms: 27.12618521538833
    mean_inference_ms: 12.954642271535183
    mean_processing_ms: 58.18964592050297
  time_since_restore: 20155.552781820297
  time_this_iter_s: 127.43549060821533
  time_total_s: 29281.56459569931
  timestamp: 1637047256
  timesteps_since_restore: 14304000
  timesteps_this_iter: 96000
  timesteps_total: 20064000
  training_iteration: 209
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    209 |          29281.6 | 20064000 |   770.84 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 72
    apples_agent-0_mean: 4.49
    apples_agent-0_min: 0
    apples_agent-1_max: 111
    apples_agent-1_mean: 29.99
    apples_agent-1_min: 0
    apples_agent-2_max: 187
    apples_agent-2_mean: 10.74
    apples_agent-2_min: 0
    apples_agent-3_max: 144
    apples_agent-3_mean: 81.78
    apples_agent-3_min: 29
    apples_agent-4_max: 82
    apples_agent-4_mean: 1.67
    apples_agent-4_min: 0
    apples_agent-5_max: 169
    apples_agent-5_mean: 92.58
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 538
    cleaning_beam_agent-0_mean: 368.73
    cleaning_beam_agent-0_min: 259
    cleaning_beam_agent-1_max: 476
    cleaning_beam_agent-1_mean: 221.2
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 567
    cleaning_beam_agent-2_mean: 318.34
    cleaning_beam_agent-2_min: 133
    cleaning_beam_agent-3_max: 82
    cleaning_beam_agent-3_mean: 30.72
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 465
    cleaning_beam_agent-4_mean: 376.82
    cleaning_beam_agent-4_min: 286
    cleaning_beam_agent-5_max: 122
    cleaning_beam_agent-5_mean: 52.37
    cleaning_beam_agent-5_min: 16
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-23-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 996.9999999999832
  episode_reward_mean: 788.1299999999864
  episode_reward_min: 407.0000000000028
  episodes_this_iter: 96
  episodes_total: 20160
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11684.223
    learner:
      agent-0:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0197303295135498
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021033594384789467
        model: {}
        policy_loss: -0.004184077028185129
        total_loss: -0.004413916729390621
        vf_explained_var: 0.07919925451278687
        vf_loss: 15.646820068359375
      agent-1:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1632251739501953
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014713130658492446
        model: {}
        policy_loss: -0.004063397645950317
        total_loss: -0.004374498035758734
        vf_explained_var: -0.01025480031967163
        vf_loss: 17.356040954589844
      agent-2:
        cur_kl_coeff: 0.0007812500116415322
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1359975337982178
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015141140902414918
        model: {}
        policy_loss: -0.0038672033697366714
        total_loss: -0.004228690173476934
        vf_explained_var: 0.03627781569957733
        vf_loss: 16.36687469482422
      agent-3:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5930144786834717
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012572940904647112
        model: {}
        policy_loss: -0.0027639465406537056
        total_loss: -0.0022852388210594654
        vf_explained_var: 0.10477069020271301
        vf_loss: 15.224124908447266
      agent-4:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0103857517242432
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015389848267659545
        model: {}
        policy_loss: -0.004366937093436718
        total_loss: -0.004483889322727919
        vf_explained_var: 0.025324419140815735
        vf_loss: 16.60728645324707
      agent-5:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9483551979064941
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013537011109292507
        model: {}
        policy_loss: -0.004241559188812971
        total_loss: -0.004414309747517109
        vf_explained_var: 0.12214735150337219
        vf_loss: 14.958252906799316
    load_time_ms: 13629.654
    num_steps_sampled: 20160000
    num_steps_trained: 20160000
    sample_time_ms: 101750.477
    update_time_ms: 14.49
  iterations_since_restore: 150
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.11318681318681
    ram_util_percent: 16.184065934065938
  pid: 30948
  policy_reward_max:
    agent-0: 166.16666666666652
    agent-1: 166.16666666666652
    agent-2: 166.16666666666652
    agent-3: 166.16666666666652
    agent-4: 166.16666666666652
    agent-5: 166.16666666666652
  policy_reward_mean:
    agent-0: 131.3550000000003
    agent-1: 131.3550000000003
    agent-2: 131.3550000000003
    agent-3: 131.3550000000003
    agent-4: 131.3550000000003
    agent-5: 131.3550000000003
  policy_reward_min:
    agent-0: 67.83333333333317
    agent-1: 67.83333333333317
    agent-2: 67.83333333333317
    agent-3: 67.83333333333317
    agent-4: 67.83333333333317
    agent-5: 67.83333333333317
  sampler_perf:
    mean_env_wait_ms: 27.124340284071977
    mean_inference_ms: 12.95488824933657
    mean_processing_ms: 58.1820334050044
  time_since_restore: 20282.91718864441
  time_this_iter_s: 127.36440682411194
  time_total_s: 29408.929002523422
  timestamp: 1637047383
  timesteps_since_restore: 14400000
  timesteps_this_iter: 96000
  timesteps_total: 20160000
  training_iteration: 210
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    210 |          29408.9 | 20160000 |   788.13 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 1.81
    apples_agent-0_min: 0
    apples_agent-1_max: 125
    apples_agent-1_mean: 33.37
    apples_agent-1_min: 0
    apples_agent-2_max: 73
    apples_agent-2_mean: 10.97
    apples_agent-2_min: 0
    apples_agent-3_max: 130
    apples_agent-3_mean: 80.35
    apples_agent-3_min: 33
    apples_agent-4_max: 37
    apples_agent-4_mean: 3.13
    apples_agent-4_min: 0
    apples_agent-5_max: 164
    apples_agent-5_mean: 95.99
    apples_agent-5_min: 23
    cleaning_beam_agent-0_max: 458
    cleaning_beam_agent-0_mean: 379.11
    cleaning_beam_agent-0_min: 218
    cleaning_beam_agent-1_max: 440
    cleaning_beam_agent-1_mean: 218.35
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 519
    cleaning_beam_agent-2_mean: 312.56
    cleaning_beam_agent-2_min: 133
    cleaning_beam_agent-3_max: 78
    cleaning_beam_agent-3_mean: 29.08
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 441
    cleaning_beam_agent-4_mean: 370.32
    cleaning_beam_agent-4_min: 252
    cleaning_beam_agent-5_max: 149
    cleaning_beam_agent-5_mean: 48.87
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-25-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 978.9999999999768
  episode_reward_mean: 790.4199999999864
  episode_reward_min: 359.00000000000466
  episodes_this_iter: 96
  episodes_total: 20256
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11559.725
    learner:
      agent-0:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9949353933334351
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023147957399487495
        model: {}
        policy_loss: -0.003882576711475849
        total_loss: -0.003973702434450388
        vf_explained_var: 0.05413609743118286
        vf_loss: 16.59850311279297
      agent-1:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1812164783477783
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020096173975616693
        model: {}
        policy_loss: -0.004258023574948311
        total_loss: -0.004526561591774225
        vf_explained_var: -0.016066819429397583
        vf_loss: 18.10015296936035
      agent-2:
        cur_kl_coeff: 0.0003906250058207661
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1339972019195557
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015166767407208681
        model: {}
        policy_loss: -0.004095124546438456
        total_loss: -0.004389370791614056
        vf_explained_var: 0.034456923604011536
        vf_loss: 17.009963989257812
      agent-3:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5806818604469299
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014175144024193287
        model: {}
        policy_loss: -0.0030195745639503
        total_loss: -0.0025145388208329678
        vf_explained_var: 0.1368684321641922
        vf_loss: 15.270367622375488
      agent-4:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0100030899047852
        entropy_coeff: 0.0017600000137463212
        kl: 0.002316541038453579
        model: {}
        policy_loss: -0.00459618866443634
        total_loss: -0.004656272940337658
        vf_explained_var: 0.03515799343585968
        vf_loss: 17.17068099975586
      agent-5:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9482369422912598
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017044720007106662
        model: {}
        policy_loss: -0.004230529069900513
        total_loss: -0.00436871312558651
        vf_explained_var: 0.13535316288471222
        vf_loss: 15.303803443908691
    load_time_ms: 13636.751
    num_steps_sampled: 20256000
    num_steps_trained: 20256000
    sample_time_ms: 101849.404
    update_time_ms: 14.495
  iterations_since_restore: 151
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.233149171270718
    ram_util_percent: 16.10939226519337
  pid: 30948
  policy_reward_max:
    agent-0: 163.16666666666632
    agent-1: 163.16666666666632
    agent-2: 163.16666666666632
    agent-3: 163.16666666666632
    agent-4: 163.16666666666632
    agent-5: 163.16666666666632
  policy_reward_mean:
    agent-0: 131.73666666666693
    agent-1: 131.73666666666693
    agent-2: 131.73666666666693
    agent-3: 131.73666666666693
    agent-4: 131.73666666666693
    agent-5: 131.73666666666693
  policy_reward_min:
    agent-0: 59.833333333333094
    agent-1: 59.833333333333094
    agent-2: 59.833333333333094
    agent-3: 59.833333333333094
    agent-4: 59.833333333333094
    agent-5: 59.833333333333094
  sampler_perf:
    mean_env_wait_ms: 27.123065240422456
    mean_inference_ms: 12.95540092010318
    mean_processing_ms: 58.17963783047483
  time_since_restore: 20410.220776557922
  time_this_iter_s: 127.30358791351318
  time_total_s: 29536.232590436935
  timestamp: 1637047511
  timesteps_since_restore: 14496000
  timesteps_this_iter: 96000
  timesteps_total: 20256000
  training_iteration: 211
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    211 |          29536.2 | 20256000 |   790.42 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 1.78
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 29.97
    apples_agent-1_min: 0
    apples_agent-2_max: 235
    apples_agent-2_mean: 10.86
    apples_agent-2_min: 0
    apples_agent-3_max: 225
    apples_agent-3_mean: 81.44
    apples_agent-3_min: 11
    apples_agent-4_max: 50
    apples_agent-4_mean: 1.14
    apples_agent-4_min: 0
    apples_agent-5_max: 162
    apples_agent-5_mean: 92.29
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 563
    cleaning_beam_agent-0_mean: 397.75
    cleaning_beam_agent-0_min: 292
    cleaning_beam_agent-1_max: 411
    cleaning_beam_agent-1_mean: 210.67
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 535
    cleaning_beam_agent-2_mean: 328.67
    cleaning_beam_agent-2_min: 101
    cleaning_beam_agent-3_max: 67
    cleaning_beam_agent-3_mean: 29.48
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 492
    cleaning_beam_agent-4_mean: 387.37
    cleaning_beam_agent-4_min: 282
    cleaning_beam_agent-5_max: 209
    cleaning_beam_agent-5_mean: 51.77
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-27-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 944.9999999999857
  episode_reward_mean: 799.6999999999845
  episode_reward_min: 404.0000000000027
  episodes_this_iter: 96
  episodes_total: 20352
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11537.359
    learner:
      agent-0:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9921469688415527
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016880370676517487
        model: {}
        policy_loss: -0.003726357128471136
        total_loss: -0.003978280816227198
        vf_explained_var: 0.09827768802642822
        vf_loss: 14.942195892333984
      agent-1:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1629695892333984
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014025085838511586
        model: {}
        policy_loss: -0.004345691297203302
        total_loss: -0.004662686493247747
        vf_explained_var: -0.02503758668899536
        vf_loss: 17.29700469970703
      agent-2:
        cur_kl_coeff: 0.00019531250291038305
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1119197607040405
        entropy_coeff: 0.0017600000137463212
        kl: 0.001403213944286108
        model: {}
        policy_loss: -0.003948984667658806
        total_loss: -0.004264450166374445
        vf_explained_var: 0.015635579824447632
        vf_loss: 16.412412643432617
      agent-3:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5595728158950806
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010342058958485723
        model: {}
        policy_loss: -0.0026660095900297165
        total_loss: -0.0021126428619027138
        vf_explained_var: 0.07452757656574249
        vf_loss: 15.382148742675781
      agent-4:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9971879124641418
        entropy_coeff: 0.0017600000137463212
        kl: 0.001444913912564516
        model: {}
        policy_loss: -0.004166624043136835
        total_loss: -0.004249968566000462
        vf_explained_var: 0.002786293625831604
        vf_loss: 16.715686798095703
      agent-5:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9277364015579224
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010031367419287562
        model: {}
        policy_loss: -0.0038545711431652308
        total_loss: -0.003891597269102931
        vf_explained_var: 0.03967630863189697
        vf_loss: 15.956987380981445
    load_time_ms: 13415.124
    num_steps_sampled: 20352000
    num_steps_trained: 20352000
    sample_time_ms: 102017.957
    update_time_ms: 14.606
  iterations_since_restore: 152
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.11703296703297
    ram_util_percent: 16.192857142857147
  pid: 30948
  policy_reward_max:
    agent-0: 157.49999999999963
    agent-1: 157.49999999999963
    agent-2: 157.49999999999963
    agent-3: 157.49999999999963
    agent-4: 157.49999999999963
    agent-5: 157.49999999999963
  policy_reward_mean:
    agent-0: 133.28333333333362
    agent-1: 133.28333333333362
    agent-2: 133.28333333333362
    agent-3: 133.28333333333362
    agent-4: 133.28333333333362
    agent-5: 133.28333333333362
  policy_reward_min:
    agent-0: 67.33333333333321
    agent-1: 67.33333333333321
    agent-2: 67.33333333333321
    agent-3: 67.33333333333321
    agent-4: 67.33333333333321
    agent-5: 67.33333333333321
  sampler_perf:
    mean_env_wait_ms: 27.12198266272844
    mean_inference_ms: 12.95593445963758
    mean_processing_ms: 58.172499900315636
  time_since_restore: 20538.13889694214
  time_this_iter_s: 127.91812038421631
  time_total_s: 29664.15071082115
  timestamp: 1637047639
  timesteps_since_restore: 14592000
  timesteps_this_iter: 96000
  timesteps_total: 20352000
  training_iteration: 212
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    212 |          29664.2 | 20352000 |    799.7 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 4.23
    apples_agent-0_min: 0
    apples_agent-1_max: 110
    apples_agent-1_mean: 27.45
    apples_agent-1_min: 0
    apples_agent-2_max: 188
    apples_agent-2_mean: 8.73
    apples_agent-2_min: 0
    apples_agent-3_max: 146
    apples_agent-3_mean: 78.77
    apples_agent-3_min: 39
    apples_agent-4_max: 41
    apples_agent-4_mean: 2.38
    apples_agent-4_min: 0
    apples_agent-5_max: 188
    apples_agent-5_mean: 89.85
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 390.25
    cleaning_beam_agent-0_min: 220
    cleaning_beam_agent-1_max: 411
    cleaning_beam_agent-1_mean: 221.96
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 515
    cleaning_beam_agent-2_mean: 341.88
    cleaning_beam_agent-2_min: 135
    cleaning_beam_agent-3_max: 73
    cleaning_beam_agent-3_mean: 30.8
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 474
    cleaning_beam_agent-4_mean: 382.07
    cleaning_beam_agent-4_min: 260
    cleaning_beam_agent-5_max: 179
    cleaning_beam_agent-5_mean: 46.48
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 3
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-29-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 962.9999999999733
  episode_reward_mean: 791.759999999985
  episode_reward_min: 416.0000000000023
  episodes_this_iter: 96
  episodes_total: 20448
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11550.601
    learner:
      agent-0:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9957039952278137
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017114004585891962
        model: {}
        policy_loss: -0.003622157499194145
        total_loss: -0.003798512741923332
        vf_explained_var: 0.09210273623466492
        vf_loss: 15.760682106018066
      agent-1:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1698230504989624
        entropy_coeff: 0.0017600000137463212
        kl: 0.001090811681933701
        model: {}
        policy_loss: -0.00407752301543951
        total_loss: -0.004285229369997978
        vf_explained_var: -0.05525916814804077
        vf_loss: 18.511356353759766
      agent-2:
        cur_kl_coeff: 9.765625145519152e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1236157417297363
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014170139329507947
        model: {}
        policy_loss: -0.0038946568965911865
        total_loss: -0.004132718313485384
        vf_explained_var: -0.0019482523202896118
        vf_loss: 17.393712997436523
      agent-3:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5637893080711365
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010908844415098429
        model: {}
        policy_loss: -0.002884411718696356
        total_loss: -0.0022856667637825012
        vf_explained_var: 0.08372688293457031
        vf_loss: 15.910109519958496
      agent-4:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0053584575653076
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013783702161163092
        model: {}
        policy_loss: -0.004293609410524368
        total_loss: -0.00434150081127882
        vf_explained_var: 0.010784998536109924
        vf_loss: 17.214744567871094
      agent-5:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.934349775314331
        entropy_coeff: 0.0017600000137463212
        kl: 0.00104736746288836
        model: {}
        policy_loss: -0.0038291739765554667
        total_loss: -0.0039033284410834312
        vf_explained_var: 0.09841090440750122
        vf_loss: 15.70254135131836
    load_time_ms: 13413.016
    num_steps_sampled: 20448000
    num_steps_trained: 20448000
    sample_time_ms: 102222.321
    update_time_ms: 14.708
  iterations_since_restore: 153
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.1478021978022
    ram_util_percent: 16.18626373626374
  pid: 30948
  policy_reward_max:
    agent-0: 160.4999999999996
    agent-1: 160.4999999999996
    agent-2: 160.4999999999996
    agent-3: 160.4999999999996
    agent-4: 160.4999999999996
    agent-5: 160.4999999999996
  policy_reward_mean:
    agent-0: 131.96000000000026
    agent-1: 131.96000000000026
    agent-2: 131.96000000000026
    agent-3: 131.96000000000026
    agent-4: 131.96000000000026
    agent-5: 131.96000000000026
  policy_reward_min:
    agent-0: 69.33333333333314
    agent-1: 69.33333333333314
    agent-2: 69.33333333333314
    agent-3: 69.33333333333314
    agent-4: 69.33333333333314
    agent-5: 69.33333333333314
  sampler_perf:
    mean_env_wait_ms: 27.122420005948292
    mean_inference_ms: 12.955886140101404
    mean_processing_ms: 58.168478702559895
  time_since_restore: 20666.326447486877
  time_this_iter_s: 128.18755054473877
  time_total_s: 29792.33826136589
  timestamp: 1637047767
  timesteps_since_restore: 14688000
  timesteps_this_iter: 96000
  timesteps_total: 20448000
  training_iteration: 213
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    213 |          29792.3 | 20448000 |   791.76 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 2.91
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 31.26
    apples_agent-1_min: 0
    apples_agent-2_max: 72
    apples_agent-2_mean: 10.29
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 79.73
    apples_agent-3_min: 33
    apples_agent-4_max: 50
    apples_agent-4_mean: 2.11
    apples_agent-4_min: 0
    apples_agent-5_max: 135
    apples_agent-5_mean: 88.58
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 495
    cleaning_beam_agent-0_mean: 387.78
    cleaning_beam_agent-0_min: 291
    cleaning_beam_agent-1_max: 409
    cleaning_beam_agent-1_mean: 207.36
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 555
    cleaning_beam_agent-2_mean: 331.45
    cleaning_beam_agent-2_min: 162
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 29.69
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 483
    cleaning_beam_agent-4_mean: 387.76
    cleaning_beam_agent-4_min: 292
    cleaning_beam_agent-5_max: 136
    cleaning_beam_agent-5_mean: 44.52
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.03
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-31-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 995.999999999971
  episode_reward_mean: 803.4199999999851
  episode_reward_min: 501.0000000000069
  episodes_this_iter: 96
  episodes_total: 20544
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11565.662
    learner:
      agent-0:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0025343894958496
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014690619427710772
        model: {}
        policy_loss: -0.0036270529963076115
        total_loss: -0.0036864320281893015
        vf_explained_var: 0.05707325041294098
        vf_loss: 17.050783157348633
      agent-1:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1802369356155396
        entropy_coeff: 0.0017600000137463212
        kl: 0.002061952371150255
        model: {}
        policy_loss: -0.004404088016599417
        total_loss: -0.004636932164430618
        vf_explained_var: -0.0029087811708450317
        vf_loss: 18.443267822265625
      agent-2:
        cur_kl_coeff: 4.882812572759576e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1235682964324951
        entropy_coeff: 0.0017600000137463212
        kl: 0.001390004064887762
        model: {}
        policy_loss: -0.0038005467504262924
        total_loss: -0.004044326022267342
        vf_explained_var: 0.045486077666282654
        vf_loss: 17.336389541625977
      agent-3:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.558899462223053
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009959592716768384
        model: {}
        policy_loss: -0.0027139533776789904
        total_loss: -0.0020803676452487707
        vf_explained_var: 0.11278180778026581
        vf_loss: 16.1724853515625
      agent-4:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0121493339538574
        entropy_coeff: 0.0017600000137463212
        kl: 0.00166391022503376
        model: {}
        policy_loss: -0.004086184781044722
        total_loss: -0.00410730205476284
        vf_explained_var: 0.0360385924577713
        vf_loss: 17.602294921875
      agent-5:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.936161994934082
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021394749637693167
        model: {}
        policy_loss: -0.004409116227179766
        total_loss: -0.0044801426120102406
        vf_explained_var: 0.13252060115337372
        vf_loss: 15.76572036743164
    load_time_ms: 13393.988
    num_steps_sampled: 20544000
    num_steps_trained: 20544000
    sample_time_ms: 102454.588
    update_time_ms: 14.564
  iterations_since_restore: 154
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.164480874316943
    ram_util_percent: 16.197267759562845
  pid: 30948
  policy_reward_max:
    agent-0: 165.99999999999966
    agent-1: 165.99999999999966
    agent-2: 165.99999999999966
    agent-3: 165.99999999999966
    agent-4: 165.99999999999966
    agent-5: 165.99999999999966
  policy_reward_mean:
    agent-0: 133.9033333333336
    agent-1: 133.9033333333336
    agent-2: 133.9033333333336
    agent-3: 133.9033333333336
    agent-4: 133.9033333333336
    agent-5: 133.9033333333336
  policy_reward_min:
    agent-0: 83.5000000000001
    agent-1: 83.5000000000001
    agent-2: 83.5000000000001
    agent-3: 83.5000000000001
    agent-4: 83.5000000000001
    agent-5: 83.5000000000001
  sampler_perf:
    mean_env_wait_ms: 27.121537538339027
    mean_inference_ms: 12.955997019341146
    mean_processing_ms: 58.16399296661194
  time_since_restore: 20795.06056046486
  time_this_iter_s: 128.73411297798157
  time_total_s: 29921.072374343872
  timestamp: 1637047896
  timesteps_since_restore: 14784000
  timesteps_this_iter: 96000
  timesteps_total: 20544000
  training_iteration: 214
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    214 |          29921.1 | 20544000 |   803.42 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 3.48
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 30.62
    apples_agent-1_min: 0
    apples_agent-2_max: 72
    apples_agent-2_mean: 5.59
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 75.83
    apples_agent-3_min: 24
    apples_agent-4_max: 87
    apples_agent-4_mean: 2.6
    apples_agent-4_min: 0
    apples_agent-5_max: 166
    apples_agent-5_mean: 96.89
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 515
    cleaning_beam_agent-0_mean: 388.11
    cleaning_beam_agent-0_min: 286
    cleaning_beam_agent-1_max: 407
    cleaning_beam_agent-1_mean: 214.16
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 550
    cleaning_beam_agent-2_mean: 365.03
    cleaning_beam_agent-2_min: 163
    cleaning_beam_agent-3_max: 88
    cleaning_beam_agent-3_mean: 29.86
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 484
    cleaning_beam_agent-4_mean: 388.58
    cleaning_beam_agent-4_min: 304
    cleaning_beam_agent-5_max: 158
    cleaning_beam_agent-5_mean: 48.12
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-33-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 979.9999999999753
  episode_reward_mean: 820.0199999999849
  episode_reward_min: 384.00000000000523
  episodes_this_iter: 96
  episodes_total: 20640
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11597.621
    learner:
      agent-0:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0049262046813965
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014228866202756763
        model: {}
        policy_loss: -0.003425230272114277
        total_loss: -0.0035331062972545624
        vf_explained_var: 0.06501176953315735
        vf_loss: 16.60793685913086
      agent-1:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.169203281402588
        entropy_coeff: 0.0017600000137463212
        kl: 0.001555867027491331
        model: {}
        policy_loss: -0.004234900698065758
        total_loss: -0.004397311247885227
        vf_explained_var: -0.045820415019989014
        vf_loss: 18.95370101928711
      agent-2:
        cur_kl_coeff: 2.441406286379788e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.112756609916687
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012571769766509533
        model: {}
        policy_loss: -0.0036351464223116636
        total_loss: -0.003825533203780651
        vf_explained_var: 0.0038750767707824707
        vf_loss: 17.680377960205078
      agent-3:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5559738874435425
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009620474884286523
        model: {}
        policy_loss: -0.002712610876187682
        total_loss: -0.0021215605083853006
        vf_explained_var: 0.12160871922969818
        vf_loss: 15.695622444152832
      agent-4:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0028307437896729
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020364427473396063
        model: {}
        policy_loss: -0.004438379779458046
        total_loss: -0.004412610083818436
        vf_explained_var: 0.003025159239768982
        vf_loss: 17.907299041748047
      agent-5:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9213845729827881
        entropy_coeff: 0.0017600000137463212
        kl: 0.001743430271744728
        model: {}
        policy_loss: -0.004530963953584433
        total_loss: -0.0045705498196184635
        vf_explained_var: 0.12246783077716827
        vf_loss: 15.820343017578125
    load_time_ms: 13469.045
    num_steps_sampled: 20640000
    num_steps_trained: 20640000
    sample_time_ms: 102479.412
    update_time_ms: 14.435
  iterations_since_restore: 155
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.94426229508197
    ram_util_percent: 16.1448087431694
  pid: 30948
  policy_reward_max:
    agent-0: 163.3333333333333
    agent-1: 163.3333333333333
    agent-2: 163.3333333333333
    agent-3: 163.3333333333333
    agent-4: 163.3333333333333
    agent-5: 163.3333333333333
  policy_reward_mean:
    agent-0: 136.67000000000021
    agent-1: 136.67000000000021
    agent-2: 136.67000000000021
    agent-3: 136.67000000000021
    agent-4: 136.67000000000021
    agent-5: 136.67000000000021
  policy_reward_min:
    agent-0: 63.99999999999978
    agent-1: 63.99999999999978
    agent-2: 63.99999999999978
    agent-3: 63.99999999999978
    agent-4: 63.99999999999978
    agent-5: 63.99999999999978
  sampler_perf:
    mean_env_wait_ms: 27.121835476315805
    mean_inference_ms: 12.956473545974037
    mean_processing_ms: 58.157609723274334
  time_since_restore: 20923.25609922409
  time_this_iter_s: 128.19553875923157
  time_total_s: 30049.267913103104
  timestamp: 1637048024
  timesteps_since_restore: 14880000
  timesteps_this_iter: 96000
  timesteps_total: 20640000
  training_iteration: 215
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    215 |          30049.3 | 20640000 |   820.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 2.33
    apples_agent-0_min: 0
    apples_agent-1_max: 121
    apples_agent-1_mean: 36.22
    apples_agent-1_min: 0
    apples_agent-2_max: 86
    apples_agent-2_mean: 8.27
    apples_agent-2_min: 0
    apples_agent-3_max: 135
    apples_agent-3_mean: 77.25
    apples_agent-3_min: 34
    apples_agent-4_max: 87
    apples_agent-4_mean: 2.67
    apples_agent-4_min: 0
    apples_agent-5_max: 164
    apples_agent-5_mean: 99.01
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 514
    cleaning_beam_agent-0_mean: 398.98
    cleaning_beam_agent-0_min: 215
    cleaning_beam_agent-1_max: 367
    cleaning_beam_agent-1_mean: 205.11
    cleaning_beam_agent-1_min: 105
    cleaning_beam_agent-2_max: 601
    cleaning_beam_agent-2_mean: 353.21
    cleaning_beam_agent-2_min: 150
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 31.12
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 475
    cleaning_beam_agent-4_mean: 385.97
    cleaning_beam_agent-4_min: 275
    cleaning_beam_agent-5_max: 119
    cleaning_beam_agent-5_mean: 45.08
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-35-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 984.9999999999816
  episode_reward_mean: 814.5299999999846
  episode_reward_min: 220.99999999999793
  episodes_this_iter: 96
  episodes_total: 20736
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11581.3
    learner:
      agent-0:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.974270761013031
        entropy_coeff: 0.0017600000137463212
        kl: 0.002162954304367304
        model: {}
        policy_loss: -0.003834344446659088
        total_loss: -0.003775784745812416
        vf_explained_var: 0.07515303790569305
        vf_loss: 17.732746124267578
      agent-1:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.175025463104248
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015746294520795345
        model: {}
        policy_loss: -0.004131702706217766
        total_loss: -0.004173867870122194
        vf_explained_var: -0.03115934133529663
        vf_loss: 20.25873374938965
      agent-2:
        cur_kl_coeff: 1.220703143189894e-05
        cur_lr: 1.2000000424450263e-05
        entropy: 1.127037763595581
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014797844924032688
        model: {}
        policy_loss: -0.003916972782462835
        total_loss: -0.004026212729513645
        vf_explained_var: 0.022211864590644836
        vf_loss: 18.743330001831055
      agent-3:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5624229907989502
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008453425252810121
        model: {}
        policy_loss: -0.0026738198939710855
        total_loss: -0.0020027991849929094
        vf_explained_var: 0.13736796379089355
        vf_loss: 16.60886001586914
      agent-4:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0045982599258423
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016680416883900762
        model: {}
        policy_loss: -0.004313201177865267
        total_loss: -0.004236431792378426
        vf_explained_var: 0.047460705041885376
        vf_loss: 18.44857406616211
      agent-5:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9235496520996094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011593104572966695
        model: {}
        policy_loss: -0.0041105132550001144
        total_loss: -0.0040330179035663605
        vf_explained_var: 0.11355280876159668
        vf_loss: 17.02935791015625
    load_time_ms: 13452.014
    num_steps_sampled: 20736000
    num_steps_trained: 20736000
    sample_time_ms: 102288.183
    update_time_ms: 14.323
  iterations_since_restore: 156
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.97262569832402
    ram_util_percent: 16.19106145251397
  pid: 30948
  policy_reward_max:
    agent-0: 164.16666666666688
    agent-1: 164.16666666666688
    agent-2: 164.16666666666688
    agent-3: 164.16666666666688
    agent-4: 164.16666666666688
    agent-5: 164.16666666666688
  policy_reward_mean:
    agent-0: 135.7550000000002
    agent-1: 135.7550000000002
    agent-2: 135.7550000000002
    agent-3: 135.7550000000002
    agent-4: 135.7550000000002
    agent-5: 135.7550000000002
  policy_reward_min:
    agent-0: 36.83333333333333
    agent-1: 36.83333333333333
    agent-2: 36.83333333333333
    agent-3: 36.83333333333333
    agent-4: 36.83333333333333
    agent-5: 36.83333333333333
  sampler_perf:
    mean_env_wait_ms: 27.119694103446232
    mean_inference_ms: 12.95559739023182
    mean_processing_ms: 58.147516430124476
  time_since_restore: 21049.231882810593
  time_this_iter_s: 125.97578358650208
  time_total_s: 30175.243696689606
  timestamp: 1637048150
  timesteps_since_restore: 14976000
  timesteps_this_iter: 96000
  timesteps_total: 20736000
  training_iteration: 216
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    216 |          30175.2 | 20736000 |   814.53 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 3.02
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 31.48
    apples_agent-1_min: 0
    apples_agent-2_max: 65
    apples_agent-2_mean: 8.02
    apples_agent-2_min: 0
    apples_agent-3_max: 131
    apples_agent-3_mean: 77.43
    apples_agent-3_min: 32
    apples_agent-4_max: 68
    apples_agent-4_mean: 2.95
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 92.26
    apples_agent-5_min: 46
    cleaning_beam_agent-0_max: 497
    cleaning_beam_agent-0_mean: 399.25
    cleaning_beam_agent-0_min: 206
    cleaning_beam_agent-1_max: 373
    cleaning_beam_agent-1_mean: 206.25
    cleaning_beam_agent-1_min: 83
    cleaning_beam_agent-2_max: 547
    cleaning_beam_agent-2_mean: 356.46
    cleaning_beam_agent-2_min: 169
    cleaning_beam_agent-3_max: 72
    cleaning_beam_agent-3_mean: 30.57
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 498
    cleaning_beam_agent-4_mean: 385.63
    cleaning_beam_agent-4_min: 280
    cleaning_beam_agent-5_max: 181
    cleaning_beam_agent-5_mean: 44.48
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-37-55
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1020.9999999999683
  episode_reward_mean: 816.6599999999846
  episode_reward_min: 292.00000000000006
  episodes_this_iter: 96
  episodes_total: 20832
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11572.069
    learner:
      agent-0:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9886012673377991
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014924835413694382
        model: {}
        policy_loss: -0.003408553544431925
        total_loss: -0.0033303992822766304
        vf_explained_var: 0.04893374443054199
        vf_loss: 18.180919647216797
      agent-1:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1650232076644897
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016957372426986694
        model: {}
        policy_loss: -0.004413797054439783
        total_loss: -0.004527131095528603
        vf_explained_var: -0.001550912857055664
        vf_loss: 19.371047973632812
      agent-2:
        cur_kl_coeff: 6.10351571594947e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1188699007034302
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014795531751587987
        model: {}
        policy_loss: -0.003951449878513813
        total_loss: -0.004097777418792248
        vf_explained_var: 0.043936312198638916
        vf_loss: 18.228778839111328
      agent-3:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5682223439216614
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011535694357007742
        model: {}
        policy_loss: -0.0029221384320408106
        total_loss: -0.0022827894426882267
        vf_explained_var: 0.14273156225681305
        vf_loss: 16.394201278686523
      agent-4:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9926835298538208
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023173820227384567
        model: {}
        policy_loss: -0.004166523925960064
        total_loss: -0.003975120838731527
        vf_explained_var: -0.007898494601249695
        vf_loss: 19.385263442993164
      agent-5:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8929369449615479
        entropy_coeff: 0.0017600000137463212
        kl: 0.00165173364803195
        model: {}
        policy_loss: -0.003818368073552847
        total_loss: -0.0037532667629420757
        vf_explained_var: 0.13617488741874695
        vf_loss: 16.36665916442871
    load_time_ms: 13452.309
    num_steps_sampled: 20832000
    num_steps_trained: 20832000
    sample_time_ms: 102104.494
    update_time_ms: 14.436
  iterations_since_restore: 157
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.5728813559322
    ram_util_percent: 16.13050847457627
  pid: 30948
  policy_reward_max:
    agent-0: 170.1666666666664
    agent-1: 170.1666666666664
    agent-2: 170.1666666666664
    agent-3: 170.1666666666664
    agent-4: 170.1666666666664
    agent-5: 170.1666666666664
  policy_reward_mean:
    agent-0: 136.11000000000018
    agent-1: 136.11000000000018
    agent-2: 136.11000000000018
    agent-3: 136.11000000000018
    agent-4: 136.11000000000018
    agent-5: 136.11000000000018
  policy_reward_min:
    agent-0: 48.66666666666656
    agent-1: 48.66666666666656
    agent-2: 48.66666666666656
    agent-3: 48.66666666666656
    agent-4: 48.66666666666656
    agent-5: 48.66666666666656
  sampler_perf:
    mean_env_wait_ms: 27.118334331904602
    mean_inference_ms: 12.954611508692954
    mean_processing_ms: 58.138670731780124
  time_since_restore: 21173.626989126205
  time_this_iter_s: 124.3951063156128
  time_total_s: 30299.63880300522
  timestamp: 1637048275
  timesteps_since_restore: 15072000
  timesteps_this_iter: 96000
  timesteps_total: 20832000
  training_iteration: 217
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    217 |          30299.6 | 20832000 |   816.66 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 68
    apples_agent-0_mean: 3.15
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 32.64
    apples_agent-1_min: 0
    apples_agent-2_max: 81
    apples_agent-2_mean: 7.31
    apples_agent-2_min: 0
    apples_agent-3_max: 145
    apples_agent-3_mean: 76.93
    apples_agent-3_min: 32
    apples_agent-4_max: 80
    apples_agent-4_mean: 3.48
    apples_agent-4_min: 0
    apples_agent-5_max: 158
    apples_agent-5_mean: 91.41
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 537
    cleaning_beam_agent-0_mean: 385.58
    cleaning_beam_agent-0_min: 249
    cleaning_beam_agent-1_max: 382
    cleaning_beam_agent-1_mean: 204.82
    cleaning_beam_agent-1_min: 73
    cleaning_beam_agent-2_max: 563
    cleaning_beam_agent-2_mean: 346.4
    cleaning_beam_agent-2_min: 190
    cleaning_beam_agent-3_max: 63
    cleaning_beam_agent-3_mean: 28.94
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 502
    cleaning_beam_agent-4_mean: 388.89
    cleaning_beam_agent-4_min: 226
    cleaning_beam_agent-5_max: 209
    cleaning_beam_agent-5_mean: 49.46
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-39-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 979.9999999999909
  episode_reward_mean: 792.639999999986
  episode_reward_min: 234.9999999999967
  episodes_this_iter: 96
  episodes_total: 20928
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11584.408
    learner:
      agent-0:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.985770583152771
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019234074279665947
        model: {}
        policy_loss: -0.0037630684673786163
        total_loss: -0.003685460891574621
        vf_explained_var: 0.03772038221359253
        vf_loss: 18.125640869140625
      agent-1:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.170323133468628
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010981772793456912
        model: {}
        policy_loss: -0.003746501635760069
        total_loss: -0.003893153741955757
        vf_explained_var: -0.011199802160263062
        vf_loss: 19.13113784790039
      agent-2:
        cur_kl_coeff: 3.051757857974735e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1194941997528076
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016209133900702
        model: {}
        policy_loss: -0.004104873165488243
        total_loss: -0.004225475713610649
        vf_explained_var: 0.016992107033729553
        vf_loss: 18.49704360961914
      agent-3:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5753543376922607
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010723794111981988
        model: {}
        policy_loss: -0.0029142757412046194
        total_loss: -0.002320419065654278
        vf_explained_var: 0.14706365764141083
        vf_loss: 16.06479835510254
      agent-4:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0071269273757935
        entropy_coeff: 0.0017600000137463212
        kl: 0.001566292718052864
        model: {}
        policy_loss: -0.0039688837714493275
        total_loss: -0.003993593621999025
        vf_explained_var: 0.07384106516838074
        vf_loss: 17.47831153869629
      agent-5:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 0.919501543045044
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011372884036973119
        model: {}
        policy_loss: -0.003921835217624903
        total_loss: -0.003913660068064928
        vf_explained_var: 0.13508756458759308
        vf_loss: 16.26500129699707
    load_time_ms: 13442.807
    num_steps_sampled: 20928000
    num_steps_trained: 20928000
    sample_time_ms: 101859.024
    update_time_ms: 15.173
  iterations_since_restore: 158
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.370056497175142
    ram_util_percent: 16.188700564971754
  pid: 30948
  policy_reward_max:
    agent-0: 163.33333333333312
    agent-1: 163.33333333333312
    agent-2: 163.33333333333312
    agent-3: 163.33333333333312
    agent-4: 163.33333333333312
    agent-5: 163.33333333333312
  policy_reward_mean:
    agent-0: 132.1066666666669
    agent-1: 132.1066666666669
    agent-2: 132.1066666666669
    agent-3: 132.1066666666669
    agent-4: 132.1066666666669
    agent-5: 132.1066666666669
  policy_reward_min:
    agent-0: 39.166666666666615
    agent-1: 39.166666666666615
    agent-2: 39.166666666666615
    agent-3: 39.166666666666615
    agent-4: 39.166666666666615
    agent-5: 39.166666666666615
  sampler_perf:
    mean_env_wait_ms: 27.11629498803875
    mean_inference_ms: 12.953011514136426
    mean_processing_ms: 58.12762195009435
  time_since_restore: 21297.996180295944
  time_this_iter_s: 124.36919116973877
  time_total_s: 30424.007994174957
  timestamp: 1637048399
  timesteps_since_restore: 15168000
  timesteps_this_iter: 96000
  timesteps_total: 20928000
  training_iteration: 218
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    218 |            30424 | 20928000 |   792.64 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 2.03
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 30.53
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 7.63
    apples_agent-2_min: 0
    apples_agent-3_max: 122
    apples_agent-3_mean: 72.85
    apples_agent-3_min: 40
    apples_agent-4_max: 90
    apples_agent-4_mean: 4.84
    apples_agent-4_min: 0
    apples_agent-5_max: 162
    apples_agent-5_mean: 94.01
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 523
    cleaning_beam_agent-0_mean: 392.53
    cleaning_beam_agent-0_min: 251
    cleaning_beam_agent-1_max: 391
    cleaning_beam_agent-1_mean: 209.17
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 602
    cleaning_beam_agent-2_mean: 355.22
    cleaning_beam_agent-2_min: 135
    cleaning_beam_agent-3_max: 59
    cleaning_beam_agent-3_mean: 25.48
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 510
    cleaning_beam_agent-4_mean: 393.51
    cleaning_beam_agent-4_min: 226
    cleaning_beam_agent-5_max: 209
    cleaning_beam_agent-5_mean: 50.01
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-42-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 975.9999999999737
  episode_reward_mean: 818.5299999999846
  episode_reward_min: 513.0000000000036
  episodes_this_iter: 96
  episodes_total: 21024
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11594.872
    learner:
      agent-0:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.973289966583252
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021458701230585575
        model: {}
        policy_loss: -0.0037363816518336535
        total_loss: -0.003775024088099599
        vf_explained_var: 0.03547406196594238
        vf_loss: 16.743453979492188
      agent-1:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.192757487297058
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010977821657434106
        model: {}
        policy_loss: -0.003881667274981737
        total_loss: -0.00411415146663785
        vf_explained_var: -0.056053370237350464
        vf_loss: 18.667736053466797
      agent-2:
        cur_kl_coeff: 1.5258789289873675e-06
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1031787395477295
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015445697354152799
        model: {}
        policy_loss: -0.0041561792604625225
        total_loss: -0.0043986476957798
        vf_explained_var: 0.028575867414474487
        vf_loss: 16.99125862121582
      agent-3:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5487080216407776
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010502783115953207
        model: {}
        policy_loss: -0.0027933570090681314
        total_loss: -0.002158989431336522
        vf_explained_var: 0.08950723707675934
        vf_loss: 16.000961303710938
      agent-4:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.002015233039856
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014749119291082025
        model: {}
        policy_loss: -0.004151957109570503
        total_loss: -0.004197725094854832
        vf_explained_var: 0.025010868906974792
        vf_loss: 17.177772521972656
      agent-5:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.917689323425293
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020893015898764133
        model: {}
        policy_loss: -0.00403505377471447
        total_loss: -0.004033138509839773
        vf_explained_var: 0.07695960998535156
        vf_loss: 16.170507431030273
    load_time_ms: 13458.491
    num_steps_sampled: 21024000
    num_steps_trained: 21024000
    sample_time_ms: 101525.615
    update_time_ms: 15.162
  iterations_since_restore: 159
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.547457627118643
    ram_util_percent: 16.134463276836158
  pid: 30948
  policy_reward_max:
    agent-0: 162.66666666666674
    agent-1: 162.66666666666674
    agent-2: 162.66666666666674
    agent-3: 162.66666666666674
    agent-4: 162.66666666666674
    agent-5: 162.66666666666674
  policy_reward_mean:
    agent-0: 136.42166666666685
    agent-1: 136.42166666666685
    agent-2: 136.42166666666685
    agent-3: 136.42166666666685
    agent-4: 136.42166666666685
    agent-5: 136.42166666666685
  policy_reward_min:
    agent-0: 85.4999999999999
    agent-1: 85.4999999999999
    agent-2: 85.4999999999999
    agent-3: 85.4999999999999
    agent-4: 85.4999999999999
    agent-5: 85.4999999999999
  sampler_perf:
    mean_env_wait_ms: 27.115792226227732
    mean_inference_ms: 12.951733490181477
    mean_processing_ms: 58.11813649106884
  time_since_restore: 21422.36217737198
  time_this_iter_s: 124.36599707603455
  time_total_s: 30548.373991250992
  timestamp: 1637048524
  timesteps_since_restore: 15264000
  timesteps_this_iter: 96000
  timesteps_total: 21024000
  training_iteration: 219
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    219 |          30548.4 | 21024000 |   818.53 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 1.64
    apples_agent-0_min: 0
    apples_agent-1_max: 117
    apples_agent-1_mean: 34.52
    apples_agent-1_min: 0
    apples_agent-2_max: 324
    apples_agent-2_mean: 10.88
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 77.45
    apples_agent-3_min: 31
    apples_agent-4_max: 51
    apples_agent-4_mean: 2.2
    apples_agent-4_min: 0
    apples_agent-5_max: 161
    apples_agent-5_mean: 100.96
    apples_agent-5_min: 58
    cleaning_beam_agent-0_max: 560
    cleaning_beam_agent-0_mean: 420.46
    cleaning_beam_agent-0_min: 234
    cleaning_beam_agent-1_max: 462
    cleaning_beam_agent-1_mean: 218.19
    cleaning_beam_agent-1_min: 101
    cleaning_beam_agent-2_max: 553
    cleaning_beam_agent-2_mean: 337.89
    cleaning_beam_agent-2_min: 142
    cleaning_beam_agent-3_max: 81
    cleaning_beam_agent-3_mean: 27.89
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 489
    cleaning_beam_agent-4_mean: 392.5
    cleaning_beam_agent-4_min: 223
    cleaning_beam_agent-5_max: 110
    cleaning_beam_agent-5_mean: 44.06
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-44-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1037.9999999999839
  episode_reward_mean: 832.8699999999853
  episode_reward_min: 467.0000000000114
  episodes_this_iter: 96
  episodes_total: 21120
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11579.351
    learner:
      agent-0:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9623147249221802
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012945699272677302
        model: {}
        policy_loss: -0.0033866045996546745
        total_loss: -0.003406535368412733
        vf_explained_var: 0.024356037378311157
        vf_loss: 16.7374267578125
      agent-1:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1596975326538086
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022957054898142815
        model: {}
        policy_loss: -0.004354757722467184
        total_loss: -0.00457589328289032
        vf_explained_var: -0.025796949863433838
        vf_loss: 18.199316024780273
      agent-2:
        cur_kl_coeff: 7.629394644936838e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.123008370399475
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017586005851626396
        model: {}
        policy_loss: -0.004121092613786459
        total_loss: -0.004436997231096029
        vf_explained_var: 0.04955655336380005
        vf_loss: 16.605911254882812
      agent-3:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5499088764190674
        entropy_coeff: 0.0017600000137463212
        kl: 0.001222363905981183
        model: {}
        policy_loss: -0.002651120536029339
        total_loss: -0.002055445685982704
        vf_explained_var: 0.09381243586540222
        vf_loss: 15.635140419006348
      agent-4:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9973534345626831
        entropy_coeff: 0.0017600000137463212
        kl: 0.001298976130783558
        model: {}
        policy_loss: -0.0039754593744874
        total_loss: -0.004059589002281427
        vf_explained_var: 0.04110172390937805
        vf_loss: 16.712127685546875
      agent-5:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9023984670639038
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015850317431613803
        model: {}
        policy_loss: -0.004040212836116552
        total_loss: -0.0040537104941904545
        vf_explained_var: 0.08962754905223846
        vf_loss: 15.747227668762207
    load_time_ms: 13454.336
    num_steps_sampled: 21120000
    num_steps_trained: 21120000
    sample_time_ms: 101181.74
    update_time_ms: 15.23
  iterations_since_restore: 160
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.406818181818185
    ram_util_percent: 16.19090909090909
  pid: 30948
  policy_reward_max:
    agent-0: 173.0000000000001
    agent-1: 173.0000000000001
    agent-2: 173.0000000000001
    agent-3: 173.0000000000001
    agent-4: 173.0000000000001
    agent-5: 173.0000000000001
  policy_reward_mean:
    agent-0: 138.81166666666684
    agent-1: 138.81166666666684
    agent-2: 138.81166666666684
    agent-3: 138.81166666666684
    agent-4: 138.81166666666684
    agent-5: 138.81166666666684
  policy_reward_min:
    agent-0: 77.83333333333334
    agent-1: 77.83333333333334
    agent-2: 77.83333333333334
    agent-3: 77.83333333333334
    agent-4: 77.83333333333334
    agent-5: 77.83333333333334
  sampler_perf:
    mean_env_wait_ms: 27.11340441921681
    mean_inference_ms: 12.95041830132503
    mean_processing_ms: 58.10784817230874
  time_since_restore: 21546.093665361404
  time_this_iter_s: 123.73148798942566
  time_total_s: 30672.105479240417
  timestamp: 1637048648
  timesteps_since_restore: 15360000
  timesteps_this_iter: 96000
  timesteps_total: 21120000
  training_iteration: 220
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    220 |          30672.1 | 21120000 |   832.87 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 1.25
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 27.36
    apples_agent-1_min: 0
    apples_agent-2_max: 326
    apples_agent-2_mean: 14.4
    apples_agent-2_min: 0
    apples_agent-3_max: 183
    apples_agent-3_mean: 84.58
    apples_agent-3_min: 46
    apples_agent-4_max: 35
    apples_agent-4_mean: 1.92
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 100.23
    apples_agent-5_min: 61
    cleaning_beam_agent-0_max: 546
    cleaning_beam_agent-0_mean: 427.69
    cleaning_beam_agent-0_min: 289
    cleaning_beam_agent-1_max: 341
    cleaning_beam_agent-1_mean: 207.86
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 570
    cleaning_beam_agent-2_mean: 341.67
    cleaning_beam_agent-2_min: 144
    cleaning_beam_agent-3_max: 53
    cleaning_beam_agent-3_mean: 23.78
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 505
    cleaning_beam_agent-4_mean: 394.93
    cleaning_beam_agent-4_min: 302
    cleaning_beam_agent-5_max: 176
    cleaning_beam_agent-5_mean: 43.38
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-46-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1015.9999999999839
  episode_reward_mean: 848.6099999999834
  episode_reward_min: 470.0000000000087
  episodes_this_iter: 96
  episodes_total: 21216
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11579.11
    learner:
      agent-0:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9691658020019531
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020404516253620386
        model: {}
        policy_loss: -0.0036010893527418375
        total_loss: -0.0036288867704570293
        vf_explained_var: 0.04408654570579529
        vf_loss: 16.779329299926758
      agent-1:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1629693508148193
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017834557220339775
        model: {}
        policy_loss: -0.0041013602167367935
        total_loss: -0.004264267161488533
        vf_explained_var: -0.024476587772369385
        vf_loss: 18.839168548583984
      agent-2:
        cur_kl_coeff: 3.814697322468419e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0948723554611206
        entropy_coeff: 0.0017600000137463212
        kl: 0.001562977908179164
        model: {}
        policy_loss: -0.0035104018170386553
        total_loss: -0.0037285301368683577
        vf_explained_var: 0.04113946855068207
        vf_loss: 17.088470458984375
      agent-3:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.521539568901062
        entropy_coeff: 0.0017600000137463212
        kl: 0.001270405249670148
        model: {}
        policy_loss: -0.00263911671936512
        total_loss: -0.0019813189283013344
        vf_explained_var: 0.0982234925031662
        vf_loss: 15.757087707519531
      agent-4:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0049018859863281
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014803748345002532
        model: {}
        policy_loss: -0.004148897714912891
        total_loss: -0.004157771356403828
        vf_explained_var: 0.01320040225982666
        vf_loss: 17.59752655029297
      agent-5:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8843151330947876
        entropy_coeff: 0.0017600000137463212
        kl: 0.002171890577301383
        model: {}
        policy_loss: -0.0038611330091953278
        total_loss: -0.0038315323181450367
        vf_explained_var: 0.0989750325679779
        vf_loss: 15.859962463378906
    load_time_ms: 13456.636
    num_steps_sampled: 21216000
    num_steps_trained: 21216000
    sample_time_ms: 100795.936
    update_time_ms: 15.235
  iterations_since_restore: 161
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.477272727272727
    ram_util_percent: 16.191477272727273
  pid: 30948
  policy_reward_max:
    agent-0: 169.33333333333306
    agent-1: 169.33333333333306
    agent-2: 169.33333333333306
    agent-3: 169.33333333333306
    agent-4: 169.33333333333306
    agent-5: 169.33333333333306
  policy_reward_mean:
    agent-0: 141.43500000000014
    agent-1: 141.43500000000014
    agent-2: 141.43500000000014
    agent-3: 141.43500000000014
    agent-4: 141.43500000000014
    agent-5: 141.43500000000014
  policy_reward_min:
    agent-0: 78.3333333333333
    agent-1: 78.3333333333333
    agent-2: 78.3333333333333
    agent-3: 78.3333333333333
    agent-4: 78.3333333333333
    agent-5: 78.3333333333333
  sampler_perf:
    mean_env_wait_ms: 27.111412578330597
    mean_inference_ms: 12.948848462129304
    mean_processing_ms: 58.09806791847288
  time_since_restore: 21669.575219631195
  time_this_iter_s: 123.48155426979065
  time_total_s: 30795.587033510208
  timestamp: 1637048772
  timesteps_since_restore: 15456000
  timesteps_this_iter: 96000
  timesteps_total: 21216000
  training_iteration: 221
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    221 |          30795.6 | 21216000 |   848.61 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 30
    apples_agent-0_mean: 2.49
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 34.49
    apples_agent-1_min: 0
    apples_agent-2_max: 77
    apples_agent-2_mean: 8.37
    apples_agent-2_min: 0
    apples_agent-3_max: 124
    apples_agent-3_mean: 76.3
    apples_agent-3_min: 42
    apples_agent-4_max: 103
    apples_agent-4_mean: 2.62
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 93.36
    apples_agent-5_min: 41
    cleaning_beam_agent-0_max: 527
    cleaning_beam_agent-0_mean: 412.38
    cleaning_beam_agent-0_min: 312
    cleaning_beam_agent-1_max: 379
    cleaning_beam_agent-1_mean: 197.36
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 607
    cleaning_beam_agent-2_mean: 338.09
    cleaning_beam_agent-2_min: 161
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 26.12
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 518
    cleaning_beam_agent-4_mean: 403.84
    cleaning_beam_agent-4_min: 259
    cleaning_beam_agent-5_max: 241
    cleaning_beam_agent-5_mean: 44.43
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-48-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 996.9999999999928
  episode_reward_mean: 840.9899999999849
  episode_reward_min: 389.99999999999886
  episodes_this_iter: 96
  episodes_total: 21312
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11588.129
    learner:
      agent-0:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.973366379737854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014007707359269261
        model: {}
        policy_loss: -0.003283850150182843
        total_loss: -0.00324611971154809
        vf_explained_var: 0.07468509674072266
        vf_loss: 17.508573532104492
      agent-1:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1672751903533936
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008706653025001287
        model: {}
        policy_loss: -0.003715861588716507
        total_loss: -0.003749626688659191
        vf_explained_var: -0.03967210650444031
        vf_loss: 20.20638656616211
      agent-2:
        cur_kl_coeff: 1.9073486612342094e-07
        cur_lr: 1.2000000424450263e-05
        entropy: 1.094584584236145
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014523904537782073
        model: {}
        policy_loss: -0.003699477296322584
        total_loss: -0.0037621192168444395
        vf_explained_var: 0.021885573863983154
        vf_loss: 18.63827896118164
      agent-3:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5539302825927734
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010104308603331447
        model: {}
        policy_loss: -0.0024821022525429726
        total_loss: -0.0017965614097192883
        vf_explained_var: 0.1225210428237915
        vf_loss: 16.604616165161133
      agent-4:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9936506152153015
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016453972784802318
        model: {}
        policy_loss: -0.004259265027940273
        total_loss: -0.004139493219554424
        vf_explained_var: 0.01921781897544861
        vf_loss: 18.685951232910156
      agent-5:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8991118669509888
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014646498020738363
        model: {}
        policy_loss: -0.003698175773024559
        total_loss: -0.003600869793444872
        vf_explained_var: 0.11070512235164642
        vf_loss: 16.797409057617188
    load_time_ms: 13458.144
    num_steps_sampled: 21312000
    num_steps_trained: 21312000
    sample_time_ms: 100454.082
    update_time_ms: 15.825
  iterations_since_restore: 162
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.317514124293783
    ram_util_percent: 16.19491525423729
  pid: 30948
  policy_reward_max:
    agent-0: 166.1666666666668
    agent-1: 166.1666666666668
    agent-2: 166.1666666666668
    agent-3: 166.1666666666668
    agent-4: 166.1666666666668
    agent-5: 166.1666666666668
  policy_reward_mean:
    agent-0: 140.16500000000013
    agent-1: 140.16500000000013
    agent-2: 140.16500000000013
    agent-3: 140.16500000000013
    agent-4: 140.16500000000013
    agent-5: 140.16500000000013
  policy_reward_min:
    agent-0: 64.99999999999997
    agent-1: 64.99999999999997
    agent-2: 64.99999999999997
    agent-3: 64.99999999999997
    agent-4: 64.99999999999997
    agent-5: 64.99999999999997
  sampler_perf:
    mean_env_wait_ms: 27.109715968158763
    mean_inference_ms: 12.94734828536568
    mean_processing_ms: 58.089243843081974
  time_since_restore: 21794.188570261
  time_this_iter_s: 124.61335062980652
  time_total_s: 30920.200384140015
  timestamp: 1637048896
  timesteps_since_restore: 15552000
  timesteps_this_iter: 96000
  timesteps_total: 21312000
  training_iteration: 222
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    222 |          30920.2 | 21312000 |   840.99 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 10
    apples_agent-0_mean: 0.54
    apples_agent-0_min: 0
    apples_agent-1_max: 125
    apples_agent-1_mean: 38.41
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 7.91
    apples_agent-2_min: 0
    apples_agent-3_max: 163
    apples_agent-3_mean: 79.19
    apples_agent-3_min: 43
    apples_agent-4_max: 103
    apples_agent-4_mean: 2.19
    apples_agent-4_min: 0
    apples_agent-5_max: 175
    apples_agent-5_mean: 96.58
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 542
    cleaning_beam_agent-0_mean: 419.49
    cleaning_beam_agent-0_min: 316
    cleaning_beam_agent-1_max: 346
    cleaning_beam_agent-1_mean: 195.27
    cleaning_beam_agent-1_min: 81
    cleaning_beam_agent-2_max: 548
    cleaning_beam_agent-2_mean: 335.54
    cleaning_beam_agent-2_min: 149
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 26.03
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 500
    cleaning_beam_agent-4_mean: 408.28
    cleaning_beam_agent-4_min: 203
    cleaning_beam_agent-5_max: 148
    cleaning_beam_agent-5_mean: 42.57
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-50-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1018.9999999999782
  episode_reward_mean: 855.6699999999818
  episode_reward_min: 389.99999999999886
  episodes_this_iter: 96
  episodes_total: 21408
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11565.563
    learner:
      agent-0:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9379081130027771
        entropy_coeff: 0.0017600000137463212
        kl: 0.002226627431809902
        model: {}
        policy_loss: -0.0031090276315808296
        total_loss: -0.003073668573051691
        vf_explained_var: 0.04487384855747223
        vf_loss: 16.860797882080078
      agent-1:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1720893383026123
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012739591766148806
        model: {}
        policy_loss: -0.00407461728900671
        total_loss: -0.004176029935479164
        vf_explained_var: -0.03672593832015991
        vf_loss: 19.614669799804688
      agent-2:
        cur_kl_coeff: 9.536743306171047e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1168781518936157
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019231067271903157
        model: {}
        policy_loss: -0.004251559264957905
        total_loss: -0.004412376321852207
        vf_explained_var: 0.0063441842794418335
        vf_loss: 18.048913955688477
      agent-3:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.525992751121521
        entropy_coeff: 0.0017600000137463212
        kl: 0.001294827088713646
        model: {}
        policy_loss: -0.0026658643037080765
        total_loss: -0.0018776943907141685
        vf_explained_var: 0.037789300084114075
        vf_loss: 17.139169692993164
      agent-4:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9957815408706665
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017547825118526816
        model: {}
        policy_loss: -0.004151179920881987
        total_loss: -0.004113543778657913
        vf_explained_var: 0.019489064812660217
        vf_loss: 17.902151107788086
      agent-5:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8832603096961975
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015313983894884586
        model: {}
        policy_loss: -0.003696886356920004
        total_loss: -0.0036741222720593214
        vf_explained_var: 0.11991623044013977
        vf_loss: 15.773004531860352
    load_time_ms: 13480.135
    num_steps_sampled: 21408000
    num_steps_trained: 21408000
    sample_time_ms: 99978.707
    update_time_ms: 15.766
  iterations_since_restore: 163
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.452272727272728
    ram_util_percent: 16.205113636363638
  pid: 30948
  policy_reward_max:
    agent-0: 169.83333333333314
    agent-1: 169.83333333333314
    agent-2: 169.83333333333314
    agent-3: 169.83333333333314
    agent-4: 169.83333333333314
    agent-5: 169.83333333333314
  policy_reward_mean:
    agent-0: 142.61166666666676
    agent-1: 142.61166666666676
    agent-2: 142.61166666666676
    agent-3: 142.61166666666676
    agent-4: 142.61166666666676
    agent-5: 142.61166666666676
  policy_reward_min:
    agent-0: 64.99999999999997
    agent-1: 64.99999999999997
    agent-2: 64.99999999999997
    agent-3: 64.99999999999997
    agent-4: 64.99999999999997
    agent-5: 64.99999999999997
  sampler_perf:
    mean_env_wait_ms: 27.10787504566581
    mean_inference_ms: 12.945766897429232
    mean_processing_ms: 58.078844093451444
  time_since_restore: 21917.64176440239
  time_this_iter_s: 123.45319414138794
  time_total_s: 31043.653578281403
  timestamp: 1637049020
  timesteps_since_restore: 15648000
  timesteps_this_iter: 96000
  timesteps_total: 21408000
  training_iteration: 223
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    223 |          31043.7 | 21408000 |   855.67 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 1.13
    apples_agent-0_min: 0
    apples_agent-1_max: 171
    apples_agent-1_mean: 30.95
    apples_agent-1_min: 0
    apples_agent-2_max: 109
    apples_agent-2_mean: 8.76
    apples_agent-2_min: 0
    apples_agent-3_max: 134
    apples_agent-3_mean: 77.42
    apples_agent-3_min: 40
    apples_agent-4_max: 60
    apples_agent-4_mean: 2.8
    apples_agent-4_min: 0
    apples_agent-5_max: 198
    apples_agent-5_mean: 98.09
    apples_agent-5_min: 54
    cleaning_beam_agent-0_max: 552
    cleaning_beam_agent-0_mean: 423.46
    cleaning_beam_agent-0_min: 286
    cleaning_beam_agent-1_max: 499
    cleaning_beam_agent-1_mean: 207.94
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 541
    cleaning_beam_agent-2_mean: 326.74
    cleaning_beam_agent-2_min: 128
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 24.14
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 496
    cleaning_beam_agent-4_mean: 411.62
    cleaning_beam_agent-4_min: 285
    cleaning_beam_agent-5_max: 108
    cleaning_beam_agent-5_mean: 44.1
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-52-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1018.9999999999782
  episode_reward_mean: 846.7599999999845
  episode_reward_min: 513.0000000000173
  episodes_this_iter: 96
  episodes_total: 21504
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11534.387
    learner:
      agent-0:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9318268895149231
        entropy_coeff: 0.0017600000137463212
        kl: 0.001812875852920115
        model: {}
        policy_loss: -0.0034178015775978565
        total_loss: -0.0033854523207992315
        vf_explained_var: 0.05123180150985718
        vf_loss: 16.723674774169922
      agent-1:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1639221906661987
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013687523314729333
        model: {}
        policy_loss: -0.003559598233550787
        total_loss: -0.0037521328777074814
        vf_explained_var: -0.023890256881713867
        vf_loss: 18.559675216674805
      agent-2:
        cur_kl_coeff: 4.7683716530855236e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.10163414478302
        entropy_coeff: 0.0017600000137463212
        kl: 0.001819120836444199
        model: {}
        policy_loss: -0.003941505216062069
        total_loss: -0.0040910132229328156
        vf_explained_var: -0.002651512622833252
        vf_loss: 17.89366912841797
      agent-3:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5331268906593323
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008599748834967613
        model: {}
        policy_loss: -0.0025398535653948784
        total_loss: -0.0019192197360098362
        vf_explained_var: 0.1162402480840683
        vf_loss: 15.589364051818848
      agent-4:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9824777841567993
        entropy_coeff: 0.0017600000137463212
        kl: 0.002206039847806096
        model: {}
        policy_loss: -0.004403139464557171
        total_loss: -0.004346958361566067
        vf_explained_var: 0.00877322256565094
        vf_loss: 17.85347557067871
      agent-5:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8864278793334961
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014368373667821288
        model: {}
        policy_loss: -0.004067456815391779
        total_loss: -0.003997020423412323
        vf_explained_var: 0.08212436735630035
        vf_loss: 16.30551528930664
    load_time_ms: 13463.314
    num_steps_sampled: 21504000
    num_steps_trained: 21504000
    sample_time_ms: 99625.821
    update_time_ms: 16.448
  iterations_since_restore: 164
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.302808988764042
    ram_util_percent: 16.20674157303371
  pid: 30948
  policy_reward_max:
    agent-0: 169.83333333333314
    agent-1: 169.83333333333314
    agent-2: 169.83333333333314
    agent-3: 169.83333333333314
    agent-4: 169.83333333333314
    agent-5: 169.83333333333314
  policy_reward_mean:
    agent-0: 141.1266666666668
    agent-1: 141.1266666666668
    agent-2: 141.1266666666668
    agent-3: 141.1266666666668
    agent-4: 141.1266666666668
    agent-5: 141.1266666666668
  policy_reward_min:
    agent-0: 85.50000000000028
    agent-1: 85.50000000000028
    agent-2: 85.50000000000028
    agent-3: 85.50000000000028
    agent-4: 85.50000000000028
    agent-5: 85.50000000000028
  sampler_perf:
    mean_env_wait_ms: 27.10572197563977
    mean_inference_ms: 12.944373753295132
    mean_processing_ms: 58.06921011153232
  time_since_restore: 22042.374636411667
  time_this_iter_s: 124.73287200927734
  time_total_s: 31168.38645029068
  timestamp: 1637049145
  timesteps_since_restore: 15744000
  timesteps_this_iter: 96000
  timesteps_total: 21504000
  training_iteration: 224
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    224 |          31168.4 | 21504000 |   846.76 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 29
    apples_agent-0_mean: 2.12
    apples_agent-0_min: 0
    apples_agent-1_max: 124
    apples_agent-1_mean: 31.18
    apples_agent-1_min: 0
    apples_agent-2_max: 70
    apples_agent-2_mean: 7.9
    apples_agent-2_min: 0
    apples_agent-3_max: 195
    apples_agent-3_mean: 77.64
    apples_agent-3_min: 40
    apples_agent-4_max: 49
    apples_agent-4_mean: 2.68
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 96.89
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 515
    cleaning_beam_agent-0_mean: 427.7
    cleaning_beam_agent-0_min: 314
    cleaning_beam_agent-1_max: 399
    cleaning_beam_agent-1_mean: 210.14
    cleaning_beam_agent-1_min: 99
    cleaning_beam_agent-2_max: 498
    cleaning_beam_agent-2_mean: 348.07
    cleaning_beam_agent-2_min: 132
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 25.37
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 538
    cleaning_beam_agent-4_mean: 409.7
    cleaning_beam_agent-4_min: 316
    cleaning_beam_agent-5_max: 193
    cleaning_beam_agent-5_mean: 47.74
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-54-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1034.9999999999704
  episode_reward_mean: 844.0799999999834
  episode_reward_min: 379.00000000000244
  episodes_this_iter: 96
  episodes_total: 21600
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11495.53
    learner:
      agent-0:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9372639060020447
        entropy_coeff: 0.0017600000137463212
        kl: 0.002272035228088498
        model: {}
        policy_loss: -0.0033256947062909603
        total_loss: -0.0033023706637322903
        vf_explained_var: 0.052365437150001526
        vf_loss: 16.729084014892578
      agent-1:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1612279415130615
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019145308760926127
        model: {}
        policy_loss: -0.004256312735378742
        total_loss: -0.004414553754031658
        vf_explained_var: -0.04007628560066223
        vf_loss: 18.855220794677734
      agent-2:
        cur_kl_coeff: 2.3841858265427618e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1001557111740112
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017667266074568033
        model: {}
        policy_loss: -0.00400942750275135
        total_loss: -0.004162781871855259
        vf_explained_var: 0.005724295973777771
        vf_loss: 17.82919692993164
      agent-3:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5350841879844666
        entropy_coeff: 0.0017600000137463212
        kl: 0.001153632765635848
        model: {}
        policy_loss: -0.0025689706671983004
        total_loss: -0.0018811651971191168
        vf_explained_var: 0.08083218336105347
        vf_loss: 16.29554557800293
      agent-4:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.979243814945221
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022578402422368526
        model: {}
        policy_loss: -0.004136587493121624
        total_loss: -0.004056239500641823
        vf_explained_var: -0.005480468273162842
        vf_loss: 18.038185119628906
      agent-5:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8945345282554626
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015613846480846405
        model: {}
        policy_loss: -0.003952197730541229
        total_loss: -0.0038640438579022884
        vf_explained_var: 0.06672778725624084
        vf_loss: 16.62537384033203
    load_time_ms: 13386.789
    num_steps_sampled: 21600000
    num_steps_trained: 21600000
    sample_time_ms: 99151.966
    update_time_ms: 16.323
  iterations_since_restore: 165
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.559195402298858
    ram_util_percent: 16.211494252873567
  pid: 30948
  policy_reward_max:
    agent-0: 172.49999999999918
    agent-1: 172.49999999999918
    agent-2: 172.49999999999918
    agent-3: 172.49999999999918
    agent-4: 172.49999999999918
    agent-5: 172.49999999999918
  policy_reward_mean:
    agent-0: 140.68000000000012
    agent-1: 140.68000000000012
    agent-2: 140.68000000000012
    agent-3: 140.68000000000012
    agent-4: 140.68000000000012
    agent-5: 140.68000000000012
  policy_reward_min:
    agent-0: 63.166666666666465
    agent-1: 63.166666666666465
    agent-2: 63.166666666666465
    agent-3: 63.166666666666465
    agent-4: 63.166666666666465
    agent-5: 63.166666666666465
  sampler_perf:
    mean_env_wait_ms: 27.103733717683998
    mean_inference_ms: 12.94266534832906
    mean_processing_ms: 58.055094760287275
  time_since_restore: 22164.675342559814
  time_this_iter_s: 122.30070614814758
  time_total_s: 31290.687156438828
  timestamp: 1637049267
  timesteps_since_restore: 15840000
  timesteps_this_iter: 96000
  timesteps_total: 21600000
  training_iteration: 225
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    225 |          31290.7 | 21600000 |   844.08 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 2.83
    apples_agent-0_min: 0
    apples_agent-1_max: 113
    apples_agent-1_mean: 31.45
    apples_agent-1_min: 0
    apples_agent-2_max: 61
    apples_agent-2_mean: 10.35
    apples_agent-2_min: 0
    apples_agent-3_max: 166
    apples_agent-3_mean: 77.82
    apples_agent-3_min: 46
    apples_agent-4_max: 66
    apples_agent-4_mean: 3.65
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 95.55
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 546
    cleaning_beam_agent-0_mean: 424.93
    cleaning_beam_agent-0_min: 325
    cleaning_beam_agent-1_max: 465
    cleaning_beam_agent-1_mean: 225.93
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 656
    cleaning_beam_agent-2_mean: 335.02
    cleaning_beam_agent-2_min: 94
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 24.28
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 536
    cleaning_beam_agent-4_mean: 421.94
    cleaning_beam_agent-4_min: 293
    cleaning_beam_agent-5_max: 235
    cleaning_beam_agent-5_mean: 54.98
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-56-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1016.9999999999757
  episode_reward_mean: 837.3399999999841
  episode_reward_min: 436.0000000000119
  episodes_this_iter: 96
  episodes_total: 21696
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11489.882
    learner:
      agent-0:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9515891671180725
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011678710579872131
        model: {}
        policy_loss: -0.0031627598218619823
        total_loss: -0.003058353438973427
        vf_explained_var: 0.08030442893505096
        vf_loss: 17.792036056518555
      agent-1:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1620572805404663
        entropy_coeff: 0.0017600000137463212
        kl: 0.001098624081350863
        model: {}
        policy_loss: -0.003996480256319046
        total_loss: -0.0039349808357656
        vf_explained_var: -0.06719523668289185
        vf_loss: 21.06717300415039
      agent-2:
        cur_kl_coeff: 1.1920929132713809e-08
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0850492715835571
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018051662482321262
        model: {}
        policy_loss: -0.0038445114623755217
        total_loss: -0.003813851857557893
        vf_explained_var: 0.011470824480056763
        vf_loss: 19.403446197509766
      agent-3:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.540643572807312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012914272956550121
        model: {}
        policy_loss: -0.002971302717924118
        total_loss: -0.0022446922957897186
        vf_explained_var: 0.13492034375667572
        vf_loss: 16.78148078918457
      agent-4:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9879414439201355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015535703860223293
        model: {}
        policy_loss: -0.003980862908065319
        total_loss: -0.00383659522049129
        vf_explained_var: 0.03846555948257446
        vf_loss: 18.8304443359375
      agent-5:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9022318720817566
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018727881833910942
        model: {}
        policy_loss: -0.004439890384674072
        total_loss: -0.004290132783353329
        vf_explained_var: 0.11058257520198822
        vf_loss: 17.37685775756836
    load_time_ms: 13386.488
    num_steps_sampled: 21696000
    num_steps_trained: 21696000
    sample_time_ms: 98920.136
    update_time_ms: 16.14
  iterations_since_restore: 166
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.60568181818182
    ram_util_percent: 16.12727272727273
  pid: 30948
  policy_reward_max:
    agent-0: 169.49999999999974
    agent-1: 169.49999999999974
    agent-2: 169.49999999999974
    agent-3: 169.49999999999974
    agent-4: 169.49999999999974
    agent-5: 169.49999999999974
  policy_reward_mean:
    agent-0: 139.55666666666676
    agent-1: 139.55666666666676
    agent-2: 139.55666666666676
    agent-3: 139.55666666666676
    agent-4: 139.55666666666676
    agent-5: 139.55666666666676
  policy_reward_min:
    agent-0: 72.6666666666666
    agent-1: 72.6666666666666
    agent-2: 72.6666666666666
    agent-3: 72.6666666666666
    agent-4: 72.6666666666666
    agent-5: 72.6666666666666
  sampler_perf:
    mean_env_wait_ms: 27.103565279958534
    mean_inference_ms: 12.94107176795365
    mean_processing_ms: 58.04382441410814
  time_since_restore: 22288.267066001892
  time_this_iter_s: 123.59172344207764
  time_total_s: 31414.278879880905
  timestamp: 1637049391
  timesteps_since_restore: 15936000
  timesteps_this_iter: 96000
  timesteps_total: 21696000
  training_iteration: 226
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    226 |          31414.3 | 21696000 |   837.34 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 2.39
    apples_agent-0_min: 0
    apples_agent-1_max: 125
    apples_agent-1_mean: 28.32
    apples_agent-1_min: 0
    apples_agent-2_max: 88
    apples_agent-2_mean: 6.91
    apples_agent-2_min: 0
    apples_agent-3_max: 123
    apples_agent-3_mean: 74.92
    apples_agent-3_min: 41
    apples_agent-4_max: 85
    apples_agent-4_mean: 1.69
    apples_agent-4_min: 0
    apples_agent-5_max: 269
    apples_agent-5_mean: 101.33
    apples_agent-5_min: 53
    cleaning_beam_agent-0_max: 547
    cleaning_beam_agent-0_mean: 428.35
    cleaning_beam_agent-0_min: 256
    cleaning_beam_agent-1_max: 423
    cleaning_beam_agent-1_mean: 224.6
    cleaning_beam_agent-1_min: 88
    cleaning_beam_agent-2_max: 582
    cleaning_beam_agent-2_mean: 370.86
    cleaning_beam_agent-2_min: 141
    cleaning_beam_agent-3_max: 44
    cleaning_beam_agent-3_mean: 23.88
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 520
    cleaning_beam_agent-4_mean: 431.0
    cleaning_beam_agent-4_min: 307
    cleaning_beam_agent-5_max: 148
    cleaning_beam_agent-5_mean: 55.02
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_02-58-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1022.9999999999842
  episode_reward_mean: 853.7199999999835
  episode_reward_min: 538.0000000000095
  episodes_this_iter: 96
  episodes_total: 21792
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11451.484
    learner:
      agent-0:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.94520103931427
        entropy_coeff: 0.0017600000137463212
        kl: 0.001587101723998785
        model: {}
        policy_loss: -0.0032795104198157787
        total_loss: -0.003293304704129696
        vf_explained_var: 0.07763108611106873
        vf_loss: 16.497568130493164
      agent-1:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1825275421142578
        entropy_coeff: 0.0017600000137463212
        kl: 0.000996546819806099
        model: {}
        policy_loss: -0.0037688836455345154
        total_loss: -0.0039274017326533794
        vf_explained_var: -0.044408053159713745
        vf_loss: 19.227325439453125
      agent-2:
        cur_kl_coeff: 5.9604645663569045e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0962364673614502
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014167290646582842
        model: {}
        policy_loss: -0.003515072399750352
        total_loss: -0.0036381480749696493
        vf_explained_var: 0.009921565651893616
        vf_loss: 18.063007354736328
      agent-3:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5187722444534302
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011696605943143368
        model: {}
        policy_loss: -0.002719322219491005
        total_loss: -0.0020045945420861244
        vf_explained_var: 0.09255050122737885
        vf_loss: 16.277677536010742
      agent-4:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.974469780921936
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021759907249361277
        model: {}
        policy_loss: -0.004288077354431152
        total_loss: -0.004188842140138149
        vf_explained_var: -0.0026799291372299194
        vf_loss: 18.143020629882812
      agent-5:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8868023157119751
        entropy_coeff: 0.0017600000137463212
        kl: 0.001589038991369307
        model: {}
        policy_loss: -0.0038353875279426575
        total_loss: -0.003749561496078968
        vf_explained_var: 0.08141447603702545
        vf_loss: 16.46600341796875
    load_time_ms: 13392.868
    num_steps_sampled: 21792000
    num_steps_trained: 21792000
    sample_time_ms: 99082.316
    update_time_ms: 16.306
  iterations_since_restore: 167
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.293854748603355
    ram_util_percent: 16.193854748603353
  pid: 30948
  policy_reward_max:
    agent-0: 170.49999999999991
    agent-1: 170.49999999999991
    agent-2: 170.49999999999991
    agent-3: 170.49999999999991
    agent-4: 170.49999999999991
    agent-5: 170.49999999999991
  policy_reward_mean:
    agent-0: 142.28666666666683
    agent-1: 142.28666666666683
    agent-2: 142.28666666666683
    agent-3: 142.28666666666683
    agent-4: 142.28666666666683
    agent-5: 142.28666666666683
  policy_reward_min:
    agent-0: 89.66666666666691
    agent-1: 89.66666666666691
    agent-2: 89.66666666666691
    agent-3: 89.66666666666691
    agent-4: 89.66666666666691
    agent-5: 89.66666666666691
  sampler_perf:
    mean_env_wait_ms: 27.104142900355647
    mean_inference_ms: 12.939827756543393
    mean_processing_ms: 58.03515692366785
  time_since_restore: 22413.89666080475
  time_this_iter_s: 125.62959480285645
  time_total_s: 31539.90847468376
  timestamp: 1637049517
  timesteps_since_restore: 16032000
  timesteps_this_iter: 96000
  timesteps_total: 21792000
  training_iteration: 227
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    227 |          31539.9 | 21792000 |   853.72 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 1.99
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 32.67
    apples_agent-1_min: 0
    apples_agent-2_max: 82
    apples_agent-2_mean: 6.79
    apples_agent-2_min: 0
    apples_agent-3_max: 123
    apples_agent-3_mean: 78.51
    apples_agent-3_min: 34
    apples_agent-4_max: 39
    apples_agent-4_mean: 2.29
    apples_agent-4_min: 0
    apples_agent-5_max: 203
    apples_agent-5_mean: 99.28
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 584
    cleaning_beam_agent-0_mean: 424.6
    cleaning_beam_agent-0_min: 300
    cleaning_beam_agent-1_max: 431
    cleaning_beam_agent-1_mean: 226.94
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 590
    cleaning_beam_agent-2_mean: 363.65
    cleaning_beam_agent-2_min: 132
    cleaning_beam_agent-3_max: 64
    cleaning_beam_agent-3_mean: 26.96
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 558
    cleaning_beam_agent-4_mean: 437.97
    cleaning_beam_agent-4_min: 285
    cleaning_beam_agent-5_max: 170
    cleaning_beam_agent-5_mean: 54.17
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-00-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1018.9999999999819
  episode_reward_mean: 857.2099999999839
  episode_reward_min: 507.00000000001234
  episodes_this_iter: 96
  episodes_total: 21888
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11432.605
    learner:
      agent-0:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9500717520713806
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014216832350939512
        model: {}
        policy_loss: -0.003313664346933365
        total_loss: -0.003258968237787485
        vf_explained_var: 0.07342833280563354
        vf_loss: 17.268217086791992
      agent-1:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1639404296875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017652945825830102
        model: {}
        policy_loss: -0.003881253767758608
        total_loss: -0.003919634036719799
        vf_explained_var: -0.04090172052383423
        vf_loss: 20.101516723632812
      agent-2:
        cur_kl_coeff: 2.9802322831784522e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1034785509109497
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015455264365300536
        model: {}
        policy_loss: -0.0038166926242411137
        total_loss: -0.0038836959283798933
        vf_explained_var: 0.016336306929588318
        vf_loss: 18.75119400024414
      agent-3:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5334416627883911
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010433030547574162
        model: {}
        policy_loss: -0.002731422893702984
        total_loss: -0.0020320452749729156
        vf_explained_var: 0.13190557062625885
        vf_loss: 16.38237953186035
      agent-4:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9663987755775452
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012204553931951523
        model: {}
        policy_loss: -0.003997861873358488
        total_loss: -0.0038415121380239725
        vf_explained_var: 0.01850469410419464
        vf_loss: 18.572147369384766
      agent-5:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 0.86833655834198
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015939808217808604
        model: {}
        policy_loss: -0.003775734454393387
        total_loss: -0.0035821213386952877
        vf_explained_var: 0.0834529846906662
        vf_loss: 17.218826293945312
    load_time_ms: 13414.484
    num_steps_sampled: 21888000
    num_steps_trained: 21888000
    sample_time_ms: 99039.983
    update_time_ms: 15.638
  iterations_since_restore: 168
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.474011299435027
    ram_util_percent: 16.20395480225989
  pid: 30948
  policy_reward_max:
    agent-0: 169.83333333333303
    agent-1: 169.83333333333303
    agent-2: 169.83333333333303
    agent-3: 169.83333333333303
    agent-4: 169.83333333333303
    agent-5: 169.83333333333303
  policy_reward_mean:
    agent-0: 142.86833333333345
    agent-1: 142.86833333333345
    agent-2: 142.86833333333345
    agent-3: 142.86833333333345
    agent-4: 142.86833333333345
    agent-5: 142.86833333333345
  policy_reward_min:
    agent-0: 84.50000000000013
    agent-1: 84.50000000000013
    agent-2: 84.50000000000013
    agent-3: 84.50000000000013
    agent-4: 84.50000000000013
    agent-5: 84.50000000000013
  sampler_perf:
    mean_env_wait_ms: 27.104807434898717
    mean_inference_ms: 12.938617906245048
    mean_processing_ms: 58.02561547343741
  time_since_restore: 22537.862906217575
  time_this_iter_s: 123.96624541282654
  time_total_s: 31663.874720096588
  timestamp: 1637049641
  timesteps_since_restore: 16128000
  timesteps_this_iter: 96000
  timesteps_total: 21888000
  training_iteration: 228
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    228 |          31663.9 | 21888000 |   857.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 1.87
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 30.67
    apples_agent-1_min: 0
    apples_agent-2_max: 91
    apples_agent-2_mean: 6.46
    apples_agent-2_min: 0
    apples_agent-3_max: 117
    apples_agent-3_mean: 73.82
    apples_agent-3_min: 31
    apples_agent-4_max: 121
    apples_agent-4_mean: 5.31
    apples_agent-4_min: 0
    apples_agent-5_max: 187
    apples_agent-5_mean: 93.93
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 560
    cleaning_beam_agent-0_mean: 434.32
    cleaning_beam_agent-0_min: 281
    cleaning_beam_agent-1_max: 385
    cleaning_beam_agent-1_mean: 223.26
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 621
    cleaning_beam_agent-2_mean: 372.2
    cleaning_beam_agent-2_min: 139
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 25.36
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 549
    cleaning_beam_agent-4_mean: 430.0
    cleaning_beam_agent-4_min: 285
    cleaning_beam_agent-5_max: 212
    cleaning_beam_agent-5_mean: 54.57
    cleaning_beam_agent-5_min: 17
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-02-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1079.9999999999825
  episode_reward_mean: 840.1799999999852
  episode_reward_min: 256.9999999999952
  episodes_this_iter: 96
  episodes_total: 21984
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11460.992
    learner:
      agent-0:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9517847895622253
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013097365153953433
        model: {}
        policy_loss: -0.00314509030431509
        total_loss: -0.0029083816334605217
        vf_explained_var: 0.06603236496448517
        vf_loss: 19.118505477905273
      agent-1:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.172134518623352
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015196918975561857
        model: {}
        policy_loss: -0.004032784141600132
        total_loss: -0.0039038611575961113
        vf_explained_var: -0.05036109685897827
        vf_loss: 21.918834686279297
      agent-2:
        cur_kl_coeff: 1.4901161415892261e-09
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0942692756652832
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014067584415897727
        model: {}
        policy_loss: -0.003525774460285902
        total_loss: -0.0034641579259186983
        vf_explained_var: 0.03376494348049164
        vf_loss: 19.875307083129883
      agent-3:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5422614812850952
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012490714434534311
        model: {}
        policy_loss: -0.0030410976614803076
        total_loss: -0.00225352356210351
        vf_explained_var: 0.15906523168087006
        vf_loss: 17.419551849365234
      agent-4:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9717555046081543
        entropy_coeff: 0.0017600000137463212
        kl: 0.002161978743970394
        model: {}
        policy_loss: -0.004317726939916611
        total_loss: -0.004091434646397829
        vf_explained_var: 0.05891050398349762
        vf_loss: 19.365821838378906
      agent-5:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8751685619354248
        entropy_coeff: 0.0017600000137463212
        kl: 0.001328017795458436
        model: {}
        policy_loss: -0.0036768317222595215
        total_loss: -0.0033900158014148474
        vf_explained_var: 0.11070261895656586
        vf_loss: 18.27117156982422
    load_time_ms: 13405.564
    num_steps_sampled: 21984000
    num_steps_trained: 21984000
    sample_time_ms: 99132.151
    update_time_ms: 15.567
  iterations_since_restore: 169
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.347752808988762
    ram_util_percent: 16.198876404494385
  pid: 30948
  policy_reward_max:
    agent-0: 179.99999999999986
    agent-1: 179.99999999999986
    agent-2: 179.99999999999986
    agent-3: 179.99999999999986
    agent-4: 179.99999999999986
    agent-5: 179.99999999999986
  policy_reward_mean:
    agent-0: 140.03000000000014
    agent-1: 140.03000000000014
    agent-2: 140.03000000000014
    agent-3: 140.03000000000014
    agent-4: 140.03000000000014
    agent-5: 140.03000000000014
  policy_reward_min:
    agent-0: 42.833333333333236
    agent-1: 42.833333333333236
    agent-2: 42.833333333333236
    agent-3: 42.833333333333236
    agent-4: 42.833333333333236
    agent-5: 42.833333333333236
  sampler_perf:
    mean_env_wait_ms: 27.106369898575075
    mean_inference_ms: 12.937381804332817
    mean_processing_ms: 58.0167442052481
  time_since_restore: 22663.312647342682
  time_this_iter_s: 125.44974112510681
  time_total_s: 31789.324461221695
  timestamp: 1637049766
  timesteps_since_restore: 16224000
  timesteps_this_iter: 96000
  timesteps_total: 21984000
  training_iteration: 229
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    229 |          31789.3 | 21984000 |   840.18 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 2.09
    apples_agent-0_min: 0
    apples_agent-1_max: 115
    apples_agent-1_mean: 35.28
    apples_agent-1_min: 0
    apples_agent-2_max: 96
    apples_agent-2_mean: 6.66
    apples_agent-2_min: 0
    apples_agent-3_max: 138
    apples_agent-3_mean: 74.72
    apples_agent-3_min: 43
    apples_agent-4_max: 52
    apples_agent-4_mean: 2.33
    apples_agent-4_min: 0
    apples_agent-5_max: 187
    apples_agent-5_mean: 97.72
    apples_agent-5_min: 53
    cleaning_beam_agent-0_max: 516
    cleaning_beam_agent-0_mean: 427.74
    cleaning_beam_agent-0_min: 325
    cleaning_beam_agent-1_max: 384
    cleaning_beam_agent-1_mean: 217.93
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 673
    cleaning_beam_agent-2_mean: 370.07
    cleaning_beam_agent-2_min: 124
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 23.79
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 536
    cleaning_beam_agent-4_mean: 448.73
    cleaning_beam_agent-4_min: 326
    cleaning_beam_agent-5_max: 183
    cleaning_beam_agent-5_mean: 53.65
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-04-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1054.9999999999818
  episode_reward_mean: 866.3499999999825
  episode_reward_min: 464.00000000001523
  episodes_this_iter: 96
  episodes_total: 22080
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11559.722
    learner:
      agent-0:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9664865732192993
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017868211725726724
        model: {}
        policy_loss: -0.0031987898983061314
        total_loss: -0.003182465210556984
        vf_explained_var: 0.09294290840625763
        vf_loss: 17.173397064208984
      agent-1:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1533396244049072
        entropy_coeff: 0.0017600000137463212
        kl: 0.001493049319833517
        model: {}
        policy_loss: -0.003974658437073231
        total_loss: -0.003967564553022385
        vf_explained_var: -0.026547402143478394
        vf_loss: 20.369735717773438
      agent-2:
        cur_kl_coeff: 7.450580707946131e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0855716466903687
        entropy_coeff: 0.0017600000137463212
        kl: 0.001432269113138318
        model: {}
        policy_loss: -0.0035233618691563606
        total_loss: -0.003566574305295944
        vf_explained_var: 0.03345498442649841
        vf_loss: 18.67394256591797
      agent-3:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5128800868988037
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011818894417956471
        model: {}
        policy_loss: -0.002657728735357523
        total_loss: -0.0018626933451741934
        vf_explained_var: 0.11274303495883942
        vf_loss: 16.977062225341797
      agent-4:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9615601301193237
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013740307185798883
        model: {}
        policy_loss: -0.003728149924427271
        total_loss: -0.0034973425790667534
        vf_explained_var: 0.00045242905616760254
        vf_loss: 19.231538772583008
      agent-5:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8804602026939392
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021045824978500605
        model: {}
        policy_loss: -0.004212259780615568
        total_loss: -0.004026866052299738
        vf_explained_var: 0.0876137763261795
        vf_loss: 17.350032806396484
    load_time_ms: 13420.705
    num_steps_sampled: 22080000
    num_steps_trained: 22080000
    sample_time_ms: 99147.522
    update_time_ms: 16.233
  iterations_since_restore: 170
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.429775280898877
    ram_util_percent: 16.205617977528092
  pid: 30948
  policy_reward_max:
    agent-0: 175.83333333333317
    agent-1: 175.83333333333317
    agent-2: 175.83333333333317
    agent-3: 175.83333333333317
    agent-4: 175.83333333333317
    agent-5: 175.83333333333317
  policy_reward_mean:
    agent-0: 144.39166666666682
    agent-1: 144.39166666666682
    agent-2: 144.39166666666682
    agent-3: 144.39166666666682
    agent-4: 144.39166666666682
    agent-5: 144.39166666666682
  policy_reward_min:
    agent-0: 77.33333333333336
    agent-1: 77.33333333333336
    agent-2: 77.33333333333336
    agent-3: 77.33333333333336
    agent-4: 77.33333333333336
    agent-5: 77.33333333333336
  sampler_perf:
    mean_env_wait_ms: 27.106543965790802
    mean_inference_ms: 12.935981505588051
    mean_processing_ms: 58.00901144522093
  time_since_restore: 22788.34053683281
  time_this_iter_s: 125.02788949012756
  time_total_s: 31914.352350711823
  timestamp: 1637049891
  timesteps_since_restore: 16320000
  timesteps_this_iter: 96000
  timesteps_total: 22080000
  training_iteration: 230
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    230 |          31914.4 | 22080000 |   866.35 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 3.0
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 34.76
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 5.7
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 78.28
    apples_agent-3_min: 32
    apples_agent-4_max: 56
    apples_agent-4_mean: 3.32
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 97.99
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 573
    cleaning_beam_agent-0_mean: 438.17
    cleaning_beam_agent-0_min: 312
    cleaning_beam_agent-1_max: 433
    cleaning_beam_agent-1_mean: 218.18
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 673
    cleaning_beam_agent-2_mean: 388.3
    cleaning_beam_agent-2_min: 126
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 23.99
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 549
    cleaning_beam_agent-4_mean: 447.39
    cleaning_beam_agent-4_min: 298
    cleaning_beam_agent-5_max: 225
    cleaning_beam_agent-5_mean: 51.44
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-06-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1009.9999999999795
  episode_reward_mean: 868.8099999999822
  episode_reward_min: 438.0000000000083
  episodes_this_iter: 96
  episodes_total: 22176
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11626.241
    learner:
      agent-0:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9318417906761169
        entropy_coeff: 0.0017600000137463212
        kl: 0.0027899513952434063
        model: {}
        policy_loss: -0.0035483636893332005
        total_loss: -0.0032799614127725363
        vf_explained_var: 0.04830385744571686
        vf_loss: 19.084430694580078
      agent-1:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1581389904022217
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011907087173312902
        model: {}
        policy_loss: -0.003844824153929949
        total_loss: -0.0036814429331570864
        vf_explained_var: -0.052757829427719116
        vf_loss: 22.017045974731445
      agent-2:
        cur_kl_coeff: 3.7252903539730653e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1036334037780762
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012393550714477897
        model: {}
        policy_loss: -0.003373466432094574
        total_loss: -0.003288975451141596
        vf_explained_var: 0.002725765109062195
        vf_loss: 20.268857955932617
      agent-3:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5302130579948425
        entropy_coeff: 0.0017600000137463212
        kl: 0.000884955283254385
        model: {}
        policy_loss: -0.002735626883804798
        total_loss: -0.0019222116097807884
        vf_explained_var: 0.13618749380111694
        vf_loss: 17.465900421142578
      agent-4:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9570977687835693
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015042931772768497
        model: {}
        policy_loss: -0.004024503752589226
        total_loss: -0.0037463302724063396
        vf_explained_var: 0.03519250452518463
        vf_loss: 19.626649856567383
      agent-5:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8754233121871948
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013932361034676433
        model: {}
        policy_loss: -0.0035925554111599922
        total_loss: -0.003392629325389862
        vf_explained_var: 0.1356046348810196
        vf_loss: 17.40672492980957
    load_time_ms: 13430.713
    num_steps_sampled: 22176000
    num_steps_trained: 22176000
    sample_time_ms: 99233.684
    update_time_ms: 16.205
  iterations_since_restore: 171
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.41257142857142
    ram_util_percent: 16.210857142857147
  pid: 30948
  policy_reward_max:
    agent-0: 168.33333333333363
    agent-1: 168.33333333333363
    agent-2: 168.33333333333363
    agent-3: 168.33333333333363
    agent-4: 168.33333333333363
    agent-5: 168.33333333333363
  policy_reward_mean:
    agent-0: 144.8016666666668
    agent-1: 144.8016666666668
    agent-2: 144.8016666666668
    agent-3: 144.8016666666668
    agent-4: 144.8016666666668
    agent-5: 144.8016666666668
  policy_reward_min:
    agent-0: 72.99999999999982
    agent-1: 72.99999999999982
    agent-2: 72.99999999999982
    agent-3: 72.99999999999982
    agent-4: 72.99999999999982
    agent-5: 72.99999999999982
  sampler_perf:
    mean_env_wait_ms: 27.107474638220825
    mean_inference_ms: 12.935027991343205
    mean_processing_ms: 57.99917566364865
  time_since_restore: 22913.450140476227
  time_this_iter_s: 125.10960364341736
  time_total_s: 32039.46195435524
  timestamp: 1637050017
  timesteps_since_restore: 16416000
  timesteps_this_iter: 96000
  timesteps_total: 22176000
  training_iteration: 231
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    231 |          32039.5 | 22176000 |   868.81 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 1.31
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 34.45
    apples_agent-1_min: 0
    apples_agent-2_max: 93
    apples_agent-2_mean: 4.81
    apples_agent-2_min: 0
    apples_agent-3_max: 130
    apples_agent-3_mean: 77.23
    apples_agent-3_min: 33
    apples_agent-4_max: 102
    apples_agent-4_mean: 2.76
    apples_agent-4_min: 0
    apples_agent-5_max: 158
    apples_agent-5_mean: 97.15
    apples_agent-5_min: 28
    cleaning_beam_agent-0_max: 569
    cleaning_beam_agent-0_mean: 454.77
    cleaning_beam_agent-0_min: 266
    cleaning_beam_agent-1_max: 401
    cleaning_beam_agent-1_mean: 210.86
    cleaning_beam_agent-1_min: 72
    cleaning_beam_agent-2_max: 600
    cleaning_beam_agent-2_mean: 376.95
    cleaning_beam_agent-2_min: 195
    cleaning_beam_agent-3_max: 45
    cleaning_beam_agent-3_mean: 23.73
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 552
    cleaning_beam_agent-4_mean: 450.42
    cleaning_beam_agent-4_min: 218
    cleaning_beam_agent-5_max: 138
    cleaning_beam_agent-5_mean: 49.21
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-09-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1077.9999999999925
  episode_reward_mean: 880.6799999999826
  episode_reward_min: 302.9999999999995
  episodes_this_iter: 96
  episodes_total: 22272
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11616.796
    learner:
      agent-0:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9536005258560181
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016328308265656233
        model: {}
        policy_loss: -0.0034045586362481117
        total_loss: -0.003391748759895563
        vf_explained_var: 0.0850994884967804
        vf_loss: 16.911500930786133
      agent-1:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1482726335525513
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016223150305449963
        model: {}
        policy_loss: -0.004032974597066641
        total_loss: -0.00395056139677763
        vf_explained_var: -0.06900414824485779
        vf_loss: 21.033733367919922
      agent-2:
        cur_kl_coeff: 1.8626451769865326e-10
        cur_lr: 1.2000000424450263e-05
        entropy: 1.094766616821289
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018901224248111248
        model: {}
        policy_loss: -0.0037743065040558577
        total_loss: -0.003798989811912179
        vf_explained_var: -0.0003716796636581421
        vf_loss: 19.02105712890625
      agent-3:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.503200352191925
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014189996290951967
        model: {}
        policy_loss: -0.0026120764669030905
        total_loss: -0.0017818931955844164
        vf_explained_var: 0.08285360038280487
        vf_loss: 17.158174514770508
      agent-4:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9640957713127136
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025134007446467876
        model: {}
        policy_loss: -0.00420721247792244
        total_loss: -0.003973530605435371
        vf_explained_var: -0.021004587411880493
        vf_loss: 19.304868698120117
      agent-5:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8700973987579346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016404647612944245
        model: {}
        policy_loss: -0.003791521303355694
        total_loss: -0.003654064843431115
        vf_explained_var: 0.10458244383335114
        vf_loss: 16.688243865966797
    load_time_ms: 13427.092
    num_steps_sampled: 22272000
    num_steps_trained: 22272000
    sample_time_ms: 99241.87
    update_time_ms: 16.158
  iterations_since_restore: 172
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.335260115606935
    ram_util_percent: 16.127167630057805
  pid: 30948
  policy_reward_max:
    agent-0: 179.66666666666606
    agent-1: 179.66666666666606
    agent-2: 179.66666666666606
    agent-3: 179.66666666666606
    agent-4: 179.66666666666606
    agent-5: 179.66666666666606
  policy_reward_mean:
    agent-0: 146.7800000000001
    agent-1: 146.7800000000001
    agent-2: 146.7800000000001
    agent-3: 146.7800000000001
    agent-4: 146.7800000000001
    agent-5: 146.7800000000001
  policy_reward_min:
    agent-0: 50.49999999999984
    agent-1: 50.49999999999984
    agent-2: 50.49999999999984
    agent-3: 50.49999999999984
    agent-4: 50.49999999999984
    agent-5: 50.49999999999984
  sampler_perf:
    mean_env_wait_ms: 27.108863332536274
    mean_inference_ms: 12.933754184149354
    mean_processing_ms: 57.989093488573445
  time_since_restore: 23038.006034612656
  time_this_iter_s: 124.55589413642883
  time_total_s: 32164.01784849167
  timestamp: 1637050141
  timesteps_since_restore: 16512000
  timesteps_this_iter: 96000
  timesteps_total: 22272000
  training_iteration: 232
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    232 |            32164 | 22272000 |   880.68 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 0.73
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 35.32
    apples_agent-1_min: 0
    apples_agent-2_max: 166
    apples_agent-2_mean: 10.36
    apples_agent-2_min: 0
    apples_agent-3_max: 143
    apples_agent-3_mean: 77.9
    apples_agent-3_min: 47
    apples_agent-4_max: 63
    apples_agent-4_mean: 3.33
    apples_agent-4_min: 0
    apples_agent-5_max: 196
    apples_agent-5_mean: 96.52
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 588
    cleaning_beam_agent-0_mean: 469.82
    cleaning_beam_agent-0_min: 365
    cleaning_beam_agent-1_max: 375
    cleaning_beam_agent-1_mean: 215.48
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 592
    cleaning_beam_agent-2_mean: 362.48
    cleaning_beam_agent-2_min: 165
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 22.69
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 533
    cleaning_beam_agent-4_mean: 435.43
    cleaning_beam_agent-4_min: 334
    cleaning_beam_agent-5_max: 163
    cleaning_beam_agent-5_mean: 46.78
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-11-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1065.999999999985
  episode_reward_mean: 872.3299999999829
  episode_reward_min: 467.0000000000091
  episodes_this_iter: 96
  episodes_total: 22368
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11615.604
    learner:
      agent-0:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9188035726547241
        entropy_coeff: 0.0017600000137463212
        kl: 0.001780267572030425
        model: {}
        policy_loss: -0.0033949408680200577
        total_loss: -0.003244960680603981
        vf_explained_var: 0.05354572832584381
        vf_loss: 17.670751571655273
      agent-1:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1720107793807983
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017702232580631971
        model: {}
        policy_loss: -0.0040713767521083355
        total_loss: -0.004043153487145901
        vf_explained_var: -0.06572631001472473
        vf_loss: 20.909616470336914
      agent-2:
        cur_kl_coeff: 9.313225884932663e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.100494146347046
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016877851448953152
        model: {}
        policy_loss: -0.003806863911449909
        total_loss: -0.003788185305893421
        vf_explained_var: -0.020626723766326904
        vf_loss: 19.555484771728516
      agent-3:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5066202878952026
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014296200824901462
        model: {}
        policy_loss: -0.0025474941357970238
        total_loss: -0.0017518894746899605
        vf_explained_var: 0.10968068242073059
        vf_loss: 16.872587203979492
      agent-4:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9726733565330505
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016956585459411144
        model: {}
        policy_loss: -0.004029085859656334
        total_loss: -0.003823294769972563
        vf_explained_var: 0.003267452120780945
        vf_loss: 19.17696762084961
      agent-5:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8686187863349915
        entropy_coeff: 0.0017600000137463212
        kl: 0.000940905709285289
        model: {}
        policy_loss: -0.00373122189193964
        total_loss: -0.0035456507466733456
        vf_explained_var: 0.09011103212833405
        vf_loss: 17.143421173095703
    load_time_ms: 13409.555
    num_steps_sampled: 22368000
    num_steps_trained: 22368000
    sample_time_ms: 99266.302
    update_time_ms: 15.945
  iterations_since_restore: 173
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.625568181818185
    ram_util_percent: 16.20852272727273
  pid: 30948
  policy_reward_max:
    agent-0: 177.66666666666634
    agent-1: 177.66666666666634
    agent-2: 177.66666666666634
    agent-3: 177.66666666666634
    agent-4: 177.66666666666634
    agent-5: 177.66666666666634
  policy_reward_mean:
    agent-0: 145.3883333333334
    agent-1: 145.3883333333334
    agent-2: 145.3883333333334
    agent-3: 145.3883333333334
    agent-4: 145.3883333333334
    agent-5: 145.3883333333334
  policy_reward_min:
    agent-0: 77.83333333333331
    agent-1: 77.83333333333331
    agent-2: 77.83333333333331
    agent-3: 77.83333333333331
    agent-4: 77.83333333333331
    agent-5: 77.83333333333331
  sampler_perf:
    mean_env_wait_ms: 27.10943417114689
    mean_inference_ms: 12.93265060753887
    mean_processing_ms: 57.97927010547352
  time_since_restore: 23161.45841050148
  time_this_iter_s: 123.45237588882446
  time_total_s: 32287.470224380493
  timestamp: 1637050265
  timesteps_since_restore: 16608000
  timesteps_this_iter: 96000
  timesteps_total: 22368000
  training_iteration: 233
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    233 |          32287.5 | 22368000 |   872.33 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 2.53
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 30.15
    apples_agent-1_min: 0
    apples_agent-2_max: 92
    apples_agent-2_mean: 6.32
    apples_agent-2_min: 0
    apples_agent-3_max: 148
    apples_agent-3_mean: 75.74
    apples_agent-3_min: 42
    apples_agent-4_max: 53
    apples_agent-4_mean: 1.25
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 99.02
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 570
    cleaning_beam_agent-0_mean: 473.02
    cleaning_beam_agent-0_min: 266
    cleaning_beam_agent-1_max: 425
    cleaning_beam_agent-1_mean: 221.6
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 630
    cleaning_beam_agent-2_mean: 382.85
    cleaning_beam_agent-2_min: 174
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 23.01
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 518
    cleaning_beam_agent-4_mean: 426.69
    cleaning_beam_agent-4_min: 313
    cleaning_beam_agent-5_max: 161
    cleaning_beam_agent-5_mean: 44.56
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 33
    fire_beam_agent-4_mean: 0.33
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-13-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1065.999999999985
  episode_reward_mean: 882.0699999999829
  episode_reward_min: 472.00000000001063
  episodes_this_iter: 96
  episodes_total: 22464
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11618.256
    learner:
      agent-0:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9269391894340515
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013552273157984018
        model: {}
        policy_loss: -0.0033515854738652706
        total_loss: -0.003247310174629092
        vf_explained_var: 0.025452613830566406
        vf_loss: 17.356887817382812
      agent-1:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1619633436203003
        entropy_coeff: 0.0017600000137463212
        kl: 0.002139539923518896
        model: {}
        policy_loss: -0.004356870427727699
        total_loss: -0.00436452915892005
        vf_explained_var: -0.08219313621520996
        vf_loss: 20.37395477294922
      agent-2:
        cur_kl_coeff: 4.6566129424663316e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.105322241783142
        entropy_coeff: 0.0017600000137463212
        kl: 0.00108359067235142
        model: {}
        policy_loss: -0.003728895913809538
        total_loss: -0.0038489610888063908
        vf_explained_var: -0.001133456826210022
        vf_loss: 18.252975463867188
      agent-3:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4956434965133667
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012261451920494437
        model: {}
        policy_loss: -0.0028310096822679043
        total_loss: -0.002065034117549658
        vf_explained_var: 0.09719178080558777
        vf_loss: 16.38307762145996
      agent-4:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9799809455871582
        entropy_coeff: 0.0017600000137463212
        kl: 0.002003324683755636
        model: {}
        policy_loss: -0.004476373549550772
        total_loss: -0.004320638719946146
        vf_explained_var: -0.01554514467716217
        vf_loss: 18.805015563964844
      agent-5:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8614620566368103
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020638511050492525
        model: {}
        policy_loss: -0.003902786411345005
        total_loss: -0.003768010064959526
        vf_explained_var: 0.08042848110198975
        vf_loss: 16.509517669677734
    load_time_ms: 13412.078
    num_steps_sampled: 22464000
    num_steps_trained: 22464000
    sample_time_ms: 99210.051
    update_time_ms: 15.317
  iterations_since_restore: 174
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.453107344632766
    ram_util_percent: 16.12768361581921
  pid: 30948
  policy_reward_max:
    agent-0: 177.66666666666634
    agent-1: 177.66666666666634
    agent-2: 177.66666666666634
    agent-3: 177.66666666666634
    agent-4: 177.66666666666634
    agent-5: 177.66666666666634
  policy_reward_mean:
    agent-0: 147.01166666666668
    agent-1: 147.01166666666668
    agent-2: 147.01166666666668
    agent-3: 147.01166666666668
    agent-4: 147.01166666666668
    agent-5: 147.01166666666668
  policy_reward_min:
    agent-0: 78.66666666666642
    agent-1: 78.66666666666642
    agent-2: 78.66666666666642
    agent-3: 78.66666666666642
    agent-4: 78.66666666666642
    agent-5: 78.66666666666642
  sampler_perf:
    mean_env_wait_ms: 27.110830580844745
    mean_inference_ms: 12.931611463766256
    mean_processing_ms: 57.97187448518158
  time_since_restore: 23285.71017432213
  time_this_iter_s: 124.2517638206482
  time_total_s: 32411.72198820114
  timestamp: 1637050389
  timesteps_since_restore: 16704000
  timesteps_this_iter: 96000
  timesteps_total: 22464000
  training_iteration: 234
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    234 |          32411.7 | 22464000 |   882.07 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 85
    apples_agent-0_mean: 2.18
    apples_agent-0_min: 0
    apples_agent-1_max: 136
    apples_agent-1_mean: 34.87
    apples_agent-1_min: 0
    apples_agent-2_max: 66
    apples_agent-2_mean: 6.59
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 72.12
    apples_agent-3_min: 38
    apples_agent-4_max: 38
    apples_agent-4_mean: 2.12
    apples_agent-4_min: 0
    apples_agent-5_max: 176
    apples_agent-5_mean: 97.4
    apples_agent-5_min: 12
    cleaning_beam_agent-0_max: 571
    cleaning_beam_agent-0_mean: 454.87
    cleaning_beam_agent-0_min: 305
    cleaning_beam_agent-1_max: 427
    cleaning_beam_agent-1_mean: 224.09
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 581
    cleaning_beam_agent-2_mean: 381.4
    cleaning_beam_agent-2_min: 200
    cleaning_beam_agent-3_max: 63
    cleaning_beam_agent-3_mean: 23.21
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 524
    cleaning_beam_agent-4_mean: 425.07
    cleaning_beam_agent-4_min: 336
    cleaning_beam_agent-5_max: 234
    cleaning_beam_agent-5_mean: 46.0
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-15-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1067.9999999999916
  episode_reward_mean: 876.9999999999825
  episode_reward_min: 576.9999999999984
  episodes_this_iter: 96
  episodes_total: 22560
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11598.018
    learner:
      agent-0:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9494611024856567
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018263838719576597
        model: {}
        policy_loss: -0.003291826695203781
        total_loss: -0.0033728834241628647
        vf_explained_var: 0.06822612881660461
        vf_loss: 15.899953842163086
      agent-1:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1532617807388306
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012434938689693809
        model: {}
        policy_loss: -0.003949687350541353
        total_loss: -0.004050934687256813
        vf_explained_var: -0.07948046922683716
        vf_loss: 19.28495979309082
      agent-2:
        cur_kl_coeff: 2.3283064712331658e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0800931453704834
        entropy_coeff: 0.0017600000137463212
        kl: 0.001787435612641275
        model: {}
        policy_loss: -0.003994018770754337
        total_loss: -0.0042066145688295364
        vf_explained_var: 0.027718886733055115
        vf_loss: 16.883705139160156
      agent-3:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49689027667045593
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008840608643367887
        model: {}
        policy_loss: -0.002299288520589471
        total_loss: -0.0015889140777289867
        vf_explained_var: 0.07906398177146912
        vf_loss: 15.848999977111816
      agent-4:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9597850441932678
        entropy_coeff: 0.0017600000137463212
        kl: 0.002344425767660141
        model: {}
        policy_loss: -0.004140510223805904
        total_loss: -0.004085496999323368
        vf_explained_var: 0.006293490529060364
        vf_loss: 17.442371368408203
      agent-5:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 0.871414303779602
        entropy_coeff: 0.0017600000137463212
        kl: 0.002407069317996502
        model: {}
        policy_loss: -0.0038269530050456524
        total_loss: -0.003761672880500555
        vf_explained_var: 0.06976592540740967
        vf_loss: 15.98970890045166
    load_time_ms: 13429.197
    num_steps_sampled: 22560000
    num_steps_trained: 22560000
    sample_time_ms: 99539.415
    update_time_ms: 15.434
  iterations_since_restore: 175
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.575842696629213
    ram_util_percent: 16.202247191011235
  pid: 30948
  policy_reward_max:
    agent-0: 177.99999999999983
    agent-1: 177.99999999999983
    agent-2: 177.99999999999983
    agent-3: 177.99999999999983
    agent-4: 177.99999999999983
    agent-5: 177.99999999999983
  policy_reward_mean:
    agent-0: 146.16666666666674
    agent-1: 146.16666666666674
    agent-2: 146.16666666666674
    agent-3: 146.16666666666674
    agent-4: 146.16666666666674
    agent-5: 146.16666666666674
  policy_reward_min:
    agent-0: 96.16666666666707
    agent-1: 96.16666666666707
    agent-2: 96.16666666666707
    agent-3: 96.16666666666707
    agent-4: 96.16666666666707
    agent-5: 96.16666666666707
  sampler_perf:
    mean_env_wait_ms: 27.112478467114585
    mean_inference_ms: 12.93049524512454
    mean_processing_ms: 57.96367350181503
  time_since_restore: 23411.248569250107
  time_this_iter_s: 125.53839492797852
  time_total_s: 32537.26038312912
  timestamp: 1637050515
  timesteps_since_restore: 16800000
  timesteps_this_iter: 96000
  timesteps_total: 22560000
  training_iteration: 235
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    235 |          32537.3 | 22560000 |      877 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 14
    apples_agent-0_mean: 0.61
    apples_agent-0_min: 0
    apples_agent-1_max: 147
    apples_agent-1_mean: 37.75
    apples_agent-1_min: 0
    apples_agent-2_max: 118
    apples_agent-2_mean: 6.97
    apples_agent-2_min: 0
    apples_agent-3_max: 122
    apples_agent-3_mean: 76.01
    apples_agent-3_min: 41
    apples_agent-4_max: 38
    apples_agent-4_mean: 1.68
    apples_agent-4_min: 0
    apples_agent-5_max: 188
    apples_agent-5_mean: 101.64
    apples_agent-5_min: 55
    cleaning_beam_agent-0_max: 543
    cleaning_beam_agent-0_mean: 458.11
    cleaning_beam_agent-0_min: 352
    cleaning_beam_agent-1_max: 352
    cleaning_beam_agent-1_mean: 214.56
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 571
    cleaning_beam_agent-2_mean: 399.27
    cleaning_beam_agent-2_min: 235
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 20.12
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 544
    cleaning_beam_agent-4_mean: 450.67
    cleaning_beam_agent-4_min: 348
    cleaning_beam_agent-5_max: 178
    cleaning_beam_agent-5_mean: 44.72
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-17-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1085.9999999999961
  episode_reward_mean: 915.679999999982
  episode_reward_min: 670.9999999999885
  episodes_this_iter: 96
  episodes_total: 22656
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11575.931
    learner:
      agent-0:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.944808840751648
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018547771032899618
        model: {}
        policy_loss: -0.0031615858897566795
        total_loss: -0.0031661142129451036
        vf_explained_var: 0.014178499579429626
        vf_loss: 16.583322525024414
      agent-1:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.139054775238037
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013308956986293197
        model: {}
        policy_loss: -0.003974032122641802
        total_loss: -0.003966251388192177
        vf_explained_var: -0.07638505101203918
        vf_loss: 20.125165939331055
      agent-2:
        cur_kl_coeff: 1.1641532356165829e-11
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0876723527908325
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019239605171605945
        model: {}
        policy_loss: -0.004009919706732035
        total_loss: -0.004077318590134382
        vf_explained_var: -0.03466823697090149
        vf_loss: 18.46907615661621
      agent-3:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47536811232566833
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011690505780279636
        model: {}
        policy_loss: -0.0024698127526789904
        total_loss: -0.00161994737572968
        vf_explained_var: 0.028034329414367676
        vf_loss: 16.86514663696289
      agent-4:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9483106136322021
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012678051134571433
        model: {}
        policy_loss: -0.004037407226860523
        total_loss: -0.00385616859421134
        vf_explained_var: -0.037308961153030396
        vf_loss: 18.502634048461914
      agent-5:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8593313694000244
        entropy_coeff: 0.0017600000137463212
        kl: 0.001296907663345337
        model: {}
        policy_loss: -0.003371667582541704
        total_loss: -0.003228327725082636
        vf_explained_var: 0.044217631220817566
        vf_loss: 16.557649612426758
    load_time_ms: 13414.164
    num_steps_sampled: 22656000
    num_steps_trained: 22656000
    sample_time_ms: 99650.253
    update_time_ms: 15.664
  iterations_since_restore: 176
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.514689265536724
    ram_util_percent: 16.18983050847458
  pid: 30948
  policy_reward_max:
    agent-0: 180.99999999999957
    agent-1: 180.99999999999957
    agent-2: 180.99999999999957
    agent-3: 180.99999999999957
    agent-4: 180.99999999999957
    agent-5: 180.99999999999957
  policy_reward_mean:
    agent-0: 152.6133333333334
    agent-1: 152.6133333333334
    agent-2: 152.6133333333334
    agent-3: 152.6133333333334
    agent-4: 152.6133333333334
    agent-5: 152.6133333333334
  policy_reward_min:
    agent-0: 111.83333333333397
    agent-1: 111.83333333333397
    agent-2: 111.83333333333397
    agent-3: 111.83333333333397
    agent-4: 111.83333333333397
    agent-5: 111.83333333333397
  sampler_perf:
    mean_env_wait_ms: 27.114730029087397
    mean_inference_ms: 12.929529238692275
    mean_processing_ms: 57.95606689760759
  time_since_restore: 23535.617634534836
  time_this_iter_s: 124.369065284729
  time_total_s: 32661.62944841385
  timestamp: 1637050639
  timesteps_since_restore: 16896000
  timesteps_this_iter: 96000
  timesteps_total: 22656000
  training_iteration: 236
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    236 |          32661.6 | 22656000 |   915.68 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 1.51
    apples_agent-0_min: 0
    apples_agent-1_max: 116
    apples_agent-1_mean: 30.47
    apples_agent-1_min: 0
    apples_agent-2_max: 89
    apples_agent-2_mean: 8.8
    apples_agent-2_min: 0
    apples_agent-3_max: 123
    apples_agent-3_mean: 72.24
    apples_agent-3_min: 34
    apples_agent-4_max: 35
    apples_agent-4_mean: 1.08
    apples_agent-4_min: 0
    apples_agent-5_max: 164
    apples_agent-5_mean: 92.07
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 553
    cleaning_beam_agent-0_mean: 436.01
    cleaning_beam_agent-0_min: 292
    cleaning_beam_agent-1_max: 462
    cleaning_beam_agent-1_mean: 219.28
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 599
    cleaning_beam_agent-2_mean: 377.63
    cleaning_beam_agent-2_min: 196
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 24.37
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 513
    cleaning_beam_agent-4_mean: 444.69
    cleaning_beam_agent-4_min: 348
    cleaning_beam_agent-5_max: 163
    cleaning_beam_agent-5_mean: 47.8
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-19-24
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1019.9999999999907
  episode_reward_mean: 873.6299999999828
  episode_reward_min: 397.999999999998
  episodes_this_iter: 96
  episodes_total: 22752
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11572.228
    learner:
      agent-0:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9443073272705078
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009693753672763705
        model: {}
        policy_loss: -0.002978370524942875
        total_loss: -0.0028707366436719894
        vf_explained_var: 0.07188069820404053
        vf_loss: 17.696125030517578
      agent-1:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1349186897277832
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009864397579804063
        model: {}
        policy_loss: -0.0035695871338248253
        total_loss: -0.00346687575802207
        vf_explained_var: -0.056322187185287476
        vf_loss: 21.001712799072266
      agent-2:
        cur_kl_coeff: 5.8207661780829145e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0942665338516235
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014579916605725884
        model: {}
        policy_loss: -0.0036077802069485188
        total_loss: -0.0036287910770624876
        vf_explained_var: 0.019379600882530212
        vf_loss: 19.048978805541992
      agent-3:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49657970666885376
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017140543786808848
        model: {}
        policy_loss: -0.0027833087369799614
        total_loss: -0.0019526733085513115
        vf_explained_var: 0.11376562714576721
        vf_loss: 17.046167373657227
      agent-4:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9547114968299866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015923911705613136
        model: {}
        policy_loss: -0.004106946289539337
        total_loss: -0.0038536451756954193
        vf_explained_var: -0.0023822039365768433
        vf_loss: 19.33592987060547
      agent-5:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8719101548194885
        entropy_coeff: 0.0017600000137463212
        kl: 0.001083934330381453
        model: {}
        policy_loss: -0.0034810614306479692
        total_loss: -0.0032963058911263943
        vf_explained_var: 0.10164064168930054
        vf_loss: 17.193172454833984
    load_time_ms: 13400.986
    num_steps_sampled: 22752000
    num_steps_trained: 22752000
    sample_time_ms: 99509.832
    update_time_ms: 15.51
  iterations_since_restore: 177
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.33276836158192
    ram_util_percent: 16.15084745762712
  pid: 30948
  policy_reward_max:
    agent-0: 169.9999999999997
    agent-1: 169.9999999999997
    agent-2: 169.9999999999997
    agent-3: 169.9999999999997
    agent-4: 169.9999999999997
    agent-5: 169.9999999999997
  policy_reward_mean:
    agent-0: 145.60500000000005
    agent-1: 145.60500000000005
    agent-2: 145.60500000000005
    agent-3: 145.60500000000005
    agent-4: 145.60500000000005
    agent-5: 145.60500000000005
  policy_reward_min:
    agent-0: 66.33333333333329
    agent-1: 66.33333333333329
    agent-2: 66.33333333333329
    agent-3: 66.33333333333329
    agent-4: 66.33333333333329
    agent-5: 66.33333333333329
  sampler_perf:
    mean_env_wait_ms: 27.114929415495645
    mean_inference_ms: 12.927907791266783
    mean_processing_ms: 57.94592931601794
  time_since_restore: 23659.675231218338
  time_this_iter_s: 124.0575966835022
  time_total_s: 32785.68704509735
  timestamp: 1637050764
  timesteps_since_restore: 16992000
  timesteps_this_iter: 96000
  timesteps_total: 22752000
  training_iteration: 237
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    237 |          32785.7 | 22752000 |   873.63 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 39
    apples_agent-0_mean: 1.47
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 35.64
    apples_agent-1_min: 0
    apples_agent-2_max: 97
    apples_agent-2_mean: 7.55
    apples_agent-2_min: 0
    apples_agent-3_max: 128
    apples_agent-3_mean: 72.39
    apples_agent-3_min: 26
    apples_agent-4_max: 34
    apples_agent-4_mean: 0.71
    apples_agent-4_min: 0
    apples_agent-5_max: 150
    apples_agent-5_mean: 94.85
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 560
    cleaning_beam_agent-0_mean: 444.68
    cleaning_beam_agent-0_min: 316
    cleaning_beam_agent-1_max: 354
    cleaning_beam_agent-1_mean: 216.4
    cleaning_beam_agent-1_min: 127
    cleaning_beam_agent-2_max: 587
    cleaning_beam_agent-2_mean: 369.34
    cleaning_beam_agent-2_min: 201
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 22.05
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 544
    cleaning_beam_agent-4_mean: 451.16
    cleaning_beam_agent-4_min: 370
    cleaning_beam_agent-5_max: 114
    cleaning_beam_agent-5_mean: 45.08
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-21-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1042.9999999999895
  episode_reward_mean: 907.6999999999821
  episode_reward_min: 582.000000000005
  episodes_this_iter: 96
  episodes_total: 22848
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11585.752
    learner:
      agent-0:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9408524036407471
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014053605264052749
        model: {}
        policy_loss: -0.003564275335520506
        total_loss: -0.003562876023352146
        vf_explained_var: 0.047888755798339844
        vf_loss: 16.57300567626953
      agent-1:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1419878005981445
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012374506331980228
        model: {}
        policy_loss: -0.0038004452362656593
        total_loss: -0.0037982845678925514
        vf_explained_var: -0.07548588514328003
        vf_loss: 20.1205997467041
      agent-2:
        cur_kl_coeff: 2.9103830890414573e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1037366390228271
        entropy_coeff: 0.0017600000137463212
        kl: 0.002128212247043848
        model: {}
        policy_loss: -0.004113148897886276
        total_loss: -0.004269407130777836
        vf_explained_var: 0.016156435012817383
        vf_loss: 17.86316680908203
      agent-3:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4748665392398834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009478658903390169
        model: {}
        policy_loss: -0.0025767155457288027
        total_loss: -0.0017387905390933156
        vf_explained_var: 0.05338817834854126
        vf_loss: 16.736888885498047
      agent-4:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9670290350914001
        entropy_coeff: 0.0017600000137463212
        kl: 0.001593882218003273
        model: {}
        policy_loss: -0.004048805683851242
        total_loss: -0.0038835196755826473
        vf_explained_var: -0.0304984450340271
        vf_loss: 18.67258071899414
      agent-5:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8648838400840759
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018788623856380582
        model: {}
        policy_loss: -0.0037200083024799824
        total_loss: -0.003573566908016801
        vf_explained_var: 0.051319390535354614
        vf_loss: 16.68634033203125
    load_time_ms: 13426.008
    num_steps_sampled: 22848000
    num_steps_trained: 22848000
    sample_time_ms: 99553.24
    update_time_ms: 15.437
  iterations_since_restore: 178
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.48870056497175
    ram_util_percent: 16.206779661016952
  pid: 30948
  policy_reward_max:
    agent-0: 173.83333333333366
    agent-1: 173.83333333333366
    agent-2: 173.83333333333366
    agent-3: 173.83333333333366
    agent-4: 173.83333333333366
    agent-5: 173.83333333333366
  policy_reward_mean:
    agent-0: 151.28333333333342
    agent-1: 151.28333333333342
    agent-2: 151.28333333333342
    agent-3: 151.28333333333342
    agent-4: 151.28333333333342
    agent-5: 151.28333333333342
  policy_reward_min:
    agent-0: 97.00000000000007
    agent-1: 97.00000000000007
    agent-2: 97.00000000000007
    agent-3: 97.00000000000007
    agent-4: 97.00000000000007
    agent-5: 97.00000000000007
  sampler_perf:
    mean_env_wait_ms: 27.115609924666067
    mean_inference_ms: 12.926576063695691
    mean_processing_ms: 57.93793672630858
  time_since_restore: 23784.45250749588
  time_this_iter_s: 124.77727627754211
  time_total_s: 32910.46432137489
  timestamp: 1637050888
  timesteps_since_restore: 17088000
  timesteps_this_iter: 96000
  timesteps_total: 22848000
  training_iteration: 238
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    238 |          32910.5 | 22848000 |    907.7 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 2.25
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 30.54
    apples_agent-1_min: 0
    apples_agent-2_max: 75
    apples_agent-2_mean: 7.71
    apples_agent-2_min: 0
    apples_agent-3_max: 113
    apples_agent-3_mean: 70.95
    apples_agent-3_min: 35
    apples_agent-4_max: 65
    apples_agent-4_mean: 2.88
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 93.67
    apples_agent-5_min: 19
    cleaning_beam_agent-0_max: 537
    cleaning_beam_agent-0_mean: 433.14
    cleaning_beam_agent-0_min: 294
    cleaning_beam_agent-1_max: 380
    cleaning_beam_agent-1_mean: 230.54
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 562
    cleaning_beam_agent-2_mean: 345.73
    cleaning_beam_agent-2_min: 183
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 23.01
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 591
    cleaning_beam_agent-4_mean: 444.39
    cleaning_beam_agent-4_min: 294
    cleaning_beam_agent-5_max: 173
    cleaning_beam_agent-5_mean: 47.98
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-23-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1046.9999999999943
  episode_reward_mean: 874.4699999999825
  episode_reward_min: 352.00000000000284
  episodes_this_iter: 96
  episodes_total: 22944
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11559.827
    learner:
      agent-0:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9306178092956543
        entropy_coeff: 0.0017600000137463212
        kl: 0.001554940827190876
        model: {}
        policy_loss: -0.0030998794827610254
        total_loss: -0.0028265106957405806
        vf_explained_var: 0.03219963610172272
        vf_loss: 19.112550735473633
      agent-1:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1343258619308472
        entropy_coeff: 0.0017600000137463212
        kl: 0.002122332341969013
        model: {}
        policy_loss: -0.004029699135571718
        total_loss: -0.0038181450217962265
        vf_explained_var: -0.08359691500663757
        vf_loss: 22.079675674438477
      agent-2:
        cur_kl_coeff: 1.4551915445207286e-12
        cur_lr: 1.2000000424450263e-05
        entropy: 1.10378098487854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019053253345191479
        model: {}
        policy_loss: -0.004005540162324905
        total_loss: -0.003901604562997818
        vf_explained_var: -0.01766890287399292
        vf_loss: 20.46586036682129
      agent-3:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5008549094200134
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010826318757608533
        model: {}
        policy_loss: -0.0026811398565769196
        total_loss: -0.0017734933644533157
        vf_explained_var: 0.10219815373420715
        vf_loss: 17.89153289794922
      agent-4:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9606484174728394
        entropy_coeff: 0.0017600000137463212
        kl: 0.001923966221511364
        model: {}
        policy_loss: -0.004298068583011627
        total_loss: -0.004016694612801075
        vf_explained_var: 0.014567792415618896
        vf_loss: 19.721120834350586
      agent-5:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.877392590045929
        entropy_coeff: 0.0017600000137463212
        kl: 0.001715082093141973
        model: {}
        policy_loss: -0.0037126413080841303
        total_loss: -0.0034855827689170837
        vf_explained_var: 0.10934537649154663
        vf_loss: 17.712722778320312
    load_time_ms: 13427.876
    num_steps_sampled: 22944000
    num_steps_trained: 22944000
    sample_time_ms: 99489.861
    update_time_ms: 15.282
  iterations_since_restore: 179
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.28698224852071
    ram_util_percent: 16.2094674556213
  pid: 30948
  policy_reward_max:
    agent-0: 174.49999999999994
    agent-1: 174.49999999999994
    agent-2: 174.49999999999994
    agent-3: 174.49999999999994
    agent-4: 174.49999999999994
    agent-5: 174.49999999999994
  policy_reward_mean:
    agent-0: 145.74500000000003
    agent-1: 145.74500000000003
    agent-2: 145.74500000000003
    agent-3: 145.74500000000003
    agent-4: 145.74500000000003
    agent-5: 145.74500000000003
  policy_reward_min:
    agent-0: 58.66666666666649
    agent-1: 58.66666666666649
    agent-2: 58.66666666666649
    agent-3: 58.66666666666649
    agent-4: 58.66666666666649
    agent-5: 58.66666666666649
  sampler_perf:
    mean_env_wait_ms: 27.11670881407895
    mean_inference_ms: 12.92544461935966
    mean_processing_ms: 57.93077790803044
  time_since_restore: 23909.03199529648
  time_this_iter_s: 124.57948780059814
  time_total_s: 33035.04380917549
  timestamp: 1637051013
  timesteps_since_restore: 17184000
  timesteps_this_iter: 96000
  timesteps_total: 22944000
  training_iteration: 239
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    239 |            33035 | 22944000 |   874.47 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 1.23
    apples_agent-0_min: 0
    apples_agent-1_max: 174
    apples_agent-1_mean: 35.88
    apples_agent-1_min: 0
    apples_agent-2_max: 67
    apples_agent-2_mean: 9.67
    apples_agent-2_min: 0
    apples_agent-3_max: 148
    apples_agent-3_mean: 71.85
    apples_agent-3_min: 29
    apples_agent-4_max: 68
    apples_agent-4_mean: 4.57
    apples_agent-4_min: 0
    apples_agent-5_max: 158
    apples_agent-5_mean: 96.45
    apples_agent-5_min: 19
    cleaning_beam_agent-0_max: 570
    cleaning_beam_agent-0_mean: 446.65
    cleaning_beam_agent-0_min: 359
    cleaning_beam_agent-1_max: 365
    cleaning_beam_agent-1_mean: 223.89
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 604
    cleaning_beam_agent-2_mean: 338.27
    cleaning_beam_agent-2_min: 140
    cleaning_beam_agent-3_max: 65
    cleaning_beam_agent-3_mean: 22.65
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 563
    cleaning_beam_agent-4_mean: 447.8
    cleaning_beam_agent-4_min: 294
    cleaning_beam_agent-5_max: 173
    cleaning_beam_agent-5_mean: 47.74
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-25-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1085.9999999999868
  episode_reward_mean: 875.5399999999836
  episode_reward_min: 352.00000000000284
  episodes_this_iter: 96
  episodes_total: 23040
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11458.808
    learner:
      agent-0:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9238353371620178
        entropy_coeff: 0.0017600000137463212
        kl: 0.002002432243898511
        model: {}
        policy_loss: -0.0033869764301925898
        total_loss: -0.003222506260499358
        vf_explained_var: 0.028119578957557678
        vf_loss: 17.904178619384766
      agent-1:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1522703170776367
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013123921817168593
        model: {}
        policy_loss: -0.004129601176828146
        total_loss: -0.004208602476865053
        vf_explained_var: -0.032323479652404785
        vf_loss: 19.489933013916016
      agent-2:
        cur_kl_coeff: 7.275957722603643e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.10336434841156
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015458291163668036
        model: {}
        policy_loss: -0.0038760723546147346
        total_loss: -0.0039238836616277695
        vf_explained_var: -0.008969932794570923
        vf_loss: 18.941089630126953
      agent-3:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.502662181854248
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010053616715595126
        model: {}
        policy_loss: -0.002619585022330284
        total_loss: -0.0018033056985586882
        vf_explained_var: 0.08166386187076569
        vf_loss: 17.009674072265625
      agent-4:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9485191702842712
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017521942500025034
        model: {}
        policy_loss: -0.003892322303727269
        total_loss: -0.0037205955013632774
        vf_explained_var: 0.014387667179107666
        vf_loss: 18.411218643188477
      agent-5:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8625798225402832
        entropy_coeff: 0.0017600000137463212
        kl: 0.001869992003776133
        model: {}
        policy_loss: -0.0037799980491399765
        total_loss: -0.0035807103849947453
        vf_explained_var: 0.07261790335178375
        vf_loss: 17.174293518066406
    load_time_ms: 13403.694
    num_steps_sampled: 23040000
    num_steps_trained: 23040000
    sample_time_ms: 99710.536
    update_time_ms: 15.165
  iterations_since_restore: 180
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.311666666666667
    ram_util_percent: 16.130555555555556
  pid: 30948
  policy_reward_max:
    agent-0: 180.99999999999932
    agent-1: 180.99999999999932
    agent-2: 180.99999999999932
    agent-3: 180.99999999999932
    agent-4: 180.99999999999932
    agent-5: 180.99999999999932
  policy_reward_mean:
    agent-0: 145.92333333333343
    agent-1: 145.92333333333343
    agent-2: 145.92333333333343
    agent-3: 145.92333333333343
    agent-4: 145.92333333333343
    agent-5: 145.92333333333343
  policy_reward_min:
    agent-0: 58.66666666666649
    agent-1: 58.66666666666649
    agent-2: 58.66666666666649
    agent-3: 58.66666666666649
    agent-4: 58.66666666666649
    agent-5: 58.66666666666649
  sampler_perf:
    mean_env_wait_ms: 27.118569942654293
    mean_inference_ms: 12.92489045968442
    mean_processing_ms: 57.925438168111704
  time_since_restore: 24035.014259576797
  time_this_iter_s: 125.98226428031921
  time_total_s: 33161.02607345581
  timestamp: 1637051139
  timesteps_since_restore: 17280000
  timesteps_this_iter: 96000
  timesteps_total: 23040000
  training_iteration: 240
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    240 |            33161 | 23040000 |   875.54 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 1.36
    apples_agent-0_min: 0
    apples_agent-1_max: 122
    apples_agent-1_mean: 33.84
    apples_agent-1_min: 0
    apples_agent-2_max: 69
    apples_agent-2_mean: 6.08
    apples_agent-2_min: 0
    apples_agent-3_max: 134
    apples_agent-3_mean: 71.77
    apples_agent-3_min: 37
    apples_agent-4_max: 47
    apples_agent-4_mean: 2.14
    apples_agent-4_min: 0
    apples_agent-5_max: 186
    apples_agent-5_mean: 93.25
    apples_agent-5_min: 38
    cleaning_beam_agent-0_max: 561
    cleaning_beam_agent-0_mean: 445.65
    cleaning_beam_agent-0_min: 320
    cleaning_beam_agent-1_max: 373
    cleaning_beam_agent-1_mean: 230.79
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 568
    cleaning_beam_agent-2_mean: 346.62
    cleaning_beam_agent-2_min: 153
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 22.51
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 532
    cleaning_beam_agent-4_mean: 442.9
    cleaning_beam_agent-4_min: 277
    cleaning_beam_agent-5_max: 189
    cleaning_beam_agent-5_mean: 51.38
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-27-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1059.9999999999882
  episode_reward_mean: 895.1499999999827
  episode_reward_min: 442.00000000000523
  episodes_this_iter: 96
  episodes_total: 23136
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11401.348
    learner:
      agent-0:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9356133341789246
        entropy_coeff: 0.0017600000137463212
        kl: 0.001562676508910954
        model: {}
        policy_loss: -0.0032831639982759953
        total_loss: -0.003122940193861723
        vf_explained_var: 0.035967886447906494
        vf_loss: 18.069046020507812
      agent-1:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1266523599624634
        entropy_coeff: 0.0017600000137463212
        kl: 0.001065149437636137
        model: {}
        policy_loss: -0.003784146159887314
        total_loss: -0.0036283601075410843
        vf_explained_var: -0.08557248115539551
        vf_loss: 21.386940002441406
      agent-2:
        cur_kl_coeff: 3.6379788613018216e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.114884614944458
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012428117915987968
        model: {}
        policy_loss: -0.0036189400125294924
        total_loss: -0.0036894360091537237
        vf_explained_var: 0.01678258180618286
        vf_loss: 18.917016983032227
      agent-3:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48240959644317627
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010723085142672062
        model: {}
        policy_loss: -0.0026789233088493347
        total_loss: -0.0018204394727945328
        vf_explained_var: 0.09028930962085724
        vf_loss: 17.075265884399414
      agent-4:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9623472690582275
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018274899339303374
        model: {}
        policy_loss: -0.0040216934867203236
        total_loss: -0.0037515126168727875
        vf_explained_var: -0.028356075286865234
        vf_loss: 19.63912582397461
      agent-5:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8523744940757751
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013020986225456
        model: {}
        policy_loss: -0.003525960259139538
        total_loss: -0.0033314330503344536
        vf_explained_var: 0.09918583929538727
        vf_loss: 16.94707489013672
    load_time_ms: 13390.738
    num_steps_sampled: 23136000
    num_steps_trained: 23136000
    sample_time_ms: 99706.937
    update_time_ms: 15.269
  iterations_since_restore: 181
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.489830508474576
    ram_util_percent: 16.189265536723163
  pid: 30948
  policy_reward_max:
    agent-0: 176.66666666666498
    agent-1: 176.66666666666498
    agent-2: 176.66666666666498
    agent-3: 176.66666666666498
    agent-4: 176.66666666666498
    agent-5: 176.66666666666498
  policy_reward_mean:
    agent-0: 149.1916666666667
    agent-1: 149.1916666666667
    agent-2: 149.1916666666667
    agent-3: 149.1916666666667
    agent-4: 149.1916666666667
    agent-5: 149.1916666666667
  policy_reward_min:
    agent-0: 73.66666666666669
    agent-1: 73.66666666666669
    agent-2: 73.66666666666669
    agent-3: 73.66666666666669
    agent-4: 73.66666666666669
    agent-5: 73.66666666666669
  sampler_perf:
    mean_env_wait_ms: 27.119798060610837
    mean_inference_ms: 12.923869132572756
    mean_processing_ms: 57.91852086336404
  time_since_restore: 24159.367224931717
  time_this_iter_s: 124.35296535491943
  time_total_s: 33285.37903881073
  timestamp: 1637051264
  timesteps_since_restore: 17376000
  timesteps_this_iter: 96000
  timesteps_total: 23136000
  training_iteration: 241
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    241 |          33285.4 | 23136000 |   895.15 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 57
    apples_agent-0_mean: 1.76
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 35.19
    apples_agent-1_min: 0
    apples_agent-2_max: 156
    apples_agent-2_mean: 9.44
    apples_agent-2_min: 0
    apples_agent-3_max: 119
    apples_agent-3_mean: 71.44
    apples_agent-3_min: 30
    apples_agent-4_max: 57
    apples_agent-4_mean: 2.38
    apples_agent-4_min: 0
    apples_agent-5_max: 194
    apples_agent-5_mean: 96.18
    apples_agent-5_min: 25
    cleaning_beam_agent-0_max: 571
    cleaning_beam_agent-0_mean: 443.95
    cleaning_beam_agent-0_min: 311
    cleaning_beam_agent-1_max: 476
    cleaning_beam_agent-1_mean: 235.07
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 587
    cleaning_beam_agent-2_mean: 342.01
    cleaning_beam_agent-2_min: 145
    cleaning_beam_agent-3_max: 44
    cleaning_beam_agent-3_mean: 22.56
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 536
    cleaning_beam_agent-4_mean: 438.46
    cleaning_beam_agent-4_min: 250
    cleaning_beam_agent-5_max: 253
    cleaning_beam_agent-5_mean: 54.18
    cleaning_beam_agent-5_min: 15
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-29-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1042.9999999999764
  episode_reward_mean: 874.2299999999835
  episode_reward_min: 233.99999999999892
  episodes_this_iter: 96
  episodes_total: 23232
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11426.074
    learner:
      agent-0:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9270564913749695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016545361140742898
        model: {}
        policy_loss: -0.0031744700390845537
        total_loss: -0.0028692944906651974
        vf_explained_var: 0.04345300793647766
        vf_loss: 19.367931365966797
      agent-1:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1400232315063477
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014764920342713594
        model: {}
        policy_loss: -0.004136070609092712
        total_loss: -0.003911973442882299
        vf_explained_var: -0.0707213282585144
        vf_loss: 22.30540657043457
      agent-2:
        cur_kl_coeff: 1.8189894306509108e-13
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1058812141418457
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017183844465762377
        model: {}
        policy_loss: -0.003722164547070861
        total_loss: -0.003598105162382126
        vf_explained_var: -0.004710972309112549
        vf_loss: 20.70409393310547
      agent-3:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49086904525756836
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013394199777394533
        model: {}
        policy_loss: -0.002973849419504404
        total_loss: -0.0020359251648187637
        vf_explained_var: 0.11181168258190155
        vf_loss: 18.018537521362305
      agent-4:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9616637825965881
        entropy_coeff: 0.0017600000137463212
        kl: 0.001939080422744155
        model: {}
        policy_loss: -0.003984969109296799
        total_loss: -0.0036968884523957968
        vf_explained_var: 0.03431747853755951
        vf_loss: 19.80609893798828
      agent-5:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.86080402135849
        entropy_coeff: 0.0017600000137463212
        kl: 0.001720736618153751
        model: {}
        policy_loss: -0.0038171582855284214
        total_loss: -0.0035017705522477627
        vf_explained_var: 0.09829968214035034
        vf_loss: 18.304035186767578
    load_time_ms: 13390.841
    num_steps_sampled: 23232000
    num_steps_trained: 23232000
    sample_time_ms: 99688.613
    update_time_ms: 14.953
  iterations_since_restore: 182
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.388135593220344
    ram_util_percent: 16.195480225988703
  pid: 30948
  policy_reward_max:
    agent-0: 173.83333333333354
    agent-1: 173.83333333333354
    agent-2: 173.83333333333354
    agent-3: 173.83333333333354
    agent-4: 173.83333333333354
    agent-5: 173.83333333333354
  policy_reward_mean:
    agent-0: 145.70500000000004
    agent-1: 145.70500000000004
    agent-2: 145.70500000000004
    agent-3: 145.70500000000004
    agent-4: 145.70500000000004
    agent-5: 145.70500000000004
  policy_reward_min:
    agent-0: 39.00000000000002
    agent-1: 39.00000000000002
    agent-2: 39.00000000000002
    agent-3: 39.00000000000002
    agent-4: 39.00000000000002
    agent-5: 39.00000000000002
  sampler_perf:
    mean_env_wait_ms: 27.11955286481483
    mean_inference_ms: 12.92265639815673
    mean_processing_ms: 57.90962252496459
  time_since_restore: 24283.988310337067
  time_this_iter_s: 124.62108540534973
  time_total_s: 33410.00012421608
  timestamp: 1637051389
  timesteps_since_restore: 17472000
  timesteps_this_iter: 96000
  timesteps_total: 23232000
  training_iteration: 242
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    242 |            33410 | 23232000 |   874.23 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 2.2
    apples_agent-0_min: 0
    apples_agent-1_max: 134
    apples_agent-1_mean: 33.24
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 6.35
    apples_agent-2_min: 0
    apples_agent-3_max: 127
    apples_agent-3_mean: 72.56
    apples_agent-3_min: 37
    apples_agent-4_max: 91
    apples_agent-4_mean: 5.14
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 93.16
    apples_agent-5_min: 38
    cleaning_beam_agent-0_max: 556
    cleaning_beam_agent-0_mean: 442.11
    cleaning_beam_agent-0_min: 303
    cleaning_beam_agent-1_max: 432
    cleaning_beam_agent-1_mean: 230.42
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 517
    cleaning_beam_agent-2_mean: 352.28
    cleaning_beam_agent-2_min: 147
    cleaning_beam_agent-3_max: 47
    cleaning_beam_agent-3_mean: 21.98
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 442.03
    cleaning_beam_agent-4_min: 287
    cleaning_beam_agent-5_max: 211
    cleaning_beam_agent-5_mean: 49.4
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-31-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1058.9999999999864
  episode_reward_mean: 867.9699999999847
  episode_reward_min: 528.0000000000028
  episodes_this_iter: 96
  episodes_total: 23328
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11428.402
    learner:
      agent-0:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9174389243125916
        entropy_coeff: 0.0017600000137463212
        kl: 0.002020271960645914
        model: {}
        policy_loss: -0.003445807844400406
        total_loss: -0.0032214312814176083
        vf_explained_var: 0.03747391700744629
        vf_loss: 18.39069175720215
      agent-1:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1296635866165161
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010750647634267807
        model: {}
        policy_loss: -0.003947754856199026
        total_loss: -0.0038460716605186462
        vf_explained_var: -0.06841552257537842
        vf_loss: 20.898910522460938
      agent-2:
        cur_kl_coeff: 9.094947153254554e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0989192724227905
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014202413149178028
        model: {}
        policy_loss: -0.003644234500825405
        total_loss: -0.0036951089277863503
        vf_explained_var: 0.017262175679206848
        vf_loss: 18.832218170166016
      agent-3:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5038660168647766
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009418696281500161
        model: {}
        policy_loss: -0.0026725749485194683
        total_loss: -0.0018249116837978363
        vf_explained_var: 0.09111331403255463
        vf_loss: 17.34467887878418
      agent-4:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9632232189178467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016308262711390853
        model: {}
        policy_loss: -0.003939385991543531
        total_loss: -0.003734322264790535
        vf_explained_var: 0.015507608652114868
        vf_loss: 19.00337028503418
      agent-5:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8653892278671265
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013053984148427844
        model: {}
        policy_loss: -0.0036575347185134888
        total_loss: -0.0034410450607538223
        vf_explained_var: 0.08729024231433868
        vf_loss: 17.395751953125
    load_time_ms: 13391.011
    num_steps_sampled: 23328000
    num_steps_trained: 23328000
    sample_time_ms: 99681.537
    update_time_ms: 15.137
  iterations_since_restore: 183
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.529545454545453
    ram_util_percent: 16.045454545454547
  pid: 30948
  policy_reward_max:
    agent-0: 176.49999999999955
    agent-1: 176.49999999999955
    agent-2: 176.49999999999955
    agent-3: 176.49999999999955
    agent-4: 176.49999999999955
    agent-5: 176.49999999999955
  policy_reward_mean:
    agent-0: 144.6616666666667
    agent-1: 144.6616666666667
    agent-2: 144.6616666666667
    agent-3: 144.6616666666667
    agent-4: 144.6616666666667
    agent-5: 144.6616666666667
  policy_reward_min:
    agent-0: 88.00000000000009
    agent-1: 88.00000000000009
    agent-2: 88.00000000000009
    agent-3: 88.00000000000009
    agent-4: 88.00000000000009
    agent-5: 88.00000000000009
  sampler_perf:
    mean_env_wait_ms: 27.119847366789305
    mean_inference_ms: 12.921348512250752
    mean_processing_ms: 57.899973126354695
  time_since_restore: 24407.441011190414
  time_this_iter_s: 123.45270085334778
  time_total_s: 33533.45282506943
  timestamp: 1637051512
  timesteps_since_restore: 17568000
  timesteps_this_iter: 96000
  timesteps_total: 23328000
  training_iteration: 243
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    243 |          33533.5 | 23328000 |   867.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 2.85
    apples_agent-0_min: 0
    apples_agent-1_max: 155
    apples_agent-1_mean: 34.18
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 6.27
    apples_agent-2_min: 0
    apples_agent-3_max: 145
    apples_agent-3_mean: 74.36
    apples_agent-3_min: 35
    apples_agent-4_max: 69
    apples_agent-4_mean: 2.42
    apples_agent-4_min: 0
    apples_agent-5_max: 196
    apples_agent-5_mean: 97.43
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 526
    cleaning_beam_agent-0_mean: 451.82
    cleaning_beam_agent-0_min: 305
    cleaning_beam_agent-1_max: 367
    cleaning_beam_agent-1_mean: 230.46
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 615
    cleaning_beam_agent-2_mean: 343.14
    cleaning_beam_agent-2_min: 177
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 22.61
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 542
    cleaning_beam_agent-4_mean: 441.97
    cleaning_beam_agent-4_min: 293
    cleaning_beam_agent-5_max: 235
    cleaning_beam_agent-5_mean: 49.18
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-33-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1077.9999999999964
  episode_reward_mean: 884.9299999999844
  episode_reward_min: 363.0000000000015
  episodes_this_iter: 96
  episodes_total: 23424
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11434.525
    learner:
      agent-0:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9174885749816895
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014010833110660315
        model: {}
        policy_loss: -0.003127397270873189
        total_loss: -0.0029692489188164473
        vf_explained_var: 0.054534927010536194
        vf_loss: 17.729278564453125
      agent-1:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1467912197113037
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009892531670629978
        model: {}
        policy_loss: -0.003846934996545315
        total_loss: -0.0038215136155486107
        vf_explained_var: -0.06132236123085022
        vf_loss: 20.43773078918457
      agent-2:
        cur_kl_coeff: 4.547473576627277e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.109056830406189
        entropy_coeff: 0.0017600000137463212
        kl: 0.00180751271545887
        model: {}
        policy_loss: -0.004032770171761513
        total_loss: -0.004145490005612373
        vf_explained_var: 0.030604824423789978
        vf_loss: 18.392169952392578
      agent-3:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4936363399028778
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014010632876306772
        model: {}
        policy_loss: -0.0030418746173381805
        total_loss: -0.002194879576563835
        vf_explained_var: 0.08603815734386444
        vf_loss: 17.157928466796875
      agent-4:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9610795974731445
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014257129514589906
        model: {}
        policy_loss: -0.003972436301410198
        total_loss: -0.0037758955731987953
        vf_explained_var: 0.005518361926078796
        vf_loss: 18.88042449951172
      agent-5:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8688145875930786
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013761030277237296
        model: {}
        policy_loss: -0.003748086281120777
        total_loss: -0.0035668262280523777
        vf_explained_var: 0.09125813841819763
        vf_loss: 17.103740692138672
    load_time_ms: 13383.381
    num_steps_sampled: 23424000
    num_steps_trained: 23424000
    sample_time_ms: 99782.475
    update_time_ms: 15.079
  iterations_since_restore: 184
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.3438202247191
    ram_util_percent: 16.189887640449438
  pid: 30948
  policy_reward_max:
    agent-0: 179.66666666666632
    agent-1: 179.66666666666632
    agent-2: 179.66666666666632
    agent-3: 179.66666666666632
    agent-4: 179.66666666666632
    agent-5: 179.66666666666632
  policy_reward_mean:
    agent-0: 147.48833333333334
    agent-1: 147.48833333333334
    agent-2: 147.48833333333334
    agent-3: 147.48833333333334
    agent-4: 147.48833333333334
    agent-5: 147.48833333333334
  policy_reward_min:
    agent-0: 60.49999999999994
    agent-1: 60.49999999999994
    agent-2: 60.49999999999994
    agent-3: 60.49999999999994
    agent-4: 60.49999999999994
    agent-5: 60.49999999999994
  sampler_perf:
    mean_env_wait_ms: 27.121099405759118
    mean_inference_ms: 12.920434653208595
    mean_processing_ms: 57.89144249850622
  time_since_restore: 24532.65445446968
  time_this_iter_s: 125.21344327926636
  time_total_s: 33658.666268348694
  timestamp: 1637051638
  timesteps_since_restore: 17664000
  timesteps_this_iter: 96000
  timesteps_total: 23424000
  training_iteration: 244
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    244 |          33658.7 | 23424000 |   884.93 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 2.02
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 33.38
    apples_agent-1_min: 0
    apples_agent-2_max: 54
    apples_agent-2_mean: 7.35
    apples_agent-2_min: 0
    apples_agent-3_max: 111
    apples_agent-3_mean: 69.88
    apples_agent-3_min: 25
    apples_agent-4_max: 82
    apples_agent-4_mean: 1.23
    apples_agent-4_min: 0
    apples_agent-5_max: 196
    apples_agent-5_mean: 94.98
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 584
    cleaning_beam_agent-0_mean: 457.4
    cleaning_beam_agent-0_min: 337
    cleaning_beam_agent-1_max: 493
    cleaning_beam_agent-1_mean: 234.27
    cleaning_beam_agent-1_min: 109
    cleaning_beam_agent-2_max: 573
    cleaning_beam_agent-2_mean: 355.6
    cleaning_beam_agent-2_min: 170
    cleaning_beam_agent-3_max: 59
    cleaning_beam_agent-3_mean: 24.33
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 448.9
    cleaning_beam_agent-4_min: 273
    cleaning_beam_agent-5_max: 118
    cleaning_beam_agent-5_mean: 44.51
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-36-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1047.9999999999854
  episode_reward_mean: 888.3299999999829
  episode_reward_min: 468.00000000000966
  episodes_this_iter: 96
  episodes_total: 23520
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11431.712
    learner:
      agent-0:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9226878881454468
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019362557213753462
        model: {}
        policy_loss: -0.0033499051351100206
        total_loss: -0.003236639779061079
        vf_explained_var: 0.05143572390079498
        vf_loss: 17.371967315673828
      agent-1:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1358051300048828
        entropy_coeff: 0.0017600000137463212
        kl: 0.001034033834002912
        model: {}
        policy_loss: -0.003732934594154358
        total_loss: -0.0037603862583637238
        vf_explained_var: -0.03984326124191284
        vf_loss: 19.715641021728516
      agent-2:
        cur_kl_coeff: 2.2737367883136385e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0915558338165283
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018589756218716502
        model: {}
        policy_loss: -0.003683722112327814
        total_loss: -0.0037671017926186323
        vf_explained_var: 0.010050341486930847
        vf_loss: 18.377614974975586
      agent-3:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49216049909591675
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009642894146963954
        model: {}
        policy_loss: -0.0024449964985251427
        total_loss: -0.0016593942418694496
        vf_explained_var: 0.09486255049705505
        vf_loss: 16.51803207397461
      agent-4:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9544594287872314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017067542066797614
        model: {}
        policy_loss: -0.004207924474030733
        total_loss: -0.003998593892902136
        vf_explained_var: -0.02084636688232422
        vf_loss: 18.891807556152344
      agent-5:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8643207550048828
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014790244167670608
        model: {}
        policy_loss: -0.003608216065913439
        total_loss: -0.0034395919647067785
        vf_explained_var: 0.08332929015159607
        vf_loss: 16.898273468017578
    load_time_ms: 13384.769
    num_steps_sampled: 23520000
    num_steps_trained: 23520000
    sample_time_ms: 99726.841
    update_time_ms: 15.021
  iterations_since_restore: 185
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.450000000000003
    ram_util_percent: 16.193258426966295
  pid: 30948
  policy_reward_max:
    agent-0: 174.6666666666663
    agent-1: 174.6666666666663
    agent-2: 174.6666666666663
    agent-3: 174.6666666666663
    agent-4: 174.6666666666663
    agent-5: 174.6666666666663
  policy_reward_mean:
    agent-0: 148.055
    agent-1: 148.055
    agent-2: 148.055
    agent-3: 148.055
    agent-4: 148.055
    agent-5: 148.055
  policy_reward_min:
    agent-0: 78.00000000000001
    agent-1: 78.00000000000001
    agent-2: 78.00000000000001
    agent-3: 78.00000000000001
    agent-4: 78.00000000000001
    agent-5: 78.00000000000001
  sampler_perf:
    mean_env_wait_ms: 27.123668182553875
    mean_inference_ms: 12.919709365800962
    mean_processing_ms: 57.88580905188047
  time_since_restore: 24657.657933473587
  time_this_iter_s: 125.00347900390625
  time_total_s: 33783.6697473526
  timestamp: 1637051763
  timesteps_since_restore: 17760000
  timesteps_this_iter: 96000
  timesteps_total: 23520000
  training_iteration: 245
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    245 |          33783.7 | 23520000 |   888.33 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 1.47
    apples_agent-0_min: 0
    apples_agent-1_max: 117
    apples_agent-1_mean: 34.65
    apples_agent-1_min: 0
    apples_agent-2_max: 84
    apples_agent-2_mean: 7.84
    apples_agent-2_min: 0
    apples_agent-3_max: 146
    apples_agent-3_mean: 74.64
    apples_agent-3_min: 48
    apples_agent-4_max: 73
    apples_agent-4_mean: 4.08
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 92.56
    apples_agent-5_min: 52
    cleaning_beam_agent-0_max: 581
    cleaning_beam_agent-0_mean: 444.6
    cleaning_beam_agent-0_min: 274
    cleaning_beam_agent-1_max: 455
    cleaning_beam_agent-1_mean: 233.79
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 626
    cleaning_beam_agent-2_mean: 372.22
    cleaning_beam_agent-2_min: 159
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 22.37
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 527
    cleaning_beam_agent-4_mean: 439.22
    cleaning_beam_agent-4_min: 340
    cleaning_beam_agent-5_max: 194
    cleaning_beam_agent-5_mean: 46.58
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-38-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1115.999999999983
  episode_reward_mean: 892.5499999999827
  episode_reward_min: 582.0000000000035
  episodes_this_iter: 96
  episodes_total: 23616
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11423.159
    learner:
      agent-0:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9181393384933472
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011821686057373881
        model: {}
        policy_loss: -0.0031295795924961567
        total_loss: -0.0030335523188114166
        vf_explained_var: 0.05710792541503906
        vf_loss: 17.119522094726562
      agent-1:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1351101398468018
        entropy_coeff: 0.0017600000137463212
        kl: 0.001428287592716515
        model: {}
        policy_loss: -0.004021255299448967
        total_loss: -0.004051086492836475
        vf_explained_var: -0.052941352128982544
        vf_loss: 19.679676055908203
      agent-2:
        cur_kl_coeff: 1.1368683941568192e-14
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0832042694091797
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015666360268369317
        model: {}
        policy_loss: -0.0036765839904546738
        total_loss: -0.0037648119032382965
        vf_explained_var: 0.008670598268508911
        vf_loss: 18.182100296020508
      agent-3:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49029797315597534
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009149820543825626
        model: {}
        policy_loss: -0.0025418810546398163
        total_loss: -0.001738551538437605
        vf_explained_var: 0.08756700158119202
        vf_loss: 16.662538528442383
      agent-4:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9640517234802246
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018695301841944456
        model: {}
        policy_loss: -0.004051936790347099
        total_loss: -0.003923689015209675
        vf_explained_var: 0.006727427244186401
        vf_loss: 18.24978256225586
      agent-5:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8587809801101685
        entropy_coeff: 0.0017600000137463212
        kl: 0.001517807599157095
        model: {}
        policy_loss: -0.0035057170316576958
        total_loss: -0.003353022038936615
        vf_explained_var: 0.08490137755870819
        vf_loss: 16.6414852142334
    load_time_ms: 13396.908
    num_steps_sampled: 23616000
    num_steps_trained: 23616000
    sample_time_ms: 99751.487
    update_time_ms: 15.13
  iterations_since_restore: 186
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.41581920903955
    ram_util_percent: 16.191525423728816
  pid: 30948
  policy_reward_max:
    agent-0: 186.0000000000001
    agent-1: 186.0000000000001
    agent-2: 186.0000000000001
    agent-3: 186.0000000000001
    agent-4: 186.0000000000001
    agent-5: 186.0000000000001
  policy_reward_mean:
    agent-0: 148.75833333333338
    agent-1: 148.75833333333338
    agent-2: 148.75833333333338
    agent-3: 148.75833333333338
    agent-4: 148.75833333333338
    agent-5: 148.75833333333338
  policy_reward_min:
    agent-0: 97.0000000000005
    agent-1: 97.0000000000005
    agent-2: 97.0000000000005
    agent-3: 97.0000000000005
    agent-4: 97.0000000000005
    agent-5: 97.0000000000005
  sampler_perf:
    mean_env_wait_ms: 27.12460225255169
    mean_inference_ms: 12.918746933325652
    mean_processing_ms: 57.877251366634475
  time_since_restore: 24782.275290966034
  time_this_iter_s: 124.6173574924469
  time_total_s: 33908.28710484505
  timestamp: 1637051888
  timesteps_since_restore: 17856000
  timesteps_this_iter: 96000
  timesteps_total: 23616000
  training_iteration: 246
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    246 |          33908.3 | 23616000 |   892.55 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 2.03
    apples_agent-0_min: 0
    apples_agent-1_max: 116
    apples_agent-1_mean: 30.92
    apples_agent-1_min: 0
    apples_agent-2_max: 78
    apples_agent-2_mean: 10.16
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 74.02
    apples_agent-3_min: 37
    apples_agent-4_max: 107
    apples_agent-4_mean: 3.75
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 94.89
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 539
    cleaning_beam_agent-0_mean: 439.53
    cleaning_beam_agent-0_min: 308
    cleaning_beam_agent-1_max: 440
    cleaning_beam_agent-1_mean: 241.68
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 589
    cleaning_beam_agent-2_mean: 347.24
    cleaning_beam_agent-2_min: 156
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 21.85
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 594
    cleaning_beam_agent-4_mean: 455.03
    cleaning_beam_agent-4_min: 335
    cleaning_beam_agent-5_max: 139
    cleaning_beam_agent-5_mean: 45.33
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-40-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1059.9999999999757
  episode_reward_mean: 875.7199999999823
  episode_reward_min: 344.0000000000025
  episodes_this_iter: 96
  episodes_total: 23712
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11426.724
    learner:
      agent-0:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9279870390892029
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018883179873228073
        model: {}
        policy_loss: -0.0035853348672389984
        total_loss: -0.003416941501200199
        vf_explained_var: 0.07990799844264984
        vf_loss: 18.016521453857422
      agent-1:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.130897879600525
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016956718172878027
        model: {}
        policy_loss: -0.0041910214349627495
        total_loss: -0.003993010148406029
        vf_explained_var: -0.09118950366973877
        vf_loss: 21.883947372436523
      agent-2:
        cur_kl_coeff: 5.684341970784096e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0978813171386719
        entropy_coeff: 0.0017600000137463212
        kl: 0.001955376472324133
        model: {}
        policy_loss: -0.0037672980688512325
        total_loss: -0.003739019390195608
        vf_explained_var: 0.009988024830818176
        vf_loss: 19.605510711669922
      agent-3:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4862980246543884
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008211473468691111
        model: {}
        policy_loss: -0.0025501223281025887
        total_loss: -0.00168598722666502
        vf_explained_var: 0.12601448595523834
        vf_loss: 17.200191497802734
      agent-4:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9583554267883301
        entropy_coeff: 0.0017600000137463212
        kl: 0.001377001404762268
        model: {}
        policy_loss: -0.003792238887399435
        total_loss: -0.0035268673673272133
        vf_explained_var: 0.01108427345752716
        vf_loss: 19.520774841308594
      agent-5:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8524115085601807
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014990507625043392
        model: {}
        policy_loss: -0.003953082021325827
        total_loss: -0.00368748651817441
        vf_explained_var: 0.10181988775730133
        vf_loss: 17.65839385986328
    load_time_ms: 13421.474
    num_steps_sampled: 23712000
    num_steps_trained: 23712000
    sample_time_ms: 99704.101
    update_time_ms: 15.062
  iterations_since_restore: 187
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.738636363636363
    ram_util_percent: 16.209659090909092
  pid: 30948
  policy_reward_max:
    agent-0: 176.6666666666666
    agent-1: 176.6666666666666
    agent-2: 176.6666666666666
    agent-3: 176.6666666666666
    agent-4: 176.6666666666666
    agent-5: 176.6666666666666
  policy_reward_mean:
    agent-0: 145.95333333333335
    agent-1: 145.95333333333335
    agent-2: 145.95333333333335
    agent-3: 145.95333333333335
    agent-4: 145.95333333333335
    agent-5: 145.95333333333335
  policy_reward_min:
    agent-0: 57.33333333333322
    agent-1: 57.33333333333322
    agent-2: 57.33333333333322
    agent-3: 57.33333333333322
    agent-4: 57.33333333333322
    agent-5: 57.33333333333322
  sampler_perf:
    mean_env_wait_ms: 27.1259750776056
    mean_inference_ms: 12.91803888595007
    mean_processing_ms: 57.86978363109907
  time_since_restore: 24906.169062137604
  time_this_iter_s: 123.89377117156982
  time_total_s: 34032.18087601662
  timestamp: 1637052012
  timesteps_since_restore: 17952000
  timesteps_this_iter: 96000
  timesteps_total: 23712000
  training_iteration: 247
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    247 |          34032.2 | 23712000 |   875.72 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 3.1
    apples_agent-0_min: 0
    apples_agent-1_max: 124
    apples_agent-1_mean: 32.22
    apples_agent-1_min: 0
    apples_agent-2_max: 120
    apples_agent-2_mean: 11.87
    apples_agent-2_min: 0
    apples_agent-3_max: 208
    apples_agent-3_mean: 72.7
    apples_agent-3_min: 30
    apples_agent-4_max: 107
    apples_agent-4_mean: 3.41
    apples_agent-4_min: 0
    apples_agent-5_max: 187
    apples_agent-5_mean: 99.09
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 559
    cleaning_beam_agent-0_mean: 443.49
    cleaning_beam_agent-0_min: 329
    cleaning_beam_agent-1_max: 446
    cleaning_beam_agent-1_mean: 248.56
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 556
    cleaning_beam_agent-2_mean: 328.32
    cleaning_beam_agent-2_min: 151
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 22.33
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 604
    cleaning_beam_agent-4_mean: 448.46
    cleaning_beam_agent-4_min: 332
    cleaning_beam_agent-5_max: 223
    cleaning_beam_agent-5_mean: 50.11
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-42-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1036.9999999999816
  episode_reward_mean: 882.7399999999835
  episode_reward_min: 546.0000000000007
  episodes_this_iter: 96
  episodes_total: 23808
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11465.534
    learner:
      agent-0:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9241019487380981
        entropy_coeff: 0.0017600000137463212
        kl: 0.001260372344404459
        model: {}
        policy_loss: -0.003140645567327738
        total_loss: -0.002996494760736823
        vf_explained_var: 0.08917652070522308
        vf_loss: 17.70573616027832
      agent-1:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1415526866912842
        entropy_coeff: 0.0017600000137463212
        kl: 0.001101040281355381
        model: {}
        policy_loss: -0.0038305879570543766
        total_loss: -0.003719617612659931
        vf_explained_var: -0.06309467554092407
        vf_loss: 21.20101547241211
      agent-2:
        cur_kl_coeff: 2.842170985392048e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0833288431167603
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016818202566355467
        model: {}
        policy_loss: -0.00389302521944046
        total_loss: -0.003815403673797846
        vf_explained_var: -0.006057038903236389
        vf_loss: 19.842796325683594
      agent-3:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49218833446502686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008614514954388142
        model: {}
        policy_loss: -0.0024990448728203773
        total_loss: -0.0016121100634336472
        vf_explained_var: 0.10326360166072845
        vf_loss: 17.531906127929688
      agent-4:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9686332941055298
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020937793888151646
        model: {}
        policy_loss: -0.004347886890172958
        total_loss: -0.004057847894728184
        vf_explained_var: -0.019671648740768433
        vf_loss: 19.948352813720703
      agent-5:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8723641633987427
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012620296329259872
        model: {}
        policy_loss: -0.0038616815581917763
        total_loss: -0.003563415724784136
        vf_explained_var: 0.06240057945251465
        vf_loss: 18.33624839782715
    load_time_ms: 13376.939
    num_steps_sampled: 23808000
    num_steps_trained: 23808000
    sample_time_ms: 99664.519
    update_time_ms: 15.196
  iterations_since_restore: 188
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.28474576271186
    ram_util_percent: 16.161016949152543
  pid: 30948
  policy_reward_max:
    agent-0: 172.83333333333292
    agent-1: 172.83333333333292
    agent-2: 172.83333333333292
    agent-3: 172.83333333333292
    agent-4: 172.83333333333292
    agent-5: 172.83333333333292
  policy_reward_mean:
    agent-0: 147.1233333333334
    agent-1: 147.1233333333334
    agent-2: 147.1233333333334
    agent-3: 147.1233333333334
    agent-4: 147.1233333333334
    agent-5: 147.1233333333334
  policy_reward_min:
    agent-0: 91.00000000000021
    agent-1: 91.00000000000021
    agent-2: 91.00000000000021
    agent-3: 91.00000000000021
    agent-4: 91.00000000000021
    agent-5: 91.00000000000021
  sampler_perf:
    mean_env_wait_ms: 27.126048235776917
    mean_inference_ms: 12.916989766115396
    mean_processing_ms: 57.860616592601865
  time_since_restore: 25030.49395418167
  time_this_iter_s: 124.32489204406738
  time_total_s: 34156.505768060684
  timestamp: 1637052136
  timesteps_since_restore: 18048000
  timesteps_this_iter: 96000
  timesteps_total: 23808000
  training_iteration: 248
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    248 |          34156.5 | 23808000 |   882.74 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 1.95
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 26.92
    apples_agent-1_min: 0
    apples_agent-2_max: 80
    apples_agent-2_mean: 9.11
    apples_agent-2_min: 0
    apples_agent-3_max: 127
    apples_agent-3_mean: 73.17
    apples_agent-3_min: 34
    apples_agent-4_max: 67
    apples_agent-4_mean: 2.5
    apples_agent-4_min: 0
    apples_agent-5_max: 223
    apples_agent-5_mean: 103.16
    apples_agent-5_min: 52
    cleaning_beam_agent-0_max: 543
    cleaning_beam_agent-0_mean: 440.82
    cleaning_beam_agent-0_min: 271
    cleaning_beam_agent-1_max: 364
    cleaning_beam_agent-1_mean: 244.29
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 631
    cleaning_beam_agent-2_mean: 357.47
    cleaning_beam_agent-2_min: 198
    cleaning_beam_agent-3_max: 59
    cleaning_beam_agent-3_mean: 23.28
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 550
    cleaning_beam_agent-4_mean: 447.09
    cleaning_beam_agent-4_min: 265
    cleaning_beam_agent-5_max: 223
    cleaning_beam_agent-5_mean: 48.21
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-44-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1048.0000000000043
  episode_reward_mean: 895.3999999999843
  episode_reward_min: 620.9999999999933
  episodes_this_iter: 96
  episodes_total: 23904
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11487.568
    learner:
      agent-0:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.927114725112915
        entropy_coeff: 0.0017600000137463212
        kl: 0.001099762157537043
        model: {}
        policy_loss: -0.003207131288945675
        total_loss: -0.0032005752436816692
        vf_explained_var: 0.0034959763288497925
        vf_loss: 16.382740020751953
      agent-1:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.158450722694397
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016367167700082064
        model: {}
        policy_loss: -0.004033293109387159
        total_loss: -0.004224913194775581
        vf_explained_var: -0.0919390618801117
        vf_loss: 18.4725284576416
      agent-2:
        cur_kl_coeff: 1.421085492696024e-15
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0810821056365967
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012385594891384244
        model: {}
        policy_loss: -0.003696109401062131
        total_loss: -0.0038683172315359116
        vf_explained_var: -0.03574422001838684
        vf_loss: 17.304973602294922
      agent-3:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47797876596450806
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010185851715505123
        model: {}
        policy_loss: -0.0022183035034686327
        total_loss: -0.0014780594501644373
        vf_explained_var: 0.04036781191825867
        vf_loss: 15.814848899841309
      agent-4:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9691493511199951
        entropy_coeff: 0.0017600000137463212
        kl: 0.00196487782523036
        model: {}
        policy_loss: -0.004393883980810642
        total_loss: -0.004394620656967163
        vf_explained_var: -0.023345261812210083
        vf_loss: 17.04963493347168
      agent-5:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8591295480728149
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017987668979912996
        model: {}
        policy_loss: -0.003738507628440857
        total_loss: -0.0036649545654654503
        vf_explained_var: 0.042584583163261414
        vf_loss: 15.856178283691406
    load_time_ms: 13379.065
    num_steps_sampled: 23904000
    num_steps_trained: 23904000
    sample_time_ms: 99600.064
    update_time_ms: 15.151
  iterations_since_restore: 189
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.391525423728808
    ram_util_percent: 16.203389830508474
  pid: 30948
  policy_reward_max:
    agent-0: 174.66666666666615
    agent-1: 174.66666666666615
    agent-2: 174.66666666666615
    agent-3: 174.66666666666615
    agent-4: 174.66666666666615
    agent-5: 174.66666666666615
  policy_reward_mean:
    agent-0: 149.2333333333334
    agent-1: 149.2333333333334
    agent-2: 149.2333333333334
    agent-3: 149.2333333333334
    agent-4: 149.2333333333334
    agent-5: 149.2333333333334
  policy_reward_min:
    agent-0: 103.50000000000055
    agent-1: 103.50000000000055
    agent-2: 103.50000000000055
    agent-3: 103.50000000000055
    agent-4: 103.50000000000055
    agent-5: 103.50000000000055
  sampler_perf:
    mean_env_wait_ms: 27.125822295644298
    mean_inference_ms: 12.915472662791114
    mean_processing_ms: 57.85177317410983
  time_since_restore: 25154.751558303833
  time_this_iter_s: 124.25760412216187
  time_total_s: 34280.763372182846
  timestamp: 1637052260
  timesteps_since_restore: 18144000
  timesteps_this_iter: 96000
  timesteps_total: 23904000
  training_iteration: 249
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    249 |          34280.8 | 23904000 |    895.4 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 1.88
    apples_agent-0_min: 0
    apples_agent-1_max: 147
    apples_agent-1_mean: 36.42
    apples_agent-1_min: 0
    apples_agent-2_max: 80
    apples_agent-2_mean: 9.59
    apples_agent-2_min: 0
    apples_agent-3_max: 133
    apples_agent-3_mean: 73.71
    apples_agent-3_min: 41
    apples_agent-4_max: 71
    apples_agent-4_mean: 1.58
    apples_agent-4_min: 0
    apples_agent-5_max: 176
    apples_agent-5_mean: 98.2
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 513
    cleaning_beam_agent-0_mean: 441.14
    cleaning_beam_agent-0_min: 328
    cleaning_beam_agent-1_max: 521
    cleaning_beam_agent-1_mean: 254.32
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 537
    cleaning_beam_agent-2_mean: 354.72
    cleaning_beam_agent-2_min: 130
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 23.52
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 545
    cleaning_beam_agent-4_mean: 438.37
    cleaning_beam_agent-4_min: 265
    cleaning_beam_agent-5_max: 136
    cleaning_beam_agent-5_mean: 51.04
    cleaning_beam_agent-5_min: 18
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-46-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1029.9999999999661
  episode_reward_mean: 890.3799999999824
  episode_reward_min: 537.0000000000026
  episodes_this_iter: 96
  episodes_total: 24000
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11506.119
    learner:
      agent-0:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9305031299591064
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011758054606616497
        model: {}
        policy_loss: -0.0030791847966611385
        total_loss: -0.0030441870912909508
        vf_explained_var: 0.02080395817756653
        vf_loss: 16.726810455322266
      agent-1:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1302449703216553
        entropy_coeff: 0.0017600000137463212
        kl: 0.001545413164421916
        model: {}
        policy_loss: -0.0041506970301270485
        total_loss: -0.004231966100633144
        vf_explained_var: -0.07002055644989014
        vf_loss: 19.079631805419922
      agent-2:
        cur_kl_coeff: 7.10542746348012e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.065926194190979
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017059561796486378
        model: {}
        policy_loss: -0.0038808463141322136
        total_loss: -0.0039799814112484455
        vf_explained_var: -0.02415454387664795
        vf_loss: 17.768924713134766
      agent-3:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4857059121131897
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012346894945949316
        model: {}
        policy_loss: -0.002506075892597437
        total_loss: -0.0017415713518857956
        vf_explained_var: 0.05628708004951477
        vf_loss: 16.193477630615234
      agent-4:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9714949131011963
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015858437400311232
        model: {}
        policy_loss: -0.00371957803145051
        total_loss: -0.003659062087535858
        vf_explained_var: -0.020659178495407104
        vf_loss: 17.703487396240234
      agent-5:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8591914176940918
        entropy_coeff: 0.0017600000137463212
        kl: 0.001823871280066669
        model: {}
        policy_loss: -0.003741808235645294
        total_loss: -0.0036358246579766273
        vf_explained_var: 0.054476022720336914
        vf_loss: 16.181610107421875
    load_time_ms: 13386.136
    num_steps_sampled: 24000000
    num_steps_trained: 24000000
    sample_time_ms: 99454.743
    update_time_ms: 14.629
  iterations_since_restore: 190
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.461797752808994
    ram_util_percent: 16.192696629213483
  pid: 30948
  policy_reward_max:
    agent-0: 171.66666666666615
    agent-1: 171.66666666666615
    agent-2: 171.66666666666615
    agent-3: 171.66666666666615
    agent-4: 171.66666666666615
    agent-5: 171.66666666666615
  policy_reward_mean:
    agent-0: 148.39666666666682
    agent-1: 148.39666666666682
    agent-2: 148.39666666666682
    agent-3: 148.39666666666682
    agent-4: 148.39666666666682
    agent-5: 148.39666666666682
  policy_reward_min:
    agent-0: 89.50000000000004
    agent-1: 89.50000000000004
    agent-2: 89.50000000000004
    agent-3: 89.50000000000004
    agent-4: 89.50000000000004
    agent-5: 89.50000000000004
  sampler_perf:
    mean_env_wait_ms: 27.127727155309504
    mean_inference_ms: 12.914504116596683
    mean_processing_ms: 57.84502086632367
  time_since_restore: 25279.53447842598
  time_this_iter_s: 124.7829201221466
  time_total_s: 34405.54629230499
  timestamp: 1637052385
  timesteps_since_restore: 18240000
  timesteps_this_iter: 96000
  timesteps_total: 24000000
  training_iteration: 250
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    250 |          34405.5 | 24000000 |   890.38 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 50
    apples_agent-0_mean: 2.18
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 36.44
    apples_agent-1_min: 0
    apples_agent-2_max: 106
    apples_agent-2_mean: 8.84
    apples_agent-2_min: 0
    apples_agent-3_max: 125
    apples_agent-3_mean: 76.72
    apples_agent-3_min: 37
    apples_agent-4_max: 91
    apples_agent-4_mean: 3.8
    apples_agent-4_min: 0
    apples_agent-5_max: 180
    apples_agent-5_mean: 95.79
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 561
    cleaning_beam_agent-0_mean: 441.31
    cleaning_beam_agent-0_min: 329
    cleaning_beam_agent-1_max: 401
    cleaning_beam_agent-1_mean: 239.57
    cleaning_beam_agent-1_min: 140
    cleaning_beam_agent-2_max: 636
    cleaning_beam_agent-2_mean: 389.86
    cleaning_beam_agent-2_min: 150
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 23.81
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 513
    cleaning_beam_agent-4_mean: 433.78
    cleaning_beam_agent-4_min: 279
    cleaning_beam_agent-5_max: 244
    cleaning_beam_agent-5_mean: 51.89
    cleaning_beam_agent-5_min: 14
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.02
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-48-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1047.0000000000005
  episode_reward_mean: 884.7799999999847
  episode_reward_min: 468.0000000000105
  episodes_this_iter: 96
  episodes_total: 24096
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11523.129
    learner:
      agent-0:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9169176816940308
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013445708900690079
        model: {}
        policy_loss: -0.003205442102625966
        total_loss: -0.003098082495853305
        vf_explained_var: 0.0776880532503128
        vf_loss: 17.21137809753418
      agent-1:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1305867433547974
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014581735013052821
        model: {}
        policy_loss: -0.0037598474882543087
        total_loss: -0.003676941618323326
        vf_explained_var: -0.08242055773735046
        vf_loss: 20.72738265991211
      agent-2:
        cur_kl_coeff: 3.55271373174006e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0608606338500977
        entropy_coeff: 0.0017600000137463212
        kl: 0.001306320889852941
        model: {}
        policy_loss: -0.0034370645880699158
        total_loss: -0.0033883508294820786
        vf_explained_var: -0.0213242769241333
        vf_loss: 19.158267974853516
      agent-3:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5104026198387146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011772055877372622
        model: {}
        policy_loss: -0.0029602027498185635
        total_loss: -0.0021460314746946096
        vf_explained_var: 0.08633317053318024
        vf_loss: 17.124794006347656
      agent-4:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9707711338996887
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012928800424560905
        model: {}
        policy_loss: -0.003962114453315735
        total_loss: -0.0038407028187066317
        vf_explained_var: 0.032168373465538025
        vf_loss: 18.299699783325195
      agent-5:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8475584983825684
        entropy_coeff: 0.0017600000137463212
        kl: 0.001210773945786059
        model: {}
        policy_loss: -0.0036887340247631073
        total_loss: -0.0034430979285389185
        vf_explained_var: 0.06926622986793518
        vf_loss: 17.373350143432617
    load_time_ms: 13395.19
    num_steps_sampled: 24096000
    num_steps_trained: 24096000
    sample_time_ms: 99338.224
    update_time_ms: 14.559
  iterations_since_restore: 191
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.573863636363637
    ram_util_percent: 16.20852272727273
  pid: 30948
  policy_reward_max:
    agent-0: 174.50000000000006
    agent-1: 174.50000000000006
    agent-2: 174.50000000000006
    agent-3: 174.50000000000006
    agent-4: 174.50000000000006
    agent-5: 174.50000000000006
  policy_reward_mean:
    agent-0: 147.46333333333337
    agent-1: 147.46333333333337
    agent-2: 147.46333333333337
    agent-3: 147.46333333333337
    agent-4: 147.46333333333337
    agent-5: 147.46333333333337
  policy_reward_min:
    agent-0: 78.00000000000004
    agent-1: 78.00000000000004
    agent-2: 78.00000000000004
    agent-3: 78.00000000000004
    agent-4: 78.00000000000004
    agent-5: 78.00000000000004
  sampler_perf:
    mean_env_wait_ms: 27.127943063866432
    mean_inference_ms: 12.913379448714045
    mean_processing_ms: 57.8342600495058
  time_since_restore: 25402.951062202454
  time_this_iter_s: 123.416583776474
  time_total_s: 34528.96287608147
  timestamp: 1637052509
  timesteps_since_restore: 18336000
  timesteps_this_iter: 96000
  timesteps_total: 24096000
  training_iteration: 251
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    251 |            34529 | 24096000 |   884.78 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 1.36
    apples_agent-0_min: 0
    apples_agent-1_max: 141
    apples_agent-1_mean: 29.71
    apples_agent-1_min: 0
    apples_agent-2_max: 96
    apples_agent-2_mean: 9.62
    apples_agent-2_min: 0
    apples_agent-3_max: 140
    apples_agent-3_mean: 73.13
    apples_agent-3_min: 29
    apples_agent-4_max: 24
    apples_agent-4_mean: 1.07
    apples_agent-4_min: 0
    apples_agent-5_max: 179
    apples_agent-5_mean: 96.01
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 611
    cleaning_beam_agent-0_mean: 450.29
    cleaning_beam_agent-0_min: 322
    cleaning_beam_agent-1_max: 478
    cleaning_beam_agent-1_mean: 260.62
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 631
    cleaning_beam_agent-2_mean: 386.1
    cleaning_beam_agent-2_min: 169
    cleaning_beam_agent-3_max: 68
    cleaning_beam_agent-3_mean: 22.91
    cleaning_beam_agent-3_min: 2
    cleaning_beam_agent-4_max: 534
    cleaning_beam_agent-4_mean: 441.05
    cleaning_beam_agent-4_min: 313
    cleaning_beam_agent-5_max: 215
    cleaning_beam_agent-5_mean: 50.12
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-50-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1062.9999999999932
  episode_reward_mean: 914.3999999999813
  episode_reward_min: 593.9999999999974
  episodes_this_iter: 96
  episodes_total: 24192
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11516.412
    learner:
      agent-0:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9068511128425598
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013542997185140848
        model: {}
        policy_loss: -0.0030324766412377357
        total_loss: -0.0028832373209297657
        vf_explained_var: 0.042952656745910645
        vf_loss: 17.452964782714844
      agent-1:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.128601312637329
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014422957319766283
        model: {}
        policy_loss: -0.003965240903198719
        total_loss: -0.003904645796865225
        vf_explained_var: -0.07725554704666138
        vf_loss: 20.46930694580078
      agent-2:
        cur_kl_coeff: 1.77635686587003e-16
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0525131225585938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016998626524582505
        model: {}
        policy_loss: -0.0037406948395073414
        total_loss: -0.003739236621186137
        vf_explained_var: -0.0003087371587753296
        vf_loss: 18.538806915283203
      agent-3:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47899308800697327
        entropy_coeff: 0.0017600000137463212
        kl: 0.000994484405964613
        model: {}
        policy_loss: -0.0024565551429986954
        total_loss: -0.0015610018745064735
        vf_explained_var: 0.05023057758808136
        vf_loss: 17.38580894470215
      agent-4:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9667932391166687
        entropy_coeff: 0.0017600000137463212
        kl: 0.001856347662396729
        model: {}
        policy_loss: -0.004015750251710415
        total_loss: -0.0038095531053841114
        vf_explained_var: -0.023563921451568604
        vf_loss: 19.07756233215332
      agent-5:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8403702974319458
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015264211688190699
        model: {}
        policy_loss: -0.003890246618539095
        total_loss: -0.0036829495802521706
        vf_explained_var: 0.07142321765422821
        vf_loss: 16.863483428955078
    load_time_ms: 13389.776
    num_steps_sampled: 24192000
    num_steps_trained: 24192000
    sample_time_ms: 99310.831
    update_time_ms: 14.155
  iterations_since_restore: 192
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.47692307692308
    ram_util_percent: 16.136686390532546
  pid: 30948
  policy_reward_max:
    agent-0: 177.16666666666666
    agent-1: 177.16666666666666
    agent-2: 177.16666666666666
    agent-3: 177.16666666666666
    agent-4: 177.16666666666666
    agent-5: 177.16666666666666
  policy_reward_mean:
    agent-0: 152.39999999999998
    agent-1: 152.39999999999998
    agent-2: 152.39999999999998
    agent-3: 152.39999999999998
    agent-4: 152.39999999999998
    agent-5: 152.39999999999998
  policy_reward_min:
    agent-0: 99.00000000000023
    agent-1: 99.00000000000023
    agent-2: 99.00000000000023
    agent-3: 99.00000000000023
    agent-4: 99.00000000000023
    agent-5: 99.00000000000023
  sampler_perf:
    mean_env_wait_ms: 27.13075798261504
    mean_inference_ms: 12.912365462242251
    mean_processing_ms: 57.82707218198606
  time_since_restore: 25527.21004796028
  time_this_iter_s: 124.25898575782776
  time_total_s: 34653.221861839294
  timestamp: 1637052633
  timesteps_since_restore: 18432000
  timesteps_this_iter: 96000
  timesteps_total: 24192000
  training_iteration: 252
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    252 |          34653.2 | 24192000 |    914.4 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 1.78
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 29.18
    apples_agent-1_min: 0
    apples_agent-2_max: 134
    apples_agent-2_mean: 8.4
    apples_agent-2_min: 0
    apples_agent-3_max: 136
    apples_agent-3_mean: 76.24
    apples_agent-3_min: 27
    apples_agent-4_max: 109
    apples_agent-4_mean: 1.91
    apples_agent-4_min: 0
    apples_agent-5_max: 199
    apples_agent-5_mean: 96.52
    apples_agent-5_min: 53
    cleaning_beam_agent-0_max: 540
    cleaning_beam_agent-0_mean: 451.8
    cleaning_beam_agent-0_min: 359
    cleaning_beam_agent-1_max: 455
    cleaning_beam_agent-1_mean: 272.88
    cleaning_beam_agent-1_min: 92
    cleaning_beam_agent-2_max: 615
    cleaning_beam_agent-2_mean: 370.81
    cleaning_beam_agent-2_min: 109
    cleaning_beam_agent-3_max: 72
    cleaning_beam_agent-3_mean: 23.23
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 441.85
    cleaning_beam_agent-4_min: 304
    cleaning_beam_agent-5_max: 139
    cleaning_beam_agent-5_mean: 49.69
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-52-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1059.9999999999861
  episode_reward_mean: 909.8699999999828
  episode_reward_min: 477.00000000000136
  episodes_this_iter: 96
  episodes_total: 24288
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11536.304
    learner:
      agent-0:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9206436276435852
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013466770760715008
        model: {}
        policy_loss: -0.00308429216966033
        total_loss: -0.002931789727881551
        vf_explained_var: 0.03525649011135101
        vf_loss: 17.72832489013672
      agent-1:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1365966796875
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015394346555694938
        model: {}
        policy_loss: -0.003917856607586145
        total_loss: -0.0038496479392051697
        vf_explained_var: -0.08215370774269104
        vf_loss: 20.686187744140625
      agent-2:
        cur_kl_coeff: 8.88178432935015e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0662331581115723
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017231416422873735
        model: {}
        policy_loss: -0.0032132272608578205
        total_loss: -0.003208721987903118
        vf_explained_var: -0.009988754987716675
        vf_loss: 18.81077003479004
      agent-3:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4893932640552521
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010343249887228012
        model: {}
        policy_loss: -0.002515612170100212
        total_loss: -0.001671348698437214
        vf_explained_var: 0.07758565247058868
        vf_loss: 17.055978775024414
      agent-4:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.962980329990387
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015726102283224463
        model: {}
        policy_loss: -0.0038330669049173594
        total_loss: -0.003655422246083617
        vf_explained_var: -0.0046754032373428345
        vf_loss: 18.72492027282715
      agent-5:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8338610529899597
        entropy_coeff: 0.0017600000137463212
        kl: 0.001076213549822569
        model: {}
        policy_loss: -0.0032795462757349014
        total_loss: -0.003080288879573345
        vf_explained_var: 0.09035931527614594
        vf_loss: 16.66851043701172
    load_time_ms: 13388.897
    num_steps_sampled: 24288000
    num_steps_trained: 24288000
    sample_time_ms: 99589.286
    update_time_ms: 14.214
  iterations_since_restore: 193
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.150555555555552
    ram_util_percent: 16.145555555555557
  pid: 30948
  policy_reward_max:
    agent-0: 176.66666666666632
    agent-1: 176.66666666666632
    agent-2: 176.66666666666632
    agent-3: 176.66666666666632
    agent-4: 176.66666666666632
    agent-5: 176.66666666666632
  policy_reward_mean:
    agent-0: 151.645
    agent-1: 151.645
    agent-2: 151.645
    agent-3: 151.645
    agent-4: 151.645
    agent-5: 151.645
  policy_reward_min:
    agent-0: 79.50000000000014
    agent-1: 79.50000000000014
    agent-2: 79.50000000000014
    agent-3: 79.50000000000014
    agent-4: 79.50000000000014
    agent-5: 79.50000000000014
  sampler_perf:
    mean_env_wait_ms: 27.13432598703642
    mean_inference_ms: 12.911672708218019
    mean_processing_ms: 57.82118840979777
  time_since_restore: 25653.594510555267
  time_this_iter_s: 126.38446259498596
  time_total_s: 34779.60632443428
  timestamp: 1637052760
  timesteps_since_restore: 18528000
  timesteps_this_iter: 96000
  timesteps_total: 24288000
  training_iteration: 253
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    253 |          34779.6 | 24288000 |   909.87 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 2.55
    apples_agent-0_min: 0
    apples_agent-1_max: 124
    apples_agent-1_mean: 34.79
    apples_agent-1_min: 0
    apples_agent-2_max: 66
    apples_agent-2_mean: 9.49
    apples_agent-2_min: 0
    apples_agent-3_max: 118
    apples_agent-3_mean: 74.45
    apples_agent-3_min: 27
    apples_agent-4_max: 38
    apples_agent-4_mean: 1.73
    apples_agent-4_min: 0
    apples_agent-5_max: 187
    apples_agent-5_mean: 94.05
    apples_agent-5_min: 26
    cleaning_beam_agent-0_max: 568
    cleaning_beam_agent-0_mean: 454.25
    cleaning_beam_agent-0_min: 336
    cleaning_beam_agent-1_max: 424
    cleaning_beam_agent-1_mean: 259.7
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 682
    cleaning_beam_agent-2_mean: 367.2
    cleaning_beam_agent-2_min: 124
    cleaning_beam_agent-3_max: 66
    cleaning_beam_agent-3_mean: 23.32
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 522
    cleaning_beam_agent-4_mean: 446.98
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 189
    cleaning_beam_agent-5_mean: 51.62
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-54-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1063.9999999999948
  episode_reward_mean: 894.6499999999838
  episode_reward_min: 475.0000000000045
  episodes_this_iter: 96
  episodes_total: 24384
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11531.484
    learner:
      agent-0:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.923719584941864
        entropy_coeff: 0.0017600000137463212
        kl: 0.001572420122101903
        model: {}
        policy_loss: -0.0033936991821974516
        total_loss: -0.0032959459349513054
        vf_explained_var: 0.0986512154340744
        vf_loss: 17.234973907470703
      agent-1:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.135810136795044
        entropy_coeff: 0.0017600000137463212
        kl: 0.001489507034420967
        model: {}
        policy_loss: -0.0038843799848109484
        total_loss: -0.0037827743217349052
        vf_explained_var: -0.07397222518920898
        vf_loss: 21.006290435791016
      agent-2:
        cur_kl_coeff: 4.440892164675075e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.053009271621704
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020515858195722103
        model: {}
        policy_loss: -0.0036807162687182426
        total_loss: -0.0036258939653635025
        vf_explained_var: 0.010170891880989075
        vf_loss: 19.081192016601562
      agent-3:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48903268575668335
        entropy_coeff: 0.0017600000137463212
        kl: 0.001065294025465846
        model: {}
        policy_loss: -0.002659186255186796
        total_loss: -0.001820507925003767
        vf_explained_var: 0.11320091784000397
        vf_loss: 16.993736267089844
      agent-4:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9624519348144531
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013345510233193636
        model: {}
        policy_loss: -0.003977093379944563
        total_loss: -0.0037656514905393124
        vf_explained_var: 0.007574036717414856
        vf_loss: 19.053585052490234
      agent-5:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8447154760360718
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012829892802983522
        model: {}
        policy_loss: -0.0035765613429248333
        total_loss: -0.0033396875951439142
        vf_explained_var: 0.10289724171161652
        vf_loss: 17.23573875427246
    load_time_ms: 13390.137
    num_steps_sampled: 24384000
    num_steps_trained: 24384000
    sample_time_ms: 99578.373
    update_time_ms: 14.228
  iterations_since_restore: 194
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.422471910112364
    ram_util_percent: 16.110112359550563
  pid: 30948
  policy_reward_max:
    agent-0: 177.33333333333348
    agent-1: 177.33333333333348
    agent-2: 177.33333333333348
    agent-3: 177.33333333333348
    agent-4: 177.33333333333348
    agent-5: 177.33333333333348
  policy_reward_mean:
    agent-0: 149.10833333333335
    agent-1: 149.10833333333335
    agent-2: 149.10833333333335
    agent-3: 149.10833333333335
    agent-4: 149.10833333333335
    agent-5: 149.10833333333335
  policy_reward_min:
    agent-0: 79.16666666666664
    agent-1: 79.16666666666664
    agent-2: 79.16666666666664
    agent-3: 79.16666666666664
    agent-4: 79.16666666666664
    agent-5: 79.16666666666664
  sampler_perf:
    mean_env_wait_ms: 27.136874165010077
    mean_inference_ms: 12.910981966057028
    mean_processing_ms: 57.81566418318027
  time_since_restore: 25778.66627550125
  time_this_iter_s: 125.07176494598389
  time_total_s: 34904.678089380264
  timestamp: 1637052885
  timesteps_since_restore: 18624000
  timesteps_this_iter: 96000
  timesteps_total: 24384000
  training_iteration: 254
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    254 |          34904.7 | 24384000 |   894.65 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 1.65
    apples_agent-0_min: 0
    apples_agent-1_max: 120
    apples_agent-1_mean: 29.78
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 7.45
    apples_agent-2_min: 0
    apples_agent-3_max: 135
    apples_agent-3_mean: 72.85
    apples_agent-3_min: 38
    apples_agent-4_max: 39
    apples_agent-4_mean: 1.99
    apples_agent-4_min: 0
    apples_agent-5_max: 240
    apples_agent-5_mean: 100.63
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 559
    cleaning_beam_agent-0_mean: 452.75
    cleaning_beam_agent-0_min: 327
    cleaning_beam_agent-1_max: 447
    cleaning_beam_agent-1_mean: 267.87
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 595
    cleaning_beam_agent-2_mean: 346.23
    cleaning_beam_agent-2_min: 141
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 22.4
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 535
    cleaning_beam_agent-4_mean: 429.98
    cleaning_beam_agent-4_min: 300
    cleaning_beam_agent-5_max: 184
    cleaning_beam_agent-5_mean: 41.96
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-56-49
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1050.999999999984
  episode_reward_mean: 913.7799999999824
  episode_reward_min: 558.0000000000039
  episodes_this_iter: 96
  episodes_total: 24480
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11537.472
    learner:
      agent-0:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9281430244445801
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012219517957419157
        model: {}
        policy_loss: -0.0033720890060067177
        total_loss: -0.003172527067363262
        vf_explained_var: 0.03270047903060913
        vf_loss: 18.330917358398438
      agent-1:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1570936441421509
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014076733496040106
        model: {}
        policy_loss: -0.003884405829012394
        total_loss: -0.003799170721322298
        vf_explained_var: -0.09777918457984924
        vf_loss: 21.217220306396484
      agent-2:
        cur_kl_coeff: 2.2204460823375376e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0804170370101929
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015238001942634583
        model: {}
        policy_loss: -0.0034252137411385775
        total_loss: -0.0033500725403428078
        vf_explained_var: -0.02890223264694214
        vf_loss: 19.766782760620117
      agent-3:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4879635274410248
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010715192183852196
        model: {}
        policy_loss: -0.0026771817356348038
        total_loss: -0.0017453450709581375
        vf_explained_var: 0.0565209835767746
        vf_loss: 17.906530380249023
      agent-4:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9801118969917297
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019274683436378837
        model: {}
        policy_loss: -0.003940235823392868
        total_loss: -0.003723105415701866
        vf_explained_var: -0.011929690837860107
        vf_loss: 19.42127799987793
      agent-5:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8127572536468506
        entropy_coeff: 0.0017600000137463212
        kl: 0.002142761368304491
        model: {}
        policy_loss: -0.003732391633093357
        total_loss: -0.003484925255179405
        vf_explained_var: 0.1135890930891037
        vf_loss: 16.779224395751953
    load_time_ms: 13387.436
    num_steps_sampled: 24480000
    num_steps_trained: 24480000
    sample_time_ms: 99452.658
    update_time_ms: 14.065
  iterations_since_restore: 195
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.53125
    ram_util_percent: 16.194886363636364
  pid: 30948
  policy_reward_max:
    agent-0: 175.16666666666688
    agent-1: 175.16666666666688
    agent-2: 175.16666666666688
    agent-3: 175.16666666666688
    agent-4: 175.16666666666688
    agent-5: 175.16666666666688
  policy_reward_mean:
    agent-0: 152.29666666666662
    agent-1: 152.29666666666662
    agent-2: 152.29666666666662
    agent-3: 152.29666666666662
    agent-4: 152.29666666666662
    agent-5: 152.29666666666662
  policy_reward_min:
    agent-0: 93.00000000000051
    agent-1: 93.00000000000051
    agent-2: 93.00000000000051
    agent-3: 93.00000000000051
    agent-4: 93.00000000000051
    agent-5: 93.00000000000051
  sampler_perf:
    mean_env_wait_ms: 27.136931433964897
    mean_inference_ms: 12.909818607776424
    mean_processing_ms: 57.80714011153859
  time_since_restore: 25902.466396331787
  time_this_iter_s: 123.80012083053589
  time_total_s: 35028.4782102108
  timestamp: 1637053009
  timesteps_since_restore: 18720000
  timesteps_this_iter: 96000
  timesteps_total: 24480000
  training_iteration: 255
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    255 |          35028.5 | 24480000 |   913.78 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 2.2
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 26.05
    apples_agent-1_min: 0
    apples_agent-2_max: 63
    apples_agent-2_mean: 9.31
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 74.42
    apples_agent-3_min: 38
    apples_agent-4_max: 36
    apples_agent-4_mean: 1.57
    apples_agent-4_min: 0
    apples_agent-5_max: 196
    apples_agent-5_mean: 94.64
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 589
    cleaning_beam_agent-0_mean: 443.35
    cleaning_beam_agent-0_min: 268
    cleaning_beam_agent-1_max: 470
    cleaning_beam_agent-1_mean: 267.58
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 556
    cleaning_beam_agent-2_mean: 336.95
    cleaning_beam_agent-2_min: 149
    cleaning_beam_agent-3_max: 47
    cleaning_beam_agent-3_mean: 22.21
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 543
    cleaning_beam_agent-4_mean: 426.68
    cleaning_beam_agent-4_min: 320
    cleaning_beam_agent-5_max: 202
    cleaning_beam_agent-5_mean: 44.86
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_03-58-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1141.9999999999866
  episode_reward_mean: 915.7599999999821
  episode_reward_min: 582.0000000000089
  episodes_this_iter: 96
  episodes_total: 24576
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11541.453
    learner:
      agent-0:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9269263744354248
        entropy_coeff: 0.0017600000137463212
        kl: 0.001559827127493918
        model: {}
        policy_loss: -0.0029725944623351097
        total_loss: -0.002867002971470356
        vf_explained_var: 0.08247125148773193
        vf_loss: 17.369800567626953
      agent-1:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1379340887069702
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014615037944167852
        model: {}
        policy_loss: -0.0038369542453438044
        total_loss: -0.0037368321791291237
        vf_explained_var: -0.0760343074798584
        vf_loss: 21.02885627746582
      agent-2:
        cur_kl_coeff: 1.1102230411687688e-17
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0885469913482666
        entropy_coeff: 0.0017600000137463212
        kl: 0.002550280885770917
        model: {}
        policy_loss: -0.003773915581405163
        total_loss: -0.003800017759203911
        vf_explained_var: 0.020120680332183838
        vf_loss: 18.897415161132812
      agent-3:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4923567771911621
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009414810338057578
        model: {}
        policy_loss: -0.002496792934834957
        total_loss: -0.001596076413989067
        vf_explained_var: 0.06942315399646759
        vf_loss: 17.672630310058594
      agent-4:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9803407788276672
        entropy_coeff: 0.0017600000137463212
        kl: 0.001586823258548975
        model: {}
        policy_loss: -0.003764938795939088
        total_loss: -0.0035232841037213802
        vf_explained_var: -0.01892666518688202
        vf_loss: 19.670541763305664
      agent-5:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8213711977005005
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017825323157012463
        model: {}
        policy_loss: -0.003787222784012556
        total_loss: -0.0034991849679499865
        vf_explained_var: 0.09105575084686279
        vf_loss: 17.336519241333008
    load_time_ms: 13389.72
    num_steps_sampled: 24576000
    num_steps_trained: 24576000
    sample_time_ms: 99424.443
    update_time_ms: 13.941
  iterations_since_restore: 196
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.379661016949157
    ram_util_percent: 16.19774011299435
  pid: 30948
  policy_reward_max:
    agent-0: 190.33333333333317
    agent-1: 190.33333333333317
    agent-2: 190.33333333333317
    agent-3: 190.33333333333317
    agent-4: 190.33333333333317
    agent-5: 190.33333333333317
  policy_reward_mean:
    agent-0: 152.62666666666664
    agent-1: 152.62666666666664
    agent-2: 152.62666666666664
    agent-3: 152.62666666666664
    agent-4: 152.62666666666664
    agent-5: 152.62666666666664
  policy_reward_min:
    agent-0: 97.00000000000033
    agent-1: 97.00000000000033
    agent-2: 97.00000000000033
    agent-3: 97.00000000000033
    agent-4: 97.00000000000033
    agent-5: 97.00000000000033
  sampler_perf:
    mean_env_wait_ms: 27.137589584180414
    mean_inference_ms: 12.908688875406485
    mean_processing_ms: 57.79910520383805
  time_since_restore: 26026.875464439392
  time_this_iter_s: 124.40906810760498
  time_total_s: 35152.887278318405
  timestamp: 1637053133
  timesteps_since_restore: 18816000
  timesteps_this_iter: 96000
  timesteps_total: 24576000
  training_iteration: 256
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    256 |          35152.9 | 24576000 |   915.76 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 1.02
    apples_agent-0_min: 0
    apples_agent-1_max: 86
    apples_agent-1_mean: 28.0
    apples_agent-1_min: 0
    apples_agent-2_max: 71
    apples_agent-2_mean: 11.43
    apples_agent-2_min: 0
    apples_agent-3_max: 120
    apples_agent-3_mean: 79.77
    apples_agent-3_min: 46
    apples_agent-4_max: 42
    apples_agent-4_mean: 0.62
    apples_agent-4_min: 0
    apples_agent-5_max: 206
    apples_agent-5_mean: 97.83
    apples_agent-5_min: 56
    cleaning_beam_agent-0_max: 542
    cleaning_beam_agent-0_mean: 439.86
    cleaning_beam_agent-0_min: 335
    cleaning_beam_agent-1_max: 489
    cleaning_beam_agent-1_mean: 268.65
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 535
    cleaning_beam_agent-2_mean: 325.59
    cleaning_beam_agent-2_min: 114
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 21.35
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 501
    cleaning_beam_agent-4_mean: 425.45
    cleaning_beam_agent-4_min: 325
    cleaning_beam_agent-5_max: 163
    cleaning_beam_agent-5_mean: 43.01
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-00-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1058.9999999999825
  episode_reward_mean: 912.8499999999824
  episode_reward_min: 492.00000000001154
  episodes_this_iter: 96
  episodes_total: 24672
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11582.611
    learner:
      agent-0:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.954883873462677
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016651764744892716
        model: {}
        policy_loss: -0.0032524503767490387
        total_loss: -0.003303603269159794
        vf_explained_var: 0.03146974742412567
        vf_loss: 16.294414520263672
      agent-1:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1444615125656128
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014897899236530066
        model: {}
        policy_loss: -0.003928559832274914
        total_loss: -0.004008496180176735
        vf_explained_var: -0.10626578330993652
        vf_loss: 19.343185424804688
      agent-2:
        cur_kl_coeff: 5.551115205843844e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.094996690750122
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019329129718244076
        model: {}
        policy_loss: -0.0036286627873778343
        total_loss: -0.0038199378177523613
        vf_explained_var: -0.0016488581895828247
        vf_loss: 17.359195709228516
      agent-3:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4986722469329834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011719856411218643
        model: {}
        policy_loss: -0.0026807172689586878
        total_loss: -0.001942646224051714
        vf_explained_var: 0.04630307853221893
        vf_loss: 16.15736961364746
      agent-4:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9850491881370544
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016609287122264504
        model: {}
        policy_loss: -0.003725834656506777
        total_loss: -0.0037487742956727743
        vf_explained_var: -0.0001862049102783203
        vf_loss: 17.107471466064453
      agent-5:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7970111966133118
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016950664576143026
        model: {}
        policy_loss: -0.0033385129645466805
        total_loss: -0.0031146658584475517
        vf_explained_var: 0.037799179553985596
        vf_loss: 16.26588249206543
    load_time_ms: 13369.479
    num_steps_sampled: 24672000
    num_steps_trained: 24672000
    sample_time_ms: 99463.074
    update_time_ms: 14.02
  iterations_since_restore: 197
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.5728813559322
    ram_util_percent: 16.19774011299435
  pid: 30948
  policy_reward_max:
    agent-0: 176.4999999999996
    agent-1: 176.4999999999996
    agent-2: 176.4999999999996
    agent-3: 176.4999999999996
    agent-4: 176.4999999999996
    agent-5: 176.4999999999996
  policy_reward_mean:
    agent-0: 152.14166666666665
    agent-1: 152.14166666666665
    agent-2: 152.14166666666665
    agent-3: 152.14166666666665
    agent-4: 152.14166666666665
    agent-5: 152.14166666666665
  policy_reward_min:
    agent-0: 82.0000000000001
    agent-1: 82.0000000000001
    agent-2: 82.0000000000001
    agent-3: 82.0000000000001
    agent-4: 82.0000000000001
    agent-5: 82.0000000000001
  sampler_perf:
    mean_env_wait_ms: 27.139251522835547
    mean_inference_ms: 12.908079932334058
    mean_processing_ms: 57.7936009661164
  time_since_restore: 26151.36888408661
  time_this_iter_s: 124.4934196472168
  time_total_s: 35277.38069796562
  timestamp: 1637053258
  timesteps_since_restore: 18912000
  timesteps_this_iter: 96000
  timesteps_total: 24672000
  training_iteration: 257
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    257 |          35277.4 | 24672000 |   912.85 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 23
    apples_agent-0_mean: 0.74
    apples_agent-0_min: 0
    apples_agent-1_max: 101
    apples_agent-1_mean: 30.99
    apples_agent-1_min: 0
    apples_agent-2_max: 63
    apples_agent-2_mean: 8.15
    apples_agent-2_min: 0
    apples_agent-3_max: 126
    apples_agent-3_mean: 77.08
    apples_agent-3_min: 42
    apples_agent-4_max: 72
    apples_agent-4_mean: 1.45
    apples_agent-4_min: 0
    apples_agent-5_max: 206
    apples_agent-5_mean: 94.18
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 533
    cleaning_beam_agent-0_mean: 431.47
    cleaning_beam_agent-0_min: 268
    cleaning_beam_agent-1_max: 523
    cleaning_beam_agent-1_mean: 270.38
    cleaning_beam_agent-1_min: 135
    cleaning_beam_agent-2_max: 554
    cleaning_beam_agent-2_mean: 306.79
    cleaning_beam_agent-2_min: 142
    cleaning_beam_agent-3_max: 67
    cleaning_beam_agent-3_mean: 23.19
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 535
    cleaning_beam_agent-4_mean: 431.11
    cleaning_beam_agent-4_min: 275
    cleaning_beam_agent-5_max: 173
    cleaning_beam_agent-5_mean: 42.18
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-03-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1069.999999999986
  episode_reward_mean: 917.3299999999829
  episode_reward_min: 387.0000000000007
  episodes_this_iter: 96
  episodes_total: 24768
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11553.953
    learner:
      agent-0:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9633350968360901
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014035823987796903
        model: {}
        policy_loss: -0.003422763664275408
        total_loss: -0.003364166710525751
        vf_explained_var: 0.03154054284095764
        vf_loss: 17.540668487548828
      agent-1:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1527881622314453
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018033741507679224
        model: {}
        policy_loss: -0.003794962540268898
        total_loss: -0.003789333626627922
        vf_explained_var: -0.08028805255889893
        vf_loss: 20.345396041870117
      agent-2:
        cur_kl_coeff: 2.775557602921922e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.115086555480957
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013275983510538936
        model: {}
        policy_loss: -0.0034595238976180553
        total_loss: -0.0035233586095273495
        vf_explained_var: -0.023693576455116272
        vf_loss: 18.987186431884766
      agent-3:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48748594522476196
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010785525664687157
        model: {}
        policy_loss: -0.002723882906138897
        total_loss: -0.0018917550332844257
        vf_explained_var: 0.0739331841468811
        vf_loss: 16.901046752929688
      agent-4:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9787998199462891
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016340838046744466
        model: {}
        policy_loss: -0.00396973080933094
        total_loss: -0.003873987589031458
        vf_explained_var: 0.01642291247844696
        vf_loss: 18.18431282043457
      agent-5:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7882615327835083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009836226236075163
        model: {}
        policy_loss: -0.0031220726668834686
        total_loss: -0.0027888689655810595
        vf_explained_var: 0.052309706807136536
        vf_loss: 17.205429077148438
    load_time_ms: 13376.539
    num_steps_sampled: 24768000
    num_steps_trained: 24768000
    sample_time_ms: 99525.59
    update_time_ms: 14.602
  iterations_since_restore: 198
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.327528089887643
    ram_util_percent: 16.200561797752812
  pid: 30948
  policy_reward_max:
    agent-0: 178.33333333333317
    agent-1: 178.33333333333317
    agent-2: 178.33333333333317
    agent-3: 178.33333333333317
    agent-4: 178.33333333333317
    agent-5: 178.33333333333317
  policy_reward_mean:
    agent-0: 152.88833333333335
    agent-1: 152.88833333333335
    agent-2: 152.88833333333335
    agent-3: 152.88833333333335
    agent-4: 152.88833333333335
    agent-5: 152.88833333333335
  policy_reward_min:
    agent-0: 64.49999999999997
    agent-1: 64.49999999999997
    agent-2: 64.49999999999997
    agent-3: 64.49999999999997
    agent-4: 64.49999999999997
    agent-5: 64.49999999999997
  sampler_perf:
    mean_env_wait_ms: 27.139144472619808
    mean_inference_ms: 12.907183079594297
    mean_processing_ms: 57.78633163041087
  time_since_restore: 26276.113984823227
  time_this_iter_s: 124.74510073661804
  time_total_s: 35402.12579870224
  timestamp: 1637053383
  timesteps_since_restore: 19008000
  timesteps_this_iter: 96000
  timesteps_total: 24768000
  training_iteration: 258
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    258 |          35402.1 | 24768000 |   917.33 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 0.99
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 31.24
    apples_agent-1_min: 0
    apples_agent-2_max: 69
    apples_agent-2_mean: 9.34
    apples_agent-2_min: 0
    apples_agent-3_max: 118
    apples_agent-3_mean: 73.52
    apples_agent-3_min: 39
    apples_agent-4_max: 82
    apples_agent-4_mean: 1.87
    apples_agent-4_min: 0
    apples_agent-5_max: 150
    apples_agent-5_mean: 92.86
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 512
    cleaning_beam_agent-0_mean: 420.31
    cleaning_beam_agent-0_min: 292
    cleaning_beam_agent-1_max: 419
    cleaning_beam_agent-1_mean: 257.27
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 569
    cleaning_beam_agent-2_mean: 292.96
    cleaning_beam_agent-2_min: 118
    cleaning_beam_agent-3_max: 66
    cleaning_beam_agent-3_mean: 23.14
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 513
    cleaning_beam_agent-4_mean: 420.58
    cleaning_beam_agent-4_min: 301
    cleaning_beam_agent-5_max: 100
    cleaning_beam_agent-5_mean: 40.29
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-05-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1062.9999999999754
  episode_reward_mean: 897.969999999983
  episode_reward_min: 423.0000000000108
  episodes_this_iter: 96
  episodes_total: 24864
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11530.7
    learner:
      agent-0:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9669691920280457
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012783277779817581
        model: {}
        policy_loss: -0.003236854914575815
        total_loss: -0.00317819369956851
        vf_explained_var: 0.052475810050964355
        vf_loss: 17.605302810668945
      agent-1:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1491656303405762
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021287209820002317
        model: {}
        policy_loss: -0.004168725572526455
        total_loss: -0.0041712019592523575
        vf_explained_var: -0.0695110559463501
        vf_loss: 20.200576782226562
      agent-2:
        cur_kl_coeff: 1.387778801460961e-18
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1063580513000488
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016352747334167361
        model: {}
        policy_loss: -0.003824415151029825
        total_loss: -0.0038788807578384876
        vf_explained_var: -0.008578717708587646
        vf_loss: 18.9272518157959
      agent-3:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4994027018547058
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011620577424764633
        model: {}
        policy_loss: -0.0027131494134664536
        total_loss: -0.001921560731716454
        vf_explained_var: 0.1062118262052536
        vf_loss: 16.70536231994629
      agent-4:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9781124591827393
        entropy_coeff: 0.0017600000137463212
        kl: 0.002148933243006468
        model: {}
        policy_loss: -0.00405195914208889
        total_loss: -0.00384689518250525
        vf_explained_var: -0.027581721544265747
        vf_loss: 19.265426635742188
      agent-5:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8007655143737793
        entropy_coeff: 0.0017600000137463212
        kl: 0.001982359681278467
        model: {}
        policy_loss: -0.00354775576852262
        total_loss: -0.003261896315962076
        vf_explained_var: 0.08780902624130249
        vf_loss: 16.952068328857422
    load_time_ms: 13382.955
    num_steps_sampled: 24864000
    num_steps_trained: 24864000
    sample_time_ms: 99707.517
    update_time_ms: 14.96
  iterations_since_restore: 199
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.460335195530728
    ram_util_percent: 16.116201117318436
  pid: 30948
  policy_reward_max:
    agent-0: 177.16666666666657
    agent-1: 177.16666666666657
    agent-2: 177.16666666666657
    agent-3: 177.16666666666657
    agent-4: 177.16666666666657
    agent-5: 177.16666666666657
  policy_reward_mean:
    agent-0: 149.6616666666667
    agent-1: 149.6616666666667
    agent-2: 149.6616666666667
    agent-3: 149.6616666666667
    agent-4: 149.6616666666667
    agent-5: 149.6616666666667
  policy_reward_min:
    agent-0: 70.49999999999982
    agent-1: 70.49999999999982
    agent-2: 70.49999999999982
    agent-3: 70.49999999999982
    agent-4: 70.49999999999982
    agent-5: 70.49999999999982
  sampler_perf:
    mean_env_wait_ms: 27.139911871912428
    mean_inference_ms: 12.906741650810464
    mean_processing_ms: 57.78298142893521
  time_since_restore: 26401.939862012863
  time_this_iter_s: 125.82587718963623
  time_total_s: 35527.951675891876
  timestamp: 1637053509
  timesteps_since_restore: 19104000
  timesteps_this_iter: 96000
  timesteps_total: 24864000
  training_iteration: 259
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    259 |            35528 | 24864000 |   897.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 2.4
    apples_agent-0_min: 0
    apples_agent-1_max: 157
    apples_agent-1_mean: 32.03
    apples_agent-1_min: 0
    apples_agent-2_max: 254
    apples_agent-2_mean: 9.35
    apples_agent-2_min: 0
    apples_agent-3_max: 262
    apples_agent-3_mean: 73.11
    apples_agent-3_min: 36
    apples_agent-4_max: 29
    apples_agent-4_mean: 0.93
    apples_agent-4_min: 0
    apples_agent-5_max: 158
    apples_agent-5_mean: 89.65
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 537
    cleaning_beam_agent-0_mean: 423.85
    cleaning_beam_agent-0_min: 265
    cleaning_beam_agent-1_max: 480
    cleaning_beam_agent-1_mean: 253.66
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 552
    cleaning_beam_agent-2_mean: 314.48
    cleaning_beam_agent-2_min: 138
    cleaning_beam_agent-3_max: 82
    cleaning_beam_agent-3_mean: 23.3
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 523
    cleaning_beam_agent-4_mean: 432.4
    cleaning_beam_agent-4_min: 285
    cleaning_beam_agent-5_max: 137
    cleaning_beam_agent-5_mean: 44.04
    cleaning_beam_agent-5_min: 13
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-07-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1045.9999999999893
  episode_reward_mean: 898.5499999999826
  episode_reward_min: 425.0000000000063
  episodes_this_iter: 96
  episodes_total: 24960
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11572.391
    learner:
      agent-0:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9640383720397949
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011947369202971458
        model: {}
        policy_loss: -0.003221448976546526
        total_loss: -0.00311831571161747
        vf_explained_var: 0.05936621129512787
        vf_loss: 17.9984130859375
      agent-1:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1490545272827148
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013086686376482248
        model: {}
        policy_loss: -0.003784661879763007
        total_loss: -0.003701961599290371
        vf_explained_var: -0.0812663733959198
        vf_loss: 21.05035400390625
      agent-2:
        cur_kl_coeff: 6.938894007304805e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1166328191757202
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018704321701079607
        model: {}
        policy_loss: -0.003815146628767252
        total_loss: -0.003917562775313854
        vf_explained_var: 0.035731300711631775
        vf_loss: 18.628578186035156
      agent-3:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49948859214782715
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015703800600022078
        model: {}
        policy_loss: -0.00293347449041903
        total_loss: -0.0020485396962612867
        vf_explained_var: 0.08540381491184235
        vf_loss: 17.640352249145508
      agent-4:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9666652679443359
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021604225039482117
        model: {}
        policy_loss: -0.004094697069376707
        total_loss: -0.003855509450659156
        vf_explained_var: -0.006660729646682739
        vf_loss: 19.40517807006836
      agent-5:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7755956649780273
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014103248249739408
        model: {}
        policy_loss: -0.00351676344871521
        total_loss: -0.003174896351993084
        vf_explained_var: 0.10485246777534485
        vf_loss: 17.069143295288086
    load_time_ms: 13380.01
    num_steps_sampled: 24960000
    num_steps_trained: 24960000
    sample_time_ms: 99655.29
    update_time_ms: 14.919
  iterations_since_restore: 200
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.30505617977528
    ram_util_percent: 16.197191011235958
  pid: 30948
  policy_reward_max:
    agent-0: 174.33333333333312
    agent-1: 174.33333333333312
    agent-2: 174.33333333333312
    agent-3: 174.33333333333312
    agent-4: 174.33333333333312
    agent-5: 174.33333333333312
  policy_reward_mean:
    agent-0: 149.75833333333333
    agent-1: 149.75833333333333
    agent-2: 149.75833333333333
    agent-3: 149.75833333333333
    agent-4: 149.75833333333333
    agent-5: 149.75833333333333
  policy_reward_min:
    agent-0: 70.83333333333331
    agent-1: 70.83333333333331
    agent-2: 70.83333333333331
    agent-3: 70.83333333333331
    agent-4: 70.83333333333331
    agent-5: 70.83333333333331
  sampler_perf:
    mean_env_wait_ms: 27.13969733551924
    mean_inference_ms: 12.905807301670196
    mean_processing_ms: 57.77489584280312
  time_since_restore: 26526.62533044815
  time_this_iter_s: 124.68546843528748
  time_total_s: 35652.637144327164
  timestamp: 1637053634
  timesteps_since_restore: 19200000
  timesteps_this_iter: 96000
  timesteps_total: 24960000
  training_iteration: 260
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    260 |          35652.6 | 24960000 |   898.55 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 46
    apples_agent-0_mean: 2.22
    apples_agent-0_min: 0
    apples_agent-1_max: 103
    apples_agent-1_mean: 32.94
    apples_agent-1_min: 0
    apples_agent-2_max: 75
    apples_agent-2_mean: 10.95
    apples_agent-2_min: 0
    apples_agent-3_max: 142
    apples_agent-3_mean: 72.14
    apples_agent-3_min: 31
    apples_agent-4_max: 79
    apples_agent-4_mean: 1.44
    apples_agent-4_min: 0
    apples_agent-5_max: 151
    apples_agent-5_mean: 88.04
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 541
    cleaning_beam_agent-0_mean: 417.5
    cleaning_beam_agent-0_min: 271
    cleaning_beam_agent-1_max: 419
    cleaning_beam_agent-1_mean: 227.32
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 481
    cleaning_beam_agent-2_mean: 294.24
    cleaning_beam_agent-2_min: 136
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 23.32
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 535
    cleaning_beam_agent-4_mean: 446.19
    cleaning_beam_agent-4_min: 313
    cleaning_beam_agent-5_max: 138
    cleaning_beam_agent-5_mean: 47.09
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-09-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1054.999999999984
  episode_reward_mean: 884.0199999999843
  episode_reward_min: 366.000000000005
  episodes_this_iter: 96
  episodes_total: 25056
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11597.535
    learner:
      agent-0:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9692814946174622
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013174685882404447
        model: {}
        policy_loss: -0.003029485233128071
        total_loss: -0.0028739178087562323
        vf_explained_var: 0.10380853712558746
        vf_loss: 18.615020751953125
      agent-1:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.151856780052185
        entropy_coeff: 0.0017600000137463212
        kl: 0.00160784344188869
        model: {}
        policy_loss: -0.004009648691862822
        total_loss: -0.003787409979850054
        vf_explained_var: -0.07443660497665405
        vf_loss: 22.495075225830078
      agent-2:
        cur_kl_coeff: 3.4694470036524025e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1108293533325195
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016594806220382452
        model: {}
        policy_loss: -0.0035077736247330904
        total_loss: -0.003386355470865965
        vf_explained_var: 0.006076455116271973
        vf_loss: 20.764766693115234
      agent-3:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5208237171173096
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008396978373639286
        model: {}
        policy_loss: -0.00258759967982769
        total_loss: -0.001689833588898182
        vf_explained_var: 0.1302725225687027
        vf_loss: 18.14417839050293
      agent-4:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9514687061309814
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019165801350027323
        model: {}
        policy_loss: -0.003888294566422701
        total_loss: -0.0035008753184229136
        vf_explained_var: 0.007245689630508423
        vf_loss: 20.62005615234375
      agent-5:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 0.8095968961715698
        entropy_coeff: 0.0017600000137463212
        kl: 0.00134499731939286
        model: {}
        policy_loss: -0.003503017360344529
        total_loss: -0.003145364811643958
        vf_explained_var: 0.14237640798091888
        vf_loss: 17.82544708251953
    load_time_ms: 13375.916
    num_steps_sampled: 25056000
    num_steps_trained: 25056000
    sample_time_ms: 99736.049
    update_time_ms: 14.986
  iterations_since_restore: 201
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.4271186440678
    ram_util_percent: 16.20395480225989
  pid: 30948
  policy_reward_max:
    agent-0: 175.83333333333275
    agent-1: 175.83333333333275
    agent-2: 175.83333333333275
    agent-3: 175.83333333333275
    agent-4: 175.83333333333275
    agent-5: 175.83333333333275
  policy_reward_mean:
    agent-0: 147.3366666666667
    agent-1: 147.3366666666667
    agent-2: 147.3366666666667
    agent-3: 147.3366666666667
    agent-4: 147.3366666666667
    agent-5: 147.3366666666667
  policy_reward_min:
    agent-0: 60.99999999999973
    agent-1: 60.99999999999973
    agent-2: 60.99999999999973
    agent-3: 60.99999999999973
    agent-4: 60.99999999999973
    agent-5: 60.99999999999973
  sampler_perf:
    mean_env_wait_ms: 27.13826753785921
    mean_inference_ms: 12.904868938796318
    mean_processing_ms: 57.76716870735342
  time_since_restore: 26651.06281352043
  time_this_iter_s: 124.43748307228088
  time_total_s: 35777.074627399445
  timestamp: 1637053759
  timesteps_since_restore: 19296000
  timesteps_this_iter: 96000
  timesteps_total: 25056000
  training_iteration: 261
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    261 |          35777.1 | 25056000 |   884.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 2.04
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 29.79
    apples_agent-1_min: 0
    apples_agent-2_max: 71
    apples_agent-2_mean: 8.06
    apples_agent-2_min: 0
    apples_agent-3_max: 117
    apples_agent-3_mean: 71.85
    apples_agent-3_min: 31
    apples_agent-4_max: 50
    apples_agent-4_mean: 1.68
    apples_agent-4_min: 0
    apples_agent-5_max: 137
    apples_agent-5_mean: 91.38
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 537
    cleaning_beam_agent-0_mean: 427.41
    cleaning_beam_agent-0_min: 320
    cleaning_beam_agent-1_max: 458
    cleaning_beam_agent-1_mean: 243.24
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 520
    cleaning_beam_agent-2_mean: 293.79
    cleaning_beam_agent-2_min: 106
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 22.46
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 533
    cleaning_beam_agent-4_mean: 450.73
    cleaning_beam_agent-4_min: 348
    cleaning_beam_agent-5_max: 161
    cleaning_beam_agent-5_mean: 45.15
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-11-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1087.9999999999927
  episode_reward_mean: 910.1299999999821
  episode_reward_min: 445.0000000000058
  episodes_this_iter: 96
  episodes_total: 25152
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11580.236
    learner:
      agent-0:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9596856832504272
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020030708983540535
        model: {}
        policy_loss: -0.003441993612796068
        total_loss: -0.0034380885772407055
        vf_explained_var: 0.05612762272357941
        vf_loss: 16.92953109741211
      agent-1:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1582601070404053
        entropy_coeff: 0.0017600000137463212
        kl: 0.001374680083245039
        model: {}
        policy_loss: -0.003919318784028292
        total_loss: -0.003998219035565853
        vf_explained_var: -0.06175622344017029
        vf_loss: 19.596363067626953
      agent-2:
        cur_kl_coeff: 1.7347235018262012e-19
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1119496822357178
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014261520700529218
        model: {}
        policy_loss: -0.0035920327063649893
        total_loss: -0.003752509132027626
        vf_explained_var: 0.013571113348007202
        vf_loss: 17.96556854248047
      agent-3:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.491906076669693
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009784766007214785
        model: {}
        policy_loss: -0.0023876242339611053
        total_loss: -0.001539874356240034
        vf_explained_var: 0.052786827087402344
        vf_loss: 17.13503646850586
      agent-4:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.95526522397995
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016431431286036968
        model: {}
        policy_loss: -0.004069919232279062
        total_loss: -0.003946214914321899
        vf_explained_var: 0.004997849464416504
        vf_loss: 18.049697875976562
      agent-5:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7810194492340088
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016027208184823394
        model: {}
        policy_loss: -0.003227061126381159
        total_loss: -0.0029424279928207397
        vf_explained_var: 0.07077004015445709
        vf_loss: 16.59230613708496
    load_time_ms: 13380.156
    num_steps_sampled: 25152000
    num_steps_trained: 25152000
    sample_time_ms: 99726.406
    update_time_ms: 15.151
  iterations_since_restore: 202
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.388700564971757
    ram_util_percent: 16.20621468926554
  pid: 30948
  policy_reward_max:
    agent-0: 181.33333333333312
    agent-1: 181.33333333333312
    agent-2: 181.33333333333312
    agent-3: 181.33333333333312
    agent-4: 181.33333333333312
    agent-5: 181.33333333333312
  policy_reward_mean:
    agent-0: 151.6883333333334
    agent-1: 151.6883333333334
    agent-2: 151.6883333333334
    agent-3: 151.6883333333334
    agent-4: 151.6883333333334
    agent-5: 151.6883333333334
  policy_reward_min:
    agent-0: 74.16666666666676
    agent-1: 74.16666666666676
    agent-2: 74.16666666666676
    agent-3: 74.16666666666676
    agent-4: 74.16666666666676
    agent-5: 74.16666666666676
  sampler_perf:
    mean_env_wait_ms: 27.137755993734284
    mean_inference_ms: 12.903984969754154
    mean_processing_ms: 57.76052274912491
  time_since_restore: 26775.062521219254
  time_this_iter_s: 123.99970769882202
  time_total_s: 35901.07433509827
  timestamp: 1637053883
  timesteps_since_restore: 19392000
  timesteps_this_iter: 96000
  timesteps_total: 25152000
  training_iteration: 262
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    262 |          35901.1 | 25152000 |   910.13 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 3.59
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 30.82
    apples_agent-1_min: 0
    apples_agent-2_max: 87
    apples_agent-2_mean: 11.0
    apples_agent-2_min: 0
    apples_agent-3_max: 123
    apples_agent-3_mean: 71.79
    apples_agent-3_min: 21
    apples_agent-4_max: 74
    apples_agent-4_mean: 2.36
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 88.82
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 517
    cleaning_beam_agent-0_mean: 414.34
    cleaning_beam_agent-0_min: 258
    cleaning_beam_agent-1_max: 423
    cleaning_beam_agent-1_mean: 234.02
    cleaning_beam_agent-1_min: 118
    cleaning_beam_agent-2_max: 514
    cleaning_beam_agent-2_mean: 287.8
    cleaning_beam_agent-2_min: 129
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 25.11
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 523
    cleaning_beam_agent-4_mean: 445.75
    cleaning_beam_agent-4_min: 288
    cleaning_beam_agent-5_max: 265
    cleaning_beam_agent-5_mean: 50.26
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-13-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1084.999999999992
  episode_reward_mean: 867.4199999999852
  episode_reward_min: 411.00000000000483
  episodes_this_iter: 96
  episodes_total: 25248
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11614.355
    learner:
      agent-0:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9605067372322083
        entropy_coeff: 0.0017600000137463212
        kl: 0.001590975560247898
        model: {}
        policy_loss: -0.0035087065771222115
        total_loss: -0.0032213162630796432
        vf_explained_var: 0.0942969024181366
        vf_loss: 19.778833389282227
      agent-1:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.153353214263916
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011270196409896016
        model: {}
        policy_loss: -0.003724674228578806
        total_loss: -0.003349759615957737
        vf_explained_var: -0.1036679744720459
        vf_loss: 24.04815673828125
      agent-2:
        cur_kl_coeff: 8.673617509131006e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1115473508834839
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015155640430748463
        model: {}
        policy_loss: -0.0037199591752141714
        total_loss: -0.0034803154412657022
        vf_explained_var: -0.008108288049697876
        vf_loss: 21.959678649902344
      agent-3:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5398755073547363
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011449744924902916
        model: {}
        policy_loss: -0.0029825870878994465
        total_loss: -0.0021273400634527206
        vf_explained_var: 0.17042912542819977
        vf_loss: 18.05426597595215
      agent-4:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9517230987548828
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013407516526058316
        model: {}
        policy_loss: -0.0036456212401390076
        total_loss: -0.0031783776357769966
        vf_explained_var: 0.017649352550506592
        vf_loss: 21.42273712158203
      agent-5:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7911839485168457
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008719292236492038
        model: {}
        policy_loss: -0.0032537670340389013
        total_loss: -0.0027755433693528175
        vf_explained_var: 0.1480189710855484
        vf_loss: 18.707061767578125
    load_time_ms: 13386.408
    num_steps_sampled: 25248000
    num_steps_trained: 25248000
    sample_time_ms: 99490.99
    update_time_ms: 15.094
  iterations_since_restore: 203
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.50508474576271
    ram_util_percent: 16.1361581920904
  pid: 30948
  policy_reward_max:
    agent-0: 180.83333333333334
    agent-1: 180.83333333333334
    agent-2: 180.83333333333334
    agent-3: 180.83333333333334
    agent-4: 180.83333333333334
    agent-5: 180.83333333333334
  policy_reward_mean:
    agent-0: 144.57000000000002
    agent-1: 144.57000000000002
    agent-2: 144.57000000000002
    agent-3: 144.57000000000002
    agent-4: 144.57000000000002
    agent-5: 144.57000000000002
  policy_reward_min:
    agent-0: 68.49999999999996
    agent-1: 68.49999999999996
    agent-2: 68.49999999999996
    agent-3: 68.49999999999996
    agent-4: 68.49999999999996
    agent-5: 68.49999999999996
  sampler_perf:
    mean_env_wait_ms: 27.137543195473707
    mean_inference_ms: 12.903178203362714
    mean_processing_ms: 57.75320778674242
  time_since_restore: 26899.543965816498
  time_this_iter_s: 124.48144459724426
  time_total_s: 36025.55577969551
  timestamp: 1637054007
  timesteps_since_restore: 19488000
  timesteps_this_iter: 96000
  timesteps_total: 25248000
  training_iteration: 263
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    263 |          36025.6 | 25248000 |   867.42 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 71
    apples_agent-0_mean: 2.54
    apples_agent-0_min: 0
    apples_agent-1_max: 88
    apples_agent-1_mean: 31.25
    apples_agent-1_min: 0
    apples_agent-2_max: 396
    apples_agent-2_mean: 13.24
    apples_agent-2_min: 0
    apples_agent-3_max: 379
    apples_agent-3_mean: 74.33
    apples_agent-3_min: 37
    apples_agent-4_max: 58
    apples_agent-4_mean: 2.31
    apples_agent-4_min: 0
    apples_agent-5_max: 174
    apples_agent-5_mean: 90.87
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 533
    cleaning_beam_agent-0_mean: 421.46
    cleaning_beam_agent-0_min: 282
    cleaning_beam_agent-1_max: 415
    cleaning_beam_agent-1_mean: 224.16
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 587
    cleaning_beam_agent-2_mean: 310.02
    cleaning_beam_agent-2_min: 109
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 25.89
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 557
    cleaning_beam_agent-4_mean: 448.25
    cleaning_beam_agent-4_min: 290
    cleaning_beam_agent-5_max: 127
    cleaning_beam_agent-5_mean: 42.85
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-15-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1074.999999999987
  episode_reward_mean: 890.7499999999844
  episode_reward_min: 298.999999999998
  episodes_this_iter: 96
  episodes_total: 25344
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11622.82
    learner:
      agent-0:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9437407851219177
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019969679415225983
        model: {}
        policy_loss: -0.0035421864595264196
        total_loss: -0.0033500934951007366
        vf_explained_var: 0.03832802176475525
        vf_loss: 18.5307559967041
      agent-1:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1392028331756592
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015308839501813054
        model: {}
        policy_loss: -0.003904071170836687
        total_loss: -0.003799597267061472
        vf_explained_var: -0.08399787545204163
        vf_loss: 21.0947265625
      agent-2:
        cur_kl_coeff: 4.336808754565503e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0938918590545654
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015264102257788181
        model: {}
        policy_loss: -0.003304295241832733
        total_loss: -0.0033087381161749363
        vf_explained_var: 0.006956472992897034
        vf_loss: 19.208057403564453
      agent-3:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5123271942138672
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012641901848837733
        model: {}
        policy_loss: -0.0027448763139545918
        total_loss: -0.0018980111926794052
        vf_explained_var: 0.09605482220649719
        vf_loss: 17.485633850097656
      agent-4:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9550867080688477
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013063986552879214
        model: {}
        policy_loss: -0.003935615997761488
        total_loss: -0.0036682067438960075
        vf_explained_var: -0.008109286427497864
        vf_loss: 19.483627319335938
      agent-5:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7793377041816711
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015626996755599976
        model: {}
        policy_loss: -0.0036714039742946625
        total_loss: -0.003288661129772663
        vf_explained_var: 0.08843562006950378
        vf_loss: 17.543752670288086
    load_time_ms: 13387.915
    num_steps_sampled: 25344000
    num_steps_trained: 25344000
    sample_time_ms: 99547.669
    update_time_ms: 15.121
  iterations_since_restore: 204
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.24078212290503
    ram_util_percent: 16.19664804469274
  pid: 30948
  policy_reward_max:
    agent-0: 179.16666666666626
    agent-1: 179.16666666666626
    agent-2: 179.16666666666626
    agent-3: 179.16666666666626
    agent-4: 179.16666666666626
    agent-5: 179.16666666666626
  policy_reward_mean:
    agent-0: 148.45833333333337
    agent-1: 148.45833333333337
    agent-2: 148.45833333333337
    agent-3: 148.45833333333337
    agent-4: 148.45833333333337
    agent-5: 148.45833333333337
  policy_reward_min:
    agent-0: 49.83333333333317
    agent-1: 49.83333333333317
    agent-2: 49.83333333333317
    agent-3: 49.83333333333317
    agent-4: 49.83333333333317
    agent-5: 49.83333333333317
  sampler_perf:
    mean_env_wait_ms: 27.13830393442342
    mean_inference_ms: 12.902441187510355
    mean_processing_ms: 57.74832481342477
  time_since_restore: 27025.279257774353
  time_this_iter_s: 125.73529195785522
  time_total_s: 36151.291071653366
  timestamp: 1637054133
  timesteps_since_restore: 19584000
  timesteps_this_iter: 96000
  timesteps_total: 25344000
  training_iteration: 264
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    264 |          36151.3 | 25344000 |   890.75 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 1.85
    apples_agent-0_min: 0
    apples_agent-1_max: 110
    apples_agent-1_mean: 29.39
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 9.78
    apples_agent-2_min: 0
    apples_agent-3_max: 142
    apples_agent-3_mean: 73.01
    apples_agent-3_min: 45
    apples_agent-4_max: 50
    apples_agent-4_mean: 2.67
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 92.95
    apples_agent-5_min: 57
    cleaning_beam_agent-0_max: 540
    cleaning_beam_agent-0_mean: 438.13
    cleaning_beam_agent-0_min: 346
    cleaning_beam_agent-1_max: 447
    cleaning_beam_agent-1_mean: 247.74
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 542
    cleaning_beam_agent-2_mean: 296.94
    cleaning_beam_agent-2_min: 137
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 27.18
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 542
    cleaning_beam_agent-4_mean: 454.83
    cleaning_beam_agent-4_min: 316
    cleaning_beam_agent-5_max: 183
    cleaning_beam_agent-5_mean: 41.46
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-17-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1074.9999999999795
  episode_reward_mean: 915.2199999999826
  episode_reward_min: 531.0000000000031
  episodes_this_iter: 96
  episodes_total: 25440
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11632.62
    learner:
      agent-0:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9445949792861938
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014211215311661363
        model: {}
        policy_loss: -0.0032785888761281967
        total_loss: -0.0032235216349363327
        vf_explained_var: 0.03135320544242859
        vf_loss: 17.17552375793457
      agent-1:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1275293827056885
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016494832234457135
        model: {}
        policy_loss: -0.0036884546279907227
        total_loss: -0.0036828317679464817
        vf_explained_var: -0.08969631791114807
        vf_loss: 19.900726318359375
      agent-2:
        cur_kl_coeff: 2.1684043772827515e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1001231670379639
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013178337831050158
        model: {}
        policy_loss: -0.00349620939232409
        total_loss: -0.0035567020531743765
        vf_explained_var: -0.039247483015060425
        vf_loss: 18.757244110107422
      agent-3:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5076085329055786
        entropy_coeff: 0.0017600000137463212
        kl: 0.001047478523105383
        model: {}
        policy_loss: -0.002504299394786358
        total_loss: -0.001697733299806714
        vf_explained_var: 0.04853449761867523
        vf_loss: 16.999570846557617
      agent-4:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9462392330169678
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014294550055637956
        model: {}
        policy_loss: -0.003921677358448505
        total_loss: -0.003806017804890871
        vf_explained_var: 0.00692102313041687
        vf_loss: 17.810428619384766
      agent-5:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7424455881118774
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018251732690259814
        model: {}
        policy_loss: -0.0032168992329388857
        total_loss: -0.0028532608412206173
        vf_explained_var: 0.05274401605129242
        vf_loss: 16.703433990478516
    load_time_ms: 13375.069
    num_steps_sampled: 25440000
    num_steps_trained: 25440000
    sample_time_ms: 99610.438
    update_time_ms: 15.192
  iterations_since_restore: 205
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.410169491525426
    ram_util_percent: 16.197740112994353
  pid: 30948
  policy_reward_max:
    agent-0: 179.16666666666637
    agent-1: 179.16666666666637
    agent-2: 179.16666666666637
    agent-3: 179.16666666666637
    agent-4: 179.16666666666637
    agent-5: 179.16666666666637
  policy_reward_mean:
    agent-0: 152.53666666666663
    agent-1: 152.53666666666663
    agent-2: 152.53666666666663
    agent-3: 152.53666666666663
    agent-4: 152.53666666666663
    agent-5: 152.53666666666663
  policy_reward_min:
    agent-0: 88.49999999999997
    agent-1: 88.49999999999997
    agent-2: 88.49999999999997
    agent-3: 88.49999999999997
    agent-4: 88.49999999999997
    agent-5: 88.49999999999997
  sampler_perf:
    mean_env_wait_ms: 27.13910667754778
    mean_inference_ms: 12.90156720985182
    mean_processing_ms: 57.7425320509815
  time_since_restore: 27149.66708445549
  time_this_iter_s: 124.38782668113708
  time_total_s: 36275.6788983345
  timestamp: 1637054258
  timesteps_since_restore: 19680000
  timesteps_this_iter: 96000
  timesteps_total: 25440000
  training_iteration: 265
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    265 |          36275.7 | 25440000 |   915.22 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 1.28
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 29.77
    apples_agent-1_min: 0
    apples_agent-2_max: 187
    apples_agent-2_mean: 12.83
    apples_agent-2_min: 0
    apples_agent-3_max: 127
    apples_agent-3_mean: 72.28
    apples_agent-3_min: 32
    apples_agent-4_max: 61
    apples_agent-4_mean: 0.84
    apples_agent-4_min: 0
    apples_agent-5_max: 145
    apples_agent-5_mean: 90.3
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 528
    cleaning_beam_agent-0_mean: 438.83
    cleaning_beam_agent-0_min: 299
    cleaning_beam_agent-1_max: 446
    cleaning_beam_agent-1_mean: 245.39
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 532
    cleaning_beam_agent-2_mean: 297.44
    cleaning_beam_agent-2_min: 120
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 26.3
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 535
    cleaning_beam_agent-4_mean: 456.55
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 132
    cleaning_beam_agent-5_mean: 40.11
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-19-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1105.9999999999864
  episode_reward_mean: 928.9699999999841
  episode_reward_min: 468.000000000012
  episodes_this_iter: 96
  episodes_total: 25536
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11649.948
    learner:
      agent-0:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.941374659538269
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016475439770147204
        model: {}
        policy_loss: -0.0033382042311131954
        total_loss: -0.0032028357964009047
        vf_explained_var: 0.026148468255996704
        vf_loss: 17.92190933227539
      agent-1:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1306108236312866
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012255292385816574
        model: {}
        policy_loss: -0.0036221949849277735
        total_loss: -0.003479442559182644
        vf_explained_var: -0.1111966073513031
        vf_loss: 21.326282501220703
      agent-2:
        cur_kl_coeff: 1.0842021886413758e-20
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0941038131713867
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016694775549694896
        model: {}
        policy_loss: -0.0034516723826527596
        total_loss: -0.003439720720052719
        vf_explained_var: -0.017957985401153564
        vf_loss: 19.37576675415039
      agent-3:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49743953347206116
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009865309111773968
        model: {}
        policy_loss: -0.0027021283749490976
        total_loss: -0.0018103324109688401
        vf_explained_var: 0.0521252304315567
        vf_loss: 17.67287254333496
      agent-4:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.946647047996521
        entropy_coeff: 0.0017600000137463212
        kl: 0.001472095842473209
        model: {}
        policy_loss: -0.0037687192671000957
        total_loss: -0.0035273097455501556
        vf_explained_var: -0.01579868793487549
        vf_loss: 19.075069427490234
      agent-5:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.717214822769165
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021266955882310867
        model: {}
        policy_loss: -0.0034456588327884674
        total_loss: -0.0029665003530681133
        vf_explained_var: 0.04790298640727997
        vf_loss: 17.41453742980957
    load_time_ms: 13816.85
    num_steps_sampled: 25536000
    num_steps_trained: 25536000
    sample_time_ms: 99574.318
    update_time_ms: 15.188
  iterations_since_restore: 206
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.832786885245905
    ram_util_percent: 16.296174863387982
  pid: 30948
  policy_reward_max:
    agent-0: 184.33333333333297
    agent-1: 184.33333333333297
    agent-2: 184.33333333333297
    agent-3: 184.33333333333297
    agent-4: 184.33333333333297
    agent-5: 184.33333333333297
  policy_reward_mean:
    agent-0: 154.8283333333333
    agent-1: 154.8283333333333
    agent-2: 154.8283333333333
    agent-3: 154.8283333333333
    agent-4: 154.8283333333333
    agent-5: 154.8283333333333
  policy_reward_min:
    agent-0: 78.00000000000004
    agent-1: 78.00000000000004
    agent-2: 78.00000000000004
    agent-3: 78.00000000000004
    agent-4: 78.00000000000004
    agent-5: 78.00000000000004
  sampler_perf:
    mean_env_wait_ms: 27.13881406666199
    mean_inference_ms: 12.900595299549993
    mean_processing_ms: 57.734280603814724
  time_since_restore: 27278.295837640762
  time_this_iter_s: 128.62875318527222
  time_total_s: 36404.307651519775
  timestamp: 1637054386
  timesteps_since_restore: 19776000
  timesteps_this_iter: 96000
  timesteps_total: 25536000
  training_iteration: 266
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    266 |          36404.3 | 25536000 |   928.97 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 1.3
    apples_agent-0_min: 0
    apples_agent-1_max: 136
    apples_agent-1_mean: 34.82
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 10.09
    apples_agent-2_min: 0
    apples_agent-3_max: 122
    apples_agent-3_mean: 69.54
    apples_agent-3_min: 37
    apples_agent-4_max: 53
    apples_agent-4_mean: 1.95
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 86.47
    apples_agent-5_min: 30
    cleaning_beam_agent-0_max: 530
    cleaning_beam_agent-0_mean: 437.74
    cleaning_beam_agent-0_min: 249
    cleaning_beam_agent-1_max: 461
    cleaning_beam_agent-1_mean: 243.1
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 587
    cleaning_beam_agent-2_mean: 312.48
    cleaning_beam_agent-2_min: 149
    cleaning_beam_agent-3_max: 85
    cleaning_beam_agent-3_mean: 27.23
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 562
    cleaning_beam_agent-4_mean: 452.6
    cleaning_beam_agent-4_min: 306
    cleaning_beam_agent-5_max: 225
    cleaning_beam_agent-5_mean: 44.67
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-21-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1089.999999999993
  episode_reward_mean: 905.9999999999843
  episode_reward_min: 396.00000000000045
  episodes_this_iter: 96
  episodes_total: 25632
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11625.358
    learner:
      agent-0:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9351536631584167
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011726201046258211
        model: {}
        policy_loss: -0.003152118530124426
        total_loss: -0.002947468776255846
        vf_explained_var: 0.06659919023513794
        vf_loss: 18.50518798828125
      agent-1:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1247669458389282
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012402123538777232
        model: {}
        policy_loss: -0.0035788239911198616
        total_loss: -0.003401845693588257
        vf_explained_var: -0.06849157810211182
        vf_loss: 21.56565284729004
      agent-2:
        cur_kl_coeff: 5.421010943206879e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0909072160720825
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013489496195688844
        model: {}
        policy_loss: -0.0035694269463419914
        total_loss: -0.0035416269674897194
        vf_explained_var: 0.024728983640670776
        vf_loss: 19.47801971435547
      agent-3:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5156158804893494
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011859263759106398
        model: {}
        policy_loss: -0.002769932383671403
        total_loss: -0.0018744482658803463
        vf_explained_var: 0.09619110822677612
        vf_loss: 18.02968406677246
      agent-4:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9396626353263855
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018502282910048962
        model: {}
        policy_loss: -0.003963077906519175
        total_loss: -0.0036101187579333782
        vf_explained_var: -0.004565492272377014
        vf_loss: 20.06764793395996
      agent-5:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7443596124649048
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013116674963384867
        model: {}
        policy_loss: -0.0032330721151083708
        total_loss: -0.002710673026740551
        vf_explained_var: 0.0739729255437851
        vf_loss: 18.324716567993164
    load_time_ms: 13842.163
    num_steps_sampled: 25632000
    num_steps_trained: 25632000
    sample_time_ms: 99607.049
    update_time_ms: 15.233
  iterations_since_restore: 207
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.275842696629216
    ram_util_percent: 16.208988764044946
  pid: 30948
  policy_reward_max:
    agent-0: 181.66666666666717
    agent-1: 181.66666666666717
    agent-2: 181.66666666666717
    agent-3: 181.66666666666717
    agent-4: 181.66666666666717
    agent-5: 181.66666666666717
  policy_reward_mean:
    agent-0: 150.99999999999997
    agent-1: 150.99999999999997
    agent-2: 150.99999999999997
    agent-3: 150.99999999999997
    agent-4: 150.99999999999997
    agent-5: 150.99999999999997
  policy_reward_min:
    agent-0: 65.99999999999984
    agent-1: 65.99999999999984
    agent-2: 65.99999999999984
    agent-3: 65.99999999999984
    agent-4: 65.99999999999984
    agent-5: 65.99999999999984
  sampler_perf:
    mean_env_wait_ms: 27.139232949643073
    mean_inference_ms: 12.89969820757145
    mean_processing_ms: 57.72845711965202
  time_since_restore: 27403.124309778214
  time_this_iter_s: 124.82847213745117
  time_total_s: 36529.13612365723
  timestamp: 1637054511
  timesteps_since_restore: 19872000
  timesteps_this_iter: 96000
  timesteps_total: 25632000
  training_iteration: 267
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    267 |          36529.1 | 25632000 |      906 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 1.17
    apples_agent-0_min: 0
    apples_agent-1_max: 90
    apples_agent-1_mean: 32.41
    apples_agent-1_min: 0
    apples_agent-2_max: 73
    apples_agent-2_mean: 7.91
    apples_agent-2_min: 0
    apples_agent-3_max: 123
    apples_agent-3_mean: 72.28
    apples_agent-3_min: 35
    apples_agent-4_max: 53
    apples_agent-4_mean: 3.05
    apples_agent-4_min: 0
    apples_agent-5_max: 157
    apples_agent-5_mean: 88.96
    apples_agent-5_min: 34
    cleaning_beam_agent-0_max: 537
    cleaning_beam_agent-0_mean: 430.11
    cleaning_beam_agent-0_min: 286
    cleaning_beam_agent-1_max: 501
    cleaning_beam_agent-1_mean: 256.55
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 488
    cleaning_beam_agent-2_mean: 317.5
    cleaning_beam_agent-2_min: 89
    cleaning_beam_agent-3_max: 73
    cleaning_beam_agent-3_mean: 26.0
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 542
    cleaning_beam_agent-4_mean: 441.62
    cleaning_beam_agent-4_min: 300
    cleaning_beam_agent-5_max: 170
    cleaning_beam_agent-5_mean: 38.99
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-23-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1074.9999999999873
  episode_reward_mean: 923.9899999999849
  episode_reward_min: 311.0000000000025
  episodes_this_iter: 96
  episodes_total: 25728
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11594.964
    learner:
      agent-0:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9337222576141357
        entropy_coeff: 0.0017600000137463212
        kl: 0.002260791137814522
        model: {}
        policy_loss: -0.003670190693810582
        total_loss: -0.0034120373893529177
        vf_explained_var: 0.04912222921848297
        vf_loss: 19.01504135131836
      agent-1:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1208581924438477
        entropy_coeff: 0.0017600000137463212
        kl: 0.001604321412742138
        model: {}
        policy_loss: -0.0038408711552619934
        total_loss: -0.003611256368458271
        vf_explained_var: -0.06689512729644775
        vf_loss: 22.023235321044922
      agent-2:
        cur_kl_coeff: 2.7105054716034394e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0983190536499023
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026339730247855186
        model: {}
        policy_loss: -0.0038963938131928444
        total_loss: -0.0038005784153938293
        vf_explained_var: 0.0007921159267425537
        vf_loss: 20.288593292236328
      agent-3:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5126901865005493
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013904188526794314
        model: {}
        policy_loss: -0.002882328350096941
        total_loss: -0.0019717304967343807
        vf_explained_var: 0.09560109674930573
        vf_loss: 18.12934112548828
      agent-4:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.94742751121521
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014735847944393754
        model: {}
        policy_loss: -0.003875487484037876
        total_loss: -0.0035207411274313927
        vf_explained_var: 0.004558399319648743
        vf_loss: 20.222196578979492
      agent-5:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7198421955108643
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015186031814664602
        model: {}
        policy_loss: -0.0034388741478323936
        total_loss: -0.0028693245258182287
        vf_explained_var: 0.07448053359985352
        vf_loss: 18.36469841003418
    load_time_ms: 13835.955
    num_steps_sampled: 25728000
    num_steps_trained: 25728000
    sample_time_ms: 99589.456
    update_time_ms: 15.135
  iterations_since_restore: 208
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.46045197740113
    ram_util_percent: 16.11864406779661
  pid: 30948
  policy_reward_max:
    agent-0: 179.16666666666654
    agent-1: 179.16666666666654
    agent-2: 179.16666666666654
    agent-3: 179.16666666666654
    agent-4: 179.16666666666654
    agent-5: 179.16666666666654
  policy_reward_mean:
    agent-0: 153.99833333333333
    agent-1: 153.99833333333333
    agent-2: 153.99833333333333
    agent-3: 153.99833333333333
    agent-4: 153.99833333333333
    agent-5: 153.99833333333333
  policy_reward_min:
    agent-0: 51.833333333333194
    agent-1: 51.833333333333194
    agent-2: 51.833333333333194
    agent-3: 51.833333333333194
    agent-4: 51.833333333333194
    agent-5: 51.833333333333194
  sampler_perf:
    mean_env_wait_ms: 27.138897193366684
    mean_inference_ms: 12.898881301635146
    mean_processing_ms: 57.721573783563215
  time_since_restore: 27527.324276447296
  time_this_iter_s: 124.19996666908264
  time_total_s: 36653.33609032631
  timestamp: 1637054636
  timesteps_since_restore: 19968000
  timesteps_this_iter: 96000
  timesteps_total: 25728000
  training_iteration: 268
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    268 |          36653.3 | 25728000 |   923.99 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 1.67
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 28.66
    apples_agent-1_min: 0
    apples_agent-2_max: 106
    apples_agent-2_mean: 9.16
    apples_agent-2_min: 0
    apples_agent-3_max: 120
    apples_agent-3_mean: 73.75
    apples_agent-3_min: 33
    apples_agent-4_max: 75
    apples_agent-4_mean: 1.17
    apples_agent-4_min: 0
    apples_agent-5_max: 157
    apples_agent-5_mean: 94.46
    apples_agent-5_min: 60
    cleaning_beam_agent-0_max: 536
    cleaning_beam_agent-0_mean: 435.84
    cleaning_beam_agent-0_min: 290
    cleaning_beam_agent-1_max: 384
    cleaning_beam_agent-1_mean: 241.2
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 492
    cleaning_beam_agent-2_mean: 330.6
    cleaning_beam_agent-2_min: 145
    cleaning_beam_agent-3_max: 63
    cleaning_beam_agent-3_mean: 29.67
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 573
    cleaning_beam_agent-4_mean: 458.62
    cleaning_beam_agent-4_min: 301
    cleaning_beam_agent-5_max: 209
    cleaning_beam_agent-5_mean: 40.29
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-25-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1133.9999999999995
  episode_reward_mean: 937.0699999999835
  episode_reward_min: 512.0000000000066
  episodes_this_iter: 96
  episodes_total: 25824
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11588.376
    learner:
      agent-0:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9382216334342957
        entropy_coeff: 0.0017600000137463212
        kl: 0.002087803091853857
        model: {}
        policy_loss: -0.0032311677932739258
        total_loss: -0.003090119920670986
        vf_explained_var: 0.05352717638015747
        vf_loss: 17.923248291015625
      agent-1:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1530687808990479
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012574995635077357
        model: {}
        policy_loss: -0.003854350419715047
        total_loss: -0.0038078785873949528
        vf_explained_var: -0.062361180782318115
        vf_loss: 20.758726119995117
      agent-2:
        cur_kl_coeff: 1.3552527358017197e-21
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1052197217941284
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017336886376142502
        model: {}
        policy_loss: -0.003816514741629362
        total_loss: -0.003864151891320944
        vf_explained_var: 0.016104266047477722
        vf_loss: 18.97550392150879
      agent-3:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5095837116241455
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009682894451543689
        model: {}
        policy_loss: -0.0025413301773369312
        total_loss: -0.0016680648550391197
        vf_explained_var: 0.06188930571079254
        vf_loss: 17.70132064819336
      agent-4:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9442946910858154
        entropy_coeff: 0.0017600000137463212
        kl: 0.001835408853366971
        model: {}
        policy_loss: -0.003939434420317411
        total_loss: -0.0036195479333400726
        vf_explained_var: -0.03547069430351257
        vf_loss: 19.81845474243164
      agent-5:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.70993971824646
        entropy_coeff: 0.0017600000137463212
        kl: 0.001144337235018611
        model: {}
        policy_loss: -0.002961650025099516
        total_loss: -0.0024582354817539454
        vf_explained_var: 0.06665174663066864
        vf_loss: 17.529077529907227
    load_time_ms: 13836.162
    num_steps_sampled: 25824000
    num_steps_trained: 25824000
    sample_time_ms: 99364.624
    update_time_ms: 15.08
  iterations_since_restore: 209
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.343181818181815
    ram_util_percent: 16.20340909090909
  pid: 30948
  policy_reward_max:
    agent-0: 188.9999999999995
    agent-1: 188.9999999999995
    agent-2: 188.9999999999995
    agent-3: 188.9999999999995
    agent-4: 188.9999999999995
    agent-5: 188.9999999999995
  policy_reward_mean:
    agent-0: 156.17833333333328
    agent-1: 156.17833333333328
    agent-2: 156.17833333333328
    agent-3: 156.17833333333328
    agent-4: 156.17833333333328
    agent-5: 156.17833333333328
  policy_reward_min:
    agent-0: 85.33333333333329
    agent-1: 85.33333333333329
    agent-2: 85.33333333333329
    agent-3: 85.33333333333329
    agent-4: 85.33333333333329
    agent-5: 85.33333333333329
  sampler_perf:
    mean_env_wait_ms: 27.138485462338075
    mean_inference_ms: 12.898009288580573
    mean_processing_ms: 57.7136070993041
  time_since_restore: 27650.93610572815
  time_this_iter_s: 123.61182928085327
  time_total_s: 36776.94791960716
  timestamp: 1637054759
  timesteps_since_restore: 20064000
  timesteps_this_iter: 96000
  timesteps_total: 25824000
  training_iteration: 269
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    269 |          36776.9 | 25824000 |   937.07 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 13
    apples_agent-0_mean: 0.73
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 33.12
    apples_agent-1_min: 0
    apples_agent-2_max: 169
    apples_agent-2_mean: 12.29
    apples_agent-2_min: 0
    apples_agent-3_max: 151
    apples_agent-3_mean: 72.61
    apples_agent-3_min: 43
    apples_agent-4_max: 27
    apples_agent-4_mean: 0.66
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 90.03
    apples_agent-5_min: 59
    cleaning_beam_agent-0_max: 587
    cleaning_beam_agent-0_mean: 436.07
    cleaning_beam_agent-0_min: 230
    cleaning_beam_agent-1_max: 431
    cleaning_beam_agent-1_mean: 240.24
    cleaning_beam_agent-1_min: 101
    cleaning_beam_agent-2_max: 571
    cleaning_beam_agent-2_mean: 340.75
    cleaning_beam_agent-2_min: 131
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 28.57
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 589
    cleaning_beam_agent-4_mean: 466.15
    cleaning_beam_agent-4_min: 374
    cleaning_beam_agent-5_max: 115
    cleaning_beam_agent-5_mean: 33.84
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-28-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1133.9999999999995
  episode_reward_mean: 961.2999999999847
  episode_reward_min: 572.0000000000027
  episodes_this_iter: 96
  episodes_total: 25920
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11525.486
    learner:
      agent-0:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9414744973182678
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010323395254090428
        model: {}
        policy_loss: -0.0029844497330486774
        total_loss: -0.0028394930996000767
        vf_explained_var: 0.043796345591545105
        vf_loss: 18.01947784423828
      agent-1:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.138071060180664
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013065759558230639
        model: {}
        policy_loss: -0.003976848907768726
        total_loss: -0.0037537976168096066
        vf_explained_var: -0.11105307936668396
        vf_loss: 22.26056671142578
      agent-2:
        cur_kl_coeff: 6.776263679008599e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.090477705001831
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016637069638818502
        model: {}
        policy_loss: -0.0036567524075508118
        total_loss: -0.0036261435598134995
        vf_explained_var: 0.011335998773574829
        vf_loss: 19.498497009277344
      agent-3:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49431926012039185
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009735039202496409
        model: {}
        policy_loss: -0.0026425858959555626
        total_loss: -0.0016797850839793682
        vf_explained_var: 0.03592647612094879
        vf_loss: 18.328004837036133
      agent-4:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9375635385513306
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016323656309396029
        model: {}
        policy_loss: -0.003618885762989521
        total_loss: -0.003304721787571907
        vf_explained_var: -0.009848788380622864
        vf_loss: 19.64276885986328
      agent-5:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.703481912612915
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009676406625658274
        model: {}
        policy_loss: -0.0032077927608042955
        total_loss: -0.0027021171990782022
        vf_explained_var: 0.07483172416687012
        vf_loss: 17.43801498413086
    load_time_ms: 13835.8
    num_steps_sampled: 25920000
    num_steps_trained: 25920000
    sample_time_ms: 99505.787
    update_time_ms: 15.17
  iterations_since_restore: 210
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.408426966292133
    ram_util_percent: 16.18932584269663
  pid: 30948
  policy_reward_max:
    agent-0: 188.9999999999995
    agent-1: 188.9999999999995
    agent-2: 188.9999999999995
    agent-3: 188.9999999999995
    agent-4: 188.9999999999995
    agent-5: 188.9999999999995
  policy_reward_mean:
    agent-0: 160.2166666666666
    agent-1: 160.2166666666666
    agent-2: 160.2166666666666
    agent-3: 160.2166666666666
    agent-4: 160.2166666666666
    agent-5: 160.2166666666666
  policy_reward_min:
    agent-0: 95.33333333333321
    agent-1: 95.33333333333321
    agent-2: 95.33333333333321
    agent-3: 95.33333333333321
    agent-4: 95.33333333333321
    agent-5: 95.33333333333321
  sampler_perf:
    mean_env_wait_ms: 27.14037255198723
    mean_inference_ms: 12.897193687735273
    mean_processing_ms: 57.70937238224937
  time_since_restore: 27776.360575914383
  time_this_iter_s: 125.42447018623352
  time_total_s: 36902.372389793396
  timestamp: 1637054885
  timesteps_since_restore: 20160000
  timesteps_this_iter: 96000
  timesteps_total: 25920000
  training_iteration: 270
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    270 |          36902.4 | 25920000 |    961.3 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 19
    apples_agent-0_mean: 1.32
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 37.92
    apples_agent-1_min: 0
    apples_agent-2_max: 146
    apples_agent-2_mean: 8.93
    apples_agent-2_min: 0
    apples_agent-3_max: 191
    apples_agent-3_mean: 76.37
    apples_agent-3_min: 45
    apples_agent-4_max: 50
    apples_agent-4_mean: 2.34
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 88.8
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 547
    cleaning_beam_agent-0_mean: 430.8
    cleaning_beam_agent-0_min: 343
    cleaning_beam_agent-1_max: 390
    cleaning_beam_agent-1_mean: 227.44
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 564
    cleaning_beam_agent-2_mean: 342.22
    cleaning_beam_agent-2_min: 134
    cleaning_beam_agent-3_max: 72
    cleaning_beam_agent-3_mean: 28.5
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 567
    cleaning_beam_agent-4_mean: 467.5
    cleaning_beam_agent-4_min: 374
    cleaning_beam_agent-5_max: 154
    cleaning_beam_agent-5_mean: 38.25
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-30-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1093.9999999999916
  episode_reward_mean: 930.0999999999835
  episode_reward_min: 449.00000000001125
  episodes_this_iter: 96
  episodes_total: 26016
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11489.925
    learner:
      agent-0:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9294800758361816
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013951535802334547
        model: {}
        policy_loss: -0.003182060085237026
        total_loss: -0.0029011359438300133
        vf_explained_var: 0.059528648853302
        vf_loss: 19.168106079101562
      agent-1:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1336597204208374
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015654857270419598
        model: {}
        policy_loss: -0.0037529049441218376
        total_loss: -0.0034319348633289337
        vf_explained_var: -0.10184931755065918
        vf_loss: 23.162124633789062
      agent-2:
        cur_kl_coeff: 3.3881318395042993e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1009023189544678
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016477270983159542
        model: {}
        policy_loss: -0.003366221208125353
        total_loss: -0.0032234350219368935
        vf_explained_var: -0.0016405880451202393
        vf_loss: 20.80374526977539
      agent-3:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.515413224697113
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006512241088785231
        model: {}
        policy_loss: -0.002534108702093363
        total_loss: -0.0015927051426842809
        vf_explained_var: 0.09532240033149719
        vf_loss: 18.4853515625
      agent-4:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9431551098823547
        entropy_coeff: 0.0017600000137463212
        kl: 0.002062444807961583
        model: {}
        policy_loss: -0.004014246165752411
        total_loss: -0.0036241281777620316
        vf_explained_var: 0.0071960389614105225
        vf_loss: 20.500743865966797
      agent-5:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 0.720729410648346
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009635327151045203
        model: {}
        policy_loss: -0.003008095547556877
        total_loss: -0.002417551353573799
        vf_explained_var: 0.09150750935077667
        vf_loss: 18.590274810791016
    load_time_ms: 13828.03
    num_steps_sampled: 26016000
    num_steps_trained: 26016000
    sample_time_ms: 99505.957
    update_time_ms: 15.074
  iterations_since_restore: 211
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.59204545454546
    ram_util_percent: 16.191477272727273
  pid: 30948
  policy_reward_max:
    agent-0: 182.3333333333328
    agent-1: 182.3333333333328
    agent-2: 182.3333333333328
    agent-3: 182.3333333333328
    agent-4: 182.3333333333328
    agent-5: 182.3333333333328
  policy_reward_mean:
    agent-0: 155.01666666666662
    agent-1: 155.01666666666662
    agent-2: 155.01666666666662
    agent-3: 155.01666666666662
    agent-4: 155.01666666666662
    agent-5: 155.01666666666662
  policy_reward_min:
    agent-0: 74.83333333333326
    agent-1: 74.83333333333326
    agent-2: 74.83333333333326
    agent-3: 74.83333333333326
    agent-4: 74.83333333333326
    agent-5: 74.83333333333326
  sampler_perf:
    mean_env_wait_ms: 27.141307018140623
    mean_inference_ms: 12.896661534533306
    mean_processing_ms: 57.703542463066114
  time_since_restore: 27900.40885567665
  time_this_iter_s: 124.04827976226807
  time_total_s: 37026.420669555664
  timestamp: 1637055009
  timesteps_since_restore: 20256000
  timesteps_this_iter: 96000
  timesteps_total: 26016000
  training_iteration: 271
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    271 |          37026.4 | 26016000 |    930.1 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 1.91
    apples_agent-0_min: 0
    apples_agent-1_max: 113
    apples_agent-1_mean: 38.09
    apples_agent-1_min: 0
    apples_agent-2_max: 55
    apples_agent-2_mean: 8.83
    apples_agent-2_min: 0
    apples_agent-3_max: 117
    apples_agent-3_mean: 71.76
    apples_agent-3_min: 34
    apples_agent-4_max: 41
    apples_agent-4_mean: 1.03
    apples_agent-4_min: 0
    apples_agent-5_max: 171
    apples_agent-5_mean: 86.58
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 558
    cleaning_beam_agent-0_mean: 442.31
    cleaning_beam_agent-0_min: 260
    cleaning_beam_agent-1_max: 336
    cleaning_beam_agent-1_mean: 217.31
    cleaning_beam_agent-1_min: 75
    cleaning_beam_agent-2_max: 597
    cleaning_beam_agent-2_mean: 317.56
    cleaning_beam_agent-2_min: 107
    cleaning_beam_agent-3_max: 65
    cleaning_beam_agent-3_mean: 29.03
    cleaning_beam_agent-3_min: 13
    cleaning_beam_agent-4_max: 553
    cleaning_beam_agent-4_mean: 460.51
    cleaning_beam_agent-4_min: 353
    cleaning_beam_agent-5_max: 113
    cleaning_beam_agent-5_mean: 36.08
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-32-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1099.9999999999898
  episode_reward_mean: 932.789999999984
  episode_reward_min: 490.00000000001324
  episodes_this_iter: 96
  episodes_total: 26112
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11491.057
    learner:
      agent-0:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9127791523933411
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013509392738342285
        model: {}
        policy_loss: -0.0033648433163762093
        total_loss: -0.0031471699476242065
        vf_explained_var: 0.07336711883544922
        vf_loss: 18.241647720336914
      agent-1:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1230592727661133
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014639467699453235
        model: {}
        policy_loss: -0.004012684337794781
        total_loss: -0.0037852111272513866
        vf_explained_var: -0.07385706901550293
        vf_loss: 22.04056739807129
      agent-2:
        cur_kl_coeff: 1.6940659197521496e-22
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0812522172927856
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017212987877428532
        model: {}
        policy_loss: -0.0035501704551279545
        total_loss: -0.003419587854295969
        vf_explained_var: -0.003196626901626587
        vf_loss: 20.33585548400879
      agent-3:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5045256614685059
        entropy_coeff: 0.0017600000137463212
        kl: 0.001076431479305029
        model: {}
        policy_loss: -0.0024992236867547035
        total_loss: -0.0015735998749732971
        vf_explained_var: 0.08140362799167633
        vf_loss: 18.135881423950195
      agent-4:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9377626180648804
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016587083227932453
        model: {}
        policy_loss: -0.003833973314613104
        total_loss: -0.0034283013083040714
        vf_explained_var: -0.02184101939201355
        vf_loss: 20.561325073242188
      agent-5:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7094850540161133
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008739895420148969
        model: {}
        policy_loss: -0.002961085643619299
        total_loss: -0.002426012884825468
        vf_explained_var: 0.09317025542259216
        vf_loss: 17.837684631347656
    load_time_ms: 13837.026
    num_steps_sampled: 26112000
    num_steps_trained: 26112000
    sample_time_ms: 99475.132
    update_time_ms: 14.965
  iterations_since_restore: 212
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.552272727272726
    ram_util_percent: 16.116477272727273
  pid: 30948
  policy_reward_max:
    agent-0: 183.3333333333326
    agent-1: 183.3333333333326
    agent-2: 183.3333333333326
    agent-3: 183.3333333333326
    agent-4: 183.3333333333326
    agent-5: 183.3333333333326
  policy_reward_mean:
    agent-0: 155.46499999999997
    agent-1: 155.46499999999997
    agent-2: 155.46499999999997
    agent-3: 155.46499999999997
    agent-4: 155.46499999999997
    agent-5: 155.46499999999997
  policy_reward_min:
    agent-0: 81.66666666666687
    agent-1: 81.66666666666687
    agent-2: 81.66666666666687
    agent-3: 81.66666666666687
    agent-4: 81.66666666666687
    agent-5: 81.66666666666687
  sampler_perf:
    mean_env_wait_ms: 27.141355233013524
    mean_inference_ms: 12.895785493092312
    mean_processing_ms: 57.69691788981888
  time_since_restore: 28024.19433617592
  time_this_iter_s: 123.78548049926758
  time_total_s: 37150.20615005493
  timestamp: 1637055133
  timesteps_since_restore: 20352000
  timesteps_this_iter: 96000
  timesteps_total: 26112000
  training_iteration: 272
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    272 |          37150.2 | 26112000 |   932.79 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 1.27
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 34.55
    apples_agent-1_min: 0
    apples_agent-2_max: 193
    apples_agent-2_mean: 12.16
    apples_agent-2_min: 0
    apples_agent-3_max: 135
    apples_agent-3_mean: 71.62
    apples_agent-3_min: 31
    apples_agent-4_max: 43
    apples_agent-4_mean: 1.1
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 90.83
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 525
    cleaning_beam_agent-0_mean: 431.29
    cleaning_beam_agent-0_min: 338
    cleaning_beam_agent-1_max: 418
    cleaning_beam_agent-1_mean: 224.08
    cleaning_beam_agent-1_min: 75
    cleaning_beam_agent-2_max: 576
    cleaning_beam_agent-2_mean: 322.24
    cleaning_beam_agent-2_min: 164
    cleaning_beam_agent-3_max: 63
    cleaning_beam_agent-3_mean: 30.21
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 550
    cleaning_beam_agent-4_mean: 467.91
    cleaning_beam_agent-4_min: 390
    cleaning_beam_agent-5_max: 164
    cleaning_beam_agent-5_mean: 37.23
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-34-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1068.9999999999889
  episode_reward_mean: 927.9199999999831
  episode_reward_min: 448.0000000000031
  episodes_this_iter: 96
  episodes_total: 26208
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11469.119
    learner:
      agent-0:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9281678199768066
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018329310696572065
        model: {}
        policy_loss: -0.003007146529853344
        total_loss: -0.002737928181886673
        vf_explained_var: 0.051063403487205505
        vf_loss: 19.027923583984375
      agent-1:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1352722644805908
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012854383094236255
        model: {}
        policy_loss: -0.0038936720229685307
        total_loss: -0.0036373110488057137
        vf_explained_var: -0.09259739518165588
        vf_loss: 22.54441261291504
      agent-2:
        cur_kl_coeff: 8.470329598760748e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0786346197128296
        entropy_coeff: 0.0017600000137463212
        kl: 0.001978923100978136
        model: {}
        policy_loss: -0.003596663475036621
        total_loss: -0.00347217358648777
        vf_explained_var: 0.0005622506141662598
        vf_loss: 20.228883743286133
      agent-3:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5062731504440308
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011520308908075094
        model: {}
        policy_loss: -0.002845484297722578
        total_loss: -0.001950263511389494
        vf_explained_var: 0.10990560054779053
        vf_loss: 17.862606048583984
      agent-4:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9401271939277649
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011867459397763014
        model: {}
        policy_loss: -0.00339217996224761
        total_loss: -0.003027407918125391
        vf_explained_var: 0.0030931979417800903
        vf_loss: 20.193981170654297
      agent-5:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7208350300788879
        entropy_coeff: 0.0017600000137463212
        kl: 0.0006920371670275927
        model: {}
        policy_loss: -0.003026602789759636
        total_loss: -0.002509070560336113
        vf_explained_var: 0.11070336401462555
        vf_loss: 17.86203384399414
    load_time_ms: 13824.342
    num_steps_sampled: 26208000
    num_steps_trained: 26208000
    sample_time_ms: 99542.412
    update_time_ms: 14.872
  iterations_since_restore: 213
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.48587570621469
    ram_util_percent: 16.18983050847458
  pid: 30948
  policy_reward_max:
    agent-0: 178.16666666666666
    agent-1: 178.16666666666666
    agent-2: 178.16666666666666
    agent-3: 178.16666666666666
    agent-4: 178.16666666666666
    agent-5: 178.16666666666666
  policy_reward_mean:
    agent-0: 154.65333333333328
    agent-1: 154.65333333333328
    agent-2: 154.65333333333328
    agent-3: 154.65333333333328
    agent-4: 154.65333333333328
    agent-5: 154.65333333333328
  policy_reward_min:
    agent-0: 74.66666666666659
    agent-1: 74.66666666666659
    agent-2: 74.66666666666659
    agent-3: 74.66666666666659
    agent-4: 74.66666666666659
    agent-5: 74.66666666666659
  sampler_perf:
    mean_env_wait_ms: 27.1419454088048
    mean_inference_ms: 12.895078622884245
    mean_processing_ms: 57.69040936388264
  time_since_restore: 28148.951951742172
  time_this_iter_s: 124.75761556625366
  time_total_s: 37274.963765621185
  timestamp: 1637055258
  timesteps_since_restore: 20448000
  timesteps_this_iter: 96000
  timesteps_total: 26208000
  training_iteration: 273
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    273 |            37275 | 26208000 |   927.92 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 42
    apples_agent-0_mean: 1.57
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 33.92
    apples_agent-1_min: 0
    apples_agent-2_max: 169
    apples_agent-2_mean: 11.14
    apples_agent-2_min: 0
    apples_agent-3_max: 120
    apples_agent-3_mean: 69.77
    apples_agent-3_min: 33
    apples_agent-4_max: 33
    apples_agent-4_mean: 0.49
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 87.33
    apples_agent-5_min: 43
    cleaning_beam_agent-0_max: 506
    cleaning_beam_agent-0_mean: 416.61
    cleaning_beam_agent-0_min: 241
    cleaning_beam_agent-1_max: 394
    cleaning_beam_agent-1_mean: 222.16
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 522
    cleaning_beam_agent-2_mean: 333.86
    cleaning_beam_agent-2_min: 146
    cleaning_beam_agent-3_max: 66
    cleaning_beam_agent-3_mean: 28.28
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 456.74
    cleaning_beam_agent-4_min: 315
    cleaning_beam_agent-5_max: 110
    cleaning_beam_agent-5_mean: 37.95
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-36-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1090.9999999999889
  episode_reward_mean: 923.9999999999843
  episode_reward_min: 566.0000000000186
  episodes_this_iter: 96
  episodes_total: 26304
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11484.914
    learner:
      agent-0:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9347123503684998
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011385378893464804
        model: {}
        policy_loss: -0.0032246215268969536
        total_loss: -0.003116672858595848
        vf_explained_var: 0.10233666002750397
        vf_loss: 17.530454635620117
      agent-1:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.137939691543579
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016296582762151957
        model: {}
        policy_loss: -0.003928756341338158
        total_loss: -0.003754488890990615
        vf_explained_var: -0.08498063683509827
        vf_loss: 21.770401000976562
      agent-2:
        cur_kl_coeff: 4.235164799380374e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0686129331588745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012396069942042232
        model: {}
        policy_loss: -0.003414037637412548
        total_loss: -0.003376398701220751
        vf_explained_var: 0.0359024703502655
        vf_loss: 19.183969497680664
      agent-3:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5083799362182617
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010607356671243906
        model: {}
        policy_loss: -0.002304753288626671
        total_loss: -0.0014255973510444164
        vf_explained_var: 0.09727828204631805
        vf_loss: 17.73900604248047
      agent-4:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.938544750213623
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018151936819776893
        model: {}
        policy_loss: -0.003897962626069784
        total_loss: -0.0035687051713466644
        vf_explained_var: -0.002552822232246399
        vf_loss: 19.81096649169922
      agent-5:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7321128845214844
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015753949992358685
        model: {}
        policy_loss: -0.003456199076026678
        total_loss: -0.003009528387337923
        vf_explained_var: 0.11368432641029358
        vf_loss: 17.351900100708008
    load_time_ms: 13833.914
    num_steps_sampled: 26304000
    num_steps_trained: 26304000
    sample_time_ms: 99420.167
    update_time_ms: 14.747
  iterations_since_restore: 214
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.51685393258427
    ram_util_percent: 16.20112359550562
  pid: 30948
  policy_reward_max:
    agent-0: 181.83333333333331
    agent-1: 181.83333333333331
    agent-2: 181.83333333333331
    agent-3: 181.83333333333331
    agent-4: 181.83333333333331
    agent-5: 181.83333333333331
  policy_reward_mean:
    agent-0: 153.99999999999997
    agent-1: 153.99999999999997
    agent-2: 153.99999999999997
    agent-3: 153.99999999999997
    agent-4: 153.99999999999997
    agent-5: 153.99999999999997
  policy_reward_min:
    agent-0: 94.33333333333366
    agent-1: 94.33333333333366
    agent-2: 94.33333333333366
    agent-3: 94.33333333333366
    agent-4: 94.33333333333366
    agent-5: 94.33333333333366
  sampler_perf:
    mean_env_wait_ms: 27.14299618469066
    mean_inference_ms: 12.894561853362264
    mean_processing_ms: 57.68624152262172
  time_since_restore: 28273.74618077278
  time_this_iter_s: 124.79422903060913
  time_total_s: 37399.757994651794
  timestamp: 1637055383
  timesteps_since_restore: 20544000
  timesteps_this_iter: 96000
  timesteps_total: 26304000
  training_iteration: 274
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    274 |          37399.8 | 26304000 |      924 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 1.28
    apples_agent-0_min: 0
    apples_agent-1_max: 117
    apples_agent-1_mean: 34.27
    apples_agent-1_min: 0
    apples_agent-2_max: 99
    apples_agent-2_mean: 10.5
    apples_agent-2_min: 0
    apples_agent-3_max: 119
    apples_agent-3_mean: 70.63
    apples_agent-3_min: 41
    apples_agent-4_max: 61
    apples_agent-4_mean: 2.56
    apples_agent-4_min: 0
    apples_agent-5_max: 181
    apples_agent-5_mean: 90.0
    apples_agent-5_min: 55
    cleaning_beam_agent-0_max: 542
    cleaning_beam_agent-0_mean: 428.6
    cleaning_beam_agent-0_min: 308
    cleaning_beam_agent-1_max: 384
    cleaning_beam_agent-1_mean: 216.88
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 612
    cleaning_beam_agent-2_mean: 346.76
    cleaning_beam_agent-2_min: 165
    cleaning_beam_agent-3_max: 62
    cleaning_beam_agent-3_mean: 29.32
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 617
    cleaning_beam_agent-4_mean: 470.17
    cleaning_beam_agent-4_min: 359
    cleaning_beam_agent-5_max: 171
    cleaning_beam_agent-5_mean: 38.4
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-38-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1096.9999999999866
  episode_reward_mean: 936.559999999984
  episode_reward_min: 558.0000000000127
  episodes_this_iter: 96
  episodes_total: 26400
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11479.597
    learner:
      agent-0:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9247756600379944
        entropy_coeff: 0.0017600000137463212
        kl: 0.001218232442624867
        model: {}
        policy_loss: -0.0030197491869330406
        total_loss: -0.002724860329180956
        vf_explained_var: 0.015579238533973694
        vf_loss: 19.22494125366211
      agent-1:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1322288513183594
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015589238610118628
        model: {}
        policy_loss: -0.004145542625337839
        total_loss: -0.003852362744510174
        vf_explained_var: -0.1340678632259369
        vf_loss: 22.85901641845703
      agent-2:
        cur_kl_coeff: 2.117582399690187e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0653765201568604
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021713185124099255
        model: {}
        policy_loss: -0.0037642279639840126
        total_loss: -0.0036840650718659163
        vf_explained_var: 0.026448875665664673
        vf_loss: 19.552288055419922
      agent-3:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5009849071502686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018820801051333547
        model: {}
        policy_loss: -0.0029390600975602865
        total_loss: -0.0020099834073334932
        vf_explained_var: 0.0787661075592041
        vf_loss: 18.10805892944336
      agent-4:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9312405586242676
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016741971485316753
        model: {}
        policy_loss: -0.0038428958505392075
        total_loss: -0.003501706290990114
        vf_explained_var: 0.004988387227058411
        vf_loss: 19.801733016967773
      agent-5:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7270089983940125
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011860521044582129
        model: {}
        policy_loss: -0.002873022574931383
        total_loss: -0.0023435186594724655
        vf_explained_var: 0.07810184359550476
        vf_loss: 18.09044075012207
    load_time_ms: 13830.124
    num_steps_sampled: 26400000
    num_steps_trained: 26400000
    sample_time_ms: 99495.357
    update_time_ms: 14.947
  iterations_since_restore: 215
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.354494382022473
    ram_util_percent: 16.18595505617978
  pid: 30948
  policy_reward_max:
    agent-0: 182.833333333333
    agent-1: 182.833333333333
    agent-2: 182.833333333333
    agent-3: 182.833333333333
    agent-4: 182.833333333333
    agent-5: 182.833333333333
  policy_reward_mean:
    agent-0: 156.09333333333328
    agent-1: 156.09333333333328
    agent-2: 156.09333333333328
    agent-3: 156.09333333333328
    agent-4: 156.09333333333328
    agent-5: 156.09333333333328
  policy_reward_min:
    agent-0: 93.00000000000041
    agent-1: 93.00000000000041
    agent-2: 93.00000000000041
    agent-3: 93.00000000000041
    agent-4: 93.00000000000041
    agent-5: 93.00000000000041
  sampler_perf:
    mean_env_wait_ms: 27.143972203946515
    mean_inference_ms: 12.893738634698614
    mean_processing_ms: 57.68097755572533
  time_since_restore: 28398.74816608429
  time_this_iter_s: 125.00198531150818
  time_total_s: 37524.7599799633
  timestamp: 1637055508
  timesteps_since_restore: 20640000
  timesteps_this_iter: 96000
  timesteps_total: 26400000
  training_iteration: 275
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    275 |          37524.8 | 26400000 |   936.56 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 1.33
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 29.89
    apples_agent-1_min: 0
    apples_agent-2_max: 83
    apples_agent-2_mean: 10.09
    apples_agent-2_min: 0
    apples_agent-3_max: 159
    apples_agent-3_mean: 67.97
    apples_agent-3_min: 36
    apples_agent-4_max: 42
    apples_agent-4_mean: 1.51
    apples_agent-4_min: 0
    apples_agent-5_max: 181
    apples_agent-5_mean: 91.51
    apples_agent-5_min: 56
    cleaning_beam_agent-0_max: 535
    cleaning_beam_agent-0_mean: 432.7
    cleaning_beam_agent-0_min: 322
    cleaning_beam_agent-1_max: 455
    cleaning_beam_agent-1_mean: 221.72
    cleaning_beam_agent-1_min: 126
    cleaning_beam_agent-2_max: 581
    cleaning_beam_agent-2_mean: 348.97
    cleaning_beam_agent-2_min: 136
    cleaning_beam_agent-3_max: 62
    cleaning_beam_agent-3_mean: 28.31
    cleaning_beam_agent-3_min: 12
    cleaning_beam_agent-4_max: 582
    cleaning_beam_agent-4_mean: 480.05
    cleaning_beam_agent-4_min: 384
    cleaning_beam_agent-5_max: 144
    cleaning_beam_agent-5_mean: 36.26
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-40-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1098.9999999999952
  episode_reward_mean: 922.8199999999837
  episode_reward_min: 683.9999999999973
  episodes_this_iter: 96
  episodes_total: 26496
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11544.949
    learner:
      agent-0:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9219608306884766
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015101920580491424
        model: {}
        policy_loss: -0.003140583634376526
        total_loss: -0.002982262521982193
        vf_explained_var: 0.032463088631629944
        vf_loss: 17.80976676940918
      agent-1:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.152313470840454
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011684028431773186
        model: {}
        policy_loss: -0.0037528518587350845
        total_loss: -0.00371362641453743
        vf_explained_var: -0.10863068699836731
        vf_loss: 20.672962188720703
      agent-2:
        cur_kl_coeff: 1.0587911998450935e-23
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0783112049102783
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021894744131714106
        model: {}
        policy_loss: -0.003697718260809779
        total_loss: -0.0037555221933871508
        vf_explained_var: 0.01365409791469574
        vf_loss: 18.40023422241211
      agent-3:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4877605438232422
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012466181069612503
        model: {}
        policy_loss: -0.002497524954378605
        total_loss: -0.001605781028047204
        vf_explained_var: 0.050309017300605774
        vf_loss: 17.502017974853516
      agent-4:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9304137229919434
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015545851783826947
        model: {}
        policy_loss: -0.003745927009731531
        total_loss: -0.003527888562530279
        vf_explained_var: 0.00041909515857696533
        vf_loss: 18.555665969848633
      agent-5:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7305190563201904
        entropy_coeff: 0.0017600000137463212
        kl: 0.002029906492680311
        model: {}
        policy_loss: -0.003273547161370516
        total_loss: -0.0028315247036516666
        vf_explained_var: 0.060200512409210205
        vf_loss: 17.277374267578125
    load_time_ms: 13392.374
    num_steps_sampled: 26496000
    num_steps_trained: 26496000
    sample_time_ms: 99459.109
    update_time_ms: 14.817
  iterations_since_restore: 216
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.584745762711865
    ram_util_percent: 16.20395480225989
  pid: 30948
  policy_reward_max:
    agent-0: 183.16666666666643
    agent-1: 183.16666666666643
    agent-2: 183.16666666666643
    agent-3: 183.16666666666643
    agent-4: 183.16666666666643
    agent-5: 183.16666666666643
  policy_reward_mean:
    agent-0: 153.8033333333333
    agent-1: 153.8033333333333
    agent-2: 153.8033333333333
    agent-3: 153.8033333333333
    agent-4: 153.8033333333333
    agent-5: 153.8033333333333
  policy_reward_min:
    agent-0: 114.00000000000038
    agent-1: 114.00000000000038
    agent-2: 114.00000000000038
    agent-3: 114.00000000000038
    agent-4: 114.00000000000038
    agent-5: 114.00000000000038
  sampler_perf:
    mean_env_wait_ms: 27.14436377918632
    mean_inference_ms: 12.892735110090898
    mean_processing_ms: 57.675378691883
  time_since_restore: 28523.33834195137
  time_this_iter_s: 124.59017586708069
  time_total_s: 37649.35015583038
  timestamp: 1637055633
  timesteps_since_restore: 20736000
  timesteps_this_iter: 96000
  timesteps_total: 26496000
  training_iteration: 276
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    276 |          37649.4 | 26496000 |   922.82 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 1.64
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 33.86
    apples_agent-1_min: 0
    apples_agent-2_max: 131
    apples_agent-2_mean: 8.91
    apples_agent-2_min: 0
    apples_agent-3_max: 106
    apples_agent-3_mean: 70.76
    apples_agent-3_min: 41
    apples_agent-4_max: 27
    apples_agent-4_mean: 0.84
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 90.57
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 526
    cleaning_beam_agent-0_mean: 426.2
    cleaning_beam_agent-0_min: 321
    cleaning_beam_agent-1_max: 378
    cleaning_beam_agent-1_mean: 225.49
    cleaning_beam_agent-1_min: 116
    cleaning_beam_agent-2_max: 559
    cleaning_beam_agent-2_mean: 368.72
    cleaning_beam_agent-2_min: 134
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 27.27
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 564
    cleaning_beam_agent-4_mean: 468.2
    cleaning_beam_agent-4_min: 342
    cleaning_beam_agent-5_max: 171
    cleaning_beam_agent-5_mean: 34.81
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-42-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1095.9999999999648
  episode_reward_mean: 947.7599999999834
  episode_reward_min: 520.000000000005
  episodes_this_iter: 96
  episodes_total: 26592
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11537.987
    learner:
      agent-0:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.931128203868866
        entropy_coeff: 0.0017600000137463212
        kl: 0.001209596754051745
        model: {}
        policy_loss: -0.0028357631526887417
        total_loss: -0.002717397641390562
        vf_explained_var: 0.0431918203830719
        vf_loss: 17.571517944335938
      agent-1:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1302261352539062
        entropy_coeff: 0.0017600000137463212
        kl: 0.001292628818191588
        model: {}
        policy_loss: -0.003635312430560589
        total_loss: -0.003527994267642498
        vf_explained_var: -0.09843122959136963
        vf_loss: 20.96515655517578
      agent-2:
        cur_kl_coeff: 5.293955999225468e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0650982856750488
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017933956114575267
        model: {}
        policy_loss: -0.003846735693514347
        total_loss: -0.003816467709839344
        vf_explained_var: -0.008499473333358765
        vf_loss: 19.04840087890625
      agent-3:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4803784191608429
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011026280699297786
        model: {}
        policy_loss: -0.002616703510284424
        total_loss: -0.0016911053098738194
        vf_explained_var: 0.04150620102882385
        vf_loss: 17.71062660217285
      agent-4:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9301881790161133
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022197726648300886
        model: {}
        policy_loss: -0.004274034406989813
        total_loss: -0.004001590423285961
        vf_explained_var: -0.015689149498939514
        vf_loss: 19.095752716064453
      agent-5:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7251496911048889
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008023771806620061
        model: {}
        policy_loss: -0.002840934321284294
        total_loss: -0.002380592282861471
        vf_explained_var: 0.05943116545677185
        vf_loss: 17.366052627563477
    load_time_ms: 13366.478
    num_steps_sampled: 26592000
    num_steps_trained: 26592000
    sample_time_ms: 99560.437
    update_time_ms: 15.428
  iterations_since_restore: 217
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.28379888268156
    ram_util_percent: 16.199441340782123
  pid: 30948
  policy_reward_max:
    agent-0: 182.6666666666665
    agent-1: 182.6666666666665
    agent-2: 182.6666666666665
    agent-3: 182.6666666666665
    agent-4: 182.6666666666665
    agent-5: 182.6666666666665
  policy_reward_mean:
    agent-0: 157.95999999999992
    agent-1: 157.95999999999992
    agent-2: 157.95999999999992
    agent-3: 157.95999999999992
    agent-4: 157.95999999999992
    agent-5: 157.95999999999992
  policy_reward_min:
    agent-0: 86.66666666666661
    agent-1: 86.66666666666661
    agent-2: 86.66666666666661
    agent-3: 86.66666666666661
    agent-4: 86.66666666666661
    agent-5: 86.66666666666661
  sampler_perf:
    mean_env_wait_ms: 27.14629195682021
    mean_inference_ms: 12.892283427541301
    mean_processing_ms: 57.6712118257481
  time_since_restore: 28648.82000875473
  time_this_iter_s: 125.48166680335999
  time_total_s: 37774.83182263374
  timestamp: 1637055758
  timesteps_since_restore: 20832000
  timesteps_this_iter: 96000
  timesteps_total: 26592000
  training_iteration: 277
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    277 |          37774.8 | 26592000 |   947.76 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 83
    apples_agent-0_mean: 2.22
    apples_agent-0_min: 0
    apples_agent-1_max: 116
    apples_agent-1_mean: 36.31
    apples_agent-1_min: 0
    apples_agent-2_max: 258
    apples_agent-2_mean: 12.42
    apples_agent-2_min: 0
    apples_agent-3_max: 193
    apples_agent-3_mean: 70.04
    apples_agent-3_min: 37
    apples_agent-4_max: 52
    apples_agent-4_mean: 2.03
    apples_agent-4_min: 0
    apples_agent-5_max: 216
    apples_agent-5_mean: 90.51
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 529
    cleaning_beam_agent-0_mean: 411.42
    cleaning_beam_agent-0_min: 223
    cleaning_beam_agent-1_max: 421
    cleaning_beam_agent-1_mean: 226.2
    cleaning_beam_agent-1_min: 121
    cleaning_beam_agent-2_max: 612
    cleaning_beam_agent-2_mean: 356.52
    cleaning_beam_agent-2_min: 111
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 28.85
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 535
    cleaning_beam_agent-4_mean: 466.75
    cleaning_beam_agent-4_min: 341
    cleaning_beam_agent-5_max: 171
    cleaning_beam_agent-5_mean: 37.74
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 3
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-44-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1063.9999999999973
  episode_reward_mean: 926.4099999999846
  episode_reward_min: 537.0000000000143
  episodes_this_iter: 96
  episodes_total: 26688
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11538.81
    learner:
      agent-0:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9291895627975464
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015010517090559006
        model: {}
        policy_loss: -0.0028649114537984133
        total_loss: -0.0027073537930846214
        vf_explained_var: 0.0869310051202774
        vf_loss: 17.929338455200195
      agent-1:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1323232650756836
        entropy_coeff: 0.0017600000137463212
        kl: 0.001098071108572185
        model: {}
        policy_loss: -0.0036519132554531097
        total_loss: -0.003465995192527771
        vf_explained_var: -0.09490320086479187
        vf_loss: 21.78805923461914
      agent-2:
        cur_kl_coeff: 2.646977999612734e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0849506855010986
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014903037808835506
        model: {}
        policy_loss: -0.003442588960751891
        total_loss: -0.003340756520628929
        vf_explained_var: -0.012836083769798279
        vf_loss: 20.113487243652344
      agent-3:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4914736747741699
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011357858311384916
        model: {}
        policy_loss: -0.002369182649999857
        total_loss: -0.0014012455940246582
        vf_explained_var: 0.07239261269569397
        vf_loss: 18.329286575317383
      agent-4:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9336051940917969
        entropy_coeff: 0.0017600000137463212
        kl: 0.001536402152851224
        model: {}
        policy_loss: -0.003777402453124523
        total_loss: -0.0034439428709447384
        vf_explained_var: 0.0022871047258377075
        vf_loss: 19.76604652404785
      agent-5:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7107174396514893
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012337606167420745
        model: {}
        policy_loss: -0.003158092964440584
        total_loss: -0.0025972952134907246
        vf_explained_var: 0.07646156847476959
        vf_loss: 18.116600036621094
    load_time_ms: 13376.621
    num_steps_sampled: 26688000
    num_steps_trained: 26688000
    sample_time_ms: 99607.735
    update_time_ms: 14.845
  iterations_since_restore: 218
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.359550561797754
    ram_util_percent: 16.202247191011235
  pid: 30948
  policy_reward_max:
    agent-0: 177.33333333333292
    agent-1: 177.33333333333292
    agent-2: 177.33333333333292
    agent-3: 177.33333333333292
    agent-4: 177.33333333333292
    agent-5: 177.33333333333292
  policy_reward_mean:
    agent-0: 154.4016666666666
    agent-1: 154.4016666666666
    agent-2: 154.4016666666666
    agent-3: 154.4016666666666
    agent-4: 154.4016666666666
    agent-5: 154.4016666666666
  policy_reward_min:
    agent-0: 89.50000000000013
    agent-1: 89.50000000000013
    agent-2: 89.50000000000013
    agent-3: 89.50000000000013
    agent-4: 89.50000000000013
    agent-5: 89.50000000000013
  sampler_perf:
    mean_env_wait_ms: 27.147023407187994
    mean_inference_ms: 12.891537905488919
    mean_processing_ms: 57.666509911368415
  time_since_restore: 28773.657523155212
  time_this_iter_s: 124.83751440048218
  time_total_s: 37899.669337034225
  timestamp: 1637055883
  timesteps_since_restore: 20928000
  timesteps_this_iter: 96000
  timesteps_total: 26688000
  training_iteration: 278
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    278 |          37899.7 | 26688000 |   926.41 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 2.6
    apples_agent-0_min: 0
    apples_agent-1_max: 115
    apples_agent-1_mean: 30.4
    apples_agent-1_min: 0
    apples_agent-2_max: 73
    apples_agent-2_mean: 7.38
    apples_agent-2_min: 0
    apples_agent-3_max: 160
    apples_agent-3_mean: 73.79
    apples_agent-3_min: 40
    apples_agent-4_max: 32
    apples_agent-4_mean: 0.88
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 91.33
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 521
    cleaning_beam_agent-0_mean: 423.47
    cleaning_beam_agent-0_min: 307
    cleaning_beam_agent-1_max: 437
    cleaning_beam_agent-1_mean: 234.12
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 611
    cleaning_beam_agent-2_mean: 357.76
    cleaning_beam_agent-2_min: 179
    cleaning_beam_agent-3_max: 57
    cleaning_beam_agent-3_mean: 25.36
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 576
    cleaning_beam_agent-4_mean: 469.87
    cleaning_beam_agent-4_min: 344
    cleaning_beam_agent-5_max: 126
    cleaning_beam_agent-5_mean: 32.79
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-46-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1090.999999999997
  episode_reward_mean: 937.7999999999832
  episode_reward_min: 508.00000000001336
  episodes_this_iter: 96
  episodes_total: 26784
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11570.429
    learner:
      agent-0:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9432531595230103
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016965133836492896
        model: {}
        policy_loss: -0.003240346908569336
        total_loss: -0.0031475508585572243
        vf_explained_var: 0.06661960482597351
        vf_loss: 17.529260635375977
      agent-1:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.137869119644165
        entropy_coeff: 0.0017600000137463212
        kl: 0.001826736843213439
        model: {}
        policy_loss: -0.003938696347177029
        total_loss: -0.0038755175191909075
        vf_explained_var: -0.08812388777732849
        vf_loss: 20.6583194732666
      agent-2:
        cur_kl_coeff: 1.323488999806367e-24
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0818943977355957
        entropy_coeff: 0.0017600000137463212
        kl: 0.001587415928952396
        model: {}
        policy_loss: -0.003973796963691711
        total_loss: -0.003972436301410198
        vf_explained_var: -0.0005300194025039673
        vf_loss: 19.054973602294922
      agent-3:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47536325454711914
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012782684061676264
        model: {}
        policy_loss: -0.0025501083582639694
        total_loss: -0.0016248589381575584
        vf_explained_var: 0.06334169209003448
        vf_loss: 17.618879318237305
      agent-4:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9299404621124268
        entropy_coeff: 0.0017600000137463212
        kl: 0.001576672657392919
        model: {}
        policy_loss: -0.003703589551150799
        total_loss: -0.0033705371897667646
        vf_explained_var: -0.03734126687049866
        vf_loss: 19.697473526000977
      agent-5:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7217729687690735
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008884135750122368
        model: {}
        policy_loss: -0.0028480691835284233
        total_loss: -0.0023836344480514526
        vf_explained_var: 0.07433751225471497
        vf_loss: 17.347537994384766
    load_time_ms: 13359.995
    num_steps_sampled: 26784000
    num_steps_trained: 26784000
    sample_time_ms: 99619.757
    update_time_ms: 14.813
  iterations_since_restore: 219
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.588636363636365
    ram_util_percent: 16.207386363636363
  pid: 30948
  policy_reward_max:
    agent-0: 181.8333333333326
    agent-1: 181.8333333333326
    agent-2: 181.8333333333326
    agent-3: 181.8333333333326
    agent-4: 181.8333333333326
    agent-5: 181.8333333333326
  policy_reward_mean:
    agent-0: 156.29999999999995
    agent-1: 156.29999999999995
    agent-2: 156.29999999999995
    agent-3: 156.29999999999995
    agent-4: 156.29999999999995
    agent-5: 156.29999999999995
  policy_reward_min:
    agent-0: 84.66666666666698
    agent-1: 84.66666666666698
    agent-2: 84.66666666666698
    agent-3: 84.66666666666698
    agent-4: 84.66666666666698
    agent-5: 84.66666666666698
  sampler_perf:
    mean_env_wait_ms: 27.147126014084233
    mean_inference_ms: 12.890546941376376
    mean_processing_ms: 57.659312180729266
  time_since_restore: 28897.436788082123
  time_this_iter_s: 123.7792649269104
  time_total_s: 38023.448601961136
  timestamp: 1637056007
  timesteps_since_restore: 21024000
  timesteps_this_iter: 96000
  timesteps_total: 26784000
  training_iteration: 279
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    279 |          38023.4 | 26784000 |    937.8 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 1.72
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 37.84
    apples_agent-1_min: 0
    apples_agent-2_max: 100
    apples_agent-2_mean: 10.81
    apples_agent-2_min: 0
    apples_agent-3_max: 113
    apples_agent-3_mean: 68.79
    apples_agent-3_min: 17
    apples_agent-4_max: 37
    apples_agent-4_mean: 0.66
    apples_agent-4_min: 0
    apples_agent-5_max: 121
    apples_agent-5_mean: 86.63
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 538
    cleaning_beam_agent-0_mean: 414.25
    cleaning_beam_agent-0_min: 203
    cleaning_beam_agent-1_max: 421
    cleaning_beam_agent-1_mean: 216.81
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 480
    cleaning_beam_agent-2_mean: 322.5
    cleaning_beam_agent-2_min: 155
    cleaning_beam_agent-3_max: 53
    cleaning_beam_agent-3_mean: 26.51
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 545
    cleaning_beam_agent-4_mean: 462.02
    cleaning_beam_agent-4_min: 361
    cleaning_beam_agent-5_max: 121
    cleaning_beam_agent-5_mean: 34.82
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-48-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1103.9999999999854
  episode_reward_mean: 930.2499999999843
  episode_reward_min: 508.00000000001336
  episodes_this_iter: 96
  episodes_total: 26880
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11573.382
    learner:
      agent-0:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9522729516029358
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011804911773651838
        model: {}
        policy_loss: -0.002932912204414606
        total_loss: -0.002853334415704012
        vf_explained_var: 0.044070467352867126
        vf_loss: 17.555805206298828
      agent-1:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1205730438232422
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011082671117037535
        model: {}
        policy_loss: -0.0037271040491759777
        total_loss: -0.0036342907696962357
        vf_explained_var: -0.1024581789970398
        vf_loss: 20.650224685668945
      agent-2:
        cur_kl_coeff: 6.617444999031835e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.108904242515564
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017299067694693804
        model: {}
        policy_loss: -0.0037343064323067665
        total_loss: -0.003827966284006834
        vf_explained_var: 0.0075054168701171875
        vf_loss: 18.580089569091797
      agent-3:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4873090386390686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008074063807725906
        model: {}
        policy_loss: -0.0025320020504295826
        total_loss: -0.001639756839722395
        vf_explained_var: 0.05598066747188568
        vf_loss: 17.499122619628906
      agent-4:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9215782284736633
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016091226134449244
        model: {}
        policy_loss: -0.0037968202959746122
        total_loss: -0.003533742856234312
        vf_explained_var: -0.010501056909561157
        vf_loss: 18.850570678710938
      agent-5:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.699506402015686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012448453344404697
        model: {}
        policy_loss: -0.0028372611850500107
        total_loss: -0.002382147591561079
        vf_explained_var: 0.08082062005996704
        vf_loss: 16.862422943115234
    load_time_ms: 13363.433
    num_steps_sampled: 26880000
    num_steps_trained: 26880000
    sample_time_ms: 99484.961
    update_time_ms: 14.784
  iterations_since_restore: 220
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.482485875706214
    ram_util_percent: 16.122598870056496
  pid: 30948
  policy_reward_max:
    agent-0: 183.99999999999977
    agent-1: 183.99999999999977
    agent-2: 183.99999999999977
    agent-3: 183.99999999999977
    agent-4: 183.99999999999977
    agent-5: 183.99999999999977
  policy_reward_mean:
    agent-0: 155.04166666666663
    agent-1: 155.04166666666663
    agent-2: 155.04166666666663
    agent-3: 155.04166666666663
    agent-4: 155.04166666666663
    agent-5: 155.04166666666663
  policy_reward_min:
    agent-0: 84.66666666666698
    agent-1: 84.66666666666698
    agent-2: 84.66666666666698
    agent-3: 84.66666666666698
    agent-4: 84.66666666666698
    agent-5: 84.66666666666698
  sampler_perf:
    mean_env_wait_ms: 27.146371692098246
    mean_inference_ms: 12.890180730277548
    mean_processing_ms: 57.65468107752287
  time_since_restore: 29021.578832626343
  time_this_iter_s: 124.14204454421997
  time_total_s: 38147.590646505356
  timestamp: 1637056131
  timesteps_since_restore: 21120000
  timesteps_this_iter: 96000
  timesteps_total: 26880000
  training_iteration: 280
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    280 |          38147.6 | 26880000 |   930.25 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 2.67
    apples_agent-0_min: 0
    apples_agent-1_max: 129
    apples_agent-1_mean: 34.56
    apples_agent-1_min: 0
    apples_agent-2_max: 87
    apples_agent-2_mean: 10.56
    apples_agent-2_min: 0
    apples_agent-3_max: 226
    apples_agent-3_mean: 72.33
    apples_agent-3_min: 35
    apples_agent-4_max: 75
    apples_agent-4_mean: 3.03
    apples_agent-4_min: 0
    apples_agent-5_max: 229
    apples_agent-5_mean: 89.96
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 513
    cleaning_beam_agent-0_mean: 412.24
    cleaning_beam_agent-0_min: 299
    cleaning_beam_agent-1_max: 435
    cleaning_beam_agent-1_mean: 215.71
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 533
    cleaning_beam_agent-2_mean: 316.18
    cleaning_beam_agent-2_min: 160
    cleaning_beam_agent-3_max: 66
    cleaning_beam_agent-3_mean: 25.79
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 553
    cleaning_beam_agent-4_mean: 457.63
    cleaning_beam_agent-4_min: 343
    cleaning_beam_agent-5_max: 239
    cleaning_beam_agent-5_mean: 36.44
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-50-57
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1087.000000000005
  episode_reward_mean: 922.6499999999826
  episode_reward_min: 543.0000000000067
  episodes_this_iter: 96
  episodes_total: 26976
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11629.877
    learner:
      agent-0:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9416125416755676
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012870890786871314
        model: {}
        policy_loss: -0.003185999346897006
        total_loss: -0.0028977887704968452
        vf_explained_var: 0.027940675616264343
        vf_loss: 19.454486846923828
      agent-1:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1285456418991089
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021766209974884987
        model: {}
        policy_loss: -0.0038428776897490025
        total_loss: -0.003637667279690504
        vf_explained_var: -0.08469480276107788
        vf_loss: 21.91450309753418
      agent-2:
        cur_kl_coeff: 3.3087224995159173e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.092430591583252
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013770569348707795
        model: {}
        policy_loss: -0.003796554636210203
        total_loss: -0.003681421745568514
        vf_explained_var: -0.01349802315235138
        vf_loss: 20.378089904785156
      agent-3:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4935729503631592
        entropy_coeff: 0.0017600000137463212
        kl: 0.001202719984576106
        model: {}
        policy_loss: -0.0027365826535969973
        total_loss: -0.0017576850950717926
        vf_explained_var: 0.07616420090198517
        vf_loss: 18.475841522216797
      agent-4:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9367881417274475
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013819729210808873
        model: {}
        policy_loss: -0.0038010277785360813
        total_loss: -0.0034484073985368013
        vf_explained_var: 0.00426638126373291
        vf_loss: 20.013662338256836
      agent-5:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7259305715560913
        entropy_coeff: 0.0017600000137463212
        kl: 0.001038423739373684
        model: {}
        policy_loss: -0.0030583173502236605
        total_loss: -0.0025103832595050335
        vf_explained_var: 0.08725075423717499
        vf_loss: 18.255718231201172
    load_time_ms: 13381.834
    num_steps_sampled: 26976000
    num_steps_trained: 26976000
    sample_time_ms: 99522.09
    update_time_ms: 14.872
  iterations_since_restore: 221
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.354494382022473
    ram_util_percent: 16.20168539325843
  pid: 30948
  policy_reward_max:
    agent-0: 181.16666666666666
    agent-1: 181.16666666666666
    agent-2: 181.16666666666666
    agent-3: 181.16666666666666
    agent-4: 181.16666666666666
    agent-5: 181.16666666666666
  policy_reward_mean:
    agent-0: 153.77499999999998
    agent-1: 153.77499999999998
    agent-2: 153.77499999999998
    agent-3: 153.77499999999998
    agent-4: 153.77499999999998
    agent-5: 153.77499999999998
  policy_reward_min:
    agent-0: 90.50000000000026
    agent-1: 90.50000000000026
    agent-2: 90.50000000000026
    agent-3: 90.50000000000026
    agent-4: 90.50000000000026
    agent-5: 90.50000000000026
  sampler_perf:
    mean_env_wait_ms: 27.146097471986895
    mean_inference_ms: 12.889543308388307
    mean_processing_ms: 57.64927275732942
  time_since_restore: 29146.761449098587
  time_this_iter_s: 125.18261647224426
  time_total_s: 38272.7732629776
  timestamp: 1637056257
  timesteps_since_restore: 21216000
  timesteps_this_iter: 96000
  timesteps_total: 26976000
  training_iteration: 281
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    281 |          38272.8 | 26976000 |   922.65 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 67
    apples_agent-0_mean: 3.03
    apples_agent-0_min: 0
    apples_agent-1_max: 188
    apples_agent-1_mean: 38.77
    apples_agent-1_min: 0
    apples_agent-2_max: 66
    apples_agent-2_mean: 7.83
    apples_agent-2_min: 0
    apples_agent-3_max: 119
    apples_agent-3_mean: 69.14
    apples_agent-3_min: 35
    apples_agent-4_max: 50
    apples_agent-4_mean: 1.93
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 87.52
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 501
    cleaning_beam_agent-0_mean: 403.27
    cleaning_beam_agent-0_min: 257
    cleaning_beam_agent-1_max: 553
    cleaning_beam_agent-1_mean: 216.81
    cleaning_beam_agent-1_min: 124
    cleaning_beam_agent-2_max: 531
    cleaning_beam_agent-2_mean: 339.51
    cleaning_beam_agent-2_min: 172
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 26.14
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 538
    cleaning_beam_agent-4_mean: 457.56
    cleaning_beam_agent-4_min: 368
    cleaning_beam_agent-5_max: 184
    cleaning_beam_agent-5_mean: 34.46
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-53-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1116.0000000000027
  episode_reward_mean: 933.3699999999843
  episode_reward_min: 434.00000000000847
  episodes_this_iter: 96
  episodes_total: 27072
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11627.506
    learner:
      agent-0:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9381206631660461
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016233962960541248
        model: {}
        policy_loss: -0.003347715362906456
        total_loss: -0.003122192807495594
        vf_explained_var: 0.06753766536712646
        vf_loss: 18.7661075592041
      agent-1:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1302292346954346
        entropy_coeff: 0.0017600000137463212
        kl: 0.00156758155208081
        model: {}
        policy_loss: -0.0037963304203003645
        total_loss: -0.0035895195323973894
        vf_explained_var: -0.0919061005115509
        vf_loss: 21.96015739440918
      agent-2:
        cur_kl_coeff: 1.6543612497579586e-25
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0928033590316772
        entropy_coeff: 0.0017600000137463212
        kl: 0.001528054242953658
        model: {}
        policy_loss: -0.003576982766389847
        total_loss: -0.003456663340330124
        vf_explained_var: -0.021211013197898865
        vf_loss: 20.436538696289062
      agent-3:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4931463599205017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009625700768083334
        model: {}
        policy_loss: -0.002561287023127079
        total_loss: -0.001573630841448903
        vf_explained_var: 0.07143761217594147
        vf_loss: 18.555919647216797
      agent-4:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9454023838043213
        entropy_coeff: 0.0017600000137463212
        kl: 0.002050081267952919
        model: {}
        policy_loss: -0.003995971288532019
        total_loss: -0.0036490578204393387
        vf_explained_var: -0.0012224912643432617
        vf_loss: 20.108203887939453
      agent-5:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7108872532844543
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008396176854148507
        model: {}
        policy_loss: -0.003097935114055872
        total_loss: -0.002574373735114932
        vf_explained_var: 0.10771414637565613
        vf_loss: 17.747234344482422
    load_time_ms: 13379.715
    num_steps_sampled: 27072000
    num_steps_trained: 27072000
    sample_time_ms: 99553.728
    update_time_ms: 15.577
  iterations_since_restore: 222
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.23465909090909
    ram_util_percent: 16.119886363636365
  pid: 30948
  policy_reward_max:
    agent-0: 185.99999999999977
    agent-1: 185.99999999999977
    agent-2: 185.99999999999977
    agent-3: 185.99999999999977
    agent-4: 185.99999999999977
    agent-5: 185.99999999999977
  policy_reward_mean:
    agent-0: 155.5616666666666
    agent-1: 155.5616666666666
    agent-2: 155.5616666666666
    agent-3: 155.5616666666666
    agent-4: 155.5616666666666
    agent-5: 155.5616666666666
  policy_reward_min:
    agent-0: 72.33333333333323
    agent-1: 72.33333333333323
    agent-2: 72.33333333333323
    agent-3: 72.33333333333323
    agent-4: 72.33333333333323
    agent-5: 72.33333333333323
  sampler_perf:
    mean_env_wait_ms: 27.14522392164703
    mean_inference_ms: 12.888584056134954
    mean_processing_ms: 57.64291481824837
  time_since_restore: 29270.831678152084
  time_this_iter_s: 124.07022905349731
  time_total_s: 38396.8434920311
  timestamp: 1637056381
  timesteps_since_restore: 21312000
  timesteps_this_iter: 96000
  timesteps_total: 27072000
  training_iteration: 282
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    282 |          38396.8 | 27072000 |   933.37 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 1.8
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 33.83
    apples_agent-1_min: 0
    apples_agent-2_max: 60
    apples_agent-2_mean: 9.23
    apples_agent-2_min: 0
    apples_agent-3_max: 130
    apples_agent-3_mean: 72.02
    apples_agent-3_min: 20
    apples_agent-4_max: 52
    apples_agent-4_mean: 3.37
    apples_agent-4_min: 0
    apples_agent-5_max: 149
    apples_agent-5_mean: 91.42
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 559
    cleaning_beam_agent-0_mean: 426.86
    cleaning_beam_agent-0_min: 308
    cleaning_beam_agent-1_max: 464
    cleaning_beam_agent-1_mean: 230.08
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 498
    cleaning_beam_agent-2_mean: 321.89
    cleaning_beam_agent-2_min: 151
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 23.92
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 530
    cleaning_beam_agent-4_mean: 450.98
    cleaning_beam_agent-4_min: 264
    cleaning_beam_agent-5_max: 115
    cleaning_beam_agent-5_mean: 32.76
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-55-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1118.9999999999902
  episode_reward_mean: 924.8299999999832
  episode_reward_min: 397.0000000000031
  episodes_this_iter: 96
  episodes_total: 27168
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11587.672
    learner:
      agent-0:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.934119701385498
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018727895803749561
        model: {}
        policy_loss: -0.0032765064388513565
        total_loss: -0.0030072228983044624
        vf_explained_var: 0.04333828389644623
        vf_loss: 19.133350372314453
      agent-1:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1264798641204834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011448704171925783
        model: {}
        policy_loss: -0.003875192254781723
        total_loss: -0.003621229901909828
        vf_explained_var: -0.10611537098884583
        vf_loss: 22.36568260192871
      agent-2:
        cur_kl_coeff: 8.271806248789793e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0995593070983887
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014980228152126074
        model: {}
        policy_loss: -0.003833481576293707
        total_loss: -0.003742368659004569
        vf_explained_var: -0.007793724536895752
        vf_loss: 20.26335334777832
      agent-3:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.500873863697052
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009342096745967865
        model: {}
        policy_loss: -0.002651928924024105
        total_loss: -0.0017383457161486149
        vf_explained_var: 0.10048273205757141
        vf_loss: 17.95117950439453
      agent-4:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9374347925186157
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013049512635916471
        model: {}
        policy_loss: -0.003568317973986268
        total_loss: -0.0032332055270671844
        vf_explained_var: 0.011981755495071411
        vf_loss: 19.849994659423828
      agent-5:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7317490577697754
        entropy_coeff: 0.0017600000137463212
        kl: 0.000983390724286437
        model: {}
        policy_loss: -0.0031896913424134254
        total_loss: -0.0026564043946564198
        vf_explained_var: 0.08839140832424164
        vf_loss: 18.211687088012695
    load_time_ms: 13378.604
    num_steps_sampled: 27168000
    num_steps_trained: 27168000
    sample_time_ms: 99456.984
    update_time_ms: 15.579
  iterations_since_restore: 223
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.555681818181824
    ram_util_percent: 16.19090909090909
  pid: 30948
  policy_reward_max:
    agent-0: 186.50000000000009
    agent-1: 186.50000000000009
    agent-2: 186.50000000000009
    agent-3: 186.50000000000009
    agent-4: 186.50000000000009
    agent-5: 186.50000000000009
  policy_reward_mean:
    agent-0: 154.13833333333332
    agent-1: 154.13833333333332
    agent-2: 154.13833333333332
    agent-3: 154.13833333333332
    agent-4: 154.13833333333332
    agent-5: 154.13833333333332
  policy_reward_min:
    agent-0: 66.1666666666666
    agent-1: 66.1666666666666
    agent-2: 66.1666666666666
    agent-3: 66.1666666666666
    agent-4: 66.1666666666666
    agent-5: 66.1666666666666
  sampler_perf:
    mean_env_wait_ms: 27.144148519931495
    mean_inference_ms: 12.887556140193066
    mean_processing_ms: 57.63668175796049
  time_since_restore: 29394.531289100647
  time_this_iter_s: 123.69961094856262
  time_total_s: 38520.54310297966
  timestamp: 1637056505
  timesteps_since_restore: 21408000
  timesteps_this_iter: 96000
  timesteps_total: 27168000
  training_iteration: 283
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    283 |          38520.5 | 27168000 |   924.83 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 1.65
    apples_agent-0_min: 0
    apples_agent-1_max: 143
    apples_agent-1_mean: 36.68
    apples_agent-1_min: 0
    apples_agent-2_max: 61
    apples_agent-2_mean: 8.07
    apples_agent-2_min: 0
    apples_agent-3_max: 130
    apples_agent-3_mean: 68.64
    apples_agent-3_min: 34
    apples_agent-4_max: 15
    apples_agent-4_mean: 0.58
    apples_agent-4_min: 0
    apples_agent-5_max: 167
    apples_agent-5_mean: 89.7
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 537
    cleaning_beam_agent-0_mean: 424.88
    cleaning_beam_agent-0_min: 284
    cleaning_beam_agent-1_max: 423
    cleaning_beam_agent-1_mean: 225.62
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 518
    cleaning_beam_agent-2_mean: 317.83
    cleaning_beam_agent-2_min: 166
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 23.14
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 548
    cleaning_beam_agent-4_mean: 464.52
    cleaning_beam_agent-4_min: 325
    cleaning_beam_agent-5_max: 133
    cleaning_beam_agent-5_mean: 34.09
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-57-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1107.9999999999854
  episode_reward_mean: 940.9999999999843
  episode_reward_min: 470.00000000000637
  episodes_this_iter: 96
  episodes_total: 27264
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11574.398
    learner:
      agent-0:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9305537343025208
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016857926966622472
        model: {}
        policy_loss: -0.0035201478749513626
        total_loss: -0.0033070091158151627
        vf_explained_var: 0.0437544584274292
        vf_loss: 18.50912094116211
      agent-1:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1215801239013672
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012603860814124346
        model: {}
        policy_loss: -0.0037902677431702614
        total_loss: -0.0036507523618638515
        vf_explained_var: -0.06635349988937378
        vf_loss: 21.134963989257812
      agent-2:
        cur_kl_coeff: 4.1359031243948966e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.10782790184021
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014703288907185197
        model: {}
        policy_loss: -0.003985504154115915
        total_loss: -0.004059158265590668
        vf_explained_var: 0.04648329317569733
        vf_loss: 18.76121711730957
      agent-3:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47846561670303345
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008660496678203344
        model: {}
        policy_loss: -0.002446469385176897
        total_loss: -0.001498389057815075
        vf_explained_var: 0.07770828902721405
        vf_loss: 17.90176773071289
      agent-4:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9301297068595886
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013598750811070204
        model: {}
        policy_loss: -0.0036629927344620228
        total_loss: -0.003291534725576639
        vf_explained_var: -0.02667650580406189
        vf_loss: 20.084869384765625
      agent-5:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7191045880317688
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010868797544389963
        model: {}
        policy_loss: -0.0030876481905579567
        total_loss: -0.002607116475701332
        vf_explained_var: 0.09381482005119324
        vf_loss: 17.461570739746094
    load_time_ms: 13521.179
    num_steps_sampled: 27264000
    num_steps_trained: 27264000
    sample_time_ms: 99334.688
    update_time_ms: 15.697
  iterations_since_restore: 224
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.27247191011236
    ram_util_percent: 16.180337078651686
  pid: 30948
  policy_reward_max:
    agent-0: 184.66666666666603
    agent-1: 184.66666666666603
    agent-2: 184.66666666666603
    agent-3: 184.66666666666603
    agent-4: 184.66666666666603
    agent-5: 184.66666666666603
  policy_reward_mean:
    agent-0: 156.8333333333333
    agent-1: 156.8333333333333
    agent-2: 156.8333333333333
    agent-3: 156.8333333333333
    agent-4: 156.8333333333333
    agent-5: 156.8333333333333
  policy_reward_min:
    agent-0: 78.33333333333353
    agent-1: 78.33333333333353
    agent-2: 78.33333333333353
    agent-3: 78.33333333333353
    agent-4: 78.33333333333353
    agent-5: 78.33333333333353
  sampler_perf:
    mean_env_wait_ms: 27.143748865768927
    mean_inference_ms: 12.88660409085379
    mean_processing_ms: 57.629809144308645
  time_since_restore: 29519.398669958115
  time_this_iter_s: 124.86738085746765
  time_total_s: 38645.41048383713
  timestamp: 1637056630
  timesteps_since_restore: 21504000
  timesteps_this_iter: 96000
  timesteps_total: 27264000
  training_iteration: 284
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    284 |          38645.4 | 27264000 |      941 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 1.9
    apples_agent-0_min: 0
    apples_agent-1_max: 137
    apples_agent-1_mean: 37.81
    apples_agent-1_min: 0
    apples_agent-2_max: 120
    apples_agent-2_mean: 6.75
    apples_agent-2_min: 0
    apples_agent-3_max: 154
    apples_agent-3_mean: 69.74
    apples_agent-3_min: 24
    apples_agent-4_max: 48
    apples_agent-4_mean: 1.97
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 88.78
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 538
    cleaning_beam_agent-0_mean: 428.63
    cleaning_beam_agent-0_min: 280
    cleaning_beam_agent-1_max: 405
    cleaning_beam_agent-1_mean: 220.88
    cleaning_beam_agent-1_min: 104
    cleaning_beam_agent-2_max: 526
    cleaning_beam_agent-2_mean: 351.25
    cleaning_beam_agent-2_min: 106
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 23.37
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 523
    cleaning_beam_agent-4_mean: 449.44
    cleaning_beam_agent-4_min: 300
    cleaning_beam_agent-5_max: 106
    cleaning_beam_agent-5_mean: 33.6
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_04-59-13
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1107.9999999999854
  episode_reward_mean: 915.6899999999846
  episode_reward_min: 235.99999999999585
  episodes_this_iter: 96
  episodes_total: 27360
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11581.535
    learner:
      agent-0:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9265531301498413
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017434151377528906
        model: {}
        policy_loss: -0.003571632318198681
        total_loss: -0.0031982995569705963
        vf_explained_var: 0.08709713816642761
        vf_loss: 20.040672302246094
      agent-1:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.121265172958374
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017932277405634522
        model: {}
        policy_loss: -0.004040321335196495
        total_loss: -0.0036870096810162067
        vf_explained_var: -0.05623430013656616
        vf_loss: 23.26740074157715
      agent-2:
        cur_kl_coeff: 2.0679515621974483e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.086122751235962
        entropy_coeff: 0.0017600000137463212
        kl: 0.002305560512468219
        model: {}
        policy_loss: -0.003908154554665089
        total_loss: -0.003615845926105976
        vf_explained_var: -0.0032536685466766357
        vf_loss: 22.038875579833984
      agent-3:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49408742785453796
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011915841605514288
        model: {}
        policy_loss: -0.003041086718440056
        total_loss: -0.002044440945610404
        vf_explained_var: 0.150319904088974
        vf_loss: 18.66241455078125
      agent-4:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9379391670227051
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012531834654510021
        model: {}
        policy_loss: -0.003519684076309204
        total_loss: -0.0029525095596909523
        vf_explained_var: -0.008703798055648804
        vf_loss: 22.179508209228516
      agent-5:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7327324151992798
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016545335529372096
        model: {}
        policy_loss: -0.003372971899807453
        total_loss: -0.0027487389743328094
        vf_explained_var: 0.12808026373386383
        vf_loss: 19.138397216796875
    load_time_ms: 13533.33
    num_steps_sampled: 27360000
    num_steps_trained: 27360000
    sample_time_ms: 99106.52
    update_time_ms: 15.608
  iterations_since_restore: 225
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.35885714285714
    ram_util_percent: 16.207428571428576
  pid: 30948
  policy_reward_max:
    agent-0: 184.66666666666603
    agent-1: 184.66666666666603
    agent-2: 184.66666666666603
    agent-3: 184.66666666666603
    agent-4: 184.66666666666603
    agent-5: 184.66666666666603
  policy_reward_mean:
    agent-0: 152.61499999999995
    agent-1: 152.61499999999995
    agent-2: 152.61499999999995
    agent-3: 152.61499999999995
    agent-4: 152.61499999999995
    agent-5: 152.61499999999995
  policy_reward_min:
    agent-0: 39.333333333333336
    agent-1: 39.333333333333336
    agent-2: 39.333333333333336
    agent-3: 39.333333333333336
    agent-4: 39.333333333333336
    agent-5: 39.333333333333336
  sampler_perf:
    mean_env_wait_ms: 27.142337139906093
    mean_inference_ms: 12.885588454574163
    mean_processing_ms: 57.62078887153792
  time_since_restore: 29642.30419421196
  time_this_iter_s: 122.90552425384521
  time_total_s: 38768.31600809097
  timestamp: 1637056753
  timesteps_since_restore: 21600000
  timesteps_this_iter: 96000
  timesteps_total: 27360000
  training_iteration: 285
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    285 |          38768.3 | 27360000 |   915.69 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 3.08
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 35.92
    apples_agent-1_min: 0
    apples_agent-2_max: 47
    apples_agent-2_mean: 6.87
    apples_agent-2_min: 0
    apples_agent-3_max: 126
    apples_agent-3_mean: 71.35
    apples_agent-3_min: 37
    apples_agent-4_max: 50
    apples_agent-4_mean: 2.66
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 90.37
    apples_agent-5_min: 52
    cleaning_beam_agent-0_max: 523
    cleaning_beam_agent-0_mean: 424.54
    cleaning_beam_agent-0_min: 286
    cleaning_beam_agent-1_max: 498
    cleaning_beam_agent-1_mean: 232.38
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 521
    cleaning_beam_agent-2_mean: 362.76
    cleaning_beam_agent-2_min: 219
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 23.18
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 548
    cleaning_beam_agent-4_mean: 453.12
    cleaning_beam_agent-4_min: 341
    cleaning_beam_agent-5_max: 154
    cleaning_beam_agent-5_mean: 34.42
    cleaning_beam_agent-5_min: 11
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-01-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1098.9999999999764
  episode_reward_mean: 937.8299999999841
  episode_reward_min: 480.0000000000018
  episodes_this_iter: 96
  episodes_total: 27456
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11500.233
    learner:
      agent-0:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9120311141014099
        entropy_coeff: 0.0017600000137463212
        kl: 0.0023784046061336994
        model: {}
        policy_loss: -0.0032049878500401974
        total_loss: -0.0028374684043228626
        vf_explained_var: 0.08175098896026611
        vf_loss: 19.726974487304688
      agent-1:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.101033091545105
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011166469193995
        model: {}
        policy_loss: -0.003918303642421961
        total_loss: -0.003539937548339367
        vf_explained_var: -0.06365397572517395
        vf_loss: 23.161842346191406
      agent-2:
        cur_kl_coeff: 1.0339757810987241e-26
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0886107683181763
        entropy_coeff: 0.0017600000137463212
        kl: 0.002292547607794404
        model: {}
        policy_loss: -0.0038368168752640486
        total_loss: -0.0036231609992682934
        vf_explained_var: 0.015619993209838867
        vf_loss: 21.296079635620117
      agent-3:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4858418107032776
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007983046816661954
        model: {}
        policy_loss: -0.0027653886936604977
        total_loss: -0.0016690179472789168
        vf_explained_var: 0.08580151200294495
        vf_loss: 19.51450538635254
      agent-4:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9384779930114746
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015626156236976385
        model: {}
        policy_loss: -0.004094239789992571
        total_loss: -0.003586212871596217
        vf_explained_var: 0.0015569627285003662
        vf_loss: 21.59745979309082
      agent-5:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7246366739273071
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009815364610403776
        model: {}
        policy_loss: -0.0032121113035827875
        total_loss: -0.00257370388135314
        vf_explained_var: 0.10280342400074005
        vf_loss: 19.137666702270508
    load_time_ms: 13529.38
    num_steps_sampled: 27456000
    num_steps_trained: 27456000
    sample_time_ms: 99210.709
    update_time_ms: 15.541
  iterations_since_restore: 226
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.464606741573032
    ram_util_percent: 16.20674157303371
  pid: 30948
  policy_reward_max:
    agent-0: 183.16666666666632
    agent-1: 183.16666666666632
    agent-2: 183.16666666666632
    agent-3: 183.16666666666632
    agent-4: 183.16666666666632
    agent-5: 183.16666666666632
  policy_reward_mean:
    agent-0: 156.305
    agent-1: 156.305
    agent-2: 156.305
    agent-3: 156.305
    agent-4: 156.305
    agent-5: 156.305
  policy_reward_min:
    agent-0: 79.99999999999976
    agent-1: 79.99999999999976
    agent-2: 79.99999999999976
    agent-3: 79.99999999999976
    agent-4: 79.99999999999976
    agent-5: 79.99999999999976
  sampler_perf:
    mean_env_wait_ms: 27.143633542416204
    mean_inference_ms: 12.885088371805262
    mean_processing_ms: 57.61639054451877
  time_since_restore: 29767.038363456726
  time_this_iter_s: 124.73416924476624
  time_total_s: 38893.05017733574
  timestamp: 1637056878
  timesteps_since_restore: 21696000
  timesteps_this_iter: 96000
  timesteps_total: 27456000
  training_iteration: 286
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    286 |          38893.1 | 27456000 |   937.83 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 1.75
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 35.81
    apples_agent-1_min: 0
    apples_agent-2_max: 110
    apples_agent-2_mean: 9.88
    apples_agent-2_min: 0
    apples_agent-3_max: 128
    apples_agent-3_mean: 72.25
    apples_agent-3_min: 40
    apples_agent-4_max: 44
    apples_agent-4_mean: 2.41
    apples_agent-4_min: 0
    apples_agent-5_max: 159
    apples_agent-5_mean: 92.42
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 591
    cleaning_beam_agent-0_mean: 436.19
    cleaning_beam_agent-0_min: 329
    cleaning_beam_agent-1_max: 395
    cleaning_beam_agent-1_mean: 242.43
    cleaning_beam_agent-1_min: 86
    cleaning_beam_agent-2_max: 595
    cleaning_beam_agent-2_mean: 378.94
    cleaning_beam_agent-2_min: 186
    cleaning_beam_agent-3_max: 53
    cleaning_beam_agent-3_mean: 24.18
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 548
    cleaning_beam_agent-4_mean: 455.18
    cleaning_beam_agent-4_min: 316
    cleaning_beam_agent-5_max: 179
    cleaning_beam_agent-5_mean: 33.82
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-03-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1121.9999999999884
  episode_reward_mean: 937.2599999999837
  episode_reward_min: 596.9999999999975
  episodes_this_iter: 96
  episodes_total: 27552
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11517.766
    learner:
      agent-0:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9224405288696289
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013236927334219217
        model: {}
        policy_loss: -0.002954021794721484
        total_loss: -0.002708155894652009
        vf_explained_var: 0.03825873136520386
        vf_loss: 18.693645477294922
      agent-1:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.116550326347351
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014425567351281643
        model: {}
        policy_loss: -0.00396705511957407
        total_loss: -0.0037221061065793037
        vf_explained_var: -0.11334091424942017
        vf_loss: 22.10076904296875
      agent-2:
        cur_kl_coeff: 5.169878905493621e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0661412477493286
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014461074024438858
        model: {}
        policy_loss: -0.0037606225814670324
        total_loss: -0.0037313743960112333
        vf_explained_var: 0.02771379053592682
        vf_loss: 19.056570053100586
      agent-3:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.486982136964798
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009589738911017776
        model: {}
        policy_loss: -0.002503233263269067
        total_loss: -0.001539656426757574
        vf_explained_var: 0.06584879755973816
        vf_loss: 18.206634521484375
      agent-4:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9388797283172607
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013640034012496471
        model: {}
        policy_loss: -0.0036250390112400055
        total_loss: -0.0032529025338590145
        vf_explained_var: -0.02741798758506775
        vf_loss: 20.245607376098633
      agent-5:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7167096734046936
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011016004718840122
        model: {}
        policy_loss: -0.00304674101062119
        total_loss: -0.002485778881236911
        vf_explained_var: 0.06049606204032898
        vf_loss: 18.223718643188477
    load_time_ms: 13534.013
    num_steps_sampled: 27552000
    num_steps_trained: 27552000
    sample_time_ms: 99056.579
    update_time_ms: 14.907
  iterations_since_restore: 227
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.520454545454548
    ram_util_percent: 16.194886363636364
  pid: 30948
  policy_reward_max:
    agent-0: 187.0
    agent-1: 187.0
    agent-2: 187.0
    agent-3: 187.0
    agent-4: 187.0
    agent-5: 187.0
  policy_reward_mean:
    agent-0: 156.20999999999998
    agent-1: 156.20999999999998
    agent-2: 156.20999999999998
    agent-3: 156.20999999999998
    agent-4: 156.20999999999998
    agent-5: 156.20999999999998
  policy_reward_min:
    agent-0: 99.50000000000054
    agent-1: 99.50000000000054
    agent-2: 99.50000000000054
    agent-3: 99.50000000000054
    agent-4: 99.50000000000054
    agent-5: 99.50000000000054
  sampler_perf:
    mean_env_wait_ms: 27.14391075653436
    mean_inference_ms: 12.88442833189311
    mean_processing_ms: 57.60969971100702
  time_since_restore: 29891.232861995697
  time_this_iter_s: 124.19449853897095
  time_total_s: 39017.24467587471
  timestamp: 1637057002
  timesteps_since_restore: 21792000
  timesteps_this_iter: 96000
  timesteps_total: 27552000
  training_iteration: 287
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    287 |          39017.2 | 27552000 |   937.26 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 1.9
    apples_agent-0_min: 0
    apples_agent-1_max: 110
    apples_agent-1_mean: 33.35
    apples_agent-1_min: 0
    apples_agent-2_max: 100
    apples_agent-2_mean: 9.42
    apples_agent-2_min: 0
    apples_agent-3_max: 274
    apples_agent-3_mean: 73.54
    apples_agent-3_min: 42
    apples_agent-4_max: 33
    apples_agent-4_mean: 0.79
    apples_agent-4_min: 0
    apples_agent-5_max: 310
    apples_agent-5_mean: 94.72
    apples_agent-5_min: 58
    cleaning_beam_agent-0_max: 538
    cleaning_beam_agent-0_mean: 438.08
    cleaning_beam_agent-0_min: 286
    cleaning_beam_agent-1_max: 455
    cleaning_beam_agent-1_mean: 242.94
    cleaning_beam_agent-1_min: 122
    cleaning_beam_agent-2_max: 543
    cleaning_beam_agent-2_mean: 383.08
    cleaning_beam_agent-2_min: 191
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 22.68
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 543
    cleaning_beam_agent-4_mean: 465.48
    cleaning_beam_agent-4_min: 375
    cleaning_beam_agent-5_max: 122
    cleaning_beam_agent-5_mean: 31.49
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-05-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1091.9999999999889
  episode_reward_mean: 956.1599999999833
  episode_reward_min: 645.9999999999975
  episodes_this_iter: 96
  episodes_total: 27648
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11534.213
    learner:
      agent-0:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9182940125465393
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014255839632824063
        model: {}
        policy_loss: -0.002944852225482464
        total_loss: -0.0027739405632019043
        vf_explained_var: 0.013887986540794373
        vf_loss: 17.871089935302734
      agent-1:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1122512817382812
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014843299286440015
        model: {}
        policy_loss: -0.0037845601327717304
        total_loss: -0.003700743429362774
        vf_explained_var: -0.09092620015144348
        vf_loss: 20.41376304626465
      agent-2:
        cur_kl_coeff: 2.5849394527468104e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0646133422851562
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015868981136009097
        model: {}
        policy_loss: -0.003755603451281786
        total_loss: -0.0037111067213118076
        vf_explained_var: -0.038661181926727295
        vf_loss: 19.18218994140625
      agent-3:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4849297106266022
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011212639510631561
        model: {}
        policy_loss: -0.002499458845704794
        total_loss: -0.0015955432318150997
        vf_explained_var: 0.030639082193374634
        vf_loss: 17.57392120361328
      agent-4:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.939460277557373
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014793702866882086
        model: {}
        policy_loss: -0.0038393191061913967
        total_loss: -0.003539526369422674
        vf_explained_var: -0.05703330039978027
        vf_loss: 19.532451629638672
      agent-5:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7209601998329163
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009511456592008471
        model: {}
        policy_loss: -0.0027797860093414783
        total_loss: -0.002300224732607603
        vf_explained_var: 0.032928138971328735
        vf_loss: 17.484516143798828
    load_time_ms: 13525.258
    num_steps_sampled: 27648000
    num_steps_trained: 27648000
    sample_time_ms: 99003.302
    update_time_ms: 15.085
  iterations_since_restore: 228
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.392090395480224
    ram_util_percent: 16.193220338983053
  pid: 30948
  policy_reward_max:
    agent-0: 181.99999999999955
    agent-1: 181.99999999999955
    agent-2: 181.99999999999955
    agent-3: 181.99999999999955
    agent-4: 181.99999999999955
    agent-5: 181.99999999999955
  policy_reward_mean:
    agent-0: 159.3599999999999
    agent-1: 159.3599999999999
    agent-2: 159.3599999999999
    agent-3: 159.3599999999999
    agent-4: 159.3599999999999
    agent-5: 159.3599999999999
  policy_reward_min:
    agent-0: 107.66666666666677
    agent-1: 107.66666666666677
    agent-2: 107.66666666666677
    agent-3: 107.66666666666677
    agent-4: 107.66666666666677
    agent-5: 107.66666666666677
  sampler_perf:
    mean_env_wait_ms: 27.14450665817089
    mean_inference_ms: 12.883554752060766
    mean_processing_ms: 57.60385033327491
  time_since_restore: 30015.556483268738
  time_this_iter_s: 124.32362127304077
  time_total_s: 39141.56829714775
  timestamp: 1637057127
  timesteps_since_restore: 21888000
  timesteps_this_iter: 96000
  timesteps_total: 27648000
  training_iteration: 288
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    288 |          39141.6 | 27648000 |   956.16 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 1.28
    apples_agent-0_min: 0
    apples_agent-1_max: 102
    apples_agent-1_mean: 36.76
    apples_agent-1_min: 0
    apples_agent-2_max: 63
    apples_agent-2_mean: 8.45
    apples_agent-2_min: 0
    apples_agent-3_max: 166
    apples_agent-3_mean: 78.23
    apples_agent-3_min: 42
    apples_agent-4_max: 23
    apples_agent-4_mean: 0.61
    apples_agent-4_min: 0
    apples_agent-5_max: 182
    apples_agent-5_mean: 91.29
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 552
    cleaning_beam_agent-0_mean: 428.33
    cleaning_beam_agent-0_min: 286
    cleaning_beam_agent-1_max: 455
    cleaning_beam_agent-1_mean: 232.73
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 577
    cleaning_beam_agent-2_mean: 401.36
    cleaning_beam_agent-2_min: 224
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 23.79
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 556
    cleaning_beam_agent-4_mean: 456.55
    cleaning_beam_agent-4_min: 364
    cleaning_beam_agent-5_max: 186
    cleaning_beam_agent-5_mean: 32.76
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-07-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1105.9999999999939
  episode_reward_mean: 944.9599999999849
  episode_reward_min: 554.0000000000023
  episodes_this_iter: 96
  episodes_total: 27744
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11565.987
    learner:
      agent-0:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9214922189712524
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012747605796903372
        model: {}
        policy_loss: -0.002847679890692234
        total_loss: -0.00259997695684433
        vf_explained_var: 0.0729038417339325
        vf_loss: 18.69530487060547
      agent-1:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1191811561584473
        entropy_coeff: 0.0017600000137463212
        kl: 0.000886295223608613
        model: {}
        policy_loss: -0.003646592143923044
        total_loss: -0.003347741672769189
        vf_explained_var: -0.08598870038986206
        vf_loss: 22.686107635498047
      agent-2:
        cur_kl_coeff: 1.2924697263734052e-27
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0605182647705078
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015672252047806978
        model: {}
        policy_loss: -0.0034649870358407497
        total_loss: -0.003304106183350086
        vf_explained_var: 0.008388236165046692
        vf_loss: 20.273937225341797
      agent-3:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5026032328605652
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009234893950633705
        model: {}
        policy_loss: -0.00249226251617074
        total_loss: -0.0015103244222700596
        vf_explained_var: 0.07853001356124878
        vf_loss: 18.665178298950195
      agent-4:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9427481889724731
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014691426185891032
        model: {}
        policy_loss: -0.00423405971378088
        total_loss: -0.0037934882566332817
        vf_explained_var: -0.025619417428970337
        vf_loss: 20.998071670532227
      agent-5:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7070894241333008
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017993355868384242
        model: {}
        policy_loss: -0.003192376345396042
        total_loss: -0.0026720925234258175
        vf_explained_var: 0.12276101112365723
        vf_loss: 17.647628784179688
    load_time_ms: 13545.77
    num_steps_sampled: 27744000
    num_steps_trained: 27744000
    sample_time_ms: 99062.656
    update_time_ms: 14.972
  iterations_since_restore: 229
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.279213483146066
    ram_util_percent: 16.139887640449437
  pid: 30948
  policy_reward_max:
    agent-0: 184.33333333333317
    agent-1: 184.33333333333317
    agent-2: 184.33333333333317
    agent-3: 184.33333333333317
    agent-4: 184.33333333333317
    agent-5: 184.33333333333317
  policy_reward_mean:
    agent-0: 157.49333333333328
    agent-1: 157.49333333333328
    agent-2: 157.49333333333328
    agent-3: 157.49333333333328
    agent-4: 157.49333333333328
    agent-5: 157.49333333333328
  policy_reward_min:
    agent-0: 92.33333333333356
    agent-1: 92.33333333333356
    agent-2: 92.33333333333356
    agent-3: 92.33333333333356
    agent-4: 92.33333333333356
    agent-5: 92.33333333333356
  sampler_perf:
    mean_env_wait_ms: 27.144709770294256
    mean_inference_ms: 12.882561345554258
    mean_processing_ms: 57.59765803602464
  time_since_restore: 30140.45403432846
  time_this_iter_s: 124.8975510597229
  time_total_s: 39266.465848207474
  timestamp: 1637057252
  timesteps_since_restore: 21984000
  timesteps_this_iter: 96000
  timesteps_total: 27744000
  training_iteration: 289
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    289 |          39266.5 | 27744000 |   944.96 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 1.13
    apples_agent-0_min: 0
    apples_agent-1_max: 143
    apples_agent-1_mean: 38.01
    apples_agent-1_min: 0
    apples_agent-2_max: 57
    apples_agent-2_mean: 4.78
    apples_agent-2_min: 0
    apples_agent-3_max: 130
    apples_agent-3_mean: 72.92
    apples_agent-3_min: 35
    apples_agent-4_max: 42
    apples_agent-4_mean: 1.98
    apples_agent-4_min: 0
    apples_agent-5_max: 179
    apples_agent-5_mean: 91.24
    apples_agent-5_min: 54
    cleaning_beam_agent-0_max: 523
    cleaning_beam_agent-0_mean: 422.56
    cleaning_beam_agent-0_min: 300
    cleaning_beam_agent-1_max: 383
    cleaning_beam_agent-1_mean: 234.04
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 575
    cleaning_beam_agent-2_mean: 416.69
    cleaning_beam_agent-2_min: 225
    cleaning_beam_agent-3_max: 71
    cleaning_beam_agent-3_mean: 24.77
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 556
    cleaning_beam_agent-4_mean: 460.06
    cleaning_beam_agent-4_min: 354
    cleaning_beam_agent-5_max: 213
    cleaning_beam_agent-5_mean: 30.31
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-09-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1128.0000000000157
  episode_reward_mean: 946.5699999999863
  episode_reward_min: 389.000000000002
  episodes_this_iter: 96
  episodes_total: 27840
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11563.998
    learner:
      agent-0:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9291597604751587
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012479826109483838
        model: {}
        policy_loss: -0.003049504477530718
        total_loss: -0.002817906904965639
        vf_explained_var: 0.05304686725139618
        vf_loss: 18.669179916381836
      agent-1:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1193735599517822
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012931000674143434
        model: {}
        policy_loss: -0.004072982352226973
        total_loss: -0.0038978434167802334
        vf_explained_var: -0.05089685320854187
        vf_loss: 21.452363967895508
      agent-2:
        cur_kl_coeff: 6.462348631867026e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0499773025512695
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017141809221357107
        model: {}
        policy_loss: -0.003600622992962599
        total_loss: -0.0034706429578363895
        vf_explained_var: 0.008172288537025452
        vf_loss: 19.779409408569336
      agent-3:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5079993009567261
        entropy_coeff: 0.0017600000137463212
        kl: 0.001146190334111452
        model: {}
        policy_loss: -0.00246115168556571
        total_loss: -0.0015395525842905045
        vf_explained_var: 0.08309000730514526
        vf_loss: 18.15678596496582
      agent-4:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9402256011962891
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018852794310078025
        model: {}
        policy_loss: -0.0037261354736983776
        total_loss: -0.0033848988823592663
        vf_explained_var: 0.010164633393287659
        vf_loss: 19.96033477783203
      agent-5:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.671538770198822
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008414650219492614
        model: {}
        policy_loss: -0.002618893515318632
        total_loss: -0.0020378446206450462
        vf_explained_var: 0.10424752533435822
        vf_loss: 17.629558563232422
    load_time_ms: 13543.343
    num_steps_sampled: 27840000
    num_steps_trained: 27840000
    sample_time_ms: 99212.11
    update_time_ms: 14.989
  iterations_since_restore: 230
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.362569832402233
    ram_util_percent: 16.193296089385477
  pid: 30948
  policy_reward_max:
    agent-0: 187.99999999999983
    agent-1: 187.99999999999983
    agent-2: 187.99999999999983
    agent-3: 187.99999999999983
    agent-4: 187.99999999999983
    agent-5: 187.99999999999983
  policy_reward_mean:
    agent-0: 157.76166666666663
    agent-1: 157.76166666666663
    agent-2: 157.76166666666663
    agent-3: 157.76166666666663
    agent-4: 157.76166666666663
    agent-5: 157.76166666666663
  policy_reward_min:
    agent-0: 64.8333333333332
    agent-1: 64.8333333333332
    agent-2: 64.8333333333332
    agent-3: 64.8333333333332
    agent-4: 64.8333333333332
    agent-5: 64.8333333333332
  sampler_perf:
    mean_env_wait_ms: 27.14738516001699
    mean_inference_ms: 12.882049148899684
    mean_processing_ms: 57.594049355386566
  time_since_restore: 30266.081023454666
  time_this_iter_s: 125.62698912620544
  time_total_s: 39392.09283733368
  timestamp: 1637057377
  timesteps_since_restore: 22080000
  timesteps_this_iter: 96000
  timesteps_total: 27840000
  training_iteration: 290
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    290 |          39392.1 | 27840000 |   946.57 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 2.89
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 38.17
    apples_agent-1_min: 0
    apples_agent-2_max: 82
    apples_agent-2_mean: 9.62
    apples_agent-2_min: 0
    apples_agent-3_max: 135
    apples_agent-3_mean: 73.5
    apples_agent-3_min: 41
    apples_agent-4_max: 71
    apples_agent-4_mean: 2.09
    apples_agent-4_min: 0
    apples_agent-5_max: 139
    apples_agent-5_mean: 91.9
    apples_agent-5_min: 56
    cleaning_beam_agent-0_max: 501
    cleaning_beam_agent-0_mean: 410.06
    cleaning_beam_agent-0_min: 252
    cleaning_beam_agent-1_max: 399
    cleaning_beam_agent-1_mean: 223.06
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 575
    cleaning_beam_agent-2_mean: 396.8
    cleaning_beam_agent-2_min: 146
    cleaning_beam_agent-3_max: 69
    cleaning_beam_agent-3_mean: 25.95
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 466.41
    cleaning_beam_agent-4_min: 385
    cleaning_beam_agent-5_max: 151
    cleaning_beam_agent-5_mean: 27.55
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-11-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1134.000000000007
  episode_reward_mean: 937.4499999999855
  episode_reward_min: 374.000000000003
  episodes_this_iter: 96
  episodes_total: 27936
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11535.715
    learner:
      agent-0:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9296757578849792
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015094708651304245
        model: {}
        policy_loss: -0.002947421744465828
        total_loss: -0.002670425921678543
        vf_explained_var: 0.06144627928733826
        vf_loss: 19.132247924804688
      agent-1:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0991182327270508
        entropy_coeff: 0.0017600000137463212
        kl: 0.001396738225594163
        model: {}
        policy_loss: -0.0036480266135185957
        total_loss: -0.0032689240761101246
        vf_explained_var: -0.11542683839797974
        vf_loss: 23.135517120361328
      agent-2:
        cur_kl_coeff: 3.231174315933513e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0556236505508423
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014627701602876186
        model: {}
        policy_loss: -0.0037441602908074856
        total_loss: -0.0036225253716111183
        vf_explained_var: 0.029775679111480713
        vf_loss: 19.795352935791016
      agent-3:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5134279131889343
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008931168704293668
        model: {}
        policy_loss: -0.002830947283655405
        total_loss: -0.0018684681272134185
        vf_explained_var: 0.08206403255462646
        vf_loss: 18.66112518310547
      agent-4:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9306844472885132
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018649643752723932
        model: {}
        policy_loss: -0.0037120727356523275
        total_loss: -0.003286841092631221
        vf_explained_var: -0.007443338632583618
        vf_loss: 20.63235092163086
      agent-5:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7111496329307556
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008441468235105276
        model: {}
        policy_loss: -0.002803226001560688
        total_loss: -0.0022490653209388256
        vf_explained_var: 0.10869920253753662
        vf_loss: 18.05786895751953
    load_time_ms: 13532.237
    num_steps_sampled: 27936000
    num_steps_trained: 27936000
    sample_time_ms: 99266.613
    update_time_ms: 15.02
  iterations_since_restore: 231
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.424157303370787
    ram_util_percent: 16.13033707865169
  pid: 30948
  policy_reward_max:
    agent-0: 188.99999999999966
    agent-1: 188.99999999999966
    agent-2: 188.99999999999966
    agent-3: 188.99999999999966
    agent-4: 188.99999999999966
    agent-5: 188.99999999999966
  policy_reward_mean:
    agent-0: 156.24166666666662
    agent-1: 156.24166666666662
    agent-2: 156.24166666666662
    agent-3: 156.24166666666662
    agent-4: 156.24166666666662
    agent-5: 156.24166666666662
  policy_reward_min:
    agent-0: 62.33333333333314
    agent-1: 62.33333333333314
    agent-2: 62.33333333333314
    agent-3: 62.33333333333314
    agent-4: 62.33333333333314
    agent-5: 62.33333333333314
  sampler_perf:
    mean_env_wait_ms: 27.14827480281869
    mean_inference_ms: 12.8816887368231
    mean_processing_ms: 57.58830101344731
  time_since_restore: 30391.352250099182
  time_this_iter_s: 125.27122664451599
  time_total_s: 39517.364063978195
  timestamp: 1637057503
  timesteps_since_restore: 22176000
  timesteps_this_iter: 96000
  timesteps_total: 27936000
  training_iteration: 291
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    291 |          39517.4 | 27936000 |   937.45 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 61
    apples_agent-0_mean: 2.25
    apples_agent-0_min: 0
    apples_agent-1_max: 113
    apples_agent-1_mean: 36.47
    apples_agent-1_min: 0
    apples_agent-2_max: 109
    apples_agent-2_mean: 8.84
    apples_agent-2_min: 0
    apples_agent-3_max: 150
    apples_agent-3_mean: 73.72
    apples_agent-3_min: 33
    apples_agent-4_max: 77
    apples_agent-4_mean: 1.22
    apples_agent-4_min: 0
    apples_agent-5_max: 158
    apples_agent-5_mean: 92.0
    apples_agent-5_min: 46
    cleaning_beam_agent-0_max: 551
    cleaning_beam_agent-0_mean: 399.3
    cleaning_beam_agent-0_min: 274
    cleaning_beam_agent-1_max: 377
    cleaning_beam_agent-1_mean: 225.81
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 648
    cleaning_beam_agent-2_mean: 408.08
    cleaning_beam_agent-2_min: 172
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 24.35
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 542
    cleaning_beam_agent-4_mean: 468.62
    cleaning_beam_agent-4_min: 358
    cleaning_beam_agent-5_max: 132
    cleaning_beam_agent-5_mean: 31.63
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-13-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1083.9999999999889
  episode_reward_mean: 948.3299999999839
  episode_reward_min: 497.00000000001063
  episodes_this_iter: 96
  episodes_total: 28032
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11557.089
    learner:
      agent-0:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9297918677330017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016788324574008584
        model: {}
        policy_loss: -0.002962022554129362
        total_loss: -0.002780192531645298
        vf_explained_var: 0.06502626836299896
        vf_loss: 18.182640075683594
      agent-1:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1076375246047974
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013854947173967957
        model: {}
        policy_loss: -0.0038730110973119736
        total_loss: -0.00358389550819993
        vf_explained_var: -0.11632773280143738
        vf_loss: 22.385581970214844
      agent-2:
        cur_kl_coeff: 1.6155871579667565e-28
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0370299816131592
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014290910912677646
        model: {}
        policy_loss: -0.0032743760384619236
        total_loss: -0.0030683628283441067
        vf_explained_var: -0.028867512941360474
        vf_loss: 20.311838150024414
      agent-3:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49293771386146545
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009651865111663938
        model: {}
        policy_loss: -0.002401852048933506
        total_loss: -0.001419310225173831
        vf_explained_var: 0.05161172151565552
        vf_loss: 18.501113891601562
      agent-4:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9286989569664001
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015953113324940205
        model: {}
        policy_loss: -0.003404725342988968
        total_loss: -0.0029558795504271984
        vf_explained_var: -0.052817583084106445
        vf_loss: 20.833545684814453
      agent-5:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7124040126800537
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009878500131890178
        model: {}
        policy_loss: -0.0029548844322562218
        total_loss: -0.0024301623925566673
        vf_explained_var: 0.0867481529712677
        vf_loss: 17.785545349121094
    load_time_ms: 13525.417
    num_steps_sampled: 28032000
    num_steps_trained: 28032000
    sample_time_ms: 99255.346
    update_time_ms: 14.402
  iterations_since_restore: 232
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.32090395480226
    ram_util_percent: 16.198305084745765
  pid: 30948
  policy_reward_max:
    agent-0: 180.66666666666634
    agent-1: 180.66666666666634
    agent-2: 180.66666666666634
    agent-3: 180.66666666666634
    agent-4: 180.66666666666634
    agent-5: 180.66666666666634
  policy_reward_mean:
    agent-0: 158.05499999999995
    agent-1: 158.05499999999995
    agent-2: 158.05499999999995
    agent-3: 158.05499999999995
    agent-4: 158.05499999999995
    agent-5: 158.05499999999995
  policy_reward_min:
    agent-0: 82.83333333333333
    agent-1: 82.83333333333333
    agent-2: 82.83333333333333
    agent-3: 82.83333333333333
    agent-4: 82.83333333333333
    agent-5: 82.83333333333333
  sampler_perf:
    mean_env_wait_ms: 27.14904559516517
    mean_inference_ms: 12.880844887480102
    mean_processing_ms: 57.58356074880239
  time_since_restore: 30515.44632101059
  time_this_iter_s: 124.09407091140747
  time_total_s: 39641.4581348896
  timestamp: 1637057627
  timesteps_since_restore: 22272000
  timesteps_this_iter: 96000
  timesteps_total: 28032000
  training_iteration: 292
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    292 |          39641.5 | 28032000 |   948.33 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 43
    apples_agent-0_mean: 0.89
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 35.47
    apples_agent-1_min: 0
    apples_agent-2_max: 82
    apples_agent-2_mean: 8.06
    apples_agent-2_min: 0
    apples_agent-3_max: 110
    apples_agent-3_mean: 68.67
    apples_agent-3_min: 43
    apples_agent-4_max: 33
    apples_agent-4_mean: 0.8
    apples_agent-4_min: 0
    apples_agent-5_max: 182
    apples_agent-5_mean: 94.56
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 497
    cleaning_beam_agent-0_mean: 409.47
    cleaning_beam_agent-0_min: 296
    cleaning_beam_agent-1_max: 403
    cleaning_beam_agent-1_mean: 241.38
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 679
    cleaning_beam_agent-2_mean: 420.44
    cleaning_beam_agent-2_min: 145
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 26.12
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 560
    cleaning_beam_agent-4_mean: 486.17
    cleaning_beam_agent-4_min: 409
    cleaning_beam_agent-5_max: 98
    cleaning_beam_agent-5_mean: 28.12
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-15-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1109.9999999999945
  episode_reward_mean: 976.4899999999833
  episode_reward_min: 685.9999999999986
  episodes_this_iter: 96
  episodes_total: 28128
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11576.022
    learner:
      agent-0:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9396719932556152
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018984805792570114
        model: {}
        policy_loss: -0.0031843583565205336
        total_loss: -0.003099199151620269
        vf_explained_var: 0.039705514907836914
        vf_loss: 17.389812469482422
      agent-1:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1165493726730347
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012681104708462954
        model: {}
        policy_loss: -0.0036181616596877575
        total_loss: -0.003434355603531003
        vf_explained_var: -0.1278449296951294
        vf_loss: 21.489316940307617
      agent-2:
        cur_kl_coeff: 8.077935789833782e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0233980417251587
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014795425813645124
        model: {}
        policy_loss: -0.003352073486894369
        total_loss: -0.0032513244077563286
        vf_explained_var: -0.025222212076187134
        vf_loss: 19.019269943237305
      agent-3:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.484933465719223
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012845881283283234
        model: {}
        policy_loss: -0.002486754208803177
        total_loss: -0.0015210267156362534
        vf_explained_var: -0.0068321675062179565
        vf_loss: 18.192123413085938
      agent-4:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9111831188201904
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019519331399351358
        model: {}
        policy_loss: -0.0037602607626467943
        total_loss: -0.0034203119575977325
        vf_explained_var: -0.047746479511260986
        vf_loss: 19.43632698059082
      agent-5:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7014868259429932
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010958547936752439
        model: {}
        policy_loss: -0.003019861411303282
        total_loss: -0.002523501869291067
        vf_explained_var: 0.0496370792388916
        vf_loss: 17.30978012084961
    load_time_ms: 13529.616
    num_steps_sampled: 28128000
    num_steps_trained: 28128000
    sample_time_ms: 99362.174
    update_time_ms: 14.489
  iterations_since_restore: 233
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.3061797752809
    ram_util_percent: 16.12808988764045
  pid: 30948
  policy_reward_max:
    agent-0: 184.99999999999972
    agent-1: 184.99999999999972
    agent-2: 184.99999999999972
    agent-3: 184.99999999999972
    agent-4: 184.99999999999972
    agent-5: 184.99999999999972
  policy_reward_mean:
    agent-0: 162.74833333333322
    agent-1: 162.74833333333322
    agent-2: 162.74833333333322
    agent-3: 162.74833333333322
    agent-4: 162.74833333333322
    agent-5: 162.74833333333322
  policy_reward_min:
    agent-0: 114.33333333333351
    agent-1: 114.33333333333351
    agent-2: 114.33333333333351
    agent-3: 114.33333333333351
    agent-4: 114.33333333333351
    agent-5: 114.33333333333351
  sampler_perf:
    mean_env_wait_ms: 27.151324517326533
    mean_inference_ms: 12.879985302233592
    mean_processing_ms: 57.57799987901154
  time_since_restore: 30640.164586544037
  time_this_iter_s: 124.71826553344727
  time_total_s: 39766.17640042305
  timestamp: 1637057752
  timesteps_since_restore: 22368000
  timesteps_this_iter: 96000
  timesteps_total: 28128000
  training_iteration: 293
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    293 |          39766.2 | 28128000 |   976.49 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 1.43
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 30.87
    apples_agent-1_min: 0
    apples_agent-2_max: 123
    apples_agent-2_mean: 11.33
    apples_agent-2_min: 0
    apples_agent-3_max: 121
    apples_agent-3_mean: 70.86
    apples_agent-3_min: 39
    apples_agent-4_max: 61
    apples_agent-4_mean: 3.17
    apples_agent-4_min: 0
    apples_agent-5_max: 150
    apples_agent-5_mean: 88.85
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 518
    cleaning_beam_agent-0_mean: 403.09
    cleaning_beam_agent-0_min: 285
    cleaning_beam_agent-1_max: 431
    cleaning_beam_agent-1_mean: 258.17
    cleaning_beam_agent-1_min: 123
    cleaning_beam_agent-2_max: 596
    cleaning_beam_agent-2_mean: 404.91
    cleaning_beam_agent-2_min: 154
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 27.42
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 567
    cleaning_beam_agent-4_mean: 479.29
    cleaning_beam_agent-4_min: 309
    cleaning_beam_agent-5_max: 183
    cleaning_beam_agent-5_mean: 30.69
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-18-00
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1130.999999999979
  episode_reward_mean: 955.7199999999841
  episode_reward_min: 440.00000000000983
  episodes_this_iter: 96
  episodes_total: 28224
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11625.676
    learner:
      agent-0:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9353273510932922
        entropy_coeff: 0.0017600000137463212
        kl: 0.0026751209516078234
        model: {}
        policy_loss: -0.003480857703834772
        total_loss: -0.003363225841894746
        vf_explained_var: 0.062109336256980896
        vf_loss: 17.638080596923828
      agent-1:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.124434471130371
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016871319385245442
        model: {}
        policy_loss: -0.004012487828731537
        total_loss: -0.0038383316714316607
        vf_explained_var: -0.11148500442504883
        vf_loss: 21.531606674194336
      agent-2:
        cur_kl_coeff: 4.038967894916891e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0241140127182007
        entropy_coeff: 0.0017600000137463212
        kl: 0.001894683577120304
        model: {}
        policy_loss: -0.003954930230975151
        total_loss: -0.003754138946533203
        vf_explained_var: -0.04062163829803467
        vf_loss: 20.032316207885742
      agent-3:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5077070593833923
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014559123665094376
        model: {}
        policy_loss: -0.0026542863342911005
        total_loss: -0.0017321384511888027
        vf_explained_var: 0.03869941830635071
        vf_loss: 18.157129287719727
      agent-4:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9259133338928223
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014689601957798004
        model: {}
        policy_loss: -0.0036839600652456284
        total_loss: -0.0033687492832541466
        vf_explained_var: -0.006297573447227478
        vf_loss: 19.44817352294922
      agent-5:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7077645063400269
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008840120863169432
        model: {}
        policy_loss: -0.002741337986662984
        total_loss: -0.0022367206402122974
        vf_explained_var: 0.0733741968870163
        vf_loss: 17.502859115600586
    load_time_ms: 13385.073
    num_steps_sampled: 28224000
    num_steps_trained: 28224000
    sample_time_ms: 99761.411
    update_time_ms: 14.571
  iterations_since_restore: 234
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.284615384615382
    ram_util_percent: 16.197802197802204
  pid: 30948
  policy_reward_max:
    agent-0: 188.49999999999955
    agent-1: 188.49999999999955
    agent-2: 188.49999999999955
    agent-3: 188.49999999999955
    agent-4: 188.49999999999955
    agent-5: 188.49999999999955
  policy_reward_mean:
    agent-0: 159.2866666666666
    agent-1: 159.2866666666666
    agent-2: 159.2866666666666
    agent-3: 159.2866666666666
    agent-4: 159.2866666666666
    agent-5: 159.2866666666666
  policy_reward_min:
    agent-0: 73.33333333333326
    agent-1: 73.33333333333326
    agent-2: 73.33333333333326
    agent-3: 73.33333333333326
    agent-4: 73.33333333333326
    agent-5: 73.33333333333326
  sampler_perf:
    mean_env_wait_ms: 27.154983633673293
    mean_inference_ms: 12.879709311129139
    mean_processing_ms: 57.577139499657385
  time_since_restore: 30768.03954100609
  time_this_iter_s: 127.87495446205139
  time_total_s: 39894.0513548851
  timestamp: 1637057880
  timesteps_since_restore: 22464000
  timesteps_this_iter: 96000
  timesteps_total: 28224000
  training_iteration: 294
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    294 |          39894.1 | 28224000 |   955.72 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 0.73
    apples_agent-0_min: 0
    apples_agent-1_max: 124
    apples_agent-1_mean: 33.1
    apples_agent-1_min: 0
    apples_agent-2_max: 131
    apples_agent-2_mean: 9.67
    apples_agent-2_min: 0
    apples_agent-3_max: 155
    apples_agent-3_mean: 72.02
    apples_agent-3_min: 44
    apples_agent-4_max: 35
    apples_agent-4_mean: 0.9
    apples_agent-4_min: 0
    apples_agent-5_max: 203
    apples_agent-5_mean: 95.92
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 403.43
    cleaning_beam_agent-0_min: 300
    cleaning_beam_agent-1_max: 481
    cleaning_beam_agent-1_mean: 264.79
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 690
    cleaning_beam_agent-2_mean: 434.46
    cleaning_beam_agent-2_min: 224
    cleaning_beam_agent-3_max: 43
    cleaning_beam_agent-3_mean: 25.9
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 556
    cleaning_beam_agent-4_mean: 476.08
    cleaning_beam_agent-4_min: 335
    cleaning_beam_agent-5_max: 194
    cleaning_beam_agent-5_mean: 32.42
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-20-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1158.0000000000048
  episode_reward_mean: 965.8899999999829
  episode_reward_min: 746.9999999999934
  episodes_this_iter: 96
  episodes_total: 28320
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11654.093
    learner:
      agent-0:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9367443919181824
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012244547251611948
        model: {}
        policy_loss: -0.003058627713471651
        total_loss: -0.0028938762843608856
        vf_explained_var: 0.03901195526123047
        vf_loss: 18.13421058654785
      agent-1:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1204805374145508
        entropy_coeff: 0.0017600000137463212
        kl: 0.00160253478679806
        model: {}
        policy_loss: -0.004187948070466518
        total_loss: -0.004027532413601875
        vf_explained_var: -0.0940777063369751
        vf_loss: 21.324615478515625
      agent-2:
        cur_kl_coeff: 2.0194839474584456e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0251662731170654
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015334432246163487
        model: {}
        policy_loss: -0.003678297158330679
        total_loss: -0.003538764314725995
        vf_explained_var: -0.009265720844268799
        vf_loss: 19.43825912475586
      agent-3:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48416605591773987
        entropy_coeff: 0.0017600000137463212
        kl: 0.00123141473159194
        model: {}
        policy_loss: -0.002515906933695078
        total_loss: -0.0015330640599131584
        vf_explained_var: 0.023256048560142517
        vf_loss: 18.3497371673584
      agent-4:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9303960204124451
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014865355333313346
        model: {}
        policy_loss: -0.0038302552420645952
        total_loss: -0.003487173467874527
        vf_explained_var: -0.027772575616836548
        vf_loss: 19.805816650390625
      agent-5:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 0.710857093334198
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013497082982212305
        model: {}
        policy_loss: -0.0030621904879808426
        total_loss: -0.002580922096967697
        vf_explained_var: 0.08366659283638
        vf_loss: 17.323734283447266
    load_time_ms: 13647.22
    num_steps_sampled: 28320000
    num_steps_trained: 28320000
    sample_time_ms: 100064.107
    update_time_ms: 14.42
  iterations_since_restore: 235
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.12786885245902
    ram_util_percent: 16.174863387978146
  pid: 30948
  policy_reward_max:
    agent-0: 192.9999999999997
    agent-1: 192.9999999999997
    agent-2: 192.9999999999997
    agent-3: 192.9999999999997
    agent-4: 192.9999999999997
    agent-5: 192.9999999999997
  policy_reward_mean:
    agent-0: 160.98166666666654
    agent-1: 160.98166666666654
    agent-2: 160.98166666666654
    agent-3: 160.98166666666654
    agent-4: 160.98166666666654
    agent-5: 160.98166666666654
  policy_reward_min:
    agent-0: 124.5000000000005
    agent-1: 124.5000000000005
    agent-2: 124.5000000000005
    agent-3: 124.5000000000005
    agent-4: 124.5000000000005
    agent-5: 124.5000000000005
  sampler_perf:
    mean_env_wait_ms: 27.158226071651256
    mean_inference_ms: 12.879255689297525
    mean_processing_ms: 57.574360220836915
  time_since_restore: 30896.910860538483
  time_this_iter_s: 128.8713195323944
  time_total_s: 40022.922674417496
  timestamp: 1637058009
  timesteps_since_restore: 22560000
  timesteps_this_iter: 96000
  timesteps_total: 28320000
  training_iteration: 295
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    295 |          40022.9 | 28320000 |   965.89 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 1.73
    apples_agent-0_min: 0
    apples_agent-1_max: 92
    apples_agent-1_mean: 34.43
    apples_agent-1_min: 0
    apples_agent-2_max: 74
    apples_agent-2_mean: 5.86
    apples_agent-2_min: 0
    apples_agent-3_max: 115
    apples_agent-3_mean: 70.71
    apples_agent-3_min: 35
    apples_agent-4_max: 64
    apples_agent-4_mean: 1.88
    apples_agent-4_min: 0
    apples_agent-5_max: 156
    apples_agent-5_mean: 94.57
    apples_agent-5_min: 57
    cleaning_beam_agent-0_max: 491
    cleaning_beam_agent-0_mean: 396.93
    cleaning_beam_agent-0_min: 253
    cleaning_beam_agent-1_max: 489
    cleaning_beam_agent-1_mean: 254.96
    cleaning_beam_agent-1_min: 90
    cleaning_beam_agent-2_max: 622
    cleaning_beam_agent-2_mean: 432.61
    cleaning_beam_agent-2_min: 150
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 25.31
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 559
    cleaning_beam_agent-4_mean: 475.06
    cleaning_beam_agent-4_min: 310
    cleaning_beam_agent-5_max: 147
    cleaning_beam_agent-5_mean: 32.17
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-22-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1094.9999999999916
  episode_reward_mean: 960.0299999999842
  episode_reward_min: 590.0000000000093
  episodes_this_iter: 96
  episodes_total: 28416
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11653.716
    learner:
      agent-0:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9464800953865051
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018113977275788784
        model: {}
        policy_loss: -0.003187614493072033
        total_loss: -0.0030097635462880135
        vf_explained_var: 0.05982813239097595
        vf_loss: 18.43655014038086
      agent-1:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1215345859527588
        entropy_coeff: 0.0017600000137463212
        kl: 0.001066711382009089
        model: {}
        policy_loss: -0.003696020692586899
        total_loss: -0.0033920500427484512
        vf_explained_var: -0.12466159462928772
        vf_loss: 22.77867317199707
      agent-2:
        cur_kl_coeff: 1.0097419737292228e-29
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0226986408233643
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017265650676563382
        model: {}
        policy_loss: -0.0034659462980926037
        total_loss: -0.0032847197726368904
        vf_explained_var: -0.004873156547546387
        vf_loss: 19.811750411987305
      agent-3:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4916476607322693
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008875442435964942
        model: {}
        policy_loss: -0.002514676656574011
        total_loss: -0.001504646148532629
        vf_explained_var: 0.04220519959926605
        vf_loss: 18.753320693969727
      agent-4:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9305914640426636
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015107420040294528
        model: {}
        policy_loss: -0.003744188230484724
        total_loss: -0.0033586258068680763
        vf_explained_var: -0.014630496501922607
        vf_loss: 20.234020233154297
      agent-5:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7124707698822021
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014070625184103847
        model: {}
        policy_loss: -0.003664793912321329
        total_loss: -0.0031438341829925776
        vf_explained_var: 0.0966658741235733
        vf_loss: 17.74905776977539
    load_time_ms: 13652.95
    num_steps_sampled: 28416000
    num_steps_trained: 28416000
    sample_time_ms: 100115.525
    update_time_ms: 14.542
  iterations_since_restore: 236
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.429608938547485
    ram_util_percent: 16.189385474860337
  pid: 30948
  policy_reward_max:
    agent-0: 182.4999999999996
    agent-1: 182.4999999999996
    agent-2: 182.4999999999996
    agent-3: 182.4999999999996
    agent-4: 182.4999999999996
    agent-5: 182.4999999999996
  policy_reward_mean:
    agent-0: 160.0049999999999
    agent-1: 160.0049999999999
    agent-2: 160.0049999999999
    agent-3: 160.0049999999999
    agent-4: 160.0049999999999
    agent-5: 160.0049999999999
  policy_reward_min:
    agent-0: 98.33333333333329
    agent-1: 98.33333333333329
    agent-2: 98.33333333333329
    agent-3: 98.33333333333329
    agent-4: 98.33333333333329
    agent-5: 98.33333333333329
  sampler_perf:
    mean_env_wait_ms: 27.160812020346693
    mean_inference_ms: 12.878816264829181
    mean_processing_ms: 57.571211787875335
  time_since_restore: 31022.205766439438
  time_this_iter_s: 125.2949059009552
  time_total_s: 40148.21758031845
  timestamp: 1637058134
  timesteps_since_restore: 22656000
  timesteps_this_iter: 96000
  timesteps_total: 28416000
  training_iteration: 296
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    296 |          40148.2 | 28416000 |   960.03 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 1.19
    apples_agent-0_min: 0
    apples_agent-1_max: 113
    apples_agent-1_mean: 30.24
    apples_agent-1_min: 0
    apples_agent-2_max: 98
    apples_agent-2_mean: 8.43
    apples_agent-2_min: 0
    apples_agent-3_max: 115
    apples_agent-3_mean: 70.83
    apples_agent-3_min: 30
    apples_agent-4_max: 34
    apples_agent-4_mean: 2.13
    apples_agent-4_min: 0
    apples_agent-5_max: 201
    apples_agent-5_mean: 96.95
    apples_agent-5_min: 21
    cleaning_beam_agent-0_max: 496
    cleaning_beam_agent-0_mean: 386.27
    cleaning_beam_agent-0_min: 241
    cleaning_beam_agent-1_max: 427
    cleaning_beam_agent-1_mean: 257.23
    cleaning_beam_agent-1_min: 103
    cleaning_beam_agent-2_max: 631
    cleaning_beam_agent-2_mean: 427.31
    cleaning_beam_agent-2_min: 209
    cleaning_beam_agent-3_max: 49
    cleaning_beam_agent-3_mean: 24.39
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 571
    cleaning_beam_agent-4_mean: 468.37
    cleaning_beam_agent-4_min: 360
    cleaning_beam_agent-5_max: 130
    cleaning_beam_agent-5_mean: 29.1
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-24-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1112.0000000000039
  episode_reward_mean: 951.6699999999831
  episode_reward_min: 151.99999999999994
  episodes_this_iter: 96
  episodes_total: 28512
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11690.222
    learner:
      agent-0:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9522661566734314
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016129649011418223
        model: {}
        policy_loss: -0.003128106240183115
        total_loss: -0.0028908313252031803
        vf_explained_var: 0.0839889645576477
        vf_loss: 19.132652282714844
      agent-1:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1434258222579956
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010452799033373594
        model: {}
        policy_loss: -0.0037608356215059757
        total_loss: -0.0034781168214976788
        vf_explained_var: -0.0802946388721466
        vf_loss: 22.95149040222168
      agent-2:
        cur_kl_coeff: 5.048709868646114e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0140748023986816
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014369392301887274
        model: {}
        policy_loss: -0.003560028737410903
        total_loss: -0.003241682192310691
        vf_explained_var: 0.005023196339607239
        vf_loss: 21.031217575073242
      agent-3:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48162442445755005
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010896441526710987
        model: {}
        policy_loss: -0.002496312139555812
        total_loss: -0.0014294167049229145
        vf_explained_var: 0.08431015908718109
        vf_loss: 19.14552116394043
      agent-4:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9312640428543091
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013499585911631584
        model: {}
        policy_loss: -0.003852741327136755
        total_loss: -0.003338384674862027
        vf_explained_var: -0.014190852642059326
        vf_loss: 21.533782958984375
      agent-5:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7194080352783203
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009788698516786098
        model: {}
        policy_loss: -0.003099303226917982
        total_loss: -0.0025074887089431286
        vf_explained_var: 0.11145690083503723
        vf_loss: 18.579742431640625
    load_time_ms: 13638.648
    num_steps_sampled: 28512000
    num_steps_trained: 28512000
    sample_time_ms: 100279.519
    update_time_ms: 14.406
  iterations_since_restore: 237
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.921666666666663
    ram_util_percent: 16.14166666666667
  pid: 30948
  policy_reward_max:
    agent-0: 185.333333333333
    agent-1: 185.333333333333
    agent-2: 185.333333333333
    agent-3: 185.333333333333
    agent-4: 185.333333333333
    agent-5: 185.333333333333
  policy_reward_mean:
    agent-0: 158.61166666666657
    agent-1: 158.61166666666657
    agent-2: 158.61166666666657
    agent-3: 158.61166666666657
    agent-4: 158.61166666666657
    agent-5: 158.61166666666657
  policy_reward_min:
    agent-0: 25.333333333333375
    agent-1: 25.333333333333375
    agent-2: 25.333333333333375
    agent-3: 25.333333333333375
    agent-4: 25.333333333333375
    agent-5: 25.333333333333375
  sampler_perf:
    mean_env_wait_ms: 27.162059979203462
    mean_inference_ms: 12.877983606569682
    mean_processing_ms: 57.567483238710864
  time_since_restore: 31148.220222473145
  time_this_iter_s: 126.01445603370667
  time_total_s: 40274.23203635216
  timestamp: 1637058260
  timesteps_since_restore: 22752000
  timesteps_this_iter: 96000
  timesteps_total: 28512000
  training_iteration: 297
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    297 |          40274.2 | 28512000 |   951.67 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 1.58
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 31.22
    apples_agent-1_min: 0
    apples_agent-2_max: 69
    apples_agent-2_mean: 6.41
    apples_agent-2_min: 0
    apples_agent-3_max: 153
    apples_agent-3_mean: 68.84
    apples_agent-3_min: 32
    apples_agent-4_max: 27
    apples_agent-4_mean: 1.25
    apples_agent-4_min: 0
    apples_agent-5_max: 194
    apples_agent-5_mean: 97.18
    apples_agent-5_min: 44
    cleaning_beam_agent-0_max: 527
    cleaning_beam_agent-0_mean: 387.57
    cleaning_beam_agent-0_min: 233
    cleaning_beam_agent-1_max: 465
    cleaning_beam_agent-1_mean: 254.72
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 628
    cleaning_beam_agent-2_mean: 427.34
    cleaning_beam_agent-2_min: 216
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 25.02
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 602
    cleaning_beam_agent-4_mean: 473.36
    cleaning_beam_agent-4_min: 396
    cleaning_beam_agent-5_max: 105
    cleaning_beam_agent-5_mean: 27.51
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-26-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1098.999999999988
  episode_reward_mean: 957.7399999999834
  episode_reward_min: 589.0000000000069
  episodes_this_iter: 96
  episodes_total: 28608
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11736.883
    learner:
      agent-0:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9406615495681763
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018079894362017512
        model: {}
        policy_loss: -0.0030979930888861418
        total_loss: -0.0029270490631461143
        vf_explained_var: 0.06794632971286774
        vf_loss: 18.265047073364258
      agent-1:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.123461127281189
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013734035892412066
        model: {}
        policy_loss: -0.003922284580767155
        total_loss: -0.0037267510779201984
        vf_explained_var: -0.08685186505317688
        vf_loss: 21.728256225585938
      agent-2:
        cur_kl_coeff: 2.524354934323057e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0101374387741089
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015082642203196883
        model: {}
        policy_loss: -0.0038518309593200684
        total_loss: -0.0036197740118950605
        vf_explained_var: -0.00962597131729126
        vf_loss: 20.09898567199707
      agent-3:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48144587874412537
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009185399394482374
        model: {}
        policy_loss: -0.0024261416401714087
        total_loss: -0.0013951424043625593
        vf_explained_var: 0.03783963620662689
        vf_loss: 18.783435821533203
      agent-4:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9301888346672058
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015511115780100226
        model: {}
        policy_loss: -0.0037794141098856926
        total_loss: -0.003401916939765215
        vf_explained_var: -0.014938518404960632
        vf_loss: 20.146324157714844
      agent-5:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 0.692328691482544
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015469614882022142
        model: {}
        policy_loss: -0.003208997193723917
        total_loss: -0.0026271676179021597
        vf_explained_var: 0.07834002375602722
        vf_loss: 18.003267288208008
    load_time_ms: 13656.831
    num_steps_sampled: 28608000
    num_steps_trained: 28608000
    sample_time_ms: 100245.965
    update_time_ms: 14.178
  iterations_since_restore: 238
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.533333333333335
    ram_util_percent: 16.207344632768365
  pid: 30948
  policy_reward_max:
    agent-0: 183.16666666666617
    agent-1: 183.16666666666617
    agent-2: 183.16666666666617
    agent-3: 183.16666666666617
    agent-4: 183.16666666666617
    agent-5: 183.16666666666617
  policy_reward_mean:
    agent-0: 159.62333333333322
    agent-1: 159.62333333333322
    agent-2: 159.62333333333322
    agent-3: 159.62333333333322
    agent-4: 159.62333333333322
    agent-5: 159.62333333333322
  policy_reward_min:
    agent-0: 98.1666666666671
    agent-1: 98.1666666666671
    agent-2: 98.1666666666671
    agent-3: 98.1666666666671
    agent-4: 98.1666666666671
    agent-5: 98.1666666666671
  sampler_perf:
    mean_env_wait_ms: 27.163685262977598
    mean_inference_ms: 12.87730395647595
    mean_processing_ms: 57.56264157577911
  time_since_restore: 31272.8579018116
  time_this_iter_s: 124.6376793384552
  time_total_s: 40398.86971569061
  timestamp: 1637058385
  timesteps_since_restore: 22848000
  timesteps_this_iter: 96000
  timesteps_total: 28608000
  training_iteration: 298
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    298 |          40398.9 | 28608000 |   957.74 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 107
    apples_agent-0_mean: 2.55
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 33.75
    apples_agent-1_min: 0
    apples_agent-2_max: 110
    apples_agent-2_mean: 8.94
    apples_agent-2_min: 0
    apples_agent-3_max: 325
    apples_agent-3_mean: 76.21
    apples_agent-3_min: 41
    apples_agent-4_max: 24
    apples_agent-4_mean: 0.39
    apples_agent-4_min: 0
    apples_agent-5_max: 304
    apples_agent-5_mean: 100.48
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 529
    cleaning_beam_agent-0_mean: 406.79
    cleaning_beam_agent-0_min: 232
    cleaning_beam_agent-1_max: 413
    cleaning_beam_agent-1_mean: 263.81
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 638
    cleaning_beam_agent-2_mean: 429.21
    cleaning_beam_agent-2_min: 241
    cleaning_beam_agent-3_max: 54
    cleaning_beam_agent-3_mean: 23.45
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 574
    cleaning_beam_agent-4_mean: 480.34
    cleaning_beam_agent-4_min: 408
    cleaning_beam_agent-5_max: 130
    cleaning_beam_agent-5_mean: 32.48
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-28-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1148.9999999999725
  episode_reward_mean: 954.7899999999827
  episode_reward_min: 383.00000000000097
  episodes_this_iter: 96
  episodes_total: 28704
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11682.353
    learner:
      agent-0:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9347681403160095
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012530124513432384
        model: {}
        policy_loss: -0.0032023824751377106
        total_loss: -0.002976763993501663
        vf_explained_var: 0.061469241976737976
        vf_loss: 18.708093643188477
      agent-1:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1196248531341553
        entropy_coeff: 0.0017600000137463212
        kl: 0.001203720341436565
        model: {}
        policy_loss: -0.003733352292329073
        total_loss: -0.00351076852530241
        vf_explained_var: -0.08778348565101624
        vf_loss: 21.93120002746582
      agent-2:
        cur_kl_coeff: 1.2621774671615285e-30
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0399893522262573
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017073582857847214
        model: {}
        policy_loss: -0.003769097849726677
        total_loss: -0.0036292080767452717
        vf_explained_var: 0.019862055778503418
        vf_loss: 19.702709197998047
      agent-3:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48522913455963135
        entropy_coeff: 0.0017600000137463212
        kl: 0.001330423867329955
        model: {}
        policy_loss: -0.002617426449432969
        total_loss: -0.0015610086265951395
        vf_explained_var: 0.03718841075897217
        vf_loss: 19.104198455810547
      agent-4:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9116466045379639
        entropy_coeff: 0.0017600000137463212
        kl: 0.001977375242859125
        model: {}
        policy_loss: -0.004042000509798527
        total_loss: -0.0035632075741887093
        vf_explained_var: -0.03772801160812378
        vf_loss: 20.832929611206055
      agent-5:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7221758961677551
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008671798277646303
        model: {}
        policy_loss: -0.0030359879601746798
        total_loss: -0.0024661060888320208
        vf_explained_var: 0.07567210495471954
        vf_loss: 18.409099578857422
    load_time_ms: 13640.687
    num_steps_sampled: 28704000
    num_steps_trained: 28704000
    sample_time_ms: 100211.639
    update_time_ms: 14.067
  iterations_since_restore: 239
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.55965909090909
    ram_util_percent: 16.19090909090909
  pid: 30948
  policy_reward_max:
    agent-0: 191.49999999999946
    agent-1: 191.49999999999946
    agent-2: 191.49999999999946
    agent-3: 191.49999999999946
    agent-4: 191.49999999999946
    agent-5: 191.49999999999946
  policy_reward_mean:
    agent-0: 159.13166666666658
    agent-1: 159.13166666666658
    agent-2: 159.13166666666658
    agent-3: 159.13166666666658
    agent-4: 159.13166666666658
    agent-5: 159.13166666666658
  policy_reward_min:
    agent-0: 63.833333333333194
    agent-1: 63.833333333333194
    agent-2: 63.833333333333194
    agent-3: 63.833333333333194
    agent-4: 63.833333333333194
    agent-5: 63.833333333333194
  sampler_perf:
    mean_env_wait_ms: 27.164872930056045
    mean_inference_ms: 12.876544310243723
    mean_processing_ms: 57.55666058383803
  time_since_restore: 31396.734855413437
  time_this_iter_s: 123.87695360183716
  time_total_s: 40522.74666929245
  timestamp: 1637058509
  timesteps_since_restore: 22944000
  timesteps_this_iter: 96000
  timesteps_total: 28704000
  training_iteration: 299
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    299 |          40522.7 | 28704000 |   954.79 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 1.33
    apples_agent-0_min: 0
    apples_agent-1_max: 98
    apples_agent-1_mean: 32.26
    apples_agent-1_min: 0
    apples_agent-2_max: 110
    apples_agent-2_mean: 7.26
    apples_agent-2_min: 0
    apples_agent-3_max: 125
    apples_agent-3_mean: 72.07
    apples_agent-3_min: 38
    apples_agent-4_max: 49
    apples_agent-4_mean: 2.78
    apples_agent-4_min: 0
    apples_agent-5_max: 145
    apples_agent-5_mean: 96.46
    apples_agent-5_min: 38
    cleaning_beam_agent-0_max: 550
    cleaning_beam_agent-0_mean: 409.59
    cleaning_beam_agent-0_min: 318
    cleaning_beam_agent-1_max: 426
    cleaning_beam_agent-1_mean: 261.49
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 603
    cleaning_beam_agent-2_mean: 416.61
    cleaning_beam_agent-2_min: 189
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 24.55
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 547
    cleaning_beam_agent-4_mean: 474.05
    cleaning_beam_agent-4_min: 280
    cleaning_beam_agent-5_max: 83
    cleaning_beam_agent-5_mean: 28.18
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-30-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1117.9999999999975
  episode_reward_mean: 944.709999999985
  episode_reward_min: 420.0000000000081
  episodes_this_iter: 96
  episodes_total: 28800
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11685.985
    learner:
      agent-0:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9358651638031006
        entropy_coeff: 0.0017600000137463212
        kl: 0.002570690121501684
        model: {}
        policy_loss: -0.0032317154109477997
        total_loss: -0.0028204284608364105
        vf_explained_var: 0.046576231718063354
        vf_loss: 20.58414077758789
      agent-1:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1414909362792969
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018554393900558352
        model: {}
        policy_loss: -0.0037940694019198418
        total_loss: -0.0034644808620214462
        vf_explained_var: -0.08021792769432068
        vf_loss: 23.38611602783203
      agent-2:
        cur_kl_coeff: 6.3108873358076425e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0490831136703491
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014030226739123464
        model: {}
        policy_loss: -0.0034596826881170273
        total_loss: -0.0031055971048772335
        vf_explained_var: -0.01875549554824829
        vf_loss: 22.004745483398438
      agent-3:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.5021244883537292
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011637065326794982
        model: {}
        policy_loss: -0.003101747017353773
        total_loss: -0.0020720711909234524
        vf_explained_var: 0.1123223602771759
        vf_loss: 19.134159088134766
      agent-4:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9325807094573975
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017686673672869802
        model: {}
        policy_loss: -0.004051485564559698
        total_loss: -0.0035522053949534893
        vf_explained_var: 0.013331800699234009
        vf_loss: 21.406204223632812
      agent-5:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7085376977920532
        entropy_coeff: 0.0017600000137463212
        kl: 0.00108938361518085
        model: {}
        policy_loss: -0.003308151848614216
        total_loss: -0.0026508886367082596
        vf_explained_var: 0.11592404544353485
        vf_loss: 19.042896270751953
    load_time_ms: 13630.868
    num_steps_sampled: 28800000
    num_steps_trained: 28800000
    sample_time_ms: 100105.872
    update_time_ms: 14.018
  iterations_since_restore: 240
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.284269662921346
    ram_util_percent: 16.193820224719104
  pid: 30948
  policy_reward_max:
    agent-0: 186.33333333333275
    agent-1: 186.33333333333275
    agent-2: 186.33333333333275
    agent-3: 186.33333333333275
    agent-4: 186.33333333333275
    agent-5: 186.33333333333275
  policy_reward_mean:
    agent-0: 157.4516666666666
    agent-1: 157.4516666666666
    agent-2: 157.4516666666666
    agent-3: 157.4516666666666
    agent-4: 157.4516666666666
    agent-5: 157.4516666666666
  policy_reward_min:
    agent-0: 69.99999999999982
    agent-1: 69.99999999999982
    agent-2: 69.99999999999982
    agent-3: 69.99999999999982
    agent-4: 69.99999999999982
    agent-5: 69.99999999999982
  sampler_perf:
    mean_env_wait_ms: 27.16606231373594
    mean_inference_ms: 12.87584461809367
    mean_processing_ms: 57.55082872696189
  time_since_restore: 31521.20832133293
  time_this_iter_s: 124.47346591949463
  time_total_s: 40647.220135211945
  timestamp: 1637058634
  timesteps_since_restore: 23040000
  timesteps_this_iter: 96000
  timesteps_total: 28800000
  training_iteration: 300
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    300 |          40647.2 | 28800000 |   944.71 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 1.11
    apples_agent-0_min: 0
    apples_agent-1_max: 117
    apples_agent-1_mean: 37.16
    apples_agent-1_min: 0
    apples_agent-2_max: 68
    apples_agent-2_mean: 6.8
    apples_agent-2_min: 0
    apples_agent-3_max: 150
    apples_agent-3_mean: 70.87
    apples_agent-3_min: 32
    apples_agent-4_max: 85
    apples_agent-4_mean: 1.55
    apples_agent-4_min: 0
    apples_agent-5_max: 163
    apples_agent-5_mean: 99.2
    apples_agent-5_min: 62
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 411.54
    cleaning_beam_agent-0_min: 228
    cleaning_beam_agent-1_max: 385
    cleaning_beam_agent-1_mean: 251.96
    cleaning_beam_agent-1_min: 130
    cleaning_beam_agent-2_max: 629
    cleaning_beam_agent-2_mean: 438.16
    cleaning_beam_agent-2_min: 207
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 25.11
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 570
    cleaning_beam_agent-4_mean: 462.34
    cleaning_beam_agent-4_min: 356
    cleaning_beam_agent-5_max: 81
    cleaning_beam_agent-5_mean: 26.44
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-32-39
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1079.99999999999
  episode_reward_mean: 959.7099999999842
  episode_reward_min: 459.0000000000092
  episodes_this_iter: 96
  episodes_total: 28896
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11657.343
    learner:
      agent-0:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9485862255096436
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019041835330426693
        model: {}
        policy_loss: -0.0034549757838249207
        total_loss: -0.003278359305113554
        vf_explained_var: 0.06598816812038422
        vf_loss: 18.461286544799805
      agent-1:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1227891445159912
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018152999691665173
        model: {}
        policy_loss: -0.003951413556933403
        total_loss: -0.0036606425419449806
        vf_explained_var: -0.11699947714805603
        vf_loss: 22.668806076049805
      agent-2:
        cur_kl_coeff: 3.1554436679038213e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0239394903182983
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015389451291412115
        model: {}
        policy_loss: -0.003820430487394333
        total_loss: -0.0036466531455516815
        vf_explained_var: 0.008099302649497986
        vf_loss: 19.759124755859375
      agent-3:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4767826497554779
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012078656582161784
        model: {}
        policy_loss: -0.0026216297410428524
        total_loss: -0.0016624103300273418
        vf_explained_var: 0.08872614800930023
        vf_loss: 17.983572006225586
      agent-4:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9425897598266602
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012923228787258267
        model: {}
        policy_loss: -0.0035323491320014
        total_loss: -0.003141830675303936
        vf_explained_var: -0.01563611626625061
        vf_loss: 20.494770050048828
      agent-5:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7168638110160828
        entropy_coeff: 0.0017600000137463212
        kl: 0.001326217083260417
        model: {}
        policy_loss: -0.003330580424517393
        total_loss: -0.0027993342373520136
        vf_explained_var: 0.09829732775688171
        vf_loss: 17.929256439208984
    load_time_ms: 13640.597
    num_steps_sampled: 28896000
    num_steps_trained: 28896000
    sample_time_ms: 100009.079
    update_time_ms: 13.958
  iterations_since_restore: 241
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.41694915254238
    ram_util_percent: 16.11864406779661
  pid: 30948
  policy_reward_max:
    agent-0: 180.00000000000003
    agent-1: 180.00000000000003
    agent-2: 180.00000000000003
    agent-3: 180.00000000000003
    agent-4: 180.00000000000003
    agent-5: 180.00000000000003
  policy_reward_mean:
    agent-0: 159.9516666666666
    agent-1: 159.9516666666666
    agent-2: 159.9516666666666
    agent-3: 159.9516666666666
    agent-4: 159.9516666666666
    agent-5: 159.9516666666666
  policy_reward_min:
    agent-0: 76.5
    agent-1: 76.5
    agent-2: 76.5
    agent-3: 76.5
    agent-4: 76.5
    agent-5: 76.5
  sampler_perf:
    mean_env_wait_ms: 27.167523692719637
    mean_inference_ms: 12.875083851534669
    mean_processing_ms: 57.544959240948316
  time_since_restore: 31645.41441988945
  time_this_iter_s: 124.20609855651855
  time_total_s: 40771.42623376846
  timestamp: 1637058759
  timesteps_since_restore: 23136000
  timesteps_this_iter: 96000
  timesteps_total: 28896000
  training_iteration: 301
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    301 |          40771.4 | 28896000 |   959.71 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 2.15
    apples_agent-0_min: 0
    apples_agent-1_max: 104
    apples_agent-1_mean: 35.3
    apples_agent-1_min: 0
    apples_agent-2_max: 154
    apples_agent-2_mean: 6.39
    apples_agent-2_min: 0
    apples_agent-3_max: 135
    apples_agent-3_mean: 71.78
    apples_agent-3_min: 44
    apples_agent-4_max: 48
    apples_agent-4_mean: 1.03
    apples_agent-4_min: 0
    apples_agent-5_max: 211
    apples_agent-5_mean: 100.61
    apples_agent-5_min: 59
    cleaning_beam_agent-0_max: 505
    cleaning_beam_agent-0_mean: 407.96
    cleaning_beam_agent-0_min: 306
    cleaning_beam_agent-1_max: 408
    cleaning_beam_agent-1_mean: 256.39
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 693
    cleaning_beam_agent-2_mean: 437.92
    cleaning_beam_agent-2_min: 177
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 22.69
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 536
    cleaning_beam_agent-4_mean: 458.29
    cleaning_beam_agent-4_min: 369
    cleaning_beam_agent-5_max: 92
    cleaning_beam_agent-5_mean: 29.87
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-34-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1110.999999999981
  episode_reward_mean: 970.0299999999822
  episode_reward_min: 633.0000000000025
  episodes_this_iter: 96
  episodes_total: 28992
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11636.644
    learner:
      agent-0:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9374586343765259
        entropy_coeff: 0.0017600000137463212
        kl: 0.001205847947858274
        model: {}
        policy_loss: -0.003192886011675
        total_loss: -0.0030773670878261328
        vf_explained_var: 0.04245257377624512
        vf_loss: 17.654481887817383
      agent-1:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.129347562789917
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021620672196149826
        model: {}
        policy_loss: -0.0036119362339377403
        total_loss: -0.0035364856012165546
        vf_explained_var: -0.11315688490867615
        vf_loss: 20.631027221679688
      agent-2:
        cur_kl_coeff: 1.5777218339519106e-31
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0254555940628052
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013851035619154572
        model: {}
        policy_loss: -0.0033403150737285614
        total_loss: -0.003264673985540867
        vf_explained_var: -0.013799235224723816
        vf_loss: 18.804386138916016
      agent-3:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47210296988487244
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009687442798167467
        model: {}
        policy_loss: -0.0023423023521900177
        total_loss: -0.0013521257787942886
        vf_explained_var: -0.0003588944673538208
        vf_loss: 18.210765838623047
      agent-4:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9434388875961304
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013003003550693393
        model: {}
        policy_loss: -0.0035795143339782953
        total_loss: -0.003302248427644372
        vf_explained_var: -0.0307481586933136
        vf_loss: 19.377166748046875
      agent-5:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7186383008956909
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010284421732649207
        model: {}
        policy_loss: -0.002986061852425337
        total_loss: -0.0025208857841789722
        vf_explained_var: 0.05706740915775299
        vf_loss: 17.299785614013672
    load_time_ms: 13647.411
    num_steps_sampled: 28992000
    num_steps_trained: 28992000
    sample_time_ms: 100032.663
    update_time_ms: 13.917
  iterations_since_restore: 242
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.59152542372881
    ram_util_percent: 16.197175141242937
  pid: 30948
  policy_reward_max:
    agent-0: 185.16666666666652
    agent-1: 185.16666666666652
    agent-2: 185.16666666666652
    agent-3: 185.16666666666652
    agent-4: 185.16666666666652
    agent-5: 185.16666666666652
  policy_reward_mean:
    agent-0: 161.67166666666654
    agent-1: 161.67166666666654
    agent-2: 161.67166666666654
    agent-3: 161.67166666666654
    agent-4: 161.67166666666654
    agent-5: 161.67166666666654
  policy_reward_min:
    agent-0: 105.50000000000023
    agent-1: 105.50000000000023
    agent-2: 105.50000000000023
    agent-3: 105.50000000000023
    agent-4: 105.50000000000023
    agent-5: 105.50000000000023
  sampler_perf:
    mean_env_wait_ms: 27.169397288443687
    mean_inference_ms: 12.874677270503577
    mean_processing_ms: 57.54003591032274
  time_since_restore: 31769.602751493454
  time_this_iter_s: 124.1883316040039
  time_total_s: 40895.61456537247
  timestamp: 1637058883
  timesteps_since_restore: 23232000
  timesteps_this_iter: 96000
  timesteps_total: 28992000
  training_iteration: 302
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    302 |          40895.6 | 28992000 |   970.03 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 1.09
    apples_agent-0_min: 0
    apples_agent-1_max: 118
    apples_agent-1_mean: 33.93
    apples_agent-1_min: 0
    apples_agent-2_max: 87
    apples_agent-2_mean: 5.55
    apples_agent-2_min: 0
    apples_agent-3_max: 127
    apples_agent-3_mean: 70.69
    apples_agent-3_min: 40
    apples_agent-4_max: 72
    apples_agent-4_mean: 3.65
    apples_agent-4_min: 0
    apples_agent-5_max: 178
    apples_agent-5_mean: 101.17
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 608
    cleaning_beam_agent-0_mean: 426.8
    cleaning_beam_agent-0_min: 325
    cleaning_beam_agent-1_max: 440
    cleaning_beam_agent-1_mean: 261.47
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 643
    cleaning_beam_agent-2_mean: 425.31
    cleaning_beam_agent-2_min: 212
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 22.2
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 532
    cleaning_beam_agent-4_mean: 452.5
    cleaning_beam_agent-4_min: 293
    cleaning_beam_agent-5_max: 137
    cleaning_beam_agent-5_mean: 30.2
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-36-48
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1146.9999999999766
  episode_reward_mean: 964.2099999999837
  episode_reward_min: 467.0000000000091
  episodes_this_iter: 96
  episodes_total: 29088
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11673.297
    learner:
      agent-0:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9247298836708069
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016764437314122915
        model: {}
        policy_loss: -0.0031757010146975517
        total_loss: -0.002951605012640357
        vf_explained_var: 0.03463040292263031
        vf_loss: 18.51619529724121
      agent-1:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1293153762817383
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014525775332003832
        model: {}
        policy_loss: -0.003291058586910367
        total_loss: -0.003091780934482813
        vf_explained_var: -0.1194722056388855
        vf_loss: 21.868736267089844
      agent-2:
        cur_kl_coeff: 7.888609169759553e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0336885452270508
        entropy_coeff: 0.0017600000137463212
        kl: 0.001583517761901021
        model: {}
        policy_loss: -0.00350321177393198
        total_loss: -0.0033363662660121918
        vf_explained_var: -0.024315178394317627
        vf_loss: 19.861356735229492
      agent-3:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4736647605895996
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012347871670499444
        model: {}
        policy_loss: -0.002677361946552992
        total_loss: -0.0016840347088873386
        vf_explained_var: 0.04760308563709259
        vf_loss: 18.269773483276367
      agent-4:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.945792019367218
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016616142820566893
        model: {}
        policy_loss: -0.0041487738490104675
        total_loss: -0.003798974910750985
        vf_explained_var: -0.02897593379020691
        vf_loss: 20.143957138061523
      agent-5:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7326318025588989
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014311091508716345
        model: {}
        policy_loss: -0.003437848761677742
        total_loss: -0.0029333108104765415
        vf_explained_var: 0.06837858259677887
        vf_loss: 17.939712524414062
    load_time_ms: 13642.672
    num_steps_sampled: 29088000
    num_steps_trained: 29088000
    sample_time_ms: 100032.793
    update_time_ms: 14.436
  iterations_since_restore: 243
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.316949152542374
    ram_util_percent: 16.188700564971754
  pid: 30948
  policy_reward_max:
    agent-0: 191.1666666666663
    agent-1: 191.1666666666663
    agent-2: 191.1666666666663
    agent-3: 191.1666666666663
    agent-4: 191.1666666666663
    agent-5: 191.1666666666663
  policy_reward_mean:
    agent-0: 160.70166666666657
    agent-1: 160.70166666666657
    agent-2: 160.70166666666657
    agent-3: 160.70166666666657
    agent-4: 160.70166666666657
    agent-5: 160.70166666666657
  policy_reward_min:
    agent-0: 77.8333333333335
    agent-1: 77.8333333333335
    agent-2: 77.8333333333335
    agent-3: 77.8333333333335
    agent-4: 77.8333333333335
    agent-5: 77.8333333333335
  sampler_perf:
    mean_env_wait_ms: 27.17076721008419
    mean_inference_ms: 12.873838846266777
    mean_processing_ms: 57.534749774341186
  time_since_restore: 31894.60567164421
  time_this_iter_s: 125.00292015075684
  time_total_s: 41020.617485523224
  timestamp: 1637059008
  timesteps_since_restore: 23328000
  timesteps_this_iter: 96000
  timesteps_total: 29088000
  training_iteration: 303
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    303 |          41020.6 | 29088000 |   964.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 35
    apples_agent-0_mean: 2.3
    apples_agent-0_min: 0
    apples_agent-1_max: 154
    apples_agent-1_mean: 34.84
    apples_agent-1_min: 0
    apples_agent-2_max: 78
    apples_agent-2_mean: 7.14
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 71.09
    apples_agent-3_min: 38
    apples_agent-4_max: 114
    apples_agent-4_mean: 2.4
    apples_agent-4_min: 0
    apples_agent-5_max: 154
    apples_agent-5_mean: 97.31
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 514
    cleaning_beam_agent-0_mean: 410.49
    cleaning_beam_agent-0_min: 267
    cleaning_beam_agent-1_max: 495
    cleaning_beam_agent-1_mean: 285.25
    cleaning_beam_agent-1_min: 157
    cleaning_beam_agent-2_max: 619
    cleaning_beam_agent-2_mean: 401.02
    cleaning_beam_agent-2_min: 192
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 23.79
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 536
    cleaning_beam_agent-4_mean: 460.29
    cleaning_beam_agent-4_min: 317
    cleaning_beam_agent-5_max: 80
    cleaning_beam_agent-5_mean: 29.6
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-38-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1107.9999999999943
  episode_reward_mean: 972.209999999983
  episode_reward_min: 503.0000000000072
  episodes_this_iter: 96
  episodes_total: 29184
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11646.968
    learner:
      agent-0:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9336109161376953
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013781398301944137
        model: {}
        policy_loss: -0.003351384773850441
        total_loss: -0.003145105205476284
        vf_explained_var: 0.06824667751789093
        vf_loss: 18.494356155395508
      agent-1:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1081033945083618
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014953031204640865
        model: {}
        policy_loss: -0.003651479259133339
        total_loss: -0.003336993046104908
        vf_explained_var: -0.11884766817092896
        vf_loss: 22.647462844848633
      agent-2:
        cur_kl_coeff: 3.9443045848797766e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0445481538772583
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019587958231568336
        model: {}
        policy_loss: -0.0036334616597741842
        total_loss: -0.0032935100607573986
        vf_explained_var: -0.07866480946540833
        vf_loss: 21.7835750579834
      agent-3:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.469698041677475
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012355106882750988
        model: {}
        policy_loss: -0.002392497146502137
        total_loss: -0.0013345449697226286
        vf_explained_var: 0.04170696437358856
        vf_loss: 18.846202850341797
      agent-4:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9323797821998596
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011085515143349767
        model: {}
        policy_loss: -0.003672616556286812
        total_loss: -0.0032099755480885506
        vf_explained_var: -0.0455859899520874
        vf_loss: 21.036293029785156
      agent-5:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7173328995704651
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012848267797380686
        model: {}
        policy_loss: -0.0033095567487180233
        total_loss: -0.0027353111654520035
        vf_explained_var: 0.0724610984325409
        vf_loss: 18.367521286010742
    load_time_ms: 13629.255
    num_steps_sampled: 29184000
    num_steps_trained: 29184000
    sample_time_ms: 99723.896
    update_time_ms: 14.479
  iterations_since_restore: 244
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.590960451977402
    ram_util_percent: 16.18983050847458
  pid: 30948
  policy_reward_max:
    agent-0: 184.66666666666612
    agent-1: 184.66666666666612
    agent-2: 184.66666666666612
    agent-3: 184.66666666666612
    agent-4: 184.66666666666612
    agent-5: 184.66666666666612
  policy_reward_mean:
    agent-0: 162.03499999999988
    agent-1: 162.03499999999988
    agent-2: 162.03499999999988
    agent-3: 162.03499999999988
    agent-4: 162.03499999999988
    agent-5: 162.03499999999988
  policy_reward_min:
    agent-0: 83.83333333333333
    agent-1: 83.83333333333333
    agent-2: 83.83333333333333
    agent-3: 83.83333333333333
    agent-4: 83.83333333333333
    agent-5: 83.83333333333333
  sampler_perf:
    mean_env_wait_ms: 27.172628018697893
    mean_inference_ms: 12.87316626306357
    mean_processing_ms: 57.52981791006551
  time_since_restore: 32018.99554514885
  time_this_iter_s: 124.38987350463867
  time_total_s: 41145.00735902786
  timestamp: 1637059132
  timesteps_since_restore: 23424000
  timesteps_this_iter: 96000
  timesteps_total: 29184000
  training_iteration: 304
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    304 |            41145 | 29184000 |   972.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 32
    apples_agent-0_mean: 1.59
    apples_agent-0_min: 0
    apples_agent-1_max: 113
    apples_agent-1_mean: 31.96
    apples_agent-1_min: 0
    apples_agent-2_max: 84
    apples_agent-2_mean: 6.38
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 71.29
    apples_agent-3_min: 41
    apples_agent-4_max: 37
    apples_agent-4_mean: 1.26
    apples_agent-4_min: 0
    apples_agent-5_max: 176
    apples_agent-5_mean: 102.55
    apples_agent-5_min: 33
    cleaning_beam_agent-0_max: 509
    cleaning_beam_agent-0_mean: 408.37
    cleaning_beam_agent-0_min: 313
    cleaning_beam_agent-1_max: 485
    cleaning_beam_agent-1_mean: 286.18
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 629
    cleaning_beam_agent-2_mean: 421.63
    cleaning_beam_agent-2_min: 238
    cleaning_beam_agent-3_max: 74
    cleaning_beam_agent-3_mean: 24.87
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 553
    cleaning_beam_agent-4_mean: 456.33
    cleaning_beam_agent-4_min: 347
    cleaning_beam_agent-5_max: 98
    cleaning_beam_agent-5_mean: 28.76
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-40-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1103.9999999999818
  episode_reward_mean: 959.3299999999839
  episode_reward_min: 415.00000000000966
  episodes_this_iter: 96
  episodes_total: 29280
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11599.373
    learner:
      agent-0:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9351440072059631
        entropy_coeff: 0.0017600000137463212
        kl: 0.001357114058919251
        model: {}
        policy_loss: -0.0029512145556509495
        total_loss: -0.002665823558345437
        vf_explained_var: 0.05337059497833252
        vf_loss: 19.312421798706055
      agent-1:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1213829517364502
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018808008171617985
        model: {}
        policy_loss: -0.003856380470097065
        total_loss: -0.0035016192123293877
        vf_explained_var: -0.12523967027664185
        vf_loss: 23.283973693847656
      agent-2:
        cur_kl_coeff: 1.9721522924398883e-32
        cur_lr: 1.2000000424450263e-05
        entropy: 1.027006983757019
        entropy_coeff: 0.0017600000137463212
        kl: 0.001528063789010048
        model: {}
        policy_loss: -0.0035073545295745134
        total_loss: -0.0031888436060398817
        vf_explained_var: -0.03610005974769592
        vf_loss: 21.260440826416016
      agent-3:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48601043224334717
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012662432854995131
        model: {}
        policy_loss: -0.0026165300514549017
        total_loss: -0.0015215461608022451
        vf_explained_var: 0.043917834758758545
        vf_loss: 19.503559112548828
      agent-4:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9434150457382202
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015426443424075842
        model: {}
        policy_loss: -0.0038128849118947983
        total_loss: -0.003354924265295267
        vf_explained_var: -0.03184783458709717
        vf_loss: 21.183696746826172
      agent-5:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7201637625694275
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010856095468625426
        model: {}
        policy_loss: -0.0033347238786518574
        total_loss: -0.002800389425829053
        vf_explained_var: 0.11837531626224518
        vf_loss: 18.018253326416016
    load_time_ms: 13374.598
    num_steps_sampled: 29280000
    num_steps_trained: 29280000
    sample_time_ms: 99522.133
    update_time_ms: 14.546
  iterations_since_restore: 245
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.33050847457627
    ram_util_percent: 16.203389830508478
  pid: 30948
  policy_reward_max:
    agent-0: 183.99999999999974
    agent-1: 183.99999999999974
    agent-2: 183.99999999999974
    agent-3: 183.99999999999974
    agent-4: 183.99999999999974
    agent-5: 183.99999999999974
  policy_reward_mean:
    agent-0: 159.8883333333332
    agent-1: 159.8883333333332
    agent-2: 159.8883333333332
    agent-3: 159.8883333333332
    agent-4: 159.8883333333332
    agent-5: 159.8883333333332
  policy_reward_min:
    agent-0: 69.1666666666666
    agent-1: 69.1666666666666
    agent-2: 69.1666666666666
    agent-3: 69.1666666666666
    agent-4: 69.1666666666666
    agent-5: 69.1666666666666
  sampler_perf:
    mean_env_wait_ms: 27.1735588199948
    mean_inference_ms: 12.872361400711352
    mean_processing_ms: 57.52376910831708
  time_since_restore: 32142.8427631855
  time_this_iter_s: 123.84721803665161
  time_total_s: 41268.854577064514
  timestamp: 1637059256
  timesteps_since_restore: 23520000
  timesteps_this_iter: 96000
  timesteps_total: 29280000
  training_iteration: 305
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    305 |          41268.9 | 29280000 |   959.33 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 0.92
    apples_agent-0_min: 0
    apples_agent-1_max: 115
    apples_agent-1_mean: 34.66
    apples_agent-1_min: 0
    apples_agent-2_max: 67
    apples_agent-2_mean: 7.14
    apples_agent-2_min: 0
    apples_agent-3_max: 137
    apples_agent-3_mean: 73.48
    apples_agent-3_min: 31
    apples_agent-4_max: 46
    apples_agent-4_mean: 3.56
    apples_agent-4_min: 0
    apples_agent-5_max: 177
    apples_agent-5_mean: 95.12
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 509
    cleaning_beam_agent-0_mean: 410.62
    cleaning_beam_agent-0_min: 282
    cleaning_beam_agent-1_max: 498
    cleaning_beam_agent-1_mean: 280.52
    cleaning_beam_agent-1_min: 117
    cleaning_beam_agent-2_max: 662
    cleaning_beam_agent-2_mean: 441.12
    cleaning_beam_agent-2_min: 184
    cleaning_beam_agent-3_max: 62
    cleaning_beam_agent-3_mean: 23.51
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 674
    cleaning_beam_agent-4_mean: 455.21
    cleaning_beam_agent-4_min: 280
    cleaning_beam_agent-5_max: 128
    cleaning_beam_agent-5_mean: 30.87
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.04
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-43-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1100.9999999999893
  episode_reward_mean: 944.2999999999853
  episode_reward_min: 500.0000000000148
  episodes_this_iter: 96
  episodes_total: 29376
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11603.682
    learner:
      agent-0:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9334482550621033
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013026903616264462
        model: {}
        policy_loss: -0.0030818309169262648
        total_loss: -0.0026833510491997004
        vf_explained_var: 0.07837016880512238
        vf_loss: 20.41348648071289
      agent-1:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1171488761901855
        entropy_coeff: 0.0017600000137463212
        kl: 0.001546130864880979
        model: {}
        policy_loss: -0.0036884844303131104
        total_loss: -0.0031819865107536316
        vf_explained_var: -0.10733437538146973
        vf_loss: 24.72682762145996
      agent-2:
        cur_kl_coeff: 9.860761462199441e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0131886005401611
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017787108663469553
        model: {}
        policy_loss: -0.003764173947274685
        total_loss: -0.0032261358574032784
        vf_explained_var: -0.04310035705566406
        vf_loss: 23.212493896484375
      agent-3:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49516722559928894
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011032891925424337
        model: {}
        policy_loss: -0.0024184193462133408
        total_loss: -0.001280471682548523
        vf_explained_var: 0.09580722451210022
        vf_loss: 20.094411849975586
      agent-4:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.94672691822052
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014295433647930622
        model: {}
        policy_loss: -0.003936676308512688
        total_loss: -0.00340559845790267
        vf_explained_var: 0.012984633445739746
        vf_loss: 21.973194122314453
      agent-5:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7096697092056274
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011267692316323519
        model: {}
        policy_loss: -0.003301311982795596
        total_loss: -0.00257319794036448
        vf_explained_var: 0.10753582417964935
        vf_loss: 19.77135467529297
    load_time_ms: 13352.287
    num_steps_sampled: 29376000
    num_steps_trained: 29376000
    sample_time_ms: 99447.618
    update_time_ms: 14.723
  iterations_since_restore: 246
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.554237288135596
    ram_util_percent: 16.20395480225989
  pid: 30948
  policy_reward_max:
    agent-0: 183.50000000000006
    agent-1: 183.50000000000006
    agent-2: 183.50000000000006
    agent-3: 183.50000000000006
    agent-4: 183.50000000000006
    agent-5: 183.50000000000006
  policy_reward_mean:
    agent-0: 157.38333333333324
    agent-1: 157.38333333333324
    agent-2: 157.38333333333324
    agent-3: 157.38333333333324
    agent-4: 157.38333333333324
    agent-5: 157.38333333333324
  policy_reward_min:
    agent-0: 83.33333333333353
    agent-1: 83.33333333333353
    agent-2: 83.33333333333353
    agent-3: 83.33333333333353
    agent-4: 83.33333333333353
    agent-5: 83.33333333333353
  sampler_perf:
    mean_env_wait_ms: 27.17466032901897
    mean_inference_ms: 12.87180007294085
    mean_processing_ms: 57.517494441961645
  time_since_restore: 32267.21913743019
  time_this_iter_s: 124.37637424468994
  time_total_s: 41393.230951309204
  timestamp: 1637059381
  timesteps_since_restore: 23616000
  timesteps_this_iter: 96000
  timesteps_total: 29376000
  training_iteration: 306
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    306 |          41393.2 | 29376000 |    944.3 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 1.67
    apples_agent-0_min: 0
    apples_agent-1_max: 116
    apples_agent-1_mean: 36.57
    apples_agent-1_min: 0
    apples_agent-2_max: 86
    apples_agent-2_mean: 5.35
    apples_agent-2_min: 0
    apples_agent-3_max: 135
    apples_agent-3_mean: 72.04
    apples_agent-3_min: 35
    apples_agent-4_max: 54
    apples_agent-4_mean: 2.94
    apples_agent-4_min: 0
    apples_agent-5_max: 224
    apples_agent-5_mean: 101.33
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 502
    cleaning_beam_agent-0_mean: 400.71
    cleaning_beam_agent-0_min: 287
    cleaning_beam_agent-1_max: 498
    cleaning_beam_agent-1_mean: 272.22
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 602
    cleaning_beam_agent-2_mean: 430.11
    cleaning_beam_agent-2_min: 259
    cleaning_beam_agent-3_max: 53
    cleaning_beam_agent-3_mean: 24.79
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 550
    cleaning_beam_agent-4_mean: 448.43
    cleaning_beam_agent-4_min: 319
    cleaning_beam_agent-5_max: 158
    cleaning_beam_agent-5_mean: 35.83
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-45-11
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1175.999999999994
  episode_reward_mean: 962.2499999999848
  episode_reward_min: 511.0000000000136
  episodes_this_iter: 96
  episodes_total: 29472
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11563.176
    learner:
      agent-0:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9438446760177612
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012024070601910353
        model: {}
        policy_loss: -0.002923373132944107
        total_loss: -0.002739335410296917
        vf_explained_var: 0.08887864649295807
        vf_loss: 18.452058792114258
      agent-1:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1295225620269775
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012809078907594085
        model: {}
        policy_loss: -0.003745022229850292
        total_loss: -0.0034922403283417225
        vf_explained_var: -0.08539798855781555
        vf_loss: 22.407394409179688
      agent-2:
        cur_kl_coeff: 4.930380731099721e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0361467599868774
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014056802028790116
        model: {}
        policy_loss: -0.003530763555318117
        total_loss: -0.0032551211770623922
        vf_explained_var: -0.028633862733840942
        vf_loss: 20.992624282836914
      agent-3:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4821772277355194
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010240989504382014
        model: {}
        policy_loss: -0.002468402497470379
        total_loss: -0.0014195707626640797
        vf_explained_var: 0.06265495717525482
        vf_loss: 18.97466278076172
      agent-4:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9482691287994385
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017021007370203733
        model: {}
        policy_loss: -0.0039300681091845036
        total_loss: -0.00350845605134964
        vf_explained_var: -0.020758599042892456
        vf_loss: 20.905641555786133
      agent-5:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7229266166687012
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010709615889936686
        model: {}
        policy_loss: -0.0030727656558156013
        total_loss: -0.0024812379851937294
        vf_explained_var: 0.08203168213367462
        vf_loss: 18.638774871826172
    load_time_ms: 13357.045
    num_steps_sampled: 29472000
    num_steps_trained: 29472000
    sample_time_ms: 99643.445
    update_time_ms: 14.618
  iterations_since_restore: 247
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 27.49081081081081
    ram_util_percent: 16.17891891891892
  pid: 30948
  policy_reward_max:
    agent-0: 195.99999999999966
    agent-1: 195.99999999999966
    agent-2: 195.99999999999966
    agent-3: 195.99999999999966
    agent-4: 195.99999999999966
    agent-5: 195.99999999999966
  policy_reward_mean:
    agent-0: 160.3749999999999
    agent-1: 160.3749999999999
    agent-2: 160.3749999999999
    agent-3: 160.3749999999999
    agent-4: 160.3749999999999
    agent-5: 160.3749999999999
  policy_reward_min:
    agent-0: 85.1666666666668
    agent-1: 85.1666666666668
    agent-2: 85.1666666666668
    agent-3: 85.1666666666668
    agent-4: 85.1666666666668
    agent-5: 85.1666666666668
  sampler_perf:
    mean_env_wait_ms: 27.175583651970893
    mean_inference_ms: 12.87098158271662
    mean_processing_ms: 57.51223167258429
  time_since_restore: 32394.836126327515
  time_this_iter_s: 127.61698889732361
  time_total_s: 41520.84794020653
  timestamp: 1637059511
  timesteps_since_restore: 23712000
  timesteps_this_iter: 96000
  timesteps_total: 29472000
  training_iteration: 307
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    307 |          41520.8 | 29472000 |   962.25 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 1.11
    apples_agent-0_min: 0
    apples_agent-1_max: 114
    apples_agent-1_mean: 31.7
    apples_agent-1_min: 0
    apples_agent-2_max: 130
    apples_agent-2_mean: 6.64
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 71.86
    apples_agent-3_min: 32
    apples_agent-4_max: 168
    apples_agent-4_mean: 4.84
    apples_agent-4_min: 0
    apples_agent-5_max: 196
    apples_agent-5_mean: 93.68
    apples_agent-5_min: 24
    cleaning_beam_agent-0_max: 512
    cleaning_beam_agent-0_mean: 404.49
    cleaning_beam_agent-0_min: 257
    cleaning_beam_agent-1_max: 528
    cleaning_beam_agent-1_mean: 276.81
    cleaning_beam_agent-1_min: 146
    cleaning_beam_agent-2_max: 693
    cleaning_beam_agent-2_mean: 417.48
    cleaning_beam_agent-2_min: 155
    cleaning_beam_agent-3_max: 91
    cleaning_beam_agent-3_mean: 25.08
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 537
    cleaning_beam_agent-4_mean: 446.44
    cleaning_beam_agent-4_min: 249
    cleaning_beam_agent-5_max: 133
    cleaning_beam_agent-5_mean: 37.37
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 2
    fire_beam_agent-0_mean: 0.02
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-47-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1109.999999999988
  episode_reward_mean: 939.8099999999841
  episode_reward_min: 324.99999999999903
  episodes_this_iter: 96
  episodes_total: 29568
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11529.422
    learner:
      agent-0:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9458466172218323
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014467244036495686
        model: {}
        policy_loss: -0.003119783941656351
        total_loss: -0.0026999302208423615
        vf_explained_var: 0.08309602737426758
        vf_loss: 20.845430374145508
      agent-1:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1194288730621338
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012084100162610412
        model: {}
        policy_loss: -0.0034950529225170612
        total_loss: -0.002897500991821289
        vf_explained_var: -0.12491035461425781
        vf_loss: 25.677461624145508
      agent-2:
        cur_kl_coeff: 2.4651903655498604e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0234322547912598
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017299671890214086
        model: {}
        policy_loss: -0.003557056188583374
        total_loss: -0.0030983611941337585
        vf_explained_var: 0.00728192925453186
        vf_loss: 22.599348068237305
      agent-3:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4886220395565033
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015939756995067
        model: {}
        policy_loss: -0.0029186303727328777
        total_loss: -0.0018264383543282747
        vf_explained_var: 0.14091436564922333
        vf_loss: 19.52169418334961
      agent-4:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9419560432434082
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018991738324984908
        model: {}
        policy_loss: -0.003735967446118593
        total_loss: -0.0031779492273926735
        vf_explained_var: 0.0278484970331192
        vf_loss: 22.158607482910156
      agent-5:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7061427235603333
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009937803260982037
        model: {}
        policy_loss: -0.0031777811236679554
        total_loss: -0.0024336916394531727
        vf_explained_var: 0.12539200484752655
        vf_loss: 19.86898422241211
    load_time_ms: 13361.116
    num_steps_sampled: 29568000
    num_steps_trained: 29568000
    sample_time_ms: 99688.62
    update_time_ms: 14.671
  iterations_since_restore: 248
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.350561797752807
    ram_util_percent: 16.203932584269666
  pid: 30948
  policy_reward_max:
    agent-0: 184.99999999999943
    agent-1: 184.99999999999943
    agent-2: 184.99999999999943
    agent-3: 184.99999999999943
    agent-4: 184.99999999999943
    agent-5: 184.99999999999943
  policy_reward_mean:
    agent-0: 156.63499999999996
    agent-1: 156.63499999999996
    agent-2: 156.63499999999996
    agent-3: 156.63499999999996
    agent-4: 156.63499999999996
    agent-5: 156.63499999999996
  policy_reward_min:
    agent-0: 54.16666666666656
    agent-1: 54.16666666666656
    agent-2: 54.16666666666656
    agent-3: 54.16666666666656
    agent-4: 54.16666666666656
    agent-5: 54.16666666666656
  sampler_perf:
    mean_env_wait_ms: 27.17699572091182
    mean_inference_ms: 12.870355534824851
    mean_processing_ms: 57.50888341044499
  time_since_restore: 32519.658690690994
  time_this_iter_s: 124.82256436347961
  time_total_s: 41645.67050457001
  timestamp: 1637059636
  timesteps_since_restore: 23808000
  timesteps_this_iter: 96000
  timesteps_total: 29568000
  training_iteration: 308
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    308 |          41645.7 | 29568000 |   939.81 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 2.6
    apples_agent-0_min: 0
    apples_agent-1_max: 142
    apples_agent-1_mean: 36.13
    apples_agent-1_min: 0
    apples_agent-2_max: 105
    apples_agent-2_mean: 6.33
    apples_agent-2_min: 0
    apples_agent-3_max: 154
    apples_agent-3_mean: 68.74
    apples_agent-3_min: 41
    apples_agent-4_max: 39
    apples_agent-4_mean: 3.01
    apples_agent-4_min: 0
    apples_agent-5_max: 191
    apples_agent-5_mean: 98.66
    apples_agent-5_min: 55
    cleaning_beam_agent-0_max: 501
    cleaning_beam_agent-0_mean: 392.56
    cleaning_beam_agent-0_min: 298
    cleaning_beam_agent-1_max: 504
    cleaning_beam_agent-1_mean: 277.25
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 684
    cleaning_beam_agent-2_mean: 437.4
    cleaning_beam_agent-2_min: 223
    cleaning_beam_agent-3_max: 61
    cleaning_beam_agent-3_mean: 24.86
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 581
    cleaning_beam_agent-4_mean: 451.78
    cleaning_beam_agent-4_min: 322
    cleaning_beam_agent-5_max: 96
    cleaning_beam_agent-5_mean: 30.46
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-49-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1114.999999999986
  episode_reward_mean: 963.0599999999837
  episode_reward_min: 540.0000000000114
  episodes_this_iter: 96
  episodes_total: 29664
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11518.147
    learner:
      agent-0:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.95804363489151
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012895278632640839
        model: {}
        policy_loss: -0.0030315611511468887
        total_loss: -0.002848643809556961
        vf_explained_var: 0.06931115686893463
        vf_loss: 18.690759658813477
      agent-1:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1319191455841064
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012138857273384929
        model: {}
        policy_loss: -0.0037731612101197243
        total_loss: -0.0034692874178290367
        vf_explained_var: -0.12170341610908508
        vf_loss: 22.960500717163086
      agent-2:
        cur_kl_coeff: 1.2325951827749302e-33
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0174180269241333
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015635222662240267
        model: {}
        policy_loss: -0.003539859317243099
        total_loss: -0.0032883211970329285
        vf_explained_var: -0.006947576999664307
        vf_loss: 20.42193031311035
      agent-3:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4657513499259949
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009408381301909685
        model: {}
        policy_loss: -0.002479500137269497
        total_loss: -0.0014096491504460573
        vf_explained_var: 0.0588788241147995
        vf_loss: 18.895721435546875
      agent-4:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9365283846855164
        entropy_coeff: 0.0017600000137463212
        kl: 0.001665092771872878
        model: {}
        policy_loss: -0.0038272589445114136
        total_loss: -0.0034167347475886345
        vf_explained_var: -0.016640692949295044
        vf_loss: 20.588134765625
      agent-5:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7258906364440918
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016137610655277967
        model: {}
        policy_loss: -0.003100398927927017
        total_loss: -0.0025273628998547792
        vf_explained_var: 0.08031804859638214
        vf_loss: 18.506017684936523
    load_time_ms: 13356.404
    num_steps_sampled: 29664000
    num_steps_trained: 29664000
    sample_time_ms: 99891.704
    update_time_ms: 14.949
  iterations_since_restore: 249
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.344134078212292
    ram_util_percent: 16.19497206703911
  pid: 30948
  policy_reward_max:
    agent-0: 185.83333333333323
    agent-1: 185.83333333333323
    agent-2: 185.83333333333323
    agent-3: 185.83333333333323
    agent-4: 185.83333333333323
    agent-5: 185.83333333333323
  policy_reward_mean:
    agent-0: 160.50999999999988
    agent-1: 160.50999999999988
    agent-2: 160.50999999999988
    agent-3: 160.50999999999988
    agent-4: 160.50999999999988
    agent-5: 160.50999999999988
  policy_reward_min:
    agent-0: 90.0000000000003
    agent-1: 90.0000000000003
    agent-2: 90.0000000000003
    agent-3: 90.0000000000003
    agent-4: 90.0000000000003
    agent-5: 90.0000000000003
  sampler_perf:
    mean_env_wait_ms: 27.178567017527577
    mean_inference_ms: 12.86971179311233
    mean_processing_ms: 57.50392406651688
  time_since_restore: 32645.37943172455
  time_this_iter_s: 125.72074103355408
  time_total_s: 41771.39124560356
  timestamp: 1637059762
  timesteps_since_restore: 23904000
  timesteps_this_iter: 96000
  timesteps_total: 29664000
  training_iteration: 309
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    309 |          41771.4 | 29664000 |   963.06 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 65
    apples_agent-0_mean: 2.46
    apples_agent-0_min: 0
    apples_agent-1_max: 136
    apples_agent-1_mean: 34.57
    apples_agent-1_min: 0
    apples_agent-2_max: 72
    apples_agent-2_mean: 7.14
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 70.28
    apples_agent-3_min: 29
    apples_agent-4_max: 76
    apples_agent-4_mean: 3.81
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 98.2
    apples_agent-5_min: 53
    cleaning_beam_agent-0_max: 531
    cleaning_beam_agent-0_mean: 396.8
    cleaning_beam_agent-0_min: 207
    cleaning_beam_agent-1_max: 411
    cleaning_beam_agent-1_mean: 268.33
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 684
    cleaning_beam_agent-2_mean: 439.74
    cleaning_beam_agent-2_min: 172
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 25.36
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 520
    cleaning_beam_agent-4_mean: 441.42
    cleaning_beam_agent-4_min: 324
    cleaning_beam_agent-5_max: 105
    cleaning_beam_agent-5_mean: 36.57
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-51-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1107.000000000024
  episode_reward_mean: 951.5199999999833
  episode_reward_min: 486.00000000001086
  episodes_this_iter: 96
  episodes_total: 29760
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11529.091
    learner:
      agent-0:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9358623027801514
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012623390648514032
        model: {}
        policy_loss: -0.003086758777499199
        total_loss: -0.0027834600768983364
        vf_explained_var: 0.0677771270275116
        vf_loss: 19.50415802001953
      agent-1:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1340816020965576
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011976652313023806
        model: {}
        policy_loss: -0.00408827792853117
        total_loss: -0.0037722692359238863
        vf_explained_var: -0.09612584114074707
        vf_loss: 23.119909286499023
      agent-2:
        cur_kl_coeff: 6.162975913874651e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0145366191864014
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013371979584917426
        model: {}
        policy_loss: -0.003426586277782917
        total_loss: -0.0030501317232847214
        vf_explained_var: -0.029581084847450256
        vf_loss: 21.62039566040039
      agent-3:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4757210612297058
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013226427836343646
        model: {}
        policy_loss: -0.0026605669409036636
        total_loss: -0.001558123156428337
        vf_explained_var: 0.07191164791584015
        vf_loss: 19.397127151489258
      agent-4:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9444940686225891
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020484179258346558
        model: {}
        policy_loss: -0.00393262691795826
        total_loss: -0.00344246719032526
        vf_explained_var: -0.02342832088470459
        vf_loss: 21.524734497070312
      agent-5:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7464160919189453
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012286505661904812
        model: {}
        policy_loss: -0.0035307779908180237
        total_loss: -0.002973485505208373
        vf_explained_var: 0.10553239285945892
        vf_loss: 18.7098388671875
    load_time_ms: 13355.799
    num_steps_sampled: 29760000
    num_steps_trained: 29760000
    sample_time_ms: 99754.785
    update_time_ms: 14.757
  iterations_since_restore: 250
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.60971428571428
    ram_util_percent: 16.19314285714286
  pid: 30948
  policy_reward_max:
    agent-0: 184.49999999999957
    agent-1: 184.49999999999957
    agent-2: 184.49999999999957
    agent-3: 184.49999999999957
    agent-4: 184.49999999999957
    agent-5: 184.49999999999957
  policy_reward_mean:
    agent-0: 158.58666666666656
    agent-1: 158.58666666666656
    agent-2: 158.58666666666656
    agent-3: 158.58666666666656
    agent-4: 158.58666666666656
    agent-5: 158.58666666666656
  policy_reward_min:
    agent-0: 81.0
    agent-1: 81.0
    agent-2: 81.0
    agent-3: 81.0
    agent-4: 81.0
    agent-5: 81.0
  sampler_perf:
    mean_env_wait_ms: 27.179715847126843
    mean_inference_ms: 12.86882780985404
    mean_processing_ms: 57.49787754366922
  time_since_restore: 32768.58028435707
  time_this_iter_s: 123.20085263252258
  time_total_s: 41894.592098236084
  timestamp: 1637059885
  timesteps_since_restore: 24000000
  timesteps_this_iter: 96000
  timesteps_total: 29760000
  training_iteration: 310
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    310 |          41894.6 | 29760000 |   951.52 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 2.04
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 29.82
    apples_agent-1_min: 0
    apples_agent-2_max: 203
    apples_agent-2_mean: 8.03
    apples_agent-2_min: 0
    apples_agent-3_max: 165
    apples_agent-3_mean: 71.65
    apples_agent-3_min: 35
    apples_agent-4_max: 85
    apples_agent-4_mean: 4.79
    apples_agent-4_min: 0
    apples_agent-5_max: 204
    apples_agent-5_mean: 100.4
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 489
    cleaning_beam_agent-0_mean: 393.42
    cleaning_beam_agent-0_min: 222
    cleaning_beam_agent-1_max: 482
    cleaning_beam_agent-1_mean: 273.21
    cleaning_beam_agent-1_min: 139
    cleaning_beam_agent-2_max: 693
    cleaning_beam_agent-2_mean: 442.75
    cleaning_beam_agent-2_min: 216
    cleaning_beam_agent-3_max: 57
    cleaning_beam_agent-3_mean: 26.76
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 547
    cleaning_beam_agent-4_mean: 441.85
    cleaning_beam_agent-4_min: 317
    cleaning_beam_agent-5_max: 87
    cleaning_beam_agent-5_mean: 32.49
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-53-31
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1135.9999999999916
  episode_reward_mean: 940.939999999984
  episode_reward_min: 449.0000000000101
  episodes_this_iter: 96
  episodes_total: 29856
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11534.276
    learner:
      agent-0:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9608876705169678
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016971362056210637
        model: {}
        policy_loss: -0.0034271045587956905
        total_loss: -0.0028120558708906174
        vf_explained_var: 0.046491220593452454
        vf_loss: 23.062088012695312
      agent-1:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1189101934432983
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014077478554099798
        model: {}
        policy_loss: -0.004029775969684124
        total_loss: -0.0034119586925953627
        vf_explained_var: -0.06421077251434326
        vf_loss: 25.87097930908203
      agent-2:
        cur_kl_coeff: 3.0814879569373254e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0168418884277344
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017848486313596368
        model: {}
        policy_loss: -0.003819853300228715
        total_loss: -0.0030966419726610184
        vf_explained_var: -0.039933472871780396
        vf_loss: 25.12854766845703
      agent-3:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4896169602870941
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010358064901083708
        model: {}
        policy_loss: -0.002847126219421625
        total_loss: -0.001618321519345045
        vf_explained_var: 0.1345788687467575
        vf_loss: 20.905309677124023
      agent-4:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9514144062995911
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011038554366678
        model: {}
        policy_loss: -0.003946554847061634
        total_loss: -0.0033198136370629072
        vf_explained_var: 0.051316291093826294
        vf_loss: 23.01228141784668
      agent-5:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7247143983840942
        entropy_coeff: 0.0017600000137463212
        kl: 0.001111820456571877
        model: {}
        policy_loss: -0.0032620723359286785
        total_loss: -0.0024220990017056465
        vf_explained_var: 0.12515227496623993
        vf_loss: 21.154727935791016
    load_time_ms: 13350.687
    num_steps_sampled: 29856000
    num_steps_trained: 29856000
    sample_time_ms: 99877.444
    update_time_ms: 14.771
  iterations_since_restore: 251
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.487078651685394
    ram_util_percent: 16.121348314606742
  pid: 30948
  policy_reward_max:
    agent-0: 189.33333333333303
    agent-1: 189.33333333333303
    agent-2: 189.33333333333303
    agent-3: 189.33333333333303
    agent-4: 189.33333333333303
    agent-5: 189.33333333333303
  policy_reward_mean:
    agent-0: 156.82333333333324
    agent-1: 156.82333333333324
    agent-2: 156.82333333333324
    agent-3: 156.82333333333324
    agent-4: 156.82333333333324
    agent-5: 156.82333333333324
  policy_reward_min:
    agent-0: 74.83333333333326
    agent-1: 74.83333333333326
    agent-2: 74.83333333333326
    agent-3: 74.83333333333326
    agent-4: 74.83333333333326
    agent-5: 74.83333333333326
  sampler_perf:
    mean_env_wait_ms: 27.1818471372522
    mean_inference_ms: 12.86835915811695
    mean_processing_ms: 57.49572528872087
  time_since_restore: 32893.97395205498
  time_this_iter_s: 125.3936676979065
  time_total_s: 42019.98576593399
  timestamp: 1637060011
  timesteps_since_restore: 24096000
  timesteps_this_iter: 96000
  timesteps_total: 29856000
  training_iteration: 311
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    311 |            42020 | 29856000 |   940.94 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 63
    apples_agent-0_mean: 2.18
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 35.47
    apples_agent-1_min: 0
    apples_agent-2_max: 101
    apples_agent-2_mean: 5.57
    apples_agent-2_min: 0
    apples_agent-3_max: 122
    apples_agent-3_mean: 70.13
    apples_agent-3_min: 38
    apples_agent-4_max: 55
    apples_agent-4_mean: 2.66
    apples_agent-4_min: 0
    apples_agent-5_max: 204
    apples_agent-5_mean: 98.0
    apples_agent-5_min: 47
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 382.48
    cleaning_beam_agent-0_min: 222
    cleaning_beam_agent-1_max: 464
    cleaning_beam_agent-1_mean: 271.64
    cleaning_beam_agent-1_min: 137
    cleaning_beam_agent-2_max: 686
    cleaning_beam_agent-2_mean: 427.18
    cleaning_beam_agent-2_min: 199
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 24.84
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 564
    cleaning_beam_agent-4_mean: 448.8
    cleaning_beam_agent-4_min: 331
    cleaning_beam_agent-5_max: 141
    cleaning_beam_agent-5_mean: 35.36
    cleaning_beam_agent-5_min: 12
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-55-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1106.999999999988
  episode_reward_mean: 954.2399999999834
  episode_reward_min: 449.00000000000244
  episodes_this_iter: 96
  episodes_total: 29952
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11528.516
    learner:
      agent-0:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9637631773948669
        entropy_coeff: 0.0017600000137463212
        kl: 0.00116718967910856
        model: {}
        policy_loss: -0.0029878364875912666
        total_loss: -0.0027884989976882935
        vf_explained_var: 0.07366088032722473
        vf_loss: 18.955615997314453
      agent-1:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1277135610580444
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015976496506482363
        model: {}
        policy_loss: -0.004136496223509312
        total_loss: -0.0038345507346093655
        vf_explained_var: -0.0989311933517456
        vf_loss: 22.86720085144043
      agent-2:
        cur_kl_coeff: 1.5407439784686627e-34
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0297352075576782
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020180190913379192
        model: {}
        policy_loss: -0.0038027833215892315
        total_loss: -0.003499697893857956
        vf_explained_var: -0.026326611638069153
        vf_loss: 21.154212951660156
      agent-3:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.471795916557312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010291163343936205
        model: {}
        policy_loss: -0.0024703931994736195
        total_loss: -0.0013948719715699553
        vf_explained_var: 0.06848461925983429
        vf_loss: 19.058815002441406
      agent-4:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9353891611099243
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012223317753523588
        model: {}
        policy_loss: -0.0036515737883746624
        total_loss: -0.003218251746147871
        vf_explained_var: -0.009371250867843628
        vf_loss: 20.79606056213379
      agent-5:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7206365466117859
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013778858119621873
        model: {}
        policy_loss: -0.0034608724527060986
        total_loss: -0.0028782421723008156
        vf_explained_var: 0.09525203704833984
        vf_loss: 18.50950050354004
    load_time_ms: 13333.765
    num_steps_sampled: 29952000
    num_steps_trained: 29952000
    sample_time_ms: 99954.096
    update_time_ms: 14.927
  iterations_since_restore: 252
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.424157303370787
    ram_util_percent: 16.188764044943824
  pid: 30948
  policy_reward_max:
    agent-0: 184.50000000000065
    agent-1: 184.50000000000065
    agent-2: 184.50000000000065
    agent-3: 184.50000000000065
    agent-4: 184.50000000000065
    agent-5: 184.50000000000065
  policy_reward_mean:
    agent-0: 159.03999999999988
    agent-1: 159.03999999999988
    agent-2: 159.03999999999988
    agent-3: 159.03999999999988
    agent-4: 159.03999999999988
    agent-5: 159.03999999999988
  policy_reward_min:
    agent-0: 74.8333333333333
    agent-1: 74.8333333333333
    agent-2: 74.8333333333333
    agent-3: 74.8333333333333
    agent-4: 74.8333333333333
    agent-5: 74.8333333333333
  sampler_perf:
    mean_env_wait_ms: 27.18308586104883
    mean_inference_ms: 12.86794478316774
    mean_processing_ms: 57.49252307910749
  time_since_restore: 33018.70131993294
  time_this_iter_s: 124.7273678779602
  time_total_s: 42144.71313381195
  timestamp: 1637060136
  timesteps_since_restore: 24192000
  timesteps_this_iter: 96000
  timesteps_total: 29952000
  training_iteration: 312
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    312 |          42144.7 | 29952000 |   954.24 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 3.84
    apples_agent-0_min: 0
    apples_agent-1_max: 116
    apples_agent-1_mean: 35.26
    apples_agent-1_min: 0
    apples_agent-2_max: 101
    apples_agent-2_mean: 8.14
    apples_agent-2_min: 0
    apples_agent-3_max: 123
    apples_agent-3_mean: 72.88
    apples_agent-3_min: 34
    apples_agent-4_max: 49
    apples_agent-4_mean: 2.56
    apples_agent-4_min: 0
    apples_agent-5_max: 229
    apples_agent-5_mean: 98.1
    apples_agent-5_min: 57
    cleaning_beam_agent-0_max: 504
    cleaning_beam_agent-0_mean: 375.0
    cleaning_beam_agent-0_min: 236
    cleaning_beam_agent-1_max: 449
    cleaning_beam_agent-1_mean: 269.15
    cleaning_beam_agent-1_min: 107
    cleaning_beam_agent-2_max: 596
    cleaning_beam_agent-2_mean: 424.72
    cleaning_beam_agent-2_min: 201
    cleaning_beam_agent-3_max: 63
    cleaning_beam_agent-3_mean: 26.96
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 543
    cleaning_beam_agent-4_mean: 452.21
    cleaning_beam_agent-4_min: 335
    cleaning_beam_agent-5_max: 102
    cleaning_beam_agent-5_mean: 34.24
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-57-40
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1108.999999999984
  episode_reward_mean: 936.1099999999838
  episode_reward_min: 527.0000000000055
  episodes_this_iter: 96
  episodes_total: 30048
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11515.926
    learner:
      agent-0:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9874351024627686
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013957215705886483
        model: {}
        policy_loss: -0.003114429535344243
        total_loss: -0.0030566900968551636
        vf_explained_var: 0.10517299175262451
        vf_loss: 17.95624542236328
      agent-1:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.127811312675476
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013198253000155091
        model: {}
        policy_loss: -0.003510987153276801
        total_loss: -0.0032529933378100395
        vf_explained_var: -0.10647353529930115
        vf_loss: 22.429426193237305
      agent-2:
        cur_kl_coeff: 7.703719892343314e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0298547744750977
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019078130135312676
        model: {}
        policy_loss: -0.003296496346592903
        total_loss: -0.00308278389275074
        vf_explained_var: -0.004999116063117981
        vf_loss: 20.26254653930664
      agent-3:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.49740874767303467
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007835940923541784
        model: {}
        policy_loss: -0.0024544158950448036
        total_loss: -0.0014901133254170418
        vf_explained_var: 0.08258137106895447
        vf_loss: 18.397418975830078
      agent-4:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9364943504333496
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012725950218737125
        model: {}
        policy_loss: -0.0036504727322608232
        total_loss: -0.0032193418592214584
        vf_explained_var: -0.0329098105430603
        vf_loss: 20.793596267700195
      agent-5:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7304004430770874
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010561515809968114
        model: {}
        policy_loss: -0.003268284723162651
        total_loss: -0.002766452496871352
        vf_explained_var: 0.10900279879570007
        vf_loss: 17.87337875366211
    load_time_ms: 13332.843
    num_steps_sampled: 30048000
    num_steps_trained: 30048000
    sample_time_ms: 99939.607
    update_time_ms: 14.292
  iterations_since_restore: 253
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.358192090395477
    ram_util_percent: 16.196045197740116
  pid: 30948
  policy_reward_max:
    agent-0: 184.83333333333294
    agent-1: 184.83333333333294
    agent-2: 184.83333333333294
    agent-3: 184.83333333333294
    agent-4: 184.83333333333294
    agent-5: 184.83333333333294
  policy_reward_mean:
    agent-0: 156.0183333333333
    agent-1: 156.0183333333333
    agent-2: 156.0183333333333
    agent-3: 156.0183333333333
    agent-4: 156.0183333333333
    agent-5: 156.0183333333333
  policy_reward_min:
    agent-0: 87.83333333333347
    agent-1: 87.83333333333347
    agent-2: 87.83333333333347
    agent-3: 87.83333333333347
    agent-4: 87.83333333333347
    agent-5: 87.83333333333347
  sampler_perf:
    mean_env_wait_ms: 27.18397041031721
    mean_inference_ms: 12.86741802779372
    mean_processing_ms: 57.488672996043945
  time_since_restore: 33143.42317008972
  time_this_iter_s: 124.72185015678406
  time_total_s: 42269.434983968735
  timestamp: 1637060260
  timesteps_since_restore: 24288000
  timesteps_this_iter: 96000
  timesteps_total: 30048000
  training_iteration: 313
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    313 |          42269.4 | 30048000 |   936.11 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 0.9
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 31.57
    apples_agent-1_min: 0
    apples_agent-2_max: 94
    apples_agent-2_mean: 6.05
    apples_agent-2_min: 0
    apples_agent-3_max: 126
    apples_agent-3_mean: 71.04
    apples_agent-3_min: 42
    apples_agent-4_max: 53
    apples_agent-4_mean: 3.99
    apples_agent-4_min: 0
    apples_agent-5_max: 160
    apples_agent-5_mean: 95.16
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 486
    cleaning_beam_agent-0_mean: 372.77
    cleaning_beam_agent-0_min: 267
    cleaning_beam_agent-1_max: 618
    cleaning_beam_agent-1_mean: 298.09
    cleaning_beam_agent-1_min: 143
    cleaning_beam_agent-2_max: 638
    cleaning_beam_agent-2_mean: 433.39
    cleaning_beam_agent-2_min: 224
    cleaning_beam_agent-3_max: 70
    cleaning_beam_agent-3_mean: 26.16
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 556
    cleaning_beam_agent-4_mean: 460.0
    cleaning_beam_agent-4_min: 362
    cleaning_beam_agent-5_max: 111
    cleaning_beam_agent-5_mean: 36.13
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_05-59-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1117.0000000000027
  episode_reward_mean: 953.2799999999837
  episode_reward_min: 459.00000000001097
  episodes_this_iter: 96
  episodes_total: 30144
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11533.355
    learner:
      agent-0:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9896639585494995
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024701892398297787
        model: {}
        policy_loss: -0.003155713900923729
        total_loss: -0.0029118661768734455
        vf_explained_var: 0.03973063826560974
        vf_loss: 19.85652732849121
      agent-1:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1146316528320312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011904584243893623
        model: {}
        policy_loss: -0.0037617445923388004
        total_loss: -0.003482281230390072
        vf_explained_var: -0.07323101162910461
        vf_loss: 22.41219139099121
      agent-2:
        cur_kl_coeff: 3.851859946171657e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0320115089416504
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017072047339752316
        model: {}
        policy_loss: -0.003371065016835928
        total_loss: -0.003084020223468542
        vf_explained_var: -0.013573169708251953
        vf_loss: 21.03386116027832
      agent-3:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.476744145154953
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011659213341772556
        model: {}
        policy_loss: -0.002634614473208785
        total_loss: -0.0015607005916535854
        vf_explained_var: 0.07578389346599579
        vf_loss: 19.129837036132812
      agent-4:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9507296085357666
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018162754131481051
        model: {}
        policy_loss: -0.004097092431038618
        total_loss: -0.003718051128089428
        vf_explained_var: 0.014551788568496704
        vf_loss: 20.523284912109375
      agent-5:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7212163209915161
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014149810886010528
        model: {}
        policy_loss: -0.0034917164593935013
        total_loss: -0.0028713184874504805
        vf_explained_var: 0.0860530287027359
        vf_loss: 18.897418975830078
    load_time_ms: 13330.704
    num_steps_sampled: 30144000
    num_steps_trained: 30144000
    sample_time_ms: 99871.153
    update_time_ms: 14.086
  iterations_since_restore: 254
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.645454545454548
    ram_util_percent: 16.19659090909091
  pid: 30948
  policy_reward_max:
    agent-0: 186.1666666666663
    agent-1: 186.1666666666663
    agent-2: 186.1666666666663
    agent-3: 186.1666666666663
    agent-4: 186.1666666666663
    agent-5: 186.1666666666663
  policy_reward_mean:
    agent-0: 158.87999999999988
    agent-1: 158.87999999999988
    agent-2: 158.87999999999988
    agent-3: 158.87999999999988
    agent-4: 158.87999999999988
    agent-5: 158.87999999999988
  policy_reward_min:
    agent-0: 76.50000000000003
    agent-1: 76.50000000000003
    agent-2: 76.50000000000003
    agent-3: 76.50000000000003
    agent-4: 76.50000000000003
    agent-5: 76.50000000000003
  sampler_perf:
    mean_env_wait_ms: 27.184969739610214
    mean_inference_ms: 12.866921720053742
    mean_processing_ms: 57.483799562844524
  time_since_restore: 33267.31508851051
  time_this_iter_s: 123.89191842079163
  time_total_s: 42393.32690238953
  timestamp: 1637060384
  timesteps_since_restore: 24384000
  timesteps_this_iter: 96000
  timesteps_total: 30144000
  training_iteration: 314
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    314 |          42393.3 | 30144000 |   953.28 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 2.4
    apples_agent-0_min: 0
    apples_agent-1_max: 133
    apples_agent-1_mean: 31.97
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 8.58
    apples_agent-2_min: 0
    apples_agent-3_max: 119
    apples_agent-3_mean: 71.35
    apples_agent-3_min: 36
    apples_agent-4_max: 38
    apples_agent-4_mean: 1.81
    apples_agent-4_min: 0
    apples_agent-5_max: 161
    apples_agent-5_mean: 91.28
    apples_agent-5_min: 46
    cleaning_beam_agent-0_max: 475
    cleaning_beam_agent-0_mean: 363.25
    cleaning_beam_agent-0_min: 202
    cleaning_beam_agent-1_max: 618
    cleaning_beam_agent-1_mean: 299.95
    cleaning_beam_agent-1_min: 157
    cleaning_beam_agent-2_max: 627
    cleaning_beam_agent-2_mean: 441.01
    cleaning_beam_agent-2_min: 186
    cleaning_beam_agent-3_max: 60
    cleaning_beam_agent-3_mean: 24.84
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 571
    cleaning_beam_agent-4_mean: 453.95
    cleaning_beam_agent-4_min: 287
    cleaning_beam_agent-5_max: 125
    cleaning_beam_agent-5_mean: 34.63
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-01-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1111.9999999999982
  episode_reward_mean: 951.2099999999859
  episode_reward_min: 291.9999999999976
  episodes_this_iter: 96
  episodes_total: 30240
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11539.463
    learner:
      agent-0:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9781187772750854
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019587769638746977
        model: {}
        policy_loss: -0.003206533845514059
        total_loss: -0.0029142391867935658
        vf_explained_var: 0.0661470890045166
        vf_loss: 20.13787078857422
      agent-1:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1321921348571777
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010260652052238584
        model: {}
        policy_loss: -0.003233557567000389
        total_loss: -0.002812896855175495
        vf_explained_var: -0.11482036113739014
        vf_loss: 24.133195877075195
      agent-2:
        cur_kl_coeff: 1.9259299730858284e-35
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0194138288497925
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012234851019456983
        model: {}
        policy_loss: -0.0035124579444527626
        total_loss: -0.0030757971107959747
        vf_explained_var: -0.029065370559692383
        vf_loss: 22.30827522277832
      agent-3:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.48457637429237366
        entropy_coeff: 0.0017600000137463212
        kl: 0.000996902585029602
        model: {}
        policy_loss: -0.002619264181703329
        total_loss: -0.0015355104114860296
        vf_explained_var: 0.10091724991798401
        vf_loss: 19.36609649658203
      agent-4:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9497652053833008
        entropy_coeff: 0.0017600000137463212
        kl: 0.001230467576533556
        model: {}
        policy_loss: -0.0035710230004042387
        total_loss: -0.0030096767004579306
        vf_explained_var: -0.029999911785125732
        vf_loss: 22.32933235168457
      agent-5:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7268376350402832
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014565025921911001
        model: {}
        policy_loss: -0.003552034730091691
        total_loss: -0.002901606960222125
        vf_explained_var: 0.10390487313270569
        vf_loss: 19.296611785888672
    load_time_ms: 13328.363
    num_steps_sampled: 30240000
    num_steps_trained: 30240000
    sample_time_ms: 99982.546
    update_time_ms: 13.965
  iterations_since_restore: 255
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.25112359550562
    ram_util_percent: 16.19101123595506
  pid: 30948
  policy_reward_max:
    agent-0: 185.333333333333
    agent-1: 185.333333333333
    agent-2: 185.333333333333
    agent-3: 185.333333333333
    agent-4: 185.333333333333
    agent-5: 185.333333333333
  policy_reward_mean:
    agent-0: 158.5349999999999
    agent-1: 158.5349999999999
    agent-2: 158.5349999999999
    agent-3: 158.5349999999999
    agent-4: 158.5349999999999
    agent-5: 158.5349999999999
  policy_reward_min:
    agent-0: 48.66666666666658
    agent-1: 48.66666666666658
    agent-2: 48.66666666666658
    agent-3: 48.66666666666658
    agent-4: 48.66666666666658
    agent-5: 48.66666666666658
  sampler_perf:
    mean_env_wait_ms: 27.186327490107818
    mean_inference_ms: 12.866278834023849
    mean_processing_ms: 57.47958778552616
  time_since_restore: 33392.265303611755
  time_this_iter_s: 124.95021510124207
  time_total_s: 42518.27711749077
  timestamp: 1637060510
  timesteps_since_restore: 24480000
  timesteps_this_iter: 96000
  timesteps_total: 30240000
  training_iteration: 315
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    315 |          42518.3 | 30240000 |   951.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 26
    apples_agent-0_mean: 2.05
    apples_agent-0_min: 0
    apples_agent-1_max: 111
    apples_agent-1_mean: 32.5
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 4.77
    apples_agent-2_min: 0
    apples_agent-3_max: 132
    apples_agent-3_mean: 71.85
    apples_agent-3_min: 44
    apples_agent-4_max: 57
    apples_agent-4_mean: 3.2
    apples_agent-4_min: 0
    apples_agent-5_max: 179
    apples_agent-5_mean: 94.15
    apples_agent-5_min: 46
    cleaning_beam_agent-0_max: 449
    cleaning_beam_agent-0_mean: 367.72
    cleaning_beam_agent-0_min: 226
    cleaning_beam_agent-1_max: 528
    cleaning_beam_agent-1_mean: 284.03
    cleaning_beam_agent-1_min: 142
    cleaning_beam_agent-2_max: 607
    cleaning_beam_agent-2_mean: 436.48
    cleaning_beam_agent-2_min: 148
    cleaning_beam_agent-3_max: 117
    cleaning_beam_agent-3_mean: 25.85
    cleaning_beam_agent-3_min: 10
    cleaning_beam_agent-4_max: 594
    cleaning_beam_agent-4_mean: 450.83
    cleaning_beam_agent-4_min: 330
    cleaning_beam_agent-5_max: 162
    cleaning_beam_agent-5_mean: 37.52
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-03-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1093.9999999999968
  episode_reward_mean: 945.6499999999833
  episode_reward_min: 411.0000000000092
  episodes_this_iter: 96
  episodes_total: 30336
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11561.422
    learner:
      agent-0:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9533179998397827
        entropy_coeff: 0.0017600000137463212
        kl: 0.002240499947220087
        model: {}
        policy_loss: -0.0032269596122205257
        total_loss: -0.003049957798793912
        vf_explained_var: 0.051881954073905945
        vf_loss: 18.548410415649414
      agent-1:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1221522092819214
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013085231184959412
        model: {}
        policy_loss: -0.003666945733129978
        total_loss: -0.0034875040873885155
        vf_explained_var: -0.09777981042861938
        vf_loss: 21.54429817199707
      agent-2:
        cur_kl_coeff: 9.629649865429142e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.025339961051941
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017439834773540497
        model: {}
        policy_loss: -0.0036445162259042263
        total_loss: -0.0034998473711311817
        vf_explained_var: 0.0038236230611801147
        vf_loss: 19.492624282836914
      agent-3:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47912248969078064
        entropy_coeff: 0.0017600000137463212
        kl: 0.001346953446045518
        model: {}
        policy_loss: -0.002825784496963024
        total_loss: -0.0018813135102391243
        vf_explained_var: 0.08382910490036011
        vf_loss: 17.877233505249023
      agent-4:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9464197754859924
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016904395306482911
        model: {}
        policy_loss: -0.003971346653997898
        total_loss: -0.0036330632865428925
        vf_explained_var: -0.021817967295646667
        vf_loss: 20.039813995361328
      agent-5:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7382665872573853
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008610750082880259
        model: {}
        policy_loss: -0.0030383486300706863
        total_loss: -0.002605752320960164
        vf_explained_var: 0.11321166157722473
        vf_loss: 17.319454193115234
    load_time_ms: 13337.558
    num_steps_sampled: 30336000
    num_steps_trained: 30336000
    sample_time_ms: 99978.606
    update_time_ms: 13.73
  iterations_since_restore: 256
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.288202247191013
    ram_util_percent: 16.12696629213483
  pid: 30948
  policy_reward_max:
    agent-0: 182.3333333333332
    agent-1: 182.3333333333332
    agent-2: 182.3333333333332
    agent-3: 182.3333333333332
    agent-4: 182.3333333333332
    agent-5: 182.3333333333332
  policy_reward_mean:
    agent-0: 157.60833333333326
    agent-1: 157.60833333333326
    agent-2: 157.60833333333326
    agent-3: 157.60833333333326
    agent-4: 157.60833333333326
    agent-5: 157.60833333333326
  policy_reward_min:
    agent-0: 68.49999999999986
    agent-1: 68.49999999999986
    agent-2: 68.49999999999986
    agent-3: 68.49999999999986
    agent-4: 68.49999999999986
    agent-5: 68.49999999999986
  sampler_perf:
    mean_env_wait_ms: 27.186845255938525
    mean_inference_ms: 12.86544505067473
    mean_processing_ms: 57.474299592935395
  time_since_restore: 33516.90699481964
  time_this_iter_s: 124.64169120788574
  time_total_s: 42642.918808698654
  timestamp: 1637060634
  timesteps_since_restore: 24576000
  timesteps_this_iter: 96000
  timesteps_total: 30336000
  training_iteration: 316
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    316 |          42642.9 | 30336000 |   945.65 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 22
    apples_agent-0_mean: 0.71
    apples_agent-0_min: 0
    apples_agent-1_max: 84
    apples_agent-1_mean: 31.95
    apples_agent-1_min: 0
    apples_agent-2_max: 87
    apples_agent-2_mean: 8.09
    apples_agent-2_min: 0
    apples_agent-3_max: 125
    apples_agent-3_mean: 69.68
    apples_agent-3_min: 35
    apples_agent-4_max: 40
    apples_agent-4_mean: 2.47
    apples_agent-4_min: 0
    apples_agent-5_max: 157
    apples_agent-5_mean: 95.27
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 469
    cleaning_beam_agent-0_mean: 371.48
    cleaning_beam_agent-0_min: 245
    cleaning_beam_agent-1_max: 488
    cleaning_beam_agent-1_mean: 278.59
    cleaning_beam_agent-1_min: 144
    cleaning_beam_agent-2_max: 664
    cleaning_beam_agent-2_mean: 419.82
    cleaning_beam_agent-2_min: 182
    cleaning_beam_agent-3_max: 87
    cleaning_beam_agent-3_mean: 25.42
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 540
    cleaning_beam_agent-4_mean: 458.5
    cleaning_beam_agent-4_min: 336
    cleaning_beam_agent-5_max: 102
    cleaning_beam_agent-5_mean: 30.62
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-05-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1151.9999999999982
  episode_reward_mean: 959.7999999999842
  episode_reward_min: 404.0000000000073
  episodes_this_iter: 96
  episodes_total: 30432
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11544.879
    learner:
      agent-0:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9582033157348633
        entropy_coeff: 0.0017600000137463212
        kl: 0.0032120528630912304
        model: {}
        policy_loss: -0.003939429298043251
        total_loss: -0.003681761212646961
        vf_explained_var: 0.06581197679042816
        vf_loss: 19.441078186035156
      agent-1:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1181740760803223
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010642484994605184
        model: {}
        policy_loss: -0.0038569732569158077
        total_loss: -0.0034400392323732376
        vf_explained_var: -0.13165169954299927
        vf_loss: 23.84920883178711
      agent-2:
        cur_kl_coeff: 4.814824932714571e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0298409461975098
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014425399713218212
        model: {}
        policy_loss: -0.0034407158382236958
        total_loss: -0.0031356550753116608
        vf_explained_var: -0.0091799795627594
        vf_loss: 21.17581558227539
      agent-3:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.467323899269104
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011129442136734724
        model: {}
        policy_loss: -0.002394120441749692
        total_loss: -0.0013392940163612366
        vf_explained_var: 0.0943516194820404
        vf_loss: 18.773157119750977
      agent-4:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9451080560684204
        entropy_coeff: 0.0017600000137463212
        kl: 0.000973632326349616
        model: {}
        policy_loss: -0.0036079389974474907
        total_loss: -0.003164820373058319
        vf_explained_var: -0.00510464608669281
        vf_loss: 21.065093994140625
      agent-5:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7244663834571838
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014524521538987756
        model: {}
        policy_loss: -0.0033850171603262424
        total_loss: -0.0028037591837346554
        vf_explained_var: 0.10505057871341705
        vf_loss: 18.563159942626953
    load_time_ms: 13341.962
    num_steps_sampled: 30432000
    num_steps_trained: 30432000
    sample_time_ms: 99567.335
    update_time_ms: 13.921
  iterations_since_restore: 257
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.47085714285714
    ram_util_percent: 16.19485714285715
  pid: 30948
  policy_reward_max:
    agent-0: 191.99999999999957
    agent-1: 191.99999999999957
    agent-2: 191.99999999999957
    agent-3: 191.99999999999957
    agent-4: 191.99999999999957
    agent-5: 191.99999999999957
  policy_reward_mean:
    agent-0: 159.96666666666658
    agent-1: 159.96666666666658
    agent-2: 159.96666666666658
    agent-3: 159.96666666666658
    agent-4: 159.96666666666658
    agent-5: 159.96666666666658
  policy_reward_min:
    agent-0: 67.33333333333304
    agent-1: 67.33333333333304
    agent-2: 67.33333333333304
    agent-3: 67.33333333333304
    agent-4: 67.33333333333304
    agent-5: 67.33333333333304
  sampler_perf:
    mean_env_wait_ms: 27.18743629669509
    mean_inference_ms: 12.864639472284939
    mean_processing_ms: 57.46867446435761
  time_since_restore: 33640.32500290871
  time_this_iter_s: 123.41800808906555
  time_total_s: 42766.33681678772
  timestamp: 1637060758
  timesteps_since_restore: 24672000
  timesteps_this_iter: 96000
  timesteps_total: 30432000
  training_iteration: 317
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    317 |          42766.3 | 30432000 |    959.8 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 21
    apples_agent-0_mean: 0.95
    apples_agent-0_min: 0
    apples_agent-1_max: 152
    apples_agent-1_mean: 34.03
    apples_agent-1_min: 0
    apples_agent-2_max: 79
    apples_agent-2_mean: 7.13
    apples_agent-2_min: 0
    apples_agent-3_max: 141
    apples_agent-3_mean: 68.37
    apples_agent-3_min: 33
    apples_agent-4_max: 49
    apples_agent-4_mean: 2.31
    apples_agent-4_min: 0
    apples_agent-5_max: 172
    apples_agent-5_mean: 100.08
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 500
    cleaning_beam_agent-0_mean: 380.82
    cleaning_beam_agent-0_min: 266
    cleaning_beam_agent-1_max: 425
    cleaning_beam_agent-1_mean: 276.03
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 647
    cleaning_beam_agent-2_mean: 421.46
    cleaning_beam_agent-2_min: 214
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 24.37
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 535
    cleaning_beam_agent-4_mean: 451.69
    cleaning_beam_agent-4_min: 317
    cleaning_beam_agent-5_max: 83
    cleaning_beam_agent-5_mean: 31.0
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-08-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1123.9999999999984
  episode_reward_mean: 946.5599999999838
  episode_reward_min: 338.9999999999987
  episodes_this_iter: 96
  episodes_total: 30528
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11594.132
    learner:
      agent-0:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9746435284614563
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010994786862283945
        model: {}
        policy_loss: -0.002906414680182934
        total_loss: -0.0026622586883604527
        vf_explained_var: 0.04853305220603943
        vf_loss: 19.595279693603516
      agent-1:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1253712177276611
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014876264613121748
        model: {}
        policy_loss: -0.004045670386403799
        total_loss: -0.0037240413948893547
        vf_explained_var: -0.11010122299194336
        vf_loss: 23.022851943969727
      agent-2:
        cur_kl_coeff: 2.4074124663572855e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.030330777168274
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012475480325520039
        model: {}
        policy_loss: -0.00324935931712389
        total_loss: -0.002923480235040188
        vf_explained_var: -0.03214702010154724
        vf_loss: 21.392608642578125
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46909675002098083
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011951818596571684
        model: {}
        policy_loss: -0.0029266802594065666
        total_loss: -0.0018486250191926956
        vf_explained_var: 0.07610408961772919
        vf_loss: 19.036623001098633
      agent-4:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9380562901496887
        entropy_coeff: 0.0017600000137463212
        kl: 0.00228887633420527
        model: {}
        policy_loss: -0.003930841106921434
        total_loss: -0.0035166447050869465
        vf_explained_var: 0.004249200224876404
        vf_loss: 20.65176773071289
      agent-5:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 0.738709568977356
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008070346666499972
        model: {}
        policy_loss: -0.0031442316249012947
        total_loss: -0.002644887426868081
        vf_explained_var: 0.1272585541009903
        vf_loss: 17.994728088378906
    load_time_ms: 13314.119
    num_steps_sampled: 30528000
    num_steps_trained: 30528000
    sample_time_ms: 99683.899
    update_time_ms: 13.988
  iterations_since_restore: 258
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.365
    ram_util_percent: 16.200555555555557
  pid: 30948
  policy_reward_max:
    agent-0: 187.33333333333272
    agent-1: 187.33333333333272
    agent-2: 187.33333333333272
    agent-3: 187.33333333333272
    agent-4: 187.33333333333272
    agent-5: 187.33333333333272
  policy_reward_mean:
    agent-0: 157.75999999999993
    agent-1: 157.75999999999993
    agent-2: 157.75999999999993
    agent-3: 157.75999999999993
    agent-4: 157.75999999999993
    agent-5: 157.75999999999993
  policy_reward_min:
    agent-0: 56.499999999999886
    agent-1: 56.499999999999886
    agent-2: 56.499999999999886
    agent-3: 56.499999999999886
    agent-4: 56.499999999999886
    agent-5: 56.499999999999886
  sampler_perf:
    mean_env_wait_ms: 27.188543395360767
    mean_inference_ms: 12.864163384057417
    mean_processing_ms: 57.46525957939271
  time_since_restore: 33766.49023771286
  time_this_iter_s: 126.16523480415344
  time_total_s: 42892.50205159187
  timestamp: 1637060884
  timesteps_since_restore: 24768000
  timesteps_this_iter: 96000
  timesteps_total: 30528000
  training_iteration: 318
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    318 |          42892.5 | 30528000 |   946.56 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 49
    apples_agent-0_mean: 2.3
    apples_agent-0_min: 0
    apples_agent-1_max: 133
    apples_agent-1_mean: 32.0
    apples_agent-1_min: 0
    apples_agent-2_max: 86
    apples_agent-2_mean: 6.55
    apples_agent-2_min: 0
    apples_agent-3_max: 262
    apples_agent-3_mean: 68.65
    apples_agent-3_min: 35
    apples_agent-4_max: 73
    apples_agent-4_mean: 3.03
    apples_agent-4_min: 0
    apples_agent-5_max: 264
    apples_agent-5_mean: 97.3
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 529
    cleaning_beam_agent-0_mean: 376.1
    cleaning_beam_agent-0_min: 202
    cleaning_beam_agent-1_max: 475
    cleaning_beam_agent-1_mean: 268.93
    cleaning_beam_agent-1_min: 96
    cleaning_beam_agent-2_max: 621
    cleaning_beam_agent-2_mean: 398.73
    cleaning_beam_agent-2_min: 174
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 24.22
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 560
    cleaning_beam_agent-4_mean: 457.37
    cleaning_beam_agent-4_min: 302
    cleaning_beam_agent-5_max: 113
    cleaning_beam_agent-5_mean: 28.35
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-10-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1137.9999999999984
  episode_reward_mean: 926.0999999999846
  episode_reward_min: 282.00000000000057
  episodes_this_iter: 96
  episodes_total: 30624
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11598.768
    learner:
      agent-0:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9805722236633301
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016323417657986283
        model: {}
        policy_loss: -0.0030506898183375597
        total_loss: -0.0027122304309159517
        vf_explained_var: 0.08905968070030212
        vf_loss: 20.642715454101562
      agent-1:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.128886342048645
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018624677322804928
        model: {}
        policy_loss: -0.004413594491779804
        total_loss: -0.003962049260735512
        vf_explained_var: -0.07627734541893005
        vf_loss: 24.383882522583008
      agent-2:
        cur_kl_coeff: 1.2037062331786428e-36
        cur_lr: 1.2000000424450263e-05
        entropy: 1.044514536857605
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020594345405697823
        model: {}
        policy_loss: -0.0039889272302389145
        total_loss: -0.0035332469269633293
        vf_explained_var: -0.012904807925224304
        vf_loss: 22.94025230407715
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47825857996940613
        entropy_coeff: 0.0017600000137463212
        kl: 0.001399659551680088
        model: {}
        policy_loss: -0.003046999219805002
        total_loss: -0.001901916228234768
        vf_explained_var: 0.12332300841808319
        vf_loss: 19.8681583404541
      agent-4:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9536147117614746
        entropy_coeff: 0.0017600000137463212
        kl: 0.001138343010097742
        model: {}
        policy_loss: -0.003599837888032198
        total_loss: -0.003043984994292259
        vf_explained_var: 0.013058096170425415
        vf_loss: 22.342182159423828
      agent-5:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7218127250671387
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012778397649526596
        model: {}
        policy_loss: -0.0036026891320943832
        total_loss: -0.0028926795348525047
        vf_explained_var: 0.12701371312141418
        vf_loss: 19.803991317749023
    load_time_ms: 13321.207
    num_steps_sampled: 30624000
    num_steps_trained: 30624000
    sample_time_ms: 99516.6
    update_time_ms: 13.638
  iterations_since_restore: 259
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.561581920903954
    ram_util_percent: 16.146892655367232
  pid: 30948
  policy_reward_max:
    agent-0: 189.66666666666652
    agent-1: 189.66666666666652
    agent-2: 189.66666666666652
    agent-3: 189.66666666666652
    agent-4: 189.66666666666652
    agent-5: 189.66666666666652
  policy_reward_mean:
    agent-0: 154.3499999999999
    agent-1: 154.3499999999999
    agent-2: 154.3499999999999
    agent-3: 154.3499999999999
    agent-4: 154.3499999999999
    agent-5: 154.3499999999999
  policy_reward_min:
    agent-0: 46.99999999999997
    agent-1: 46.99999999999997
    agent-2: 46.99999999999997
    agent-3: 46.99999999999997
    agent-4: 46.99999999999997
    agent-5: 46.99999999999997
  sampler_perf:
    mean_env_wait_ms: 27.189179950671793
    mean_inference_ms: 12.863581969042448
    mean_processing_ms: 57.46141484066334
  time_since_restore: 33890.65354561806
  time_this_iter_s: 124.16330790519714
  time_total_s: 43016.66535949707
  timestamp: 1637061008
  timesteps_since_restore: 24864000
  timesteps_this_iter: 96000
  timesteps_total: 30624000
  training_iteration: 319
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    319 |          43016.7 | 30624000 |    926.1 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 62
    apples_agent-0_mean: 2.88
    apples_agent-0_min: 0
    apples_agent-1_max: 133
    apples_agent-1_mean: 28.82
    apples_agent-1_min: 0
    apples_agent-2_max: 112
    apples_agent-2_mean: 7.22
    apples_agent-2_min: 0
    apples_agent-3_max: 108
    apples_agent-3_mean: 71.59
    apples_agent-3_min: 39
    apples_agent-4_max: 55
    apples_agent-4_mean: 1.95
    apples_agent-4_min: 0
    apples_agent-5_max: 208
    apples_agent-5_mean: 100.12
    apples_agent-5_min: 49
    cleaning_beam_agent-0_max: 515
    cleaning_beam_agent-0_mean: 373.14
    cleaning_beam_agent-0_min: 231
    cleaning_beam_agent-1_max: 526
    cleaning_beam_agent-1_mean: 284.53
    cleaning_beam_agent-1_min: 141
    cleaning_beam_agent-2_max: 621
    cleaning_beam_agent-2_mean: 403.9
    cleaning_beam_agent-2_min: 182
    cleaning_beam_agent-3_max: 53
    cleaning_beam_agent-3_mean: 23.01
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 538
    cleaning_beam_agent-4_mean: 455.59
    cleaning_beam_agent-4_min: 343
    cleaning_beam_agent-5_max: 102
    cleaning_beam_agent-5_mean: 30.9
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-12-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1163.0000000000023
  episode_reward_mean: 962.3499999999837
  episode_reward_min: 504.0000000000108
  episodes_this_iter: 96
  episodes_total: 30720
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11597.757
    learner:
      agent-0:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9854792356491089
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024564347695559263
        model: {}
        policy_loss: -0.0030870288610458374
        total_loss: -0.0028503299690783024
        vf_explained_var: 0.03693503141403198
        vf_loss: 19.711423873901367
      agent-1:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1121078729629517
        entropy_coeff: 0.0017600000137463212
        kl: 0.00156061549205333
        model: {}
        policy_loss: -0.004119613207876682
        total_loss: -0.0037767242174595594
        vf_explained_var: -0.11040052771568298
        vf_loss: 23.001968383789062
      agent-2:
        cur_kl_coeff: 6.018531165893214e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0115690231323242
        entropy_coeff: 0.0017600000137463212
        kl: 0.001274951035156846
        model: {}
        policy_loss: -0.0035245055332779884
        total_loss: -0.0033146864734590054
        vf_explained_var: 0.03021080791950226
        vf_loss: 19.901817321777344
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4664768874645233
        entropy_coeff: 0.0017600000137463212
        kl: 0.001133448095060885
        model: {}
        policy_loss: -0.0024969829246401787
        total_loss: -0.0014139031991362572
        vf_explained_var: 0.07094958424568176
        vf_loss: 19.0407657623291
      agent-4:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9511762857437134
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015904635656625032
        model: {}
        policy_loss: -0.0038478774949908257
        total_loss: -0.0034555098973214626
        vf_explained_var: 0.0005449652671813965
        vf_loss: 20.664430618286133
      agent-5:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7047305703163147
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011879020603373647
        model: {}
        policy_loss: -0.0029404445085674524
        total_loss: -0.00227616960182786
        vf_explained_var: 0.06969501078128815
        vf_loss: 19.046039581298828
    load_time_ms: 13343.026
    num_steps_sampled: 30720000
    num_steps_trained: 30720000
    sample_time_ms: 99562.387
    update_time_ms: 13.762
  iterations_since_restore: 260
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.360795454545453
    ram_util_percent: 16.198863636363637
  pid: 30948
  policy_reward_max:
    agent-0: 193.83333333333306
    agent-1: 193.83333333333306
    agent-2: 193.83333333333306
    agent-3: 193.83333333333306
    agent-4: 193.83333333333306
    agent-5: 193.83333333333306
  policy_reward_mean:
    agent-0: 160.3916666666666
    agent-1: 160.3916666666666
    agent-2: 160.3916666666666
    agent-3: 160.3916666666666
    agent-4: 160.3916666666666
    agent-5: 160.3916666666666
  policy_reward_min:
    agent-0: 84.00000000000009
    agent-1: 84.00000000000009
    agent-2: 84.00000000000009
    agent-3: 84.00000000000009
    agent-4: 84.00000000000009
    agent-5: 84.00000000000009
  sampler_perf:
    mean_env_wait_ms: 27.189004733417452
    mean_inference_ms: 12.862968915681675
    mean_processing_ms: 57.45606296474371
  time_since_restore: 34014.55869293213
  time_this_iter_s: 123.90514731407166
  time_total_s: 43140.57050681114
  timestamp: 1637061132
  timesteps_since_restore: 24960000
  timesteps_this_iter: 96000
  timesteps_total: 30720000
  training_iteration: 320
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    320 |          43140.6 | 30720000 |   962.35 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 75
    apples_agent-0_mean: 2.62
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 34.22
    apples_agent-1_min: 0
    apples_agent-2_max: 112
    apples_agent-2_mean: 8.37
    apples_agent-2_min: 0
    apples_agent-3_max: 126
    apples_agent-3_mean: 70.02
    apples_agent-3_min: 38
    apples_agent-4_max: 40
    apples_agent-4_mean: 2.81
    apples_agent-4_min: 0
    apples_agent-5_max: 145
    apples_agent-5_mean: 91.0
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 518
    cleaning_beam_agent-0_mean: 380.47
    cleaning_beam_agent-0_min: 241
    cleaning_beam_agent-1_max: 468
    cleaning_beam_agent-1_mean: 273.5
    cleaning_beam_agent-1_min: 143
    cleaning_beam_agent-2_max: 664
    cleaning_beam_agent-2_mean: 413.44
    cleaning_beam_agent-2_min: 204
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 23.42
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 507
    cleaning_beam_agent-4_mean: 442.42
    cleaning_beam_agent-4_min: 346
    cleaning_beam_agent-5_max: 129
    cleaning_beam_agent-5_mean: 33.3
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-14-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1132.0000000000011
  episode_reward_mean: 944.2599999999853
  episode_reward_min: 491.00000000000927
  episodes_this_iter: 96
  episodes_total: 30816
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11632.196
    learner:
      agent-0:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9873062372207642
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019483409123495221
        model: {}
        policy_loss: -0.0032188636250793934
        total_loss: -0.003039219882339239
        vf_explained_var: 0.0706576555967331
        vf_loss: 19.173038482666016
      agent-1:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.127469778060913
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012667779810726643
        model: {}
        policy_loss: -0.0041121551766991615
        total_loss: -0.0038221487775444984
        vf_explained_var: -0.09740850329399109
        vf_loss: 22.743518829345703
      agent-2:
        cur_kl_coeff: 3.009265582946607e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0239968299865723
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014278495218604803
        model: {}
        policy_loss: -0.0037145144306123257
        total_loss: -0.003446088172495365
        vf_explained_var: -0.0019727498292922974
        vf_loss: 20.706584930419922
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4843514561653137
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009229148272424936
        model: {}
        policy_loss: -0.0024865586310625076
        total_loss: -0.001470436342060566
        vf_explained_var: 0.0946933776140213
        vf_loss: 18.685823440551758
      agent-4:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9581843614578247
        entropy_coeff: 0.0017600000137463212
        kl: 0.001442662556655705
        model: {}
        policy_loss: -0.0036034618970006704
        total_loss: -0.003138855332508683
        vf_explained_var: -0.03823983669281006
        vf_loss: 21.510120391845703
      agent-5:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7459650039672852
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010941964574158192
        model: {}
        policy_loss: -0.00319615937769413
        total_loss: -0.0026831068098545074
        vf_explained_var: 0.11549746990203857
        vf_loss: 18.259550094604492
    load_time_ms: 13336.508
    num_steps_sampled: 30816000
    num_steps_trained: 30816000
    sample_time_ms: 99693.648
    update_time_ms: 13.584
  iterations_since_restore: 261
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.29281767955801
    ram_util_percent: 16.189502762430944
  pid: 30948
  policy_reward_max:
    agent-0: 188.6666666666666
    agent-1: 188.6666666666666
    agent-2: 188.6666666666666
    agent-3: 188.6666666666666
    agent-4: 188.6666666666666
    agent-5: 188.6666666666666
  policy_reward_mean:
    agent-0: 157.37666666666664
    agent-1: 157.37666666666664
    agent-2: 157.37666666666664
    agent-3: 157.37666666666664
    agent-4: 157.37666666666664
    agent-5: 157.37666666666664
  policy_reward_min:
    agent-0: 81.83333333333351
    agent-1: 81.83333333333351
    agent-2: 81.83333333333351
    agent-3: 81.83333333333351
    agent-4: 81.83333333333351
    agent-5: 81.83333333333351
  sampler_perf:
    mean_env_wait_ms: 27.19073214341392
    mean_inference_ms: 12.862589964658653
    mean_processing_ms: 57.454845639978785
  time_since_restore: 34141.49043107033
  time_this_iter_s: 126.93173813819885
  time_total_s: 43267.50224494934
  timestamp: 1637061260
  timesteps_since_restore: 25056000
  timesteps_this_iter: 96000
  timesteps_total: 30816000
  training_iteration: 321
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    321 |          43267.5 | 30816000 |   944.26 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 44
    apples_agent-0_mean: 1.49
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 33.34
    apples_agent-1_min: 0
    apples_agent-2_max: 109
    apples_agent-2_mean: 8.63
    apples_agent-2_min: 0
    apples_agent-3_max: 134
    apples_agent-3_mean: 70.61
    apples_agent-3_min: 39
    apples_agent-4_max: 85
    apples_agent-4_mean: 3.03
    apples_agent-4_min: 0
    apples_agent-5_max: 188
    apples_agent-5_mean: 97.46
    apples_agent-5_min: 54
    cleaning_beam_agent-0_max: 458
    cleaning_beam_agent-0_mean: 365.82
    cleaning_beam_agent-0_min: 237
    cleaning_beam_agent-1_max: 435
    cleaning_beam_agent-1_mean: 270.22
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 590
    cleaning_beam_agent-2_mean: 405.37
    cleaning_beam_agent-2_min: 229
    cleaning_beam_agent-3_max: 48
    cleaning_beam_agent-3_mean: 24.27
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 537
    cleaning_beam_agent-4_mean: 448.92
    cleaning_beam_agent-4_min: 348
    cleaning_beam_agent-5_max: 136
    cleaning_beam_agent-5_mean: 33.52
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.04
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-16-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1142.9999999999798
  episode_reward_mean: 945.9799999999836
  episode_reward_min: 546.0000000000035
  episodes_this_iter: 96
  episodes_total: 30912
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11660.889
    learner:
      agent-0:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9894998073577881
        entropy_coeff: 0.0017600000137463212
        kl: 0.0025693345814943314
        model: {}
        policy_loss: -0.0032069701701402664
        total_loss: -0.0030906759202480316
        vf_explained_var: 0.03125573694705963
        vf_loss: 18.5781192779541
      agent-1:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1344980001449585
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019201772520318627
        model: {}
        policy_loss: -0.004263934679329395
        total_loss: -0.004032722674310207
        vf_explained_var: -0.15030315518379211
        vf_loss: 22.27930450439453
      agent-2:
        cur_kl_coeff: 1.5046327914733034e-37
        cur_lr: 1.2000000424450263e-05
        entropy: 1.037602424621582
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014535256195813417
        model: {}
        policy_loss: -0.0037546195089817047
        total_loss: -0.0036273377481848
        vf_explained_var: -0.013820111751556396
        vf_loss: 19.53461456298828
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46538180112838745
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012191820424050093
        model: {}
        policy_loss: -0.0025669746100902557
        total_loss: -0.001566518098115921
        vf_explained_var: 0.05645468831062317
        vf_loss: 18.195255279541016
      agent-4:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9456066489219666
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012893769890069962
        model: {}
        policy_loss: -0.0037140417844057083
        total_loss: -0.0034029430244117975
        vf_explained_var: -0.017422735691070557
        vf_loss: 19.753662109375
      agent-5:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7302544116973877
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009138536988757551
        model: {}
        policy_loss: -0.002927485853433609
        total_loss: -0.0024634646251797676
        vf_explained_var: 0.08974497020244598
        vf_loss: 17.4926815032959
    load_time_ms: 13355.851
    num_steps_sampled: 30912000
    num_steps_trained: 30912000
    sample_time_ms: 99658.864
    update_time_ms: 13.357
  iterations_since_restore: 262
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.373595505617978
    ram_util_percent: 16.112921348314607
  pid: 30948
  policy_reward_max:
    agent-0: 190.49999999999997
    agent-1: 190.49999999999997
    agent-2: 190.49999999999997
    agent-3: 190.49999999999997
    agent-4: 190.49999999999997
    agent-5: 190.49999999999997
  policy_reward_mean:
    agent-0: 157.66333333333327
    agent-1: 157.66333333333327
    agent-2: 157.66333333333327
    agent-3: 157.66333333333327
    agent-4: 157.66333333333327
    agent-5: 157.66333333333327
  policy_reward_min:
    agent-0: 91.0000000000001
    agent-1: 91.0000000000001
    agent-2: 91.0000000000001
    agent-3: 91.0000000000001
    agent-4: 91.0000000000001
    agent-5: 91.0000000000001
  sampler_perf:
    mean_env_wait_ms: 27.191210206600495
    mean_inference_ms: 12.861906903363302
    mean_processing_ms: 57.45162982292562
  time_since_restore: 34266.38584113121
  time_this_iter_s: 124.89541006088257
  time_total_s: 43392.39765501022
  timestamp: 1637061385
  timesteps_since_restore: 25152000
  timesteps_this_iter: 96000
  timesteps_total: 30912000
  training_iteration: 322
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    322 |          43392.4 | 30912000 |   945.98 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 1.23
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 30.42
    apples_agent-1_min: 0
    apples_agent-2_max: 85
    apples_agent-2_mean: 8.53
    apples_agent-2_min: 0
    apples_agent-3_max: 113
    apples_agent-3_mean: 70.83
    apples_agent-3_min: 34
    apples_agent-4_max: 78
    apples_agent-4_mean: 4.87
    apples_agent-4_min: 0
    apples_agent-5_max: 144
    apples_agent-5_mean: 93.22
    apples_agent-5_min: 47
    cleaning_beam_agent-0_max: 562
    cleaning_beam_agent-0_mean: 381.49
    cleaning_beam_agent-0_min: 282
    cleaning_beam_agent-1_max: 514
    cleaning_beam_agent-1_mean: 271.56
    cleaning_beam_agent-1_min: 125
    cleaning_beam_agent-2_max: 587
    cleaning_beam_agent-2_mean: 413.83
    cleaning_beam_agent-2_min: 187
    cleaning_beam_agent-3_max: 71
    cleaning_beam_agent-3_mean: 22.69
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 525
    cleaning_beam_agent-4_mean: 444.03
    cleaning_beam_agent-4_min: 341
    cleaning_beam_agent-5_max: 88
    cleaning_beam_agent-5_mean: 31.2
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.05
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-18-30
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1146.0000000000039
  episode_reward_mean: 948.179999999984
  episode_reward_min: 621.9999999999948
  episodes_this_iter: 96
  episodes_total: 31008
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11641.031
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0070338249206543
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015425101155415177
        model: {}
        policy_loss: -0.002741534262895584
        total_loss: -0.002588252304121852
        vf_explained_var: 0.030161887407302856
        vf_loss: 19.256637573242188
      agent-1:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1391143798828125
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015411815838888288
        model: {}
        policy_loss: -0.003942274488508701
        total_loss: -0.0037667197175323963
        vf_explained_var: -0.09131419658660889
        vf_loss: 21.803970336914062
      agent-2:
        cur_kl_coeff: 7.523163957366517e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0364017486572266
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012277897913008928
        model: {}
        policy_loss: -0.0035129643511027098
        total_loss: -0.0033346707932651043
        vf_explained_var: -0.0052478015422821045
        vf_loss: 20.023637771606445
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47510889172554016
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010703621665015817
        model: {}
        policy_loss: -0.0026190467178821564
        total_loss: -0.0016078613698482513
        vf_explained_var: 0.0685044676065445
        vf_loss: 18.473735809326172
      agent-4:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9518129825592041
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016257410170510411
        model: {}
        policy_loss: -0.003923314157873392
        total_loss: -0.0035581146366894245
        vf_explained_var: -0.01563277840614319
        vf_loss: 20.403873443603516
      agent-5:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7283773422241211
        entropy_coeff: 0.0017600000137463212
        kl: 0.000888345530256629
        model: {}
        policy_loss: -0.003236273303627968
        total_loss: -0.0027517639100551605
        vf_explained_var: 0.10953706502914429
        vf_loss: 17.664539337158203
    load_time_ms: 13365.723
    num_steps_sampled: 31008000
    num_steps_trained: 31008000
    sample_time_ms: 99722.401
    update_time_ms: 13.583
  iterations_since_restore: 263
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.514606741573036
    ram_util_percent: 16.192696629213483
  pid: 30948
  policy_reward_max:
    agent-0: 190.9999999999994
    agent-1: 190.9999999999994
    agent-2: 190.9999999999994
    agent-3: 190.9999999999994
    agent-4: 190.9999999999994
    agent-5: 190.9999999999994
  policy_reward_mean:
    agent-0: 158.02999999999992
    agent-1: 158.02999999999992
    agent-2: 158.02999999999992
    agent-3: 158.02999999999992
    agent-4: 158.02999999999992
    agent-5: 158.02999999999992
  policy_reward_min:
    agent-0: 103.66666666666691
    agent-1: 103.66666666666691
    agent-2: 103.66666666666691
    agent-3: 103.66666666666691
    agent-4: 103.66666666666691
    agent-5: 103.66666666666691
  sampler_perf:
    mean_env_wait_ms: 27.192621068574518
    mean_inference_ms: 12.861210377604012
    mean_processing_ms: 57.44976259576622
  time_since_restore: 34391.642567157745
  time_this_iter_s: 125.25672602653503
  time_total_s: 43517.65438103676
  timestamp: 1637061510
  timesteps_since_restore: 25248000
  timesteps_this_iter: 96000
  timesteps_total: 31008000
  training_iteration: 323
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    323 |          43517.7 | 31008000 |   948.18 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 47
    apples_agent-0_mean: 2.96
    apples_agent-0_min: 0
    apples_agent-1_max: 127
    apples_agent-1_mean: 30.48
    apples_agent-1_min: 0
    apples_agent-2_max: 90
    apples_agent-2_mean: 6.65
    apples_agent-2_min: 0
    apples_agent-3_max: 145
    apples_agent-3_mean: 72.56
    apples_agent-3_min: 37
    apples_agent-4_max: 53
    apples_agent-4_mean: 2.12
    apples_agent-4_min: 0
    apples_agent-5_max: 185
    apples_agent-5_mean: 91.39
    apples_agent-5_min: 37
    cleaning_beam_agent-0_max: 476
    cleaning_beam_agent-0_mean: 367.62
    cleaning_beam_agent-0_min: 245
    cleaning_beam_agent-1_max: 468
    cleaning_beam_agent-1_mean: 254.72
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 633
    cleaning_beam_agent-2_mean: 430.03
    cleaning_beam_agent-2_min: 221
    cleaning_beam_agent-3_max: 78
    cleaning_beam_agent-3_mean: 25.78
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 542
    cleaning_beam_agent-4_mean: 445.34
    cleaning_beam_agent-4_min: 188
    cleaning_beam_agent-5_max: 139
    cleaning_beam_agent-5_mean: 33.24
    cleaning_beam_agent-5_min: 9
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-20-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1130.9999999999725
  episode_reward_mean: 963.3199999999853
  episode_reward_min: 333.00000000000443
  episodes_this_iter: 96
  episodes_total: 31104
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11642.654
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0079110860824585
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018457858823239803
        model: {}
        policy_loss: -0.002743158955127001
        total_loss: -0.0025518876500427723
        vf_explained_var: 0.010652914643287659
        vf_loss: 19.6519718170166
      agent-1:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1333118677139282
        entropy_coeff: 0.0017600000137463212
        kl: 0.002021787455305457
        model: {}
        policy_loss: -0.004117573611438274
        total_loss: -0.003804444335401058
        vf_explained_var: -0.13936001062393188
        vf_loss: 23.077571868896484
      agent-2:
        cur_kl_coeff: 3.7615819786832586e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.018011450767517
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016336868284270167
        model: {}
        policy_loss: -0.0036285207606852055
        total_loss: -0.003441757522523403
        vf_explained_var: 0.015110626816749573
        vf_loss: 19.78461456298828
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4588709771633148
        entropy_coeff: 0.0017600000137463212
        kl: 0.001452367170713842
        model: {}
        policy_loss: -0.002730967476963997
        total_loss: -0.0016565639525651932
        vf_explained_var: 0.05247427523136139
        vf_loss: 18.820125579833984
      agent-4:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9575753808021545
        entropy_coeff: 0.0017600000137463212
        kl: 0.00200209254398942
        model: {}
        policy_loss: -0.003754981327801943
        total_loss: -0.0033946242183446884
        vf_explained_var: -0.009929746389389038
        vf_loss: 20.45692253112793
      agent-5:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7166357040405273
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021290485747158527
        model: {}
        policy_loss: -0.0032648122869431973
        total_loss: -0.002708661835640669
        vf_explained_var: 0.08316761255264282
        vf_loss: 18.17424774169922
    load_time_ms: 13375.964
    num_steps_sampled: 31104000
    num_steps_trained: 31104000
    sample_time_ms: 99996.729
    update_time_ms: 13.614
  iterations_since_restore: 264
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.276795580110495
    ram_util_percent: 16.201104972375692
  pid: 30948
  policy_reward_max:
    agent-0: 188.4999999999997
    agent-1: 188.4999999999997
    agent-2: 188.4999999999997
    agent-3: 188.4999999999997
    agent-4: 188.4999999999997
    agent-5: 188.4999999999997
  policy_reward_mean:
    agent-0: 160.55333333333326
    agent-1: 160.55333333333326
    agent-2: 160.55333333333326
    agent-3: 160.55333333333326
    agent-4: 160.55333333333326
    agent-5: 160.55333333333326
  policy_reward_min:
    agent-0: 55.49999999999994
    agent-1: 55.49999999999994
    agent-2: 55.49999999999994
    agent-3: 55.49999999999994
    agent-4: 55.49999999999994
    agent-5: 55.49999999999994
  sampler_perf:
    mean_env_wait_ms: 27.194965767942595
    mean_inference_ms: 12.861193259353957
    mean_processing_ms: 57.449029862641275
  time_since_restore: 34518.36600923538
  time_this_iter_s: 126.72344207763672
  time_total_s: 43644.377823114395
  timestamp: 1637061637
  timesteps_since_restore: 25344000
  timesteps_this_iter: 96000
  timesteps_total: 31104000
  training_iteration: 324
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    324 |          43644.4 | 31104000 |   963.32 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 0.94
    apples_agent-0_min: 0
    apples_agent-1_max: 99
    apples_agent-1_mean: 29.05
    apples_agent-1_min: 0
    apples_agent-2_max: 63
    apples_agent-2_mean: 7.1
    apples_agent-2_min: 0
    apples_agent-3_max: 121
    apples_agent-3_mean: 69.66
    apples_agent-3_min: 41
    apples_agent-4_max: 74
    apples_agent-4_mean: 3.5
    apples_agent-4_min: 0
    apples_agent-5_max: 265
    apples_agent-5_mean: 93.73
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 486
    cleaning_beam_agent-0_mean: 359.81
    cleaning_beam_agent-0_min: 260
    cleaning_beam_agent-1_max: 521
    cleaning_beam_agent-1_mean: 271.99
    cleaning_beam_agent-1_min: 132
    cleaning_beam_agent-2_max: 667
    cleaning_beam_agent-2_mean: 435.2
    cleaning_beam_agent-2_min: 233
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 24.59
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 542
    cleaning_beam_agent-4_mean: 446.0
    cleaning_beam_agent-4_min: 335
    cleaning_beam_agent-5_max: 98
    cleaning_beam_agent-5_mean: 30.28
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-22-42
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1128.9999999999977
  episode_reward_mean: 955.8299999999853
  episode_reward_min: 549.0000000000078
  episodes_this_iter: 96
  episodes_total: 31200
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11662.924
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0192482471466064
        entropy_coeff: 0.0017600000137463212
        kl: 0.001429213909432292
        model: {}
        policy_loss: -0.0031352974474430084
        total_loss: -0.0029377713799476624
        vf_explained_var: 0.021010830998420715
        vf_loss: 19.91402816772461
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.136466383934021
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010764183243736625
        model: {}
        policy_loss: -0.0037683057598769665
        total_loss: -0.0034654303453862667
        vf_explained_var: -0.12093064188957214
        vf_loss: 23.030561447143555
      agent-2:
        cur_kl_coeff: 1.8807909893416293e-38
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0249286890029907
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011799904750660062
        model: {}
        policy_loss: -0.003522972110658884
        total_loss: -0.003322581760585308
        vf_explained_var: 0.01869100332260132
        vf_loss: 20.042695999145508
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47402727603912354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011888202279806137
        model: {}
        policy_loss: -0.002529554534703493
        total_loss: -0.0015048063360154629
        vf_explained_var: 0.08675596117973328
        vf_loss: 18.59034538269043
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.958214282989502
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015637862961739302
        model: {}
        policy_loss: -0.0038467487320303917
        total_loss: -0.0034767144825309515
        vf_explained_var: -0.003869280219078064
        vf_loss: 20.56491470336914
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7030279636383057
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011087533785030246
        model: {}
        policy_loss: -0.002924397587776184
        total_loss: -0.0023637907579541206
        vf_explained_var: 0.11540712416172028
        vf_loss: 17.979351043701172
    load_time_ms: 13380.818
    num_steps_sampled: 31200000
    num_steps_trained: 31200000
    sample_time_ms: 99968.185
    update_time_ms: 13.806
  iterations_since_restore: 265
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.423595505617975
    ram_util_percent: 16.198314606741572
  pid: 30948
  policy_reward_max:
    agent-0: 188.166666666666
    agent-1: 188.166666666666
    agent-2: 188.166666666666
    agent-3: 188.166666666666
    agent-4: 188.166666666666
    agent-5: 188.166666666666
  policy_reward_mean:
    agent-0: 159.30499999999992
    agent-1: 159.30499999999992
    agent-2: 159.30499999999992
    agent-3: 159.30499999999992
    agent-4: 159.30499999999992
    agent-5: 159.30499999999992
  policy_reward_min:
    agent-0: 91.49999999999993
    agent-1: 91.49999999999993
    agent-2: 91.49999999999993
    agent-3: 91.49999999999993
    agent-4: 91.49999999999993
    agent-5: 91.49999999999993
  sampler_perf:
    mean_env_wait_ms: 27.196574972632924
    mean_inference_ms: 12.860778863950845
    mean_processing_ms: 57.44659141586722
  time_since_restore: 34643.320712804794
  time_this_iter_s: 124.95470356941223
  time_total_s: 43769.33252668381
  timestamp: 1637061762
  timesteps_since_restore: 25440000
  timesteps_this_iter: 96000
  timesteps_total: 31200000
  training_iteration: 325
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    325 |          43769.3 | 31200000 |   955.83 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 1.5
    apples_agent-0_min: 0
    apples_agent-1_max: 106
    apples_agent-1_mean: 36.38
    apples_agent-1_min: 0
    apples_agent-2_max: 44
    apples_agent-2_mean: 4.38
    apples_agent-2_min: 0
    apples_agent-3_max: 144
    apples_agent-3_mean: 71.96
    apples_agent-3_min: 42
    apples_agent-4_max: 74
    apples_agent-4_mean: 3.25
    apples_agent-4_min: 0
    apples_agent-5_max: 165
    apples_agent-5_mean: 92.49
    apples_agent-5_min: 55
    cleaning_beam_agent-0_max: 468
    cleaning_beam_agent-0_mean: 370.3
    cleaning_beam_agent-0_min: 263
    cleaning_beam_agent-1_max: 431
    cleaning_beam_agent-1_mean: 247.56
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 600
    cleaning_beam_agent-2_mean: 438.4
    cleaning_beam_agent-2_min: 206
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 25.87
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 527
    cleaning_beam_agent-4_mean: 440.25
    cleaning_beam_agent-4_min: 301
    cleaning_beam_agent-5_max: 112
    cleaning_beam_agent-5_mean: 29.32
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-24-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1126.9999999999964
  episode_reward_mean: 958.2099999999847
  episode_reward_min: 561.0000000000013
  episodes_this_iter: 96
  episodes_total: 31296
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11643.344
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9791861176490784
        entropy_coeff: 0.0017600000137463212
        kl: 0.0033396075014024973
        model: {}
        policy_loss: -0.0034472723491489887
        total_loss: -0.003283372148871422
        vf_explained_var: 0.03553993999958038
        vf_loss: 18.872655868530273
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.133601427078247
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013182710390537977
        model: {}
        policy_loss: -0.003994167782366276
        total_loss: -0.003722200635820627
        vf_explained_var: -0.13114526867866516
        vf_loss: 22.671110153198242
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0154695510864258
        entropy_coeff: 0.0017600000137463212
        kl: 0.001169621362350881
        model: {}
        policy_loss: -0.003189442679286003
        total_loss: -0.0030111735686659813
        vf_explained_var: 0.0013539493083953857
        vf_loss: 19.654926300048828
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4752253592014313
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011269995011389256
        model: {}
        policy_loss: -0.00258931377902627
        total_loss: -0.0015356708317995071
        vf_explained_var: 0.034591227769851685
        vf_loss: 18.90036392211914
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9617139101028442
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018458825070410967
        model: {}
        policy_loss: -0.0037546565290540457
        total_loss: -0.003451299387961626
        vf_explained_var: -0.014676228165626526
        vf_loss: 19.95972442626953
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6846367716789246
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014191704103723168
        model: {}
        policy_loss: -0.0030616801232099533
        total_loss: -0.002503499388694763
        vf_explained_var: 0.0972253829240799
        vf_loss: 17.631439208984375
    load_time_ms: 13377.975
    num_steps_sampled: 31296000
    num_steps_trained: 31296000
    sample_time_ms: 99993.799
    update_time_ms: 13.913
  iterations_since_restore: 266
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.307865168539323
    ram_util_percent: 16.200561797752812
  pid: 30948
  policy_reward_max:
    agent-0: 187.8333333333332
    agent-1: 187.8333333333332
    agent-2: 187.8333333333332
    agent-3: 187.8333333333332
    agent-4: 187.8333333333332
    agent-5: 187.8333333333332
  policy_reward_mean:
    agent-0: 159.70166666666657
    agent-1: 159.70166666666657
    agent-2: 159.70166666666657
    agent-3: 159.70166666666657
    agent-4: 159.70166666666657
    agent-5: 159.70166666666657
  policy_reward_min:
    agent-0: 93.50000000000021
    agent-1: 93.50000000000021
    agent-2: 93.50000000000021
    agent-3: 93.50000000000021
    agent-4: 93.50000000000021
    agent-5: 93.50000000000021
  sampler_perf:
    mean_env_wait_ms: 27.19665682555404
    mean_inference_ms: 12.8601801227378
    mean_processing_ms: 57.442650201346495
  time_since_restore: 34768.00032734871
  time_this_iter_s: 124.6796145439148
  time_total_s: 43894.01214122772
  timestamp: 1637061887
  timesteps_since_restore: 25536000
  timesteps_this_iter: 96000
  timesteps_total: 31296000
  training_iteration: 326
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    326 |            43894 | 31296000 |   958.21 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 1.61
    apples_agent-0_min: 0
    apples_agent-1_max: 87
    apples_agent-1_mean: 30.82
    apples_agent-1_min: 0
    apples_agent-2_max: 76
    apples_agent-2_mean: 9.78
    apples_agent-2_min: 0
    apples_agent-3_max: 135
    apples_agent-3_mean: 71.7
    apples_agent-3_min: 33
    apples_agent-4_max: 88
    apples_agent-4_mean: 3.79
    apples_agent-4_min: 0
    apples_agent-5_max: 138
    apples_agent-5_mean: 90.59
    apples_agent-5_min: 28
    cleaning_beam_agent-0_max: 538
    cleaning_beam_agent-0_mean: 388.98
    cleaning_beam_agent-0_min: 256
    cleaning_beam_agent-1_max: 531
    cleaning_beam_agent-1_mean: 253.98
    cleaning_beam_agent-1_min: 102
    cleaning_beam_agent-2_max: 614
    cleaning_beam_agent-2_mean: 432.51
    cleaning_beam_agent-2_min: 217
    cleaning_beam_agent-3_max: 64
    cleaning_beam_agent-3_mean: 24.44
    cleaning_beam_agent-3_min: 11
    cleaning_beam_agent-4_max: 527
    cleaning_beam_agent-4_mean: 437.47
    cleaning_beam_agent-4_min: 317
    cleaning_beam_agent-5_max: 242
    cleaning_beam_agent-5_mean: 34.19
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-26-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1109.999999999993
  episode_reward_mean: 954.4199999999838
  episode_reward_min: 574.0000000000017
  episodes_this_iter: 96
  episodes_total: 31392
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11646.911
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9906333684921265
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019101046491414309
        model: {}
        policy_loss: -0.0032223984599113464
        total_loss: -0.002926914021372795
        vf_explained_var: -0.0070667564868927
        vf_loss: 20.390003204345703
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1388640403747559
        entropy_coeff: 0.0017600000137463212
        kl: 0.00162496417760849
        model: {}
        policy_loss: -0.003993590362370014
        total_loss: -0.003698213491588831
        vf_explained_var: -0.11752516031265259
        vf_loss: 22.997764587402344
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0150413513183594
        entropy_coeff: 0.0017600000137463212
        kl: 0.001800884259864688
        model: {}
        policy_loss: -0.0035898881033062935
        total_loss: -0.003265708452090621
        vf_explained_var: -0.03493809700012207
        vf_loss: 21.106565475463867
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4642937183380127
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011694269487634301
        model: {}
        policy_loss: -0.0026975260116159916
        total_loss: -0.0016235137591138482
        vf_explained_var: 0.0674184262752533
        vf_loss: 18.911705017089844
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9654994606971741
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011745757656171918
        model: {}
        policy_loss: -0.003661155002191663
        total_loss: -0.0033616749569773674
        vf_explained_var: 0.019640430808067322
        vf_loss: 19.987600326538086
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.7069849371910095
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008188661886379123
        model: {}
        policy_loss: -0.002993778558447957
        total_loss: -0.0024454877711832523
        vf_explained_var: 0.11631114780902863
        vf_loss: 17.925846099853516
    load_time_ms: 13378.209
    num_steps_sampled: 31392000
    num_steps_trained: 31392000
    sample_time_ms: 100075.209
    update_time_ms: 13.877
  iterations_since_restore: 267
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.49604519774011
    ram_util_percent: 16.201129943502828
  pid: 30948
  policy_reward_max:
    agent-0: 184.99999999999972
    agent-1: 184.99999999999972
    agent-2: 184.99999999999972
    agent-3: 184.99999999999972
    agent-4: 184.99999999999972
    agent-5: 184.99999999999972
  policy_reward_mean:
    agent-0: 159.06999999999994
    agent-1: 159.06999999999994
    agent-2: 159.06999999999994
    agent-3: 159.06999999999994
    agent-4: 159.06999999999994
    agent-5: 159.06999999999994
  policy_reward_min:
    agent-0: 95.66666666666644
    agent-1: 95.66666666666644
    agent-2: 95.66666666666644
    agent-3: 95.66666666666644
    agent-4: 95.66666666666644
    agent-5: 95.66666666666644
  sampler_perf:
    mean_env_wait_ms: 27.19768969196184
    mean_inference_ms: 12.859608394210408
    mean_processing_ms: 57.438102242942435
  time_since_restore: 34892.23632931709
  time_this_iter_s: 124.23600196838379
  time_total_s: 44018.248143196106
  timestamp: 1637062011
  timesteps_since_restore: 25632000
  timesteps_this_iter: 96000
  timesteps_total: 31392000
  training_iteration: 327
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.6/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    327 |          44018.2 | 31392000 |   954.42 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 72
    apples_agent-0_mean: 2.68
    apples_agent-0_min: 0
    apples_agent-1_max: 121
    apples_agent-1_mean: 32.19
    apples_agent-1_min: 0
    apples_agent-2_max: 73
    apples_agent-2_mean: 5.6
    apples_agent-2_min: 0
    apples_agent-3_max: 131
    apples_agent-3_mean: 68.93
    apples_agent-3_min: 37
    apples_agent-4_max: 50
    apples_agent-4_mean: 2.48
    apples_agent-4_min: 0
    apples_agent-5_max: 153
    apples_agent-5_mean: 92.51
    apples_agent-5_min: 31
    cleaning_beam_agent-0_max: 517
    cleaning_beam_agent-0_mean: 384.75
    cleaning_beam_agent-0_min: 287
    cleaning_beam_agent-1_max: 417
    cleaning_beam_agent-1_mean: 244.39
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 676
    cleaning_beam_agent-2_mean: 464.53
    cleaning_beam_agent-2_min: 195
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 26.43
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 536
    cleaning_beam_agent-4_mean: 438.02
    cleaning_beam_agent-4_min: 235
    cleaning_beam_agent-5_max: 234
    cleaning_beam_agent-5_mean: 30.92
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-28-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1140.0000000000073
  episode_reward_mean: 969.7199999999866
  episode_reward_min: 461.000000000005
  episodes_this_iter: 96
  episodes_total: 31488
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11571.515
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9739465713500977
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019702170975506306
        model: {}
        policy_loss: -0.003193797543644905
        total_loss: -0.00298107392154634
        vf_explained_var: 0.0544956773519516
        vf_loss: 19.268672943115234
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1447086334228516
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021300395019352436
        model: {}
        policy_loss: -0.0040736012160778046
        total_loss: -0.0038735712878406048
        vf_explained_var: -0.07714822888374329
        vf_loss: 22.147140502929688
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9884808659553528
        entropy_coeff: 0.0017600000137463212
        kl: 0.001248420448973775
        model: {}
        policy_loss: -0.003522577928379178
        total_loss: -0.003203519620001316
        vf_explained_var: -0.007659465074539185
        vf_loss: 20.587831497192383
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4792323410511017
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014387528644874692
        model: {}
        policy_loss: -0.0028332718648016453
        total_loss: -0.0017748652026057243
        vf_explained_var: 0.06486701965332031
        vf_loss: 19.01856231689453
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9690210819244385
        entropy_coeff: 0.0017600000137463212
        kl: 0.001332116313278675
        model: {}
        policy_loss: -0.0035925712436437607
        total_loss: -0.003253784030675888
        vf_explained_var: 0.004376217722892761
        vf_loss: 20.44265365600586
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6815399527549744
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010034136939793825
        model: {}
        policy_loss: -0.002735552843660116
        total_loss: -0.002128876280039549
        vf_explained_var: 0.10825076699256897
        vf_loss: 18.061899185180664
    load_time_ms: 13388.078
    num_steps_sampled: 31488000
    num_steps_trained: 31488000
    sample_time_ms: 100123.225
    update_time_ms: 14.075
  iterations_since_restore: 268
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.44357541899441
    ram_util_percent: 16.109497206703914
  pid: 30948
  policy_reward_max:
    agent-0: 189.99999999999977
    agent-1: 189.99999999999977
    agent-2: 189.99999999999977
    agent-3: 189.99999999999977
    agent-4: 189.99999999999977
    agent-5: 189.99999999999977
  policy_reward_mean:
    agent-0: 161.61999999999992
    agent-1: 161.61999999999992
    agent-2: 161.61999999999992
    agent-3: 161.61999999999992
    agent-4: 161.61999999999992
    agent-5: 161.61999999999992
  policy_reward_min:
    agent-0: 76.83333333333337
    agent-1: 76.83333333333337
    agent-2: 76.83333333333337
    agent-3: 76.83333333333337
    agent-4: 76.83333333333337
    agent-5: 76.83333333333337
  sampler_perf:
    mean_env_wait_ms: 27.200269455872462
    mean_inference_ms: 12.85937523997445
    mean_processing_ms: 57.43727989821326
  time_since_restore: 35018.26787185669
  time_this_iter_s: 126.03154253959656
  time_total_s: 44144.2796857357
  timestamp: 1637062138
  timesteps_since_restore: 25728000
  timesteps_this_iter: 96000
  timesteps_total: 31488000
  training_iteration: 328
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    328 |          44144.3 | 31488000 |   969.72 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 1.33
    apples_agent-0_min: 0
    apples_agent-1_max: 109
    apples_agent-1_mean: 27.73
    apples_agent-1_min: 0
    apples_agent-2_max: 61
    apples_agent-2_mean: 5.82
    apples_agent-2_min: 0
    apples_agent-3_max: 128
    apples_agent-3_mean: 68.62
    apples_agent-3_min: 36
    apples_agent-4_max: 55
    apples_agent-4_mean: 2.66
    apples_agent-4_min: 0
    apples_agent-5_max: 147
    apples_agent-5_mean: 91.92
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 498
    cleaning_beam_agent-0_mean: 384.19
    cleaning_beam_agent-0_min: 288
    cleaning_beam_agent-1_max: 467
    cleaning_beam_agent-1_mean: 246.81
    cleaning_beam_agent-1_min: 144
    cleaning_beam_agent-2_max: 665
    cleaning_beam_agent-2_mean: 461.21
    cleaning_beam_agent-2_min: 243
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 23.63
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 495
    cleaning_beam_agent-4_mean: 434.07
    cleaning_beam_agent-4_min: 348
    cleaning_beam_agent-5_max: 79
    cleaning_beam_agent-5_mean: 27.42
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-31-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1119.9999999999882
  episode_reward_mean: 977.8599999999841
  episode_reward_min: 599.9999999999884
  episodes_this_iter: 96
  episodes_total: 31584
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11578.805
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9825390577316284
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018521472811698914
        model: {}
        policy_loss: -0.0031419056467711926
        total_loss: -0.003004770027473569
        vf_explained_var: 0.052346572279930115
        vf_loss: 18.664011001586914
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.146104097366333
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010042042704299092
        model: {}
        policy_loss: -0.003460168605670333
        total_loss: -0.0032077946234494448
        vf_explained_var: -0.13176241517066956
        vf_loss: 22.695180892944336
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0121722221374512
        entropy_coeff: 0.0017600000137463212
        kl: 0.001954203937202692
        model: {}
        policy_loss: -0.003940205555409193
        total_loss: -0.003753370838239789
        vf_explained_var: 0.008395925164222717
        vf_loss: 19.682567596435547
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4660089313983917
        entropy_coeff: 0.0017600000137463212
        kl: 0.001041565090417862
        model: {}
        policy_loss: -0.002301451750099659
        total_loss: -0.0012833733344450593
        vf_explained_var: 0.06438162922859192
        vf_loss: 18.382530212402344
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9665763974189758
        entropy_coeff: 0.0017600000137463212
        kl: 0.001723735360428691
        model: {}
        policy_loss: -0.0037231158930808306
        total_loss: -0.003403909970074892
        vf_explained_var: -0.015763089060783386
        vf_loss: 20.203792572021484
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6746944785118103
        entropy_coeff: 0.0017600000137463212
        kl: 0.0007021567435003817
        model: {}
        policy_loss: -0.0024477699771523476
        total_loss: -0.0018877857364714146
        vf_explained_var: 0.1111886203289032
        vf_loss: 17.474454879760742
    load_time_ms: 13377.789
    num_steps_sampled: 31584000
    num_steps_trained: 31584000
    sample_time_ms: 100247.81
    update_time_ms: 14.263
  iterations_since_restore: 269
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.18379888268156
    ram_util_percent: 16.2
  pid: 30948
  policy_reward_max:
    agent-0: 186.66666666666632
    agent-1: 186.66666666666632
    agent-2: 186.66666666666632
    agent-3: 186.66666666666632
    agent-4: 186.66666666666632
    agent-5: 186.66666666666632
  policy_reward_mean:
    agent-0: 162.97666666666657
    agent-1: 162.97666666666657
    agent-2: 162.97666666666657
    agent-3: 162.97666666666657
    agent-4: 162.97666666666657
    agent-5: 162.97666666666657
  policy_reward_min:
    agent-0: 100.00000000000043
    agent-1: 100.00000000000043
    agent-2: 100.00000000000043
    agent-3: 100.00000000000043
    agent-4: 100.00000000000043
    agent-5: 100.00000000000043
  sampler_perf:
    mean_env_wait_ms: 27.201360849808253
    mean_inference_ms: 12.858693906279719
    mean_processing_ms: 57.433335951273385
  time_since_restore: 35143.64812707901
  time_this_iter_s: 125.38025522232056
  time_total_s: 44269.65994095802
  timestamp: 1637062263
  timesteps_since_restore: 25824000
  timesteps_this_iter: 96000
  timesteps_total: 31584000
  training_iteration: 329
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    329 |          44269.7 | 31584000 |   977.86 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 41
    apples_agent-0_mean: 2.65
    apples_agent-0_min: 0
    apples_agent-1_max: 140
    apples_agent-1_mean: 31.2
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 5.11
    apples_agent-2_min: 0
    apples_agent-3_max: 128
    apples_agent-3_mean: 73.4
    apples_agent-3_min: 41
    apples_agent-4_max: 34
    apples_agent-4_mean: 1.83
    apples_agent-4_min: 0
    apples_agent-5_max: 161
    apples_agent-5_mean: 87.52
    apples_agent-5_min: 35
    cleaning_beam_agent-0_max: 488
    cleaning_beam_agent-0_mean: 371.8
    cleaning_beam_agent-0_min: 209
    cleaning_beam_agent-1_max: 474
    cleaning_beam_agent-1_mean: 235.41
    cleaning_beam_agent-1_min: 133
    cleaning_beam_agent-2_max: 613
    cleaning_beam_agent-2_mean: 450.66
    cleaning_beam_agent-2_min: 285
    cleaning_beam_agent-3_max: 53
    cleaning_beam_agent-3_mean: 24.86
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 521
    cleaning_beam_agent-4_mean: 435.78
    cleaning_beam_agent-4_min: 299
    cleaning_beam_agent-5_max: 198
    cleaning_beam_agent-5_mean: 35.27
    cleaning_beam_agent-5_min: 8
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 2
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-33-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1124.99999999999
  episode_reward_mean: 961.8699999999864
  episode_reward_min: 532.0000000000052
  episodes_this_iter: 96
  episodes_total: 31680
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11574.618
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9910793304443359
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016756076365709305
        model: {}
        policy_loss: -0.0029191551730036736
        total_loss: -0.0026776501908898354
        vf_explained_var: 0.03982166945934296
        vf_loss: 19.858030319213867
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.139782190322876
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015568947419524193
        model: {}
        policy_loss: -0.004015319049358368
        total_loss: -0.003762077074497938
        vf_explained_var: -0.08535578846931458
        vf_loss: 22.592601776123047
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0247540473937988
        entropy_coeff: 0.0017600000137463212
        kl: 0.002567625604569912
        model: {}
        policy_loss: -0.003830626839771867
        total_loss: -0.003567239735275507
        vf_explained_var: 0.0009026080369949341
        vf_loss: 20.66952896118164
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.47129130363464355
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011238612933084369
        model: {}
        policy_loss: -0.002537087071686983
        total_loss: -0.0014381327200680971
        vf_explained_var: 0.06530120968818665
        vf_loss: 19.284290313720703
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9662052392959595
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014040268724784255
        model: {}
        policy_loss: -0.0036795497871935368
        total_loss: -0.0032945116981863976
        vf_explained_var: -0.007080584764480591
        vf_loss: 20.855571746826172
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6692720055580139
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013936415780335665
        model: {}
        policy_loss: -0.002803521230816841
        total_loss: -0.002104642102494836
        vf_explained_var: 0.09006957709789276
        vf_loss: 18.767980575561523
    load_time_ms: 13360.599
    num_steps_sampled: 31680000
    num_steps_trained: 31680000
    sample_time_ms: 100326.188
    update_time_ms: 14.373
  iterations_since_restore: 270
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.419774011299435
    ram_util_percent: 16.18757062146893
  pid: 30948
  policy_reward_max:
    agent-0: 187.4999999999993
    agent-1: 187.4999999999993
    agent-2: 187.4999999999993
    agent-3: 187.4999999999993
    agent-4: 187.4999999999993
    agent-5: 187.4999999999993
  policy_reward_mean:
    agent-0: 160.3116666666666
    agent-1: 160.3116666666666
    agent-2: 160.3116666666666
    agent-3: 160.3116666666666
    agent-4: 160.3116666666666
    agent-5: 160.3116666666666
  policy_reward_min:
    agent-0: 88.6666666666667
    agent-1: 88.6666666666667
    agent-2: 88.6666666666667
    agent-3: 88.6666666666667
    agent-4: 88.6666666666667
    agent-5: 88.6666666666667
  sampler_perf:
    mean_env_wait_ms: 27.202217348807864
    mean_inference_ms: 12.858411762038118
    mean_processing_ms: 57.42928857682064
  time_since_restore: 35268.11552453041
  time_this_iter_s: 124.46739745140076
  time_total_s: 44394.127338409424
  timestamp: 1637062388
  timesteps_since_restore: 25920000
  timesteps_this_iter: 96000
  timesteps_total: 31680000
  training_iteration: 330
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    330 |          44394.1 | 31680000 |   961.87 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 2.15
    apples_agent-0_min: 0
    apples_agent-1_max: 136
    apples_agent-1_mean: 34.11
    apples_agent-1_min: 0
    apples_agent-2_max: 90
    apples_agent-2_mean: 7.82
    apples_agent-2_min: 0
    apples_agent-3_max: 132
    apples_agent-3_mean: 72.94
    apples_agent-3_min: 34
    apples_agent-4_max: 74
    apples_agent-4_mean: 2.91
    apples_agent-4_min: 0
    apples_agent-5_max: 155
    apples_agent-5_mean: 87.68
    apples_agent-5_min: 32
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 379.1
    cleaning_beam_agent-0_min: 226
    cleaning_beam_agent-1_max: 388
    cleaning_beam_agent-1_mean: 227.47
    cleaning_beam_agent-1_min: 80
    cleaning_beam_agent-2_max: 648
    cleaning_beam_agent-2_mean: 409.8
    cleaning_beam_agent-2_min: 189
    cleaning_beam_agent-3_max: 63
    cleaning_beam_agent-3_mean: 26.33
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 512
    cleaning_beam_agent-4_mean: 423.69
    cleaning_beam_agent-4_min: 236
    cleaning_beam_agent-5_max: 145
    cleaning_beam_agent-5_mean: 30.27
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-35-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1129.999999999988
  episode_reward_mean: 962.0199999999878
  episode_reward_min: 532.0000000000106
  episodes_this_iter: 96
  episodes_total: 31776
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11567.262
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9703906774520874
        entropy_coeff: 0.0017600000137463212
        kl: 0.001964107621461153
        model: {}
        policy_loss: -0.003405788680538535
        total_loss: -0.0030253257136791945
        vf_explained_var: 0.05000746250152588
        vf_loss: 20.883472442626953
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1286205053329468
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017807611729949713
        model: {}
        policy_loss: -0.003922738134860992
        total_loss: -0.003490688279271126
        vf_explained_var: -0.08957540988922119
        vf_loss: 24.1842041015625
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.047196865081787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014632053207606077
        model: {}
        policy_loss: -0.0034209571313112974
        total_loss: -0.0030356342904269695
        vf_explained_var: -0.007907524704933167
        vf_loss: 22.283891677856445
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.484406441450119
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008401707164011896
        model: {}
        policy_loss: -0.00270633352920413
        total_loss: -0.00159393809735775
        vf_explained_var: 0.10735657811164856
        vf_loss: 19.649518966674805
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.977265477180481
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014956854283809662
        model: {}
        policy_loss: -0.003777069039642811
        total_loss: -0.003331320360302925
        vf_explained_var: 0.02100023627281189
        vf_loss: 21.6573486328125
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6656591296195984
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008001082460395992
        model: {}
        policy_loss: -0.002818042179569602
        total_loss: -0.0020667612552642822
        vf_explained_var: 0.12579482793807983
        vf_loss: 19.228410720825195
    load_time_ms: 13355.008
    num_steps_sampled: 31776000
    num_steps_trained: 31776000
    sample_time_ms: 100094.624
    update_time_ms: 15.04
  iterations_since_restore: 271
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.37514124293785
    ram_util_percent: 16.201129943502828
  pid: 30948
  policy_reward_max:
    agent-0: 188.33333333333337
    agent-1: 188.33333333333337
    agent-2: 188.33333333333337
    agent-3: 188.33333333333337
    agent-4: 188.33333333333337
    agent-5: 188.33333333333337
  policy_reward_mean:
    agent-0: 160.33666666666662
    agent-1: 160.33666666666662
    agent-2: 160.33666666666662
    agent-3: 160.33666666666662
    agent-4: 160.33666666666662
    agent-5: 160.33666666666662
  policy_reward_min:
    agent-0: 88.66666666666677
    agent-1: 88.66666666666677
    agent-2: 88.66666666666677
    agent-3: 88.66666666666677
    agent-4: 88.66666666666677
    agent-5: 88.66666666666677
  sampler_perf:
    mean_env_wait_ms: 27.202185907333202
    mean_inference_ms: 12.857772249280952
    mean_processing_ms: 57.42481104979128
  time_since_restore: 35392.608946084976
  time_this_iter_s: 124.49342155456543
  time_total_s: 44518.62075996399
  timestamp: 1637062512
  timesteps_since_restore: 26016000
  timesteps_this_iter: 96000
  timesteps_total: 31776000
  training_iteration: 331
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    331 |          44518.6 | 31776000 |   962.02 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 55
    apples_agent-0_mean: 3.18
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 31.71
    apples_agent-1_min: 0
    apples_agent-2_max: 150
    apples_agent-2_mean: 7.43
    apples_agent-2_min: 0
    apples_agent-3_max: 138
    apples_agent-3_mean: 72.66
    apples_agent-3_min: 34
    apples_agent-4_max: 56
    apples_agent-4_mean: 2.22
    apples_agent-4_min: 0
    apples_agent-5_max: 131
    apples_agent-5_mean: 86.07
    apples_agent-5_min: 12
    cleaning_beam_agent-0_max: 463
    cleaning_beam_agent-0_mean: 373.87
    cleaning_beam_agent-0_min: 224
    cleaning_beam_agent-1_max: 432
    cleaning_beam_agent-1_mean: 238.03
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 587
    cleaning_beam_agent-2_mean: 437.05
    cleaning_beam_agent-2_min: 217
    cleaning_beam_agent-3_max: 56
    cleaning_beam_agent-3_mean: 24.44
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 541
    cleaning_beam_agent-4_mean: 429.98
    cleaning_beam_agent-4_min: 312
    cleaning_beam_agent-5_max: 198
    cleaning_beam_agent-5_mean: 33.28
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-37-18
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1101.999999999991
  episode_reward_mean: 956.1499999999855
  episode_reward_min: 589.0000000000009
  episodes_this_iter: 96
  episodes_total: 31872
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11577.09
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9722701907157898
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018837391398847103
        model: {}
        policy_loss: -0.0033386219292879105
        total_loss: -0.003031910862773657
        vf_explained_var: 0.04683394730091095
        vf_loss: 20.179092407226562
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1397391557693481
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014030029997229576
        model: {}
        policy_loss: -0.00410468690097332
        total_loss: -0.0037970682606101036
        vf_explained_var: -0.08796557784080505
        vf_loss: 23.13562774658203
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0199947357177734
        entropy_coeff: 0.0017600000137463212
        kl: 0.001950058969669044
        model: {}
        policy_loss: -0.0035339435562491417
        total_loss: -0.003238449804484844
        vf_explained_var: 0.013137206435203552
        vf_loss: 20.90681266784668
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.483295202255249
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011312876595184207
        model: {}
        policy_loss: -0.0028281928971409798
        total_loss: -0.001766812987625599
        vf_explained_var: 0.09572255611419678
        vf_loss: 19.119813919067383
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9640913009643555
        entropy_coeff: 0.0017600000137463212
        kl: 0.0021674365270882845
        model: {}
        policy_loss: -0.003792754141613841
        total_loss: -0.003389170626178384
        vf_explained_var: 0.007295191287994385
        vf_loss: 21.003822326660156
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6771032810211182
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010614010971039534
        model: {}
        policy_loss: -0.002812613034620881
        total_loss: -0.002152437809854746
        vf_explained_var: 0.12424705922603607
        vf_loss: 18.51876449584961
    load_time_ms: 13344.72
    num_steps_sampled: 31872000
    num_steps_trained: 31872000
    sample_time_ms: 100129.452
    update_time_ms: 15.197
  iterations_since_restore: 272
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.561235955056176
    ram_util_percent: 16.18932584269663
  pid: 30948
  policy_reward_max:
    agent-0: 183.66666666666637
    agent-1: 183.66666666666637
    agent-2: 183.66666666666637
    agent-3: 183.66666666666637
    agent-4: 183.66666666666637
    agent-5: 183.66666666666637
  policy_reward_mean:
    agent-0: 159.35833333333326
    agent-1: 159.35833333333326
    agent-2: 159.35833333333326
    agent-3: 159.35833333333326
    agent-4: 159.35833333333326
    agent-5: 159.35833333333326
  policy_reward_min:
    agent-0: 98.16666666666671
    agent-1: 98.16666666666671
    agent-2: 98.16666666666671
    agent-3: 98.16666666666671
    agent-4: 98.16666666666671
    agent-5: 98.16666666666671
  sampler_perf:
    mean_env_wait_ms: 27.203878503711156
    mean_inference_ms: 12.857412682200179
    mean_processing_ms: 57.42213576093365
  time_since_restore: 35517.81336188316
  time_this_iter_s: 125.20441579818726
  time_total_s: 44643.82517576218
  timestamp: 1637062638
  timesteps_since_restore: 26112000
  timesteps_this_iter: 96000
  timesteps_total: 31872000
  training_iteration: 332
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    332 |          44643.8 | 31872000 |   956.15 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 2.93
    apples_agent-0_min: 0
    apples_agent-1_max: 94
    apples_agent-1_mean: 30.34
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 5.71
    apples_agent-2_min: 0
    apples_agent-3_max: 122
    apples_agent-3_mean: 70.52
    apples_agent-3_min: 39
    apples_agent-4_max: 106
    apples_agent-4_mean: 2.65
    apples_agent-4_min: 0
    apples_agent-5_max: 128
    apples_agent-5_mean: 91.63
    apples_agent-5_min: 36
    cleaning_beam_agent-0_max: 485
    cleaning_beam_agent-0_mean: 382.53
    cleaning_beam_agent-0_min: 217
    cleaning_beam_agent-1_max: 479
    cleaning_beam_agent-1_mean: 254.64
    cleaning_beam_agent-1_min: 120
    cleaning_beam_agent-2_max: 614
    cleaning_beam_agent-2_mean: 416.88
    cleaning_beam_agent-2_min: 233
    cleaning_beam_agent-3_max: 45
    cleaning_beam_agent-3_mean: 23.86
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 530
    cleaning_beam_agent-4_mean: 448.34
    cleaning_beam_agent-4_min: 273
    cleaning_beam_agent-5_max: 189
    cleaning_beam_agent-5_mean: 31.31
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-39-22
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1145.999999999979
  episode_reward_mean: 991.6199999999841
  episode_reward_min: 700.9999999999843
  episodes_this_iter: 96
  episodes_total: 31968
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11589.904
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9576737880706787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019859622698277235
        model: {}
        policy_loss: -0.0035172333009541035
        total_loss: -0.003375230822712183
        vf_explained_var: 0.050951212644577026
        vf_loss: 18.27508544921875
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1371943950653076
        entropy_coeff: 0.0017600000137463212
        kl: 0.001220916979946196
        model: {}
        policy_loss: -0.0034799673594534397
        total_loss: -0.003284890204668045
        vf_explained_var: -0.11807161569595337
        vf_loss: 21.965389251708984
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0419527292251587
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014067422598600388
        model: {}
        policy_loss: -0.0037147002294659615
        total_loss: -0.0036475984379649162
        vf_explained_var: 0.023637905716896057
        vf_loss: 19.009361267089844
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4556305706501007
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010960539802908897
        model: {}
        policy_loss: -0.0024557113647460938
        total_loss: -0.0013823993504047394
        vf_explained_var: 0.019835099577903748
        vf_loss: 18.75221824645996
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9557721614837646
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022967916447669268
        model: {}
        policy_loss: -0.0037844660691916943
        total_loss: -0.003531861351802945
        vf_explained_var: -0.004018872976303101
        vf_loss: 19.347625732421875
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6663272380828857
        entropy_coeff: 0.0017600000137463212
        kl: 0.000889949151314795
        model: {}
        policy_loss: -0.002980488585308194
        total_loss: -0.002417058451101184
        vf_explained_var: 0.09171804785728455
        vf_loss: 17.361696243286133
    load_time_ms: 13351.196
    num_steps_sampled: 31968000
    num_steps_trained: 31968000
    sample_time_ms: 100049.398
    update_time_ms: 15.07
  iterations_since_restore: 273
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.47514124293785
    ram_util_percent: 16.18813559322034
  pid: 30948
  policy_reward_max:
    agent-0: 191.0000000000002
    agent-1: 191.0000000000002
    agent-2: 191.0000000000002
    agent-3: 191.0000000000002
    agent-4: 191.0000000000002
    agent-5: 191.0000000000002
  policy_reward_mean:
    agent-0: 165.2699999999999
    agent-1: 165.2699999999999
    agent-2: 165.2699999999999
    agent-3: 165.2699999999999
    agent-4: 165.2699999999999
    agent-5: 165.2699999999999
  policy_reward_min:
    agent-0: 116.83333333333394
    agent-1: 116.83333333333394
    agent-2: 116.83333333333394
    agent-3: 116.83333333333394
    agent-4: 116.83333333333394
    agent-5: 116.83333333333394
  sampler_perf:
    mean_env_wait_ms: 27.204669765024295
    mean_inference_ms: 12.856810962316816
    mean_processing_ms: 57.418968253286124
  time_since_restore: 35642.49302268028
  time_this_iter_s: 124.67966079711914
  time_total_s: 44768.504836559296
  timestamp: 1637062762
  timesteps_since_restore: 26208000
  timesteps_this_iter: 96000
  timesteps_total: 31968000
  training_iteration: 333
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    333 |          44768.5 | 31968000 |   991.62 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 2.88
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 29.3
    apples_agent-1_min: 0
    apples_agent-2_max: 64
    apples_agent-2_mean: 5.9
    apples_agent-2_min: 0
    apples_agent-3_max: 121
    apples_agent-3_mean: 70.19
    apples_agent-3_min: 28
    apples_agent-4_max: 106
    apples_agent-4_mean: 2.61
    apples_agent-4_min: 0
    apples_agent-5_max: 166
    apples_agent-5_mean: 87.88
    apples_agent-5_min: 50
    cleaning_beam_agent-0_max: 520
    cleaning_beam_agent-0_mean: 376.93
    cleaning_beam_agent-0_min: 217
    cleaning_beam_agent-1_max: 435
    cleaning_beam_agent-1_mean: 240.22
    cleaning_beam_agent-1_min: 144
    cleaning_beam_agent-2_max: 622
    cleaning_beam_agent-2_mean: 404.04
    cleaning_beam_agent-2_min: 191
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 23.73
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 534
    cleaning_beam_agent-4_mean: 444.07
    cleaning_beam_agent-4_min: 273
    cleaning_beam_agent-5_max: 102
    cleaning_beam_agent-5_mean: 31.09
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-41-28
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1146.0000000000095
  episode_reward_mean: 977.9799999999854
  episode_reward_min: 525.0000000000106
  episodes_this_iter: 96
  episodes_total: 32064
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11586.674
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9571409821510315
        entropy_coeff: 0.0017600000137463212
        kl: 0.001639843569137156
        model: {}
        policy_loss: -0.003114853985607624
        total_loss: -0.002864138688892126
        vf_explained_var: 0.07309027016162872
        vf_loss: 19.352872848510742
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1362266540527344
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011658028233796358
        model: {}
        policy_loss: -0.0036536287516355515
        total_loss: -0.003296172246336937
        vf_explained_var: -0.10681304335594177
        vf_loss: 23.572189331054688
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0480657815933228
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017532509518787265
        model: {}
        policy_loss: -0.00397955859079957
        total_loss: -0.0036919331178069115
        vf_explained_var: -0.012630462646484375
        vf_loss: 21.322202682495117
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45242011547088623
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009905113838613033
        model: {}
        policy_loss: -0.002666773274540901
        total_loss: -0.0015024049207568169
        vf_explained_var: 0.06075601279735565
        vf_loss: 19.606285095214844
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9465470910072327
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018086322816088796
        model: {}
        policy_loss: -0.00407513789832592
        total_loss: -0.003634459339082241
        vf_explained_var: -0.005281373858451843
        vf_loss: 21.0660400390625
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6507784128189087
        entropy_coeff: 0.0017600000137463212
        kl: 0.001172335585579276
        model: {}
        policy_loss: -0.002878443105146289
        total_loss: -0.002156425267457962
        vf_explained_var: 0.10256421566009521
        vf_loss: 18.673906326293945
    load_time_ms: 13341.03
    num_steps_sampled: 32064000
    num_steps_trained: 32064000
    sample_time_ms: 99945.05
    update_time_ms: 15.189
  iterations_since_restore: 274
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.359776536312854
    ram_util_percent: 16.201117318435756
  pid: 30948
  policy_reward_max:
    agent-0: 190.99999999999935
    agent-1: 190.99999999999935
    agent-2: 190.99999999999935
    agent-3: 190.99999999999935
    agent-4: 190.99999999999935
    agent-5: 190.99999999999935
  policy_reward_mean:
    agent-0: 162.99666666666656
    agent-1: 162.99666666666656
    agent-2: 162.99666666666656
    agent-3: 162.99666666666656
    agent-4: 162.99666666666656
    agent-5: 162.99666666666656
  policy_reward_min:
    agent-0: 87.49999999999983
    agent-1: 87.49999999999983
    agent-2: 87.49999999999983
    agent-3: 87.49999999999983
    agent-4: 87.49999999999983
    agent-5: 87.49999999999983
  sampler_perf:
    mean_env_wait_ms: 27.20536509178807
    mean_inference_ms: 12.85631784491876
    mean_processing_ms: 57.41624327047167
  time_since_restore: 35768.03649139404
  time_this_iter_s: 125.54346871376038
  time_total_s: 44894.048305273056
  timestamp: 1637062888
  timesteps_since_restore: 26304000
  timesteps_this_iter: 96000
  timesteps_total: 32064000
  training_iteration: 334
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    334 |            44894 | 32064000 |   977.98 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 38
    apples_agent-0_mean: 2.26
    apples_agent-0_min: 0
    apples_agent-1_max: 135
    apples_agent-1_mean: 35.84
    apples_agent-1_min: 0
    apples_agent-2_max: 200
    apples_agent-2_mean: 8.14
    apples_agent-2_min: 0
    apples_agent-3_max: 147
    apples_agent-3_mean: 68.81
    apples_agent-3_min: 38
    apples_agent-4_max: 77
    apples_agent-4_mean: 3.05
    apples_agent-4_min: 0
    apples_agent-5_max: 174
    apples_agent-5_mean: 88.53
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 496
    cleaning_beam_agent-0_mean: 384.63
    cleaning_beam_agent-0_min: 252
    cleaning_beam_agent-1_max: 402
    cleaning_beam_agent-1_mean: 235.86
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 576
    cleaning_beam_agent-2_mean: 408.58
    cleaning_beam_agent-2_min: 195
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 23.31
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 539
    cleaning_beam_agent-4_mean: 444.92
    cleaning_beam_agent-4_min: 310
    cleaning_beam_agent-5_max: 141
    cleaning_beam_agent-5_mean: 32.28
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-43-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1144.9999999999925
  episode_reward_mean: 984.8299999999866
  episode_reward_min: 569.9999999999977
  episodes_this_iter: 96
  episodes_total: 32160
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11567.587
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9458922743797302
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017676058923825622
        model: {}
        policy_loss: -0.0030779647640883923
        total_loss: -0.002772280713543296
        vf_explained_var: 0.08566465973854065
        vf_loss: 19.70454978942871
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1368951797485352
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009680237853899598
        model: {}
        policy_loss: -0.003351848805323243
        total_loss: -0.0029503547120839357
        vf_explained_var: -0.09636226296424866
        vf_loss: 24.024280548095703
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0427343845367432
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013851961120963097
        model: {}
        policy_loss: -0.003481570165604353
        total_loss: -0.00323428213596344
        vf_explained_var: 0.04435907304286957
        vf_loss: 20.82501983642578
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4574841260910034
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010954239405691624
        model: {}
        policy_loss: -0.002509934827685356
        total_loss: -0.0013323123566806316
        vf_explained_var: 0.08143618702888489
        vf_loss: 19.827930450439453
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9476895332336426
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015476671978831291
        model: {}
        policy_loss: -0.003602266777306795
        total_loss: -0.003117169253528118
        vf_explained_var: 0.002554848790168762
        vf_loss: 21.53030776977539
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6574333310127258
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008585924515500665
        model: {}
        policy_loss: -0.0028455164283514023
        total_loss: -0.0021160277538001537
        vf_explained_var: 0.12401309609413147
        vf_loss: 18.86571502685547
    load_time_ms: 13340.915
    num_steps_sampled: 32160000
    num_steps_trained: 32160000
    sample_time_ms: 99851.981
    update_time_ms: 15.083
  iterations_since_restore: 275
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.34011299435028
    ram_util_percent: 16.200564971751415
  pid: 30948
  policy_reward_max:
    agent-0: 190.83333333333312
    agent-1: 190.83333333333312
    agent-2: 190.83333333333312
    agent-3: 190.83333333333312
    agent-4: 190.83333333333312
    agent-5: 190.83333333333312
  policy_reward_mean:
    agent-0: 164.1383333333332
    agent-1: 164.1383333333332
    agent-2: 164.1383333333332
    agent-3: 164.1383333333332
    agent-4: 164.1383333333332
    agent-5: 164.1383333333332
  policy_reward_min:
    agent-0: 95.00000000000024
    agent-1: 95.00000000000024
    agent-2: 95.00000000000024
    agent-3: 95.00000000000024
    agent-4: 95.00000000000024
    agent-5: 95.00000000000024
  sampler_perf:
    mean_env_wait_ms: 27.205287127536117
    mean_inference_ms: 12.855859830874612
    mean_processing_ms: 57.411655222563674
  time_since_restore: 35891.83324623108
  time_this_iter_s: 123.79675483703613
  time_total_s: 45017.84506011009
  timestamp: 1637063012
  timesteps_since_restore: 26400000
  timesteps_this_iter: 96000
  timesteps_total: 32160000
  training_iteration: 335
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    335 |          45017.8 | 32160000 |   984.83 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 36
    apples_agent-0_mean: 1.44
    apples_agent-0_min: 0
    apples_agent-1_max: 114
    apples_agent-1_mean: 32.25
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 7.11
    apples_agent-2_min: 0
    apples_agent-3_max: 116
    apples_agent-3_mean: 65.76
    apples_agent-3_min: 37
    apples_agent-4_max: 63
    apples_agent-4_mean: 3.7
    apples_agent-4_min: 0
    apples_agent-5_max: 140
    apples_agent-5_mean: 89.71
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 551
    cleaning_beam_agent-0_mean: 382.39
    cleaning_beam_agent-0_min: 274
    cleaning_beam_agent-1_max: 436
    cleaning_beam_agent-1_mean: 250.33
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 572
    cleaning_beam_agent-2_mean: 407.87
    cleaning_beam_agent-2_min: 188
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 24.05
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 566
    cleaning_beam_agent-4_mean: 454.56
    cleaning_beam_agent-4_min: 334
    cleaning_beam_agent-5_max: 136
    cleaning_beam_agent-5_mean: 33.18
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-45-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1126.9999999999989
  episode_reward_mean: 987.5499999999847
  episode_reward_min: 621.9999999999917
  episodes_this_iter: 96
  episodes_total: 32256
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11582.0
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9489385485649109
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012844899902120233
        model: {}
        policy_loss: -0.003199749393388629
        total_loss: -0.0029795283917337656
        vf_explained_var: 0.05379380285739899
        vf_loss: 18.903532028198242
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1402634382247925
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009689244907349348
        model: {}
        policy_loss: -0.00349310040473938
        total_loss: -0.0031859418377280235
        vf_explained_var: -0.13355818390846252
        vf_loss: 23.140220642089844
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0389333963394165
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014215540140867233
        model: {}
        policy_loss: -0.003503987565636635
        total_loss: -0.003276587463915348
        vf_explained_var: -0.014670535922050476
        vf_loss: 20.55923843383789
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.45083314180374146
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009075475391000509
        model: {}
        policy_loss: -0.00253958604298532
        total_loss: -0.0014551370404660702
        vf_explained_var: 0.06395074725151062
        vf_loss: 18.77916717529297
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.929151177406311
        entropy_coeff: 0.0017600000137463212
        kl: 0.002263526199385524
        model: {}
        policy_loss: -0.003740202635526657
        total_loss: -0.0033579785376787186
        vf_explained_var: -0.0032418817281723022
        vf_loss: 20.17528533935547
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6600625514984131
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015026828041300178
        model: {}
        policy_loss: -0.0030044931918382645
        total_loss: -0.0023829718120396137
        vf_explained_var: 0.11092738807201385
        vf_loss: 17.832313537597656
    load_time_ms: 13355.985
    num_steps_sampled: 32256000
    num_steps_trained: 32256000
    sample_time_ms: 99741.054
    update_time_ms: 15.012
  iterations_since_restore: 276
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.35
    ram_util_percent: 16.200568181818184
  pid: 30948
  policy_reward_max:
    agent-0: 187.83333333333303
    agent-1: 187.83333333333303
    agent-2: 187.83333333333303
    agent-3: 187.83333333333303
    agent-4: 187.83333333333303
    agent-5: 187.83333333333303
  policy_reward_mean:
    agent-0: 164.59166666666658
    agent-1: 164.59166666666658
    agent-2: 164.59166666666658
    agent-3: 164.59166666666658
    agent-4: 164.59166666666658
    agent-5: 164.59166666666658
  policy_reward_min:
    agent-0: 103.66666666666721
    agent-1: 103.66666666666721
    agent-2: 103.66666666666721
    agent-3: 103.66666666666721
    agent-4: 103.66666666666721
    agent-5: 103.66666666666721
  sampler_perf:
    mean_env_wait_ms: 27.205337252415408
    mean_inference_ms: 12.855069335807132
    mean_processing_ms: 57.407506512988945
  time_since_restore: 36015.719745635986
  time_this_iter_s: 123.88649940490723
  time_total_s: 45141.731559515
  timestamp: 1637063136
  timesteps_since_restore: 26496000
  timesteps_this_iter: 96000
  timesteps_total: 32256000
  training_iteration: 336
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    336 |          45141.7 | 32256000 |   987.55 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 33
    apples_agent-0_mean: 1.62
    apples_agent-0_min: 0
    apples_agent-1_max: 151
    apples_agent-1_mean: 33.73
    apples_agent-1_min: 0
    apples_agent-2_max: 63
    apples_agent-2_mean: 7.45
    apples_agent-2_min: 0
    apples_agent-3_max: 158
    apples_agent-3_mean: 67.8
    apples_agent-3_min: 31
    apples_agent-4_max: 30
    apples_agent-4_mean: 1.95
    apples_agent-4_min: 0
    apples_agent-5_max: 178
    apples_agent-5_mean: 91.85
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 517
    cleaning_beam_agent-0_mean: 395.38
    cleaning_beam_agent-0_min: 197
    cleaning_beam_agent-1_max: 379
    cleaning_beam_agent-1_mean: 242.53
    cleaning_beam_agent-1_min: 138
    cleaning_beam_agent-2_max: 593
    cleaning_beam_agent-2_mean: 406.33
    cleaning_beam_agent-2_min: 200
    cleaning_beam_agent-3_max: 103
    cleaning_beam_agent-3_mean: 23.83
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 551
    cleaning_beam_agent-4_mean: 460.59
    cleaning_beam_agent-4_min: 304
    cleaning_beam_agent-5_max: 145
    cleaning_beam_agent-5_mean: 35.07
    cleaning_beam_agent-5_min: 2
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-47-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1138.000000000008
  episode_reward_mean: 973.8799999999866
  episode_reward_min: 342.00000000000455
  episodes_this_iter: 96
  episodes_total: 32352
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11598.761
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9355196356773376
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018001208081841469
        model: {}
        policy_loss: -0.0033079609274864197
        total_loss: -0.0030252723954617977
        vf_explained_var: 0.0693514347076416
        vf_loss: 19.292041778564453
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.148267388343811
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010721551952883601
        model: {}
        policy_loss: -0.0033579471055418253
        total_loss: -0.003049344290047884
        vf_explained_var: -0.11200341582298279
        vf_loss: 23.295530319213867
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0484263896942139
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012191608548164368
        model: {}
        policy_loss: -0.003578504081815481
        total_loss: -0.003354016225785017
        vf_explained_var: 0.004143327474594116
        vf_loss: 20.697200775146484
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44830557703971863
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010873958235606551
        model: {}
        policy_loss: -0.0026127174496650696
        total_loss: -0.0014818403869867325
        vf_explained_var: 0.07104611396789551
        vf_loss: 19.19895362854004
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9401633739471436
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016386333154514432
        model: {}
        policy_loss: -0.0037372433580458164
        total_loss: -0.0033493200317025185
        vf_explained_var: 0.014745444059371948
        vf_loss: 20.426124572753906
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6744008660316467
        entropy_coeff: 0.0017600000137463212
        kl: 0.000916035904083401
        model: {}
        policy_loss: -0.0028751546051353216
        total_loss: -0.0022098924964666367
        vf_explained_var: 0.10661444067955017
        vf_loss: 18.52206039428711
    load_time_ms: 13358.678
    num_steps_sampled: 32352000
    num_steps_trained: 32352000
    sample_time_ms: 99841.258
    update_time_ms: 16.168
  iterations_since_restore: 277
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.275977653631283
    ram_util_percent: 16.18882681564246
  pid: 30948
  policy_reward_max:
    agent-0: 189.6666666666663
    agent-1: 189.6666666666663
    agent-2: 189.6666666666663
    agent-3: 189.6666666666663
    agent-4: 189.6666666666663
    agent-5: 189.6666666666663
  policy_reward_mean:
    agent-0: 162.31333333333325
    agent-1: 162.31333333333325
    agent-2: 162.31333333333325
    agent-3: 162.31333333333325
    agent-4: 162.31333333333325
    agent-5: 162.31333333333325
  policy_reward_min:
    agent-0: 56.99999999999988
    agent-1: 56.99999999999988
    agent-2: 56.99999999999988
    agent-3: 56.99999999999988
    agent-4: 56.99999999999988
    agent-5: 56.99999999999988
  sampler_perf:
    mean_env_wait_ms: 27.205941177009485
    mean_inference_ms: 12.854516783089034
    mean_processing_ms: 57.40454873897726
  time_since_restore: 36141.16390943527
  time_this_iter_s: 125.44416379928589
  time_total_s: 45267.175723314285
  timestamp: 1637063261
  timesteps_since_restore: 26592000
  timesteps_this_iter: 96000
  timesteps_total: 32352000
  training_iteration: 337
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    337 |          45267.2 | 32352000 |   973.88 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 1.71
    apples_agent-0_min: 0
    apples_agent-1_max: 96
    apples_agent-1_mean: 33.57
    apples_agent-1_min: 0
    apples_agent-2_max: 106
    apples_agent-2_mean: 7.76
    apples_agent-2_min: 0
    apples_agent-3_max: 119
    apples_agent-3_mean: 68.25
    apples_agent-3_min: 39
    apples_agent-4_max: 50
    apples_agent-4_mean: 2.43
    apples_agent-4_min: 0
    apples_agent-5_max: 138
    apples_agent-5_mean: 88.25
    apples_agent-5_min: 53
    cleaning_beam_agent-0_max: 517
    cleaning_beam_agent-0_mean: 379.95
    cleaning_beam_agent-0_min: 245
    cleaning_beam_agent-1_max: 452
    cleaning_beam_agent-1_mean: 253.28
    cleaning_beam_agent-1_min: 136
    cleaning_beam_agent-2_max: 579
    cleaning_beam_agent-2_mean: 391.22
    cleaning_beam_agent-2_min: 187
    cleaning_beam_agent-3_max: 58
    cleaning_beam_agent-3_mean: 23.12
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 539
    cleaning_beam_agent-4_mean: 459.31
    cleaning_beam_agent-4_min: 322
    cleaning_beam_agent-5_max: 154
    cleaning_beam_agent-5_mean: 29.82
    cleaning_beam_agent-5_min: 1
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-49-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1136.9999999999907
  episode_reward_mean: 989.139999999985
  episode_reward_min: 640.0
  episodes_this_iter: 96
  episodes_total: 32448
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11620.145
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9239869713783264
        entropy_coeff: 0.0017600000137463212
        kl: 0.002336998702958226
        model: {}
        policy_loss: -0.0030370033346116543
        total_loss: -0.0027120006270706654
        vf_explained_var: 0.05077101290225983
        vf_loss: 19.512205123901367
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1282742023468018
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009748645825311542
        model: {}
        policy_loss: -0.0038258987478911877
        total_loss: -0.0034720483236014843
        vf_explained_var: -0.11525410413742065
        vf_loss: 23.39613914489746
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0455420017242432
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012863100273534656
        model: {}
        policy_loss: -0.0034668156877160072
        total_loss: -0.0032475353218615055
        vf_explained_var: 0.014162033796310425
        vf_loss: 20.594345092773438
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44225385785102844
        entropy_coeff: 0.0017600000137463212
        kl: 0.001190563547424972
        model: {}
        policy_loss: -0.002512948354706168
        total_loss: -0.0013755029067397118
        vf_explained_var: 0.06608003377914429
        vf_loss: 19.15812110900879
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9466829299926758
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013690456980839372
        model: {}
        policy_loss: -0.00373503053560853
        total_loss: -0.003297111950814724
        vf_explained_var: -0.017247334122657776
        vf_loss: 21.040813446044922
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6455717086791992
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011261457111686468
        model: {}
        policy_loss: -0.0031432337127625942
        total_loss: -0.002386555541306734
        vf_explained_var: 0.08135659992694855
        vf_loss: 18.928836822509766
    load_time_ms: 13362.416
    num_steps_sampled: 32448000
    num_steps_trained: 32448000
    sample_time_ms: 99624.338
    update_time_ms: 15.825
  iterations_since_restore: 278
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.57045454545454
    ram_util_percent: 16.119886363636365
  pid: 30948
  policy_reward_max:
    agent-0: 189.49999999999986
    agent-1: 189.49999999999986
    agent-2: 189.49999999999986
    agent-3: 189.49999999999986
    agent-4: 189.49999999999986
    agent-5: 189.49999999999986
  policy_reward_mean:
    agent-0: 164.85666666666654
    agent-1: 164.85666666666654
    agent-2: 164.85666666666654
    agent-3: 164.85666666666654
    agent-4: 164.85666666666654
    agent-5: 164.85666666666654
  policy_reward_min:
    agent-0: 106.66666666666704
    agent-1: 106.66666666666704
    agent-2: 106.66666666666704
    agent-3: 106.66666666666704
    agent-4: 106.66666666666704
    agent-5: 106.66666666666704
  sampler_perf:
    mean_env_wait_ms: 27.206219585219873
    mean_inference_ms: 12.854098050749956
    mean_processing_ms: 57.40073640263096
  time_since_restore: 36265.29155921936
  time_this_iter_s: 124.12764978408813
  time_total_s: 45391.30337309837
  timestamp: 1637063386
  timesteps_since_restore: 26688000
  timesteps_this_iter: 96000
  timesteps_total: 32448000
  training_iteration: 338
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    338 |          45391.3 | 32448000 |   989.14 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 54
    apples_agent-0_mean: 2.34
    apples_agent-0_min: 0
    apples_agent-1_max: 119
    apples_agent-1_mean: 33.46
    apples_agent-1_min: 0
    apples_agent-2_max: 162
    apples_agent-2_mean: 11.5
    apples_agent-2_min: 0
    apples_agent-3_max: 197
    apples_agent-3_mean: 69.54
    apples_agent-3_min: 41
    apples_agent-4_max: 27
    apples_agent-4_mean: 2.03
    apples_agent-4_min: 0
    apples_agent-5_max: 142
    apples_agent-5_mean: 90.32
    apples_agent-5_min: 58
    cleaning_beam_agent-0_max: 540
    cleaning_beam_agent-0_mean: 383.13
    cleaning_beam_agent-0_min: 201
    cleaning_beam_agent-1_max: 506
    cleaning_beam_agent-1_mean: 252.81
    cleaning_beam_agent-1_min: 106
    cleaning_beam_agent-2_max: 583
    cleaning_beam_agent-2_mean: 395.67
    cleaning_beam_agent-2_min: 141
    cleaning_beam_agent-3_max: 70
    cleaning_beam_agent-3_mean: 27.03
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 535
    cleaning_beam_agent-4_mean: 449.73
    cleaning_beam_agent-4_min: 334
    cleaning_beam_agent-5_max: 118
    cleaning_beam_agent-5_mean: 31.77
    cleaning_beam_agent-5_min: 5
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-51-51
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1128.0000000000027
  episode_reward_mean: 976.7599999999856
  episode_reward_min: 667.9999999999972
  episodes_this_iter: 96
  episodes_total: 32544
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11675.181
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9532978534698486
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011218155268579721
        model: {}
        policy_loss: -0.002933497540652752
        total_loss: -0.002677672542631626
        vf_explained_var: 0.0541079044342041
        vf_loss: 19.3363094329834
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.140804648399353
        entropy_coeff: 0.0017600000137463212
        kl: 0.001657217857427895
        model: {}
        policy_loss: -0.0037817922420799732
        total_loss: -0.0035388669930398464
        vf_explained_var: -0.09191370010375977
        vf_loss: 22.507402420043945
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.042029857635498
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014585379976779222
        model: {}
        policy_loss: -0.003442503046244383
        total_loss: -0.0032255365513265133
        vf_explained_var: 0.0021471381187438965
        vf_loss: 20.509397506713867
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4541141390800476
        entropy_coeff: 0.0017600000137463212
        kl: 0.001211081980727613
        model: {}
        policy_loss: -0.0027803676202893257
        total_loss: -0.0016591411549597979
        vf_explained_var: 0.0611487478017807
        vf_loss: 19.20466423034668
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9431208372116089
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015074783004820347
        model: {}
        policy_loss: -0.0036279261112213135
        total_loss: -0.003272506408393383
        vf_explained_var: 0.015250951051712036
        vf_loss: 20.15312957763672
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6464455127716064
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009462989983148873
        model: {}
        policy_loss: -0.002788937883451581
        total_loss: -0.0020752311684191227
        vf_explained_var: 0.09567716717720032
        vf_loss: 18.514507293701172
    load_time_ms: 13377.571
    num_steps_sampled: 32544000
    num_steps_trained: 32544000
    sample_time_ms: 99546.74
    update_time_ms: 15.792
  iterations_since_restore: 279
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.31620111731844
    ram_util_percent: 16.21284916201117
  pid: 30948
  policy_reward_max:
    agent-0: 187.99999999999994
    agent-1: 187.99999999999994
    agent-2: 187.99999999999994
    agent-3: 187.99999999999994
    agent-4: 187.99999999999994
    agent-5: 187.99999999999994
  policy_reward_mean:
    agent-0: 162.79333333333324
    agent-1: 162.79333333333324
    agent-2: 162.79333333333324
    agent-3: 162.79333333333324
    agent-4: 162.79333333333324
    agent-5: 162.79333333333324
  policy_reward_min:
    agent-0: 111.33333333333348
    agent-1: 111.33333333333348
    agent-2: 111.33333333333348
    agent-3: 111.33333333333348
    agent-4: 111.33333333333348
    agent-5: 111.33333333333348
  sampler_perf:
    mean_env_wait_ms: 27.206514075258482
    mean_inference_ms: 12.853724419627628
    mean_processing_ms: 57.39639338386473
  time_since_restore: 36390.5896589756
  time_this_iter_s: 125.29809975624084
  time_total_s: 45516.601472854614
  timestamp: 1637063511
  timesteps_since_restore: 26784000
  timesteps_this_iter: 96000
  timesteps_total: 32544000
  training_iteration: 339
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    339 |          45516.6 | 32544000 |   976.76 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 59
    apples_agent-0_mean: 2.91
    apples_agent-0_min: 0
    apples_agent-1_max: 122
    apples_agent-1_mean: 27.91
    apples_agent-1_min: 0
    apples_agent-2_max: 95
    apples_agent-2_mean: 8.34
    apples_agent-2_min: 0
    apples_agent-3_max: 102
    apples_agent-3_mean: 69.5
    apples_agent-3_min: 35
    apples_agent-4_max: 77
    apples_agent-4_mean: 3.35
    apples_agent-4_min: 0
    apples_agent-5_max: 134
    apples_agent-5_mean: 87.26
    apples_agent-5_min: 60
    cleaning_beam_agent-0_max: 498
    cleaning_beam_agent-0_mean: 382.66
    cleaning_beam_agent-0_min: 201
    cleaning_beam_agent-1_max: 501
    cleaning_beam_agent-1_mean: 268.31
    cleaning_beam_agent-1_min: 145
    cleaning_beam_agent-2_max: 624
    cleaning_beam_agent-2_mean: 416.77
    cleaning_beam_agent-2_min: 211
    cleaning_beam_agent-3_max: 55
    cleaning_beam_agent-3_mean: 22.32
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 536
    cleaning_beam_agent-4_mean: 446.57
    cleaning_beam_agent-4_min: 370
    cleaning_beam_agent-5_max: 111
    cleaning_beam_agent-5_mean: 30.13
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-53-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1129.999999999989
  episode_reward_mean: 987.0099999999856
  episode_reward_min: 628.9999999999992
  episodes_this_iter: 96
  episodes_total: 32640
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11741.449
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9552128314971924
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018152364064007998
        model: {}
        policy_loss: -0.0031204752158373594
        total_loss: -0.0029058558866381645
        vf_explained_var: 0.07071632146835327
        vf_loss: 18.957901000976562
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1412417888641357
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015290894079953432
        model: {}
        policy_loss: -0.0036037780810147524
        total_loss: -0.0032721401657909155
        vf_explained_var: -0.12512555718421936
        vf_loss: 23.402204513549805
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0389115810394287
        entropy_coeff: 0.0017600000137463212
        kl: 0.001887560705654323
        model: {}
        policy_loss: -0.0034021660685539246
        total_loss: -0.0031156325712800026
        vf_explained_var: -0.02352219820022583
        vf_loss: 21.150188446044922
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44842174649238586
        entropy_coeff: 0.0017600000137463212
        kl: 0.001003820332698524
        model: {}
        policy_loss: -0.002416586969047785
        total_loss: -0.0012717435602098703
        vf_explained_var: 0.05470740795135498
        vf_loss: 19.34063720703125
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9329798817634583
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019201753893867135
        model: {}
        policy_loss: -0.003936481196433306
        total_loss: -0.0035238706041127443
        vf_explained_var: -0.0006944090127944946
        vf_loss: 20.54656982421875
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6340682506561279
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009677418856881559
        model: {}
        policy_loss: -0.0027943411841988564
        total_loss: -0.0021040067076683044
        vf_explained_var: 0.11578406393527985
        vf_loss: 18.062942504882812
    load_time_ms: 13395.119
    num_steps_sampled: 32640000
    num_steps_trained: 32640000
    sample_time_ms: 99462.853
    update_time_ms: 15.62
  iterations_since_restore: 280
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.96553672316384
    ram_util_percent: 16.228813559322035
  pid: 30948
  policy_reward_max:
    agent-0: 188.33333333333312
    agent-1: 188.33333333333312
    agent-2: 188.33333333333312
    agent-3: 188.33333333333312
    agent-4: 188.33333333333312
    agent-5: 188.33333333333312
  policy_reward_mean:
    agent-0: 164.5016666666665
    agent-1: 164.5016666666665
    agent-2: 164.5016666666665
    agent-3: 164.5016666666665
    agent-4: 164.5016666666665
    agent-5: 164.5016666666665
  policy_reward_min:
    agent-0: 104.83333333333378
    agent-1: 104.83333333333378
    agent-2: 104.83333333333378
    agent-3: 104.83333333333378
    agent-4: 104.83333333333378
    agent-5: 104.83333333333378
  sampler_perf:
    mean_env_wait_ms: 27.20657641104052
    mean_inference_ms: 12.853021682884492
    mean_processing_ms: 57.39147839861786
  time_since_restore: 36515.02941918373
  time_this_iter_s: 124.43976020812988
  time_total_s: 45641.041233062744
  timestamp: 1637063636
  timesteps_since_restore: 26880000
  timesteps_this_iter: 96000
  timesteps_total: 32640000
  training_iteration: 340
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    340 |            45641 | 32640000 |   987.01 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 1.76
    apples_agent-0_min: 0
    apples_agent-1_max: 117
    apples_agent-1_mean: 32.37
    apples_agent-1_min: 0
    apples_agent-2_max: 64
    apples_agent-2_mean: 8.09
    apples_agent-2_min: 0
    apples_agent-3_max: 115
    apples_agent-3_mean: 70.39
    apples_agent-3_min: 36
    apples_agent-4_max: 81
    apples_agent-4_mean: 3.61
    apples_agent-4_min: 0
    apples_agent-5_max: 144
    apples_agent-5_mean: 88.0
    apples_agent-5_min: 40
    cleaning_beam_agent-0_max: 495
    cleaning_beam_agent-0_mean: 386.59
    cleaning_beam_agent-0_min: 287
    cleaning_beam_agent-1_max: 455
    cleaning_beam_agent-1_mean: 257.25
    cleaning_beam_agent-1_min: 114
    cleaning_beam_agent-2_max: 609
    cleaning_beam_agent-2_mean: 411.28
    cleaning_beam_agent-2_min: 222
    cleaning_beam_agent-3_max: 52
    cleaning_beam_agent-3_mean: 22.72
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 544
    cleaning_beam_agent-4_mean: 455.98
    cleaning_beam_agent-4_min: 342
    cleaning_beam_agent-5_max: 176
    cleaning_beam_agent-5_mean: 30.84
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-56-03
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1135.9999999999957
  episode_reward_mean: 989.4199999999868
  episode_reward_min: 606.0000000000057
  episodes_this_iter: 96
  episodes_total: 32736
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11726.187
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.952852725982666
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015718669164925814
        model: {}
        policy_loss: -0.002869661897420883
        total_loss: -0.002567836781963706
        vf_explained_var: 0.016525983810424805
        vf_loss: 19.7884464263916
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1241893768310547
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016661311965435743
        model: {}
        policy_loss: -0.0038306270726025105
        total_loss: -0.003565310500562191
        vf_explained_var: -0.09787753224372864
        vf_loss: 22.438920974731445
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0491691827774048
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013640518300235271
        model: {}
        policy_loss: -0.0031426455825567245
        total_loss: -0.002926919609308243
        vf_explained_var: -0.01854783296585083
        vf_loss: 20.622644424438477
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44298967719078064
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011614167597144842
        model: {}
        policy_loss: -0.0027425549924373627
        total_loss: -0.0016267793253064156
        vf_explained_var: 0.05784384906291962
        vf_loss: 18.954364776611328
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9374609589576721
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016789563233032823
        model: {}
        policy_loss: -0.0038671379443258047
        total_loss: -0.003488993737846613
        vf_explained_var: -0.004411488771438599
        vf_loss: 20.280757904052734
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6644502282142639
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010899719782173634
        model: {}
        policy_loss: -0.0028558713383972645
        total_loss: -0.0021895074751228094
        vf_explained_var: 0.09277135133743286
        vf_loss: 18.35797882080078
    load_time_ms: 13412.149
    num_steps_sampled: 32736000
    num_steps_trained: 32736000
    sample_time_ms: 99671.247
    update_time_ms: 14.935
  iterations_since_restore: 281
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.29888888888889
    ram_util_percent: 16.118333333333336
  pid: 30948
  policy_reward_max:
    agent-0: 189.33333333333312
    agent-1: 189.33333333333312
    agent-2: 189.33333333333312
    agent-3: 189.33333333333312
    agent-4: 189.33333333333312
    agent-5: 189.33333333333312
  policy_reward_mean:
    agent-0: 164.90333333333322
    agent-1: 164.90333333333322
    agent-2: 164.90333333333322
    agent-3: 164.90333333333322
    agent-4: 164.90333333333322
    agent-5: 164.90333333333322
  policy_reward_min:
    agent-0: 101.00000000000003
    agent-1: 101.00000000000003
    agent-2: 101.00000000000003
    agent-3: 101.00000000000003
    agent-4: 101.00000000000003
    agent-5: 101.00000000000003
  sampler_perf:
    mean_env_wait_ms: 27.20801739547304
    mean_inference_ms: 12.852661888116462
    mean_processing_ms: 57.38943543555026
  time_since_restore: 36641.67151379585
  time_this_iter_s: 126.64209461212158
  time_total_s: 45767.683327674866
  timestamp: 1637063763
  timesteps_since_restore: 26976000
  timesteps_this_iter: 96000
  timesteps_total: 32736000
  training_iteration: 341
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    341 |          45767.7 | 32736000 |   989.42 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 48
    apples_agent-0_mean: 3.19
    apples_agent-0_min: 0
    apples_agent-1_max: 107
    apples_agent-1_mean: 26.98
    apples_agent-1_min: 0
    apples_agent-2_max: 62
    apples_agent-2_mean: 5.25
    apples_agent-2_min: 0
    apples_agent-3_max: 183
    apples_agent-3_mean: 70.78
    apples_agent-3_min: 37
    apples_agent-4_max: 67
    apples_agent-4_mean: 4.24
    apples_agent-4_min: 0
    apples_agent-5_max: 192
    apples_agent-5_mean: 89.79
    apples_agent-5_min: 39
    cleaning_beam_agent-0_max: 534
    cleaning_beam_agent-0_mean: 379.76
    cleaning_beam_agent-0_min: 184
    cleaning_beam_agent-1_max: 450
    cleaning_beam_agent-1_mean: 274.53
    cleaning_beam_agent-1_min: 158
    cleaning_beam_agent-2_max: 609
    cleaning_beam_agent-2_mean: 401.54
    cleaning_beam_agent-2_min: 205
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 21.8
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 520
    cleaning_beam_agent-4_mean: 446.38
    cleaning_beam_agent-4_min: 292
    cleaning_beam_agent-5_max: 177
    cleaning_beam_agent-5_mean: 30.94
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.03
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_06-58-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1118.000000000007
  episode_reward_mean: 961.0799999999861
  episode_reward_min: 317.9999999999983
  episodes_this_iter: 96
  episodes_total: 32832
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11698.791
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9480828046798706
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012824495788663626
        model: {}
        policy_loss: -0.002864271402359009
        total_loss: -0.0024462612345814705
        vf_explained_var: 0.0739908516407013
        vf_loss: 20.866348266601562
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1298036575317383
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013264238368719816
        model: {}
        policy_loss: -0.003395477309823036
        total_loss: -0.002918977290391922
        vf_explained_var: -0.09045186638832092
        vf_loss: 24.64954376220703
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.054787278175354
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011658290168270469
        model: {}
        policy_loss: -0.0035812321584671736
        total_loss: -0.003197082784026861
        vf_explained_var: 0.006697848439216614
        vf_loss: 22.40574073791504
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4649513363838196
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013258541002869606
        model: {}
        policy_loss: -0.002932550385594368
        total_loss: -0.0017421306110918522
        vf_explained_var: 0.10935798287391663
        vf_loss: 20.087295532226562
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9425377249717712
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012846427271142602
        model: {}
        policy_loss: -0.003702618880197406
        total_loss: -0.003071179613471031
        vf_explained_var: -0.01616370677947998
        vf_loss: 22.903026580810547
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6574980020523071
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012196688912808895
        model: {}
        policy_loss: -0.0033073853701353073
        total_loss: -0.002526841126382351
        vf_explained_var: 0.1410723626613617
        vf_loss: 19.37741470336914
    load_time_ms: 13414.072
    num_steps_sampled: 32832000
    num_steps_trained: 32832000
    sample_time_ms: 99735.296
    update_time_ms: 15.524
  iterations_since_restore: 282
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.401117318435755
    ram_util_percent: 16.189944134078214
  pid: 30948
  policy_reward_max:
    agent-0: 186.33333333333314
    agent-1: 186.33333333333314
    agent-2: 186.33333333333314
    agent-3: 186.33333333333314
    agent-4: 186.33333333333314
    agent-5: 186.33333333333314
  policy_reward_mean:
    agent-0: 160.1799999999999
    agent-1: 160.1799999999999
    agent-2: 160.1799999999999
    agent-3: 160.1799999999999
    agent-4: 160.1799999999999
    agent-5: 160.1799999999999
  policy_reward_min:
    agent-0: 52.999999999999886
    agent-1: 52.999999999999886
    agent-2: 52.999999999999886
    agent-3: 52.999999999999886
    agent-4: 52.999999999999886
    agent-5: 52.999999999999886
  sampler_perf:
    mean_env_wait_ms: 27.208901228757068
    mean_inference_ms: 12.852368871467533
    mean_processing_ms: 57.386158641646624
  time_since_restore: 36767.28418421745
  time_this_iter_s: 125.61267042160034
  time_total_s: 45893.295998096466
  timestamp: 1637063889
  timesteps_since_restore: 27072000
  timesteps_this_iter: 96000
  timesteps_total: 32832000
  training_iteration: 342
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    342 |          45893.3 | 32832000 |   961.08 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 1.32
    apples_agent-0_min: 0
    apples_agent-1_max: 125
    apples_agent-1_mean: 29.5
    apples_agent-1_min: 0
    apples_agent-2_max: 67
    apples_agent-2_mean: 6.03
    apples_agent-2_min: 0
    apples_agent-3_max: 111
    apples_agent-3_mean: 67.43
    apples_agent-3_min: 30
    apples_agent-4_max: 62
    apples_agent-4_mean: 4.21
    apples_agent-4_min: 0
    apples_agent-5_max: 123
    apples_agent-5_mean: 86.58
    apples_agent-5_min: 52
    cleaning_beam_agent-0_max: 506
    cleaning_beam_agent-0_mean: 375.82
    cleaning_beam_agent-0_min: 262
    cleaning_beam_agent-1_max: 472
    cleaning_beam_agent-1_mean: 256.57
    cleaning_beam_agent-1_min: 119
    cleaning_beam_agent-2_max: 602
    cleaning_beam_agent-2_mean: 390.33
    cleaning_beam_agent-2_min: 201
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 23.07
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 546
    cleaning_beam_agent-4_mean: 446.09
    cleaning_beam_agent-4_min: 275
    cleaning_beam_agent-5_max: 116
    cleaning_beam_agent-5_mean: 31.29
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.02
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-00-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1135.0000000000052
  episode_reward_mean: 965.7399999999851
  episode_reward_min: 518.0000000000119
  episodes_this_iter: 96
  episodes_total: 32928
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11718.164
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9469217658042908
        entropy_coeff: 0.0017600000137463212
        kl: 0.0024364623241126537
        model: {}
        policy_loss: -0.003259953111410141
        total_loss: -0.0029238350689411163
        vf_explained_var: 0.05803696811199188
        vf_loss: 20.026988983154297
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1365442276000977
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018306463025510311
        model: {}
        policy_loss: -0.0037748157046735287
        total_loss: -0.003456959268078208
        vf_explained_var: -0.08379796147346497
        vf_loss: 23.181730270385742
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0559821128845215
        entropy_coeff: 0.0017600000137463212
        kl: 0.0022597017232328653
        model: {}
        policy_loss: -0.00373907876200974
        total_loss: -0.0035030385479331017
        vf_explained_var: 0.017957821488380432
        vf_loss: 20.945697784423828
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.46989721059799194
        entropy_coeff: 0.0017600000137463212
        kl: 0.001305358950048685
        model: {}
        policy_loss: -0.0027312426827847958
        total_loss: -0.0015909611247479916
        vf_explained_var: 0.07612261176109314
        vf_loss: 19.673017501831055
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9484049081802368
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018181202467530966
        model: {}
        policy_loss: -0.003926149569451809
        total_loss: -0.003506606910377741
        vf_explained_var: 0.020235151052474976
        vf_loss: 20.88736343383789
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.667477011680603
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008497831877321005
        model: {}
        policy_loss: -0.003086811164394021
        total_loss: -0.0023459591902792454
        vf_explained_var: 0.09969039261341095
        vf_loss: 19.156126022338867
    load_time_ms: 13401.983
    num_steps_sampled: 32928000
    num_steps_trained: 32928000
    sample_time_ms: 99822.549
    update_time_ms: 15.499
  iterations_since_restore: 283
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.452247191011235
    ram_util_percent: 16.20056179775281
  pid: 30948
  policy_reward_max:
    agent-0: 189.16666666666643
    agent-1: 189.16666666666643
    agent-2: 189.16666666666643
    agent-3: 189.16666666666643
    agent-4: 189.16666666666643
    agent-5: 189.16666666666643
  policy_reward_mean:
    agent-0: 160.95666666666656
    agent-1: 160.95666666666656
    agent-2: 160.95666666666656
    agent-3: 160.95666666666656
    agent-4: 160.95666666666656
    agent-5: 160.95666666666656
  policy_reward_min:
    agent-0: 86.33333333333348
    agent-1: 86.33333333333348
    agent-2: 86.33333333333348
    agent-3: 86.33333333333348
    agent-4: 86.33333333333348
    agent-5: 86.33333333333348
  sampler_perf:
    mean_env_wait_ms: 27.209643832787854
    mean_inference_ms: 12.851939904921416
    mean_processing_ms: 57.38295205598411
  time_since_restore: 36892.87646269798
  time_this_iter_s: 125.59227848052979
  time_total_s: 46018.888276576996
  timestamp: 1637064014
  timesteps_since_restore: 27168000
  timesteps_this_iter: 96000
  timesteps_total: 32928000
  training_iteration: 343
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    343 |          46018.9 | 32928000 |   965.74 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 2.23
    apples_agent-0_min: 0
    apples_agent-1_max: 116
    apples_agent-1_mean: 35.04
    apples_agent-1_min: 0
    apples_agent-2_max: 75
    apples_agent-2_mean: 6.86
    apples_agent-2_min: 0
    apples_agent-3_max: 179
    apples_agent-3_mean: 69.96
    apples_agent-3_min: 36
    apples_agent-4_max: 34
    apples_agent-4_mean: 2.16
    apples_agent-4_min: 0
    apples_agent-5_max: 146
    apples_agent-5_mean: 89.85
    apples_agent-5_min: 45
    cleaning_beam_agent-0_max: 507
    cleaning_beam_agent-0_mean: 390.44
    cleaning_beam_agent-0_min: 278
    cleaning_beam_agent-1_max: 444
    cleaning_beam_agent-1_mean: 254.5
    cleaning_beam_agent-1_min: 129
    cleaning_beam_agent-2_max: 654
    cleaning_beam_agent-2_mean: 425.01
    cleaning_beam_agent-2_min: 214
    cleaning_beam_agent-3_max: 50
    cleaning_beam_agent-3_mean: 20.47
    cleaning_beam_agent-3_min: 9
    cleaning_beam_agent-4_max: 532
    cleaning_beam_agent-4_mean: 444.49
    cleaning_beam_agent-4_min: 360
    cleaning_beam_agent-5_max: 181
    cleaning_beam_agent-5_mean: 34.68
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-02-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1144.9999999999948
  episode_reward_mean: 990.6799999999863
  episode_reward_min: 635.9999999999951
  episodes_this_iter: 96
  episodes_total: 33024
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11701.871
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9512697458267212
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018589545506983995
        model: {}
        policy_loss: -0.0028564888052642345
        total_loss: -0.002603094559162855
        vf_explained_var: 0.08296345174312592
        vf_loss: 19.276302337646484
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1311606168746948
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011343727819621563
        model: {}
        policy_loss: -0.0038494556210935116
        total_loss: -0.003424371127039194
        vf_explained_var: -0.12357890605926514
        vf_loss: 24.159305572509766
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0426197052001953
        entropy_coeff: 0.0017600000137463212
        kl: 0.0017696369905024767
        model: {}
        policy_loss: -0.0033972810488194227
        total_loss: -0.0031059354078024626
        vf_explained_var: -0.00517137348651886
        vf_loss: 21.26357078552246
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44270068407058716
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010662178974598646
        model: {}
        policy_loss: -0.0024999072775244713
        total_loss: -0.0013534465106204152
        vf_explained_var: 0.08725827932357788
        vf_loss: 19.256160736083984
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9500239491462708
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020009514410048723
        model: {}
        policy_loss: -0.003510288195684552
        total_loss: -0.0030130865052342415
        vf_explained_var: -0.027309805154800415
        vf_loss: 21.69241714477539
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6536010503768921
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013108885614201427
        model: {}
        policy_loss: -0.002967675682157278
        total_loss: -0.0022463123314082623
        vf_explained_var: 0.11457766592502594
        vf_loss: 18.717010498046875
    load_time_ms: 13422.254
    num_steps_sampled: 33024000
    num_steps_trained: 33024000
    sample_time_ms: 99720.571
    update_time_ms: 15.515
  iterations_since_restore: 284
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.22134831460674
    ram_util_percent: 16.20112359550562
  pid: 30948
  policy_reward_max:
    agent-0: 190.8333333333333
    agent-1: 190.8333333333333
    agent-2: 190.8333333333333
    agent-3: 190.8333333333333
    agent-4: 190.8333333333333
    agent-5: 190.8333333333333
  policy_reward_mean:
    agent-0: 165.1133333333332
    agent-1: 165.1133333333332
    agent-2: 165.1133333333332
    agent-3: 165.1133333333332
    agent-4: 165.1133333333332
    agent-5: 165.1133333333332
  policy_reward_min:
    agent-0: 106.00000000000021
    agent-1: 106.00000000000021
    agent-2: 106.00000000000021
    agent-3: 106.00000000000021
    agent-4: 106.00000000000021
    agent-5: 106.00000000000021
  sampler_perf:
    mean_env_wait_ms: 27.210473224575235
    mean_inference_ms: 12.851369344981093
    mean_processing_ms: 57.37913641570561
  time_since_restore: 37017.47500705719
  time_this_iter_s: 124.59854435920715
  time_total_s: 46143.4868209362
  timestamp: 1637064139
  timesteps_since_restore: 27264000
  timesteps_this_iter: 96000
  timesteps_total: 33024000
  training_iteration: 344
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    344 |          46143.5 | 33024000 |   990.68 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 25
    apples_agent-0_mean: 0.83
    apples_agent-0_min: 0
    apples_agent-1_max: 105
    apples_agent-1_mean: 31.37
    apples_agent-1_min: 0
    apples_agent-2_max: 95
    apples_agent-2_mean: 7.97
    apples_agent-2_min: 0
    apples_agent-3_max: 125
    apples_agent-3_mean: 73.85
    apples_agent-3_min: 36
    apples_agent-4_max: 60
    apples_agent-4_mean: 2.8
    apples_agent-4_min: 0
    apples_agent-5_max: 161
    apples_agent-5_mean: 88.24
    apples_agent-5_min: 42
    cleaning_beam_agent-0_max: 517
    cleaning_beam_agent-0_mean: 390.97
    cleaning_beam_agent-0_min: 249
    cleaning_beam_agent-1_max: 552
    cleaning_beam_agent-1_mean: 259.45
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 641
    cleaning_beam_agent-2_mean: 425.57
    cleaning_beam_agent-2_min: 275
    cleaning_beam_agent-3_max: 51
    cleaning_beam_agent-3_mean: 20.29
    cleaning_beam_agent-3_min: 8
    cleaning_beam_agent-4_max: 525
    cleaning_beam_agent-4_mean: 440.09
    cleaning_beam_agent-4_min: 333
    cleaning_beam_agent-5_max: 89
    cleaning_beam_agent-5_mean: 29.61
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 1
    fire_beam_agent-2_mean: 0.01
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-04-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1145.999999999995
  episode_reward_mean: 999.1399999999862
  episode_reward_min: 684.0000000000003
  episodes_this_iter: 96
  episodes_total: 33120
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11715.297
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9473493695259094
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011802847729995847
        model: {}
        policy_loss: -0.0031489734537899494
        total_loss: -0.0029199067503213882
        vf_explained_var: 0.04580846428871155
        vf_loss: 18.96396827697754
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1456959247589111
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011391884181648493
        model: {}
        policy_loss: -0.00360454386100173
        total_loss: -0.0034173307940363884
        vf_explained_var: -0.08327233791351318
        vf_loss: 22.036365509033203
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0454695224761963
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011305921943858266
        model: {}
        policy_loss: -0.0035862550139427185
        total_loss: -0.0034156162291765213
        vf_explained_var: -0.0029452145099639893
        vf_loss: 20.106637954711914
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44168803095817566
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008785121026448905
        model: {}
        policy_loss: -0.0022017471492290497
        total_loss: -0.0010727012995630503
        vf_explained_var: 0.03768736124038696
        vf_loss: 19.064176559448242
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.957301139831543
        entropy_coeff: 0.0017600000137463212
        kl: 0.002340089762583375
        model: {}
        policy_loss: -0.003707360476255417
        total_loss: -0.0033859480172395706
        vf_explained_var: -0.0034214258193969727
        vf_loss: 20.062639236450195
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6168476939201355
        entropy_coeff: 0.0017600000137463212
        kl: 0.001632035244256258
        model: {}
        policy_loss: -0.0028258031234145164
        total_loss: -0.002136831171810627
        vf_explained_var: 0.1024194061756134
        vf_loss: 17.746227264404297
    load_time_ms: 13393.872
    num_steps_sampled: 33120000
    num_steps_trained: 33120000
    sample_time_ms: 99750.204
    update_time_ms: 15.536
  iterations_since_restore: 285
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.625
    ram_util_percent: 16.130681818181817
  pid: 30948
  policy_reward_max:
    agent-0: 190.9999999999994
    agent-1: 190.9999999999994
    agent-2: 190.9999999999994
    agent-3: 190.9999999999994
    agent-4: 190.9999999999994
    agent-5: 190.9999999999994
  policy_reward_mean:
    agent-0: 166.5233333333332
    agent-1: 166.5233333333332
    agent-2: 166.5233333333332
    agent-3: 166.5233333333332
    agent-4: 166.5233333333332
    agent-5: 166.5233333333332
  policy_reward_min:
    agent-0: 114.00000000000013
    agent-1: 114.00000000000013
    agent-2: 114.00000000000013
    agent-3: 114.00000000000013
    agent-4: 114.00000000000013
    agent-5: 114.00000000000013
  sampler_perf:
    mean_env_wait_ms: 27.210505022895475
    mean_inference_ms: 12.850830643474621
    mean_processing_ms: 57.374297121215015
  time_since_restore: 37141.41560602188
  time_this_iter_s: 123.94059896469116
  time_total_s: 46267.427419900894
  timestamp: 1637064263
  timesteps_since_restore: 27360000
  timesteps_this_iter: 96000
  timesteps_total: 33120000
  training_iteration: 345
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.9/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    345 |          46267.4 | 33120000 |   999.14 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 34
    apples_agent-0_mean: 1.47
    apples_agent-0_min: 0
    apples_agent-1_max: 140
    apples_agent-1_mean: 33.62
    apples_agent-1_min: 0
    apples_agent-2_max: 115
    apples_agent-2_mean: 7.84
    apples_agent-2_min: 0
    apples_agent-3_max: 117
    apples_agent-3_mean: 67.79
    apples_agent-3_min: 33
    apples_agent-4_max: 91
    apples_agent-4_mean: 3.72
    apples_agent-4_min: 0
    apples_agent-5_max: 121
    apples_agent-5_mean: 85.65
    apples_agent-5_min: 48
    cleaning_beam_agent-0_max: 475
    cleaning_beam_agent-0_mean: 377.98
    cleaning_beam_agent-0_min: 244
    cleaning_beam_agent-1_max: 525
    cleaning_beam_agent-1_mean: 255.24
    cleaning_beam_agent-1_min: 110
    cleaning_beam_agent-2_max: 646
    cleaning_beam_agent-2_mean: 421.56
    cleaning_beam_agent-2_min: 260
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 21.41
    cleaning_beam_agent-3_min: 5
    cleaning_beam_agent-4_max: 525
    cleaning_beam_agent-4_mean: 429.2
    cleaning_beam_agent-4_min: 231
    cleaning_beam_agent-5_max: 121
    cleaning_beam_agent-5_mean: 30.14
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.01
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-06-27
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1151.0000000000016
  episode_reward_mean: 983.4899999999856
  episode_reward_min: 527.0000000000136
  episodes_this_iter: 96
  episodes_total: 33216
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11695.047
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9731360673904419
        entropy_coeff: 0.0017600000137463212
        kl: 0.001821405254304409
        model: {}
        policy_loss: -0.003093196079134941
        total_loss: -0.0026679541915655136
        vf_explained_var: 0.050581932067871094
        vf_loss: 21.379594802856445
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1452887058258057
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018365937285125256
        model: {}
        policy_loss: -0.0038794148713350296
        total_loss: -0.003490426577627659
        vf_explained_var: -0.053908973932266235
        vf_loss: 24.046972274780273
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0418859720230103
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019113598391413689
        model: {}
        policy_loss: -0.0036070942878723145
        total_loss: -0.003142720088362694
        vf_explained_var: -0.019548088312149048
        vf_loss: 22.980972290039062
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44401729106903076
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011491136392578483
        model: {}
        policy_loss: -0.0025178794749081135
        total_loss: -0.0012852908112108707
        vf_explained_var: 0.10709980130195618
        vf_loss: 20.140575408935547
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9507156014442444
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019507264951243997
        model: {}
        policy_loss: -0.00382399233058095
        total_loss: -0.003315191948786378
        vf_explained_var: 0.03384721279144287
        vf_loss: 21.820594787597656
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6148742437362671
        entropy_coeff: 0.0017600000137463212
        kl: 0.000721669290214777
        model: {}
        policy_loss: -0.0027888924814760685
        total_loss: -0.0018515277188271284
        vf_explained_var: 0.10255758464336395
        vf_loss: 20.195453643798828
    load_time_ms: 13395.265
    num_steps_sampled: 33216000
    num_steps_trained: 33216000
    sample_time_ms: 99704.922
    update_time_ms: 15.54
  iterations_since_restore: 286
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.463428571428572
    ram_util_percent: 16.198285714285717
  pid: 30948
  policy_reward_max:
    agent-0: 191.83333333333334
    agent-1: 191.83333333333334
    agent-2: 191.83333333333334
    agent-3: 191.83333333333334
    agent-4: 191.83333333333334
    agent-5: 191.83333333333334
  policy_reward_mean:
    agent-0: 163.91499999999985
    agent-1: 163.91499999999985
    agent-2: 163.91499999999985
    agent-3: 163.91499999999985
    agent-4: 163.91499999999985
    agent-5: 163.91499999999985
  policy_reward_min:
    agent-0: 87.8333333333336
    agent-1: 87.8333333333336
    agent-2: 87.8333333333336
    agent-3: 87.8333333333336
    agent-4: 87.8333333333336
    agent-5: 87.8333333333336
  sampler_perf:
    mean_env_wait_ms: 27.2101981166416
    mean_inference_ms: 12.850331728251147
    mean_processing_ms: 57.370119505201465
  time_since_restore: 37264.66765356064
  time_this_iter_s: 123.25204753875732
  time_total_s: 46390.67946743965
  timestamp: 1637064387
  timesteps_since_restore: 27456000
  timesteps_this_iter: 96000
  timesteps_total: 33216000
  training_iteration: 346
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    346 |          46390.7 | 33216000 |   983.49 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 45
    apples_agent-0_mean: 2.2
    apples_agent-0_min: 0
    apples_agent-1_max: 95
    apples_agent-1_mean: 29.59
    apples_agent-1_min: 0
    apples_agent-2_max: 77
    apples_agent-2_mean: 8.35
    apples_agent-2_min: 0
    apples_agent-3_max: 123
    apples_agent-3_mean: 67.97
    apples_agent-3_min: 40
    apples_agent-4_max: 68
    apples_agent-4_mean: 2.88
    apples_agent-4_min: 0
    apples_agent-5_max: 145
    apples_agent-5_mean: 87.48
    apples_agent-5_min: 54
    cleaning_beam_agent-0_max: 465
    cleaning_beam_agent-0_mean: 360.0
    cleaning_beam_agent-0_min: 215
    cleaning_beam_agent-1_max: 469
    cleaning_beam_agent-1_mean: 250.81
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 626
    cleaning_beam_agent-2_mean: 422.96
    cleaning_beam_agent-2_min: 183
    cleaning_beam_agent-3_max: 57
    cleaning_beam_agent-3_mean: 21.3
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 518
    cleaning_beam_agent-4_mean: 434.0
    cleaning_beam_agent-4_min: 329
    cleaning_beam_agent-5_max: 174
    cleaning_beam_agent-5_mean: 35.17
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.01
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.02
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-08-32
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1127.0000000000002
  episode_reward_mean: 975.6199999999869
  episode_reward_min: 572.9999999999992
  episodes_this_iter: 96
  episodes_total: 33312
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11688.79
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9911048412322998
        entropy_coeff: 0.0017600000137463212
        kl: 0.002868730342015624
        model: {}
        policy_loss: -0.0033496501855552197
        total_loss: -0.0030949285719543695
        vf_explained_var: 0.04601135849952698
        vf_loss: 19.990650177001953
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.156571865081787
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015672669978812337
        model: {}
        policy_loss: -0.004037819802761078
        total_loss: -0.0037902221083641052
        vf_explained_var: -0.0810396671295166
        vf_loss: 22.831632614135742
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0243735313415527
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016889185644686222
        model: {}
        policy_loss: -0.003679822199046612
        total_loss: -0.003472020849585533
        vf_explained_var: 0.03928281366825104
        vf_loss: 20.107019424438477
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44592350721359253
        entropy_coeff: 0.0017600000137463212
        kl: 0.0009083616314455867
        model: {}
        policy_loss: -0.002535770647227764
        total_loss: -0.0013845227658748627
        vf_explained_var: 0.07435484230518341
        vf_loss: 19.360729217529297
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9551034569740295
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014433145988732576
        model: {}
        policy_loss: -0.0035357936285436153
        total_loss: -0.003104663686826825
        vf_explained_var: -0.007973805069923401
        vf_loss: 21.12112808227539
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6311296224594116
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011924111749976873
        model: {}
        policy_loss: -0.0030789021402597427
        total_loss: -0.0023352240677922964
        vf_explained_var: 0.1108241081237793
        vf_loss: 18.544666290283203
    load_time_ms: 13396.093
    num_steps_sampled: 33312000
    num_steps_trained: 33312000
    sample_time_ms: 99673.901
    update_time_ms: 14.569
  iterations_since_restore: 287
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.416292134831462
    ram_util_percent: 16.140449438202246
  pid: 30948
  policy_reward_max:
    agent-0: 187.83333333333334
    agent-1: 187.83333333333334
    agent-2: 187.83333333333334
    agent-3: 187.83333333333334
    agent-4: 187.83333333333334
    agent-5: 187.83333333333334
  policy_reward_mean:
    agent-0: 162.6033333333332
    agent-1: 162.6033333333332
    agent-2: 162.6033333333332
    agent-3: 162.6033333333332
    agent-4: 162.6033333333332
    agent-5: 162.6033333333332
  policy_reward_min:
    agent-0: 95.49999999999997
    agent-1: 95.49999999999997
    agent-2: 95.49999999999997
    agent-3: 95.49999999999997
    agent-4: 95.49999999999997
    agent-5: 95.49999999999997
  sampler_perf:
    mean_env_wait_ms: 27.210691544783568
    mean_inference_ms: 12.849912117755236
    mean_processing_ms: 57.36739768052813
  time_since_restore: 37389.73434948921
  time_this_iter_s: 125.06669592857361
  time_total_s: 46515.746163368225
  timestamp: 1637064512
  timesteps_since_restore: 27552000
  timesteps_this_iter: 96000
  timesteps_total: 33312000
  training_iteration: 347
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    347 |          46515.7 | 33312000 |   975.62 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 24
    apples_agent-0_mean: 0.68
    apples_agent-0_min: 0
    apples_agent-1_max: 111
    apples_agent-1_mean: 33.17
    apples_agent-1_min: 0
    apples_agent-2_max: 106
    apples_agent-2_mean: 7.89
    apples_agent-2_min: 0
    apples_agent-3_max: 122
    apples_agent-3_mean: 73.47
    apples_agent-3_min: 41
    apples_agent-4_max: 78
    apples_agent-4_mean: 2.88
    apples_agent-4_min: 0
    apples_agent-5_max: 135
    apples_agent-5_mean: 89.33
    apples_agent-5_min: 54
    cleaning_beam_agent-0_max: 447
    cleaning_beam_agent-0_mean: 356.77
    cleaning_beam_agent-0_min: 253
    cleaning_beam_agent-1_max: 480
    cleaning_beam_agent-1_mean: 258.33
    cleaning_beam_agent-1_min: 108
    cleaning_beam_agent-2_max: 605
    cleaning_beam_agent-2_mean: 432.89
    cleaning_beam_agent-2_min: 196
    cleaning_beam_agent-3_max: 46
    cleaning_beam_agent-3_mean: 20.09
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 529
    cleaning_beam_agent-4_mean: 432.7
    cleaning_beam_agent-4_min: 292
    cleaning_beam_agent-5_max: 155
    cleaning_beam_agent-5_mean: 35.1
    cleaning_beam_agent-5_min: 6
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-10-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1191.0000000000027
  episode_reward_mean: 993.7699999999866
  episode_reward_min: 516.0000000000131
  episodes_this_iter: 96
  episodes_total: 33408
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11666.311
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.970988392829895
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018033210653811693
        model: {}
        policy_loss: -0.0028837225399911404
        total_loss: -0.002639635931700468
        vf_explained_var: 0.05038551986217499
        vf_loss: 19.530277252197266
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1306092739105225
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011354249436408281
        model: {}
        policy_loss: -0.0035131536424160004
        total_loss: -0.003129946766421199
        vf_explained_var: -0.12695160508155823
        vf_loss: 23.730789184570312
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0333905220031738
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014124150620773435
        model: {}
        policy_loss: -0.0034732255153357983
        total_loss: -0.0032665375620126724
        vf_explained_var: 0.019094377756118774
        vf_loss: 20.254528045654297
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44363364577293396
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012708342401310802
        model: {}
        policy_loss: -0.00262665469199419
        total_loss: -0.0014679646119475365
        vf_explained_var: 0.05783194303512573
        vf_loss: 19.39487075805664
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9561902284622192
        entropy_coeff: 0.0017600000137463212
        kl: 0.002153646433725953
        model: {}
        policy_loss: -0.0040580197237432
        total_loss: -0.003694424405694008
        vf_explained_var: 0.015259996056556702
        vf_loss: 20.46490478515625
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6283513307571411
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018750947201624513
        model: {}
        policy_loss: -0.003045042045414448
        total_loss: -0.002294725738465786
        vf_explained_var: 0.09793378412723541
        vf_loss: 18.56215476989746
    load_time_ms: 13392.749
    num_steps_sampled: 33408000
    num_steps_trained: 33408000
    sample_time_ms: 99758.057
    update_time_ms: 14.754
  iterations_since_restore: 288
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.405617977528088
    ram_util_percent: 16.182022471910113
  pid: 30948
  policy_reward_max:
    agent-0: 198.4999999999999
    agent-1: 198.4999999999999
    agent-2: 198.4999999999999
    agent-3: 198.4999999999999
    agent-4: 198.4999999999999
    agent-5: 198.4999999999999
  policy_reward_mean:
    agent-0: 165.62833333333322
    agent-1: 165.62833333333322
    agent-2: 165.62833333333322
    agent-3: 165.62833333333322
    agent-4: 165.62833333333322
    agent-5: 165.62833333333322
  policy_reward_min:
    agent-0: 86.00000000000017
    agent-1: 86.00000000000017
    agent-2: 86.00000000000017
    agent-3: 86.00000000000017
    agent-4: 86.00000000000017
    agent-5: 86.00000000000017
  sampler_perf:
    mean_env_wait_ms: 27.21121057497611
    mean_inference_ms: 12.849454555227071
    mean_processing_ms: 57.364867412524006
  time_since_restore: 37514.39538693428
  time_this_iter_s: 124.66103744506836
  time_total_s: 46640.40720081329
  timestamp: 1637064637
  timesteps_since_restore: 27648000
  timesteps_this_iter: 96000
  timesteps_total: 33408000
  training_iteration: 348
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    348 |          46640.4 | 33408000 |   993.77 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 37
    apples_agent-0_mean: 1.63
    apples_agent-0_min: 0
    apples_agent-1_max: 112
    apples_agent-1_mean: 29.01
    apples_agent-1_min: 0
    apples_agent-2_max: 52
    apples_agent-2_mean: 7.69
    apples_agent-2_min: 0
    apples_agent-3_max: 132
    apples_agent-3_mean: 71.51
    apples_agent-3_min: 39
    apples_agent-4_max: 50
    apples_agent-4_mean: 2.71
    apples_agent-4_min: 0
    apples_agent-5_max: 158
    apples_agent-5_mean: 91.19
    apples_agent-5_min: 60
    cleaning_beam_agent-0_max: 471
    cleaning_beam_agent-0_mean: 364.45
    cleaning_beam_agent-0_min: 241
    cleaning_beam_agent-1_max: 498
    cleaning_beam_agent-1_mean: 255.07
    cleaning_beam_agent-1_min: 111
    cleaning_beam_agent-2_max: 675
    cleaning_beam_agent-2_mean: 428.11
    cleaning_beam_agent-2_min: 206
    cleaning_beam_agent-3_max: 45
    cleaning_beam_agent-3_mean: 21.35
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 506
    cleaning_beam_agent-4_mean: 437.24
    cleaning_beam_agent-4_min: 326
    cleaning_beam_agent-5_max: 84
    cleaning_beam_agent-5_mean: 29.47
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-12-41
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1155.9999999999986
  episode_reward_mean: 992.3599999999856
  episode_reward_min: 698.9999999999818
  episodes_this_iter: 96
  episodes_total: 33504
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11639.94
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9764081239700317
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018402314744889736
        model: {}
        policy_loss: -0.0031761431600898504
        total_loss: -0.0029749504756182432
        vf_explained_var: 0.05160023272037506
        vf_loss: 19.196731567382812
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1494084596633911
        entropy_coeff: 0.0017600000137463212
        kl: 0.00167113880161196
        model: {}
        policy_loss: -0.004045149311423302
        total_loss: -0.0038835741579532623
        vf_explained_var: -0.0630117654800415
        vf_loss: 21.8453311920166
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0212249755859375
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015309322625398636
        model: {}
        policy_loss: -0.003336381632834673
        total_loss: -0.003232041373848915
        vf_explained_var: 0.06068021059036255
        vf_loss: 19.016963958740234
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4487473964691162
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008766885730437934
        model: {}
        policy_loss: -0.002283666282892227
        total_loss: -0.0011639760341495275
        vf_explained_var: 0.05775377154350281
        vf_loss: 19.094839096069336
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9536249041557312
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020316005684435368
        model: {}
        policy_loss: -0.0039037468377500772
        total_loss: -0.0035618888214230537
        vf_explained_var: 0.008429005742073059
        vf_loss: 20.202402114868164
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.633658766746521
        entropy_coeff: 0.0017600000137463212
        kl: 0.000925883068703115
        model: {}
        policy_loss: -0.002820233814418316
        total_loss: -0.002135889371857047
        vf_explained_var: 0.10822857916355133
        vf_loss: 17.995887756347656
    load_time_ms: 13380.325
    num_steps_sampled: 33504000
    num_steps_trained: 33504000
    sample_time_ms: 99693.156
    update_time_ms: 14.834
  iterations_since_restore: 289
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.55795454545455
    ram_util_percent: 16.138068181818184
  pid: 30948
  policy_reward_max:
    agent-0: 192.66666666666646
    agent-1: 192.66666666666646
    agent-2: 192.66666666666646
    agent-3: 192.66666666666646
    agent-4: 192.66666666666646
    agent-5: 192.66666666666646
  policy_reward_mean:
    agent-0: 165.39333333333326
    agent-1: 165.39333333333326
    agent-2: 165.39333333333326
    agent-3: 165.39333333333326
    agent-4: 165.39333333333326
    agent-5: 165.39333333333326
  policy_reward_min:
    agent-0: 116.5000000000007
    agent-1: 116.5000000000007
    agent-2: 116.5000000000007
    agent-3: 116.5000000000007
    agent-4: 116.5000000000007
    agent-5: 116.5000000000007
  sampler_perf:
    mean_env_wait_ms: 27.210957923451964
    mean_inference_ms: 12.849029815276129
    mean_processing_ms: 57.36168743095149
  time_since_restore: 37638.7113339901
  time_this_iter_s: 124.31594705581665
  time_total_s: 46764.72314786911
  timestamp: 1637064761
  timesteps_since_restore: 27744000
  timesteps_this_iter: 96000
  timesteps_total: 33504000
  training_iteration: 349
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    349 |          46764.7 | 33504000 |   992.36 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 18
    apples_agent-0_mean: 1.16
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 31.13
    apples_agent-1_min: 0
    apples_agent-2_max: 69
    apples_agent-2_mean: 6.95
    apples_agent-2_min: 0
    apples_agent-3_max: 127
    apples_agent-3_mean: 71.11
    apples_agent-3_min: 30
    apples_agent-4_max: 27
    apples_agent-4_mean: 1.68
    apples_agent-4_min: 0
    apples_agent-5_max: 168
    apples_agent-5_mean: 90.31
    apples_agent-5_min: 46
    cleaning_beam_agent-0_max: 493
    cleaning_beam_agent-0_mean: 373.46
    cleaning_beam_agent-0_min: 231
    cleaning_beam_agent-1_max: 362
    cleaning_beam_agent-1_mean: 251.81
    cleaning_beam_agent-1_min: 131
    cleaning_beam_agent-2_max: 616
    cleaning_beam_agent-2_mean: 428.23
    cleaning_beam_agent-2_min: 216
    cleaning_beam_agent-3_max: 59
    cleaning_beam_agent-3_mean: 23.2
    cleaning_beam_agent-3_min: 7
    cleaning_beam_agent-4_max: 553
    cleaning_beam_agent-4_mean: 441.2
    cleaning_beam_agent-4_min: 311
    cleaning_beam_agent-5_max: 243
    cleaning_beam_agent-5_mean: 32.71
    cleaning_beam_agent-5_min: 10
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 2
    fire_beam_agent-1_mean: 0.03
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-14-47
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1125.000000000005
  episode_reward_mean: 980.579999999985
  episode_reward_min: 652.9999999999891
  episodes_this_iter: 96
  episodes_total: 33600
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11606.37
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9465989470481873
        entropy_coeff: 0.0017600000137463212
        kl: 0.0012599157635122538
        model: {}
        policy_loss: -0.0028812880627810955
        total_loss: -0.0025718859396874905
        vf_explained_var: 0.05524726212024689
        vf_loss: 19.754138946533203
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1398190259933472
        entropy_coeff: 0.0017600000137463212
        kl: 0.0018021776340901852
        model: {}
        policy_loss: -0.0036040358245372772
        total_loss: -0.003207284025847912
        vf_explained_var: -0.134898841381073
        vf_loss: 24.028316497802734
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0183027982711792
        entropy_coeff: 0.0017600000137463212
        kl: 0.001268314546905458
        model: {}
        policy_loss: -0.003202416468411684
        total_loss: -0.002950887195765972
        vf_explained_var: 0.02039186656475067
        vf_loss: 20.437406539916992
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4491329789161682
        entropy_coeff: 0.0017600000137463212
        kl: 0.000995142268948257
        model: {}
        policy_loss: -0.0026053637266159058
        total_loss: -0.0014085983857512474
        vf_explained_var: 0.047767817974090576
        vf_loss: 19.872398376464844
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9416168332099915
        entropy_coeff: 0.0017600000137463212
        kl: 0.0019028941169381142
        model: {}
        policy_loss: -0.003743607085198164
        total_loss: -0.0032805425580590963
        vf_explained_var: -0.013788089156150818
        vf_loss: 21.203109741210938
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6507328748703003
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008207730716094375
        model: {}
        policy_loss: -0.0025841430760920048
        total_loss: -0.0019017045851796865
        vf_explained_var: 0.12365458905696869
        vf_loss: 18.277297973632812
    load_time_ms: 13359.692
    num_steps_sampled: 33600000
    num_steps_trained: 33600000
    sample_time_ms: 99927.365
    update_time_ms: 15.175
  iterations_since_restore: 290
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.13222222222222
    ram_util_percent: 16.190000000000005
  pid: 30948
  policy_reward_max:
    agent-0: 187.49999999999943
    agent-1: 187.49999999999943
    agent-2: 187.49999999999943
    agent-3: 187.49999999999943
    agent-4: 187.49999999999943
    agent-5: 187.49999999999943
  policy_reward_mean:
    agent-0: 163.42999999999992
    agent-1: 163.42999999999992
    agent-2: 163.42999999999992
    agent-3: 163.42999999999992
    agent-4: 163.42999999999992
    agent-5: 163.42999999999992
  policy_reward_min:
    agent-0: 108.83333333333368
    agent-1: 108.83333333333368
    agent-2: 108.83333333333368
    agent-3: 108.83333333333368
    agent-4: 108.83333333333368
    agent-5: 108.83333333333368
  sampler_perf:
    mean_env_wait_ms: 27.2114619211868
    mean_inference_ms: 12.848701758443688
    mean_processing_ms: 57.359658609706486
  time_since_restore: 37764.951691150665
  time_this_iter_s: 126.24035716056824
  time_total_s: 46890.96350502968
  timestamp: 1637064887
  timesteps_since_restore: 27840000
  timesteps_this_iter: 96000
  timesteps_total: 33600000
  training_iteration: 350
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.7/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    350 |            46891 | 33600000 |   980.58 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 56
    apples_agent-0_mean: 2.67
    apples_agent-0_min: 0
    apples_agent-1_max: 93
    apples_agent-1_mean: 27.13
    apples_agent-1_min: 0
    apples_agent-2_max: 58
    apples_agent-2_mean: 4.61
    apples_agent-2_min: 0
    apples_agent-3_max: 116
    apples_agent-3_mean: 70.18
    apples_agent-3_min: 38
    apples_agent-4_max: 59
    apples_agent-4_mean: 3.64
    apples_agent-4_min: 0
    apples_agent-5_max: 128
    apples_agent-5_mean: 86.76
    apples_agent-5_min: 46
    cleaning_beam_agent-0_max: 482
    cleaning_beam_agent-0_mean: 368.53
    cleaning_beam_agent-0_min: 229
    cleaning_beam_agent-1_max: 461
    cleaning_beam_agent-1_mean: 261.9
    cleaning_beam_agent-1_min: 91
    cleaning_beam_agent-2_max: 684
    cleaning_beam_agent-2_mean: 452.3
    cleaning_beam_agent-2_min: 194
    cleaning_beam_agent-3_max: 45
    cleaning_beam_agent-3_mean: 18.65
    cleaning_beam_agent-3_min: 4
    cleaning_beam_agent-4_max: 520
    cleaning_beam_agent-4_mean: 440.34
    cleaning_beam_agent-4_min: 259
    cleaning_beam_agent-5_max: 264
    cleaning_beam_agent-5_mean: 38.2
    cleaning_beam_agent-5_min: 3
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 1
    fire_beam_agent-3_mean: 0.01
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 1
    fire_beam_agent-5_mean: 0.02
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-16-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1203.9999999999995
  episode_reward_mean: 986.4599999999862
  episode_reward_min: 601.9999999999969
  episodes_this_iter: 96
  episodes_total: 33696
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11604.434
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9528595209121704
        entropy_coeff: 0.0017600000137463212
        kl: 0.0016606900608167052
        model: {}
        policy_loss: -0.002923146355897188
        total_loss: -0.00231129489839077
        vf_explained_var: 0.0382622629404068
        vf_loss: 22.888826370239258
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1487600803375244
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013753699604421854
        model: {}
        policy_loss: -0.0036016928497701883
        total_loss: -0.00293826125562191
        vf_explained_var: -0.12289121747016907
        vf_loss: 26.852468490600586
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0119138956069946
        entropy_coeff: 0.0017600000137463212
        kl: 0.00191887142136693
        model: {}
        policy_loss: -0.003196278354153037
        total_loss: -0.0026049823500216007
        vf_explained_var: -0.0018137544393539429
        vf_loss: 23.72262191772461
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4454794228076935
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010472401045262814
        model: {}
        policy_loss: -0.0028875921852886677
        total_loss: -0.0015948228538036346
        vf_explained_var: 0.12307573854923248
        vf_loss: 20.76815414428711
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9446208477020264
        entropy_coeff: 0.0017600000137463212
        kl: 0.0020022718235850334
        model: {}
        policy_loss: -0.0037942808121442795
        total_loss: -0.003142333822324872
        vf_explained_var: 0.025778397917747498
        vf_loss: 23.144824981689453
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6278924345970154
        entropy_coeff: 0.0017600000137463212
        kl: 0.001191378920339048
        model: {}
        policy_loss: -0.0030626622028648853
        total_loss: -0.002146666869521141
        vf_explained_var: 0.14531639218330383
        vf_loss: 20.210865020751953
    load_time_ms: 13341.687
    num_steps_sampled: 33696000
    num_steps_trained: 33696000
    sample_time_ms: 99732.797
    update_time_ms: 15.25
  iterations_since_restore: 291
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.517514124293786
    ram_util_percent: 16.112429378531075
  pid: 30948
  policy_reward_max:
    agent-0: 200.6666666666665
    agent-1: 200.6666666666665
    agent-2: 200.6666666666665
    agent-3: 200.6666666666665
    agent-4: 200.6666666666665
    agent-5: 200.6666666666665
  policy_reward_mean:
    agent-0: 164.40999999999988
    agent-1: 164.40999999999988
    agent-2: 164.40999999999988
    agent-3: 164.40999999999988
    agent-4: 164.40999999999988
    agent-5: 164.40999999999988
  policy_reward_min:
    agent-0: 100.33333333333381
    agent-1: 100.33333333333381
    agent-2: 100.33333333333381
    agent-3: 100.33333333333381
    agent-4: 100.33333333333381
    agent-5: 100.33333333333381
  sampler_perf:
    mean_env_wait_ms: 27.212243041093235
    mean_inference_ms: 12.848328864492641
    mean_processing_ms: 57.35633337453979
  time_since_restore: 37889.40263223648
  time_this_iter_s: 124.45094108581543
  time_total_s: 47015.414446115494
  timestamp: 1637065012
  timesteps_since_restore: 27936000
  timesteps_this_iter: 96000
  timesteps_total: 33696000
  training_iteration: 351
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    351 |          47015.4 | 33696000 |   986.46 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 53
    apples_agent-0_mean: 2.77
    apples_agent-0_min: 0
    apples_agent-1_max: 108
    apples_agent-1_mean: 31.23
    apples_agent-1_min: 0
    apples_agent-2_max: 53
    apples_agent-2_mean: 6.78
    apples_agent-2_min: 0
    apples_agent-3_max: 129
    apples_agent-3_mean: 69.1
    apples_agent-3_min: 35
    apples_agent-4_max: 67
    apples_agent-4_mean: 2.48
    apples_agent-4_min: 0
    apples_agent-5_max: 143
    apples_agent-5_mean: 86.42
    apples_agent-5_min: 51
    cleaning_beam_agent-0_max: 486
    cleaning_beam_agent-0_mean: 371.08
    cleaning_beam_agent-0_min: 256
    cleaning_beam_agent-1_max: 399
    cleaning_beam_agent-1_mean: 259.08
    cleaning_beam_agent-1_min: 77
    cleaning_beam_agent-2_max: 591
    cleaning_beam_agent-2_mean: 459.56
    cleaning_beam_agent-2_min: 216
    cleaning_beam_agent-3_max: 40
    cleaning_beam_agent-3_mean: 18.42
    cleaning_beam_agent-3_min: 3
    cleaning_beam_agent-4_max: 529
    cleaning_beam_agent-4_mean: 448.09
    cleaning_beam_agent-4_min: 318
    cleaning_beam_agent-5_max: 206
    cleaning_beam_agent-5_mean: 34.91
    cleaning_beam_agent-5_min: 4
    fire_beam_agent-0_max: 0
    fire_beam_agent-0_mean: 0.0
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 1
    fire_beam_agent-1_mean: 0.02
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 1
    fire_beam_agent-4_mean: 0.01
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-18-56
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1173.999999999987
  episode_reward_mean: 979.9999999999872
  episode_reward_min: 586.0000000000043
  episodes_this_iter: 96
  episodes_total: 33792
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11632.059
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9427287578582764
        entropy_coeff: 0.0017600000137463212
        kl: 0.001203519175760448
        model: {}
        policy_loss: -0.003131701610982418
        total_loss: -0.0026577948592603207
        vf_explained_var: -0.011029064655303955
        vf_loss: 21.331111907958984
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1465725898742676
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015735761262476444
        model: {}
        policy_loss: -0.0036724647507071495
        total_loss: -0.00337276142090559
        vf_explained_var: -0.08878076076507568
        vf_loss: 23.176740646362305
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0204027891159058
        entropy_coeff: 0.0017600000137463212
        kl: 0.0015297430800274014
        model: {}
        policy_loss: -0.003439180552959442
        total_loss: -0.0031465976499021053
        vf_explained_var: 0.00586777925491333
        vf_loss: 20.884923934936523
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.44145673513412476
        entropy_coeff: 0.0017600000137463212
        kl: 0.000882750260643661
        model: {}
        policy_loss: -0.002435593167319894
        total_loss: -0.0012602664064615965
        vf_explained_var: 0.07024236023426056
        vf_loss: 19.522899627685547
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9549047350883484
        entropy_coeff: 0.0017600000137463212
        kl: 0.0013989197323098779
        model: {}
        policy_loss: -0.0037183528766036034
        total_loss: -0.0033083739690482616
        vf_explained_var: 0.006373360753059387
        vf_loss: 20.906124114990234
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6321585774421692
        entropy_coeff: 0.0017600000137463212
        kl: 0.0010835828725248575
        model: {}
        policy_loss: -0.0028729038313031197
        total_loss: -0.0020745093934237957
        vf_explained_var: 0.08890193700790405
        vf_loss: 19.10992431640625
    load_time_ms: 13350.218
    num_steps_sampled: 33792000
    num_steps_trained: 33792000
    sample_time_ms: 99516.854
    update_time_ms: 14.453
  iterations_since_restore: 292
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.53409090909091
    ram_util_percent: 16.200000000000003
  pid: 30948
  policy_reward_max:
    agent-0: 195.66666666666666
    agent-1: 195.66666666666666
    agent-2: 195.66666666666666
    agent-3: 195.66666666666666
    agent-4: 195.66666666666666
    agent-5: 195.66666666666666
  policy_reward_mean:
    agent-0: 163.33333333333326
    agent-1: 163.33333333333326
    agent-2: 163.33333333333326
    agent-3: 163.33333333333326
    agent-4: 163.33333333333326
    agent-5: 163.33333333333326
  policy_reward_min:
    agent-0: 97.66666666666686
    agent-1: 97.66666666666686
    agent-2: 97.66666666666686
    agent-3: 97.66666666666686
    agent-4: 97.66666666666686
    agent-5: 97.66666666666686
  sampler_perf:
    mean_env_wait_ms: 27.212733078303245
    mean_inference_ms: 12.847902746685108
    mean_processing_ms: 57.352213403727575
  time_since_restore: 38013.23371720314
  time_this_iter_s: 123.83108496665955
  time_total_s: 47139.24553108215
  timestamp: 1637065136
  timesteps_since_restore: 28032000
  timesteps_this_iter: 96000
  timesteps_total: 33792000
  training_iteration: 352
  trial_id: '00000'
  
[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
== Status ==
Memory usage on this node: 29.8/188.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/37.7 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_1teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |       ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+----------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.43:30948 |    352 |          47139.2 | 33792000 |      980 |
+--------------------------------------+----------+-------------------+--------+------------------+----------+----------+


[2m[36m(pid=30948)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7fd898e94588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 40
    apples_agent-0_mean: 2.33
    apples_agent-0_min: 0
    apples_agent-1_max: 111
    apples_agent-1_mean: 30.09
    apples_agent-1_min: 0
    apples_agent-2_max: 100
    apples_agent-2_mean: 5.87
    apples_agent-2_min: 0
    apples_agent-3_max: 112
    apples_agent-3_mean: 68.65
    apples_agent-3_min: 30
    apples_agent-4_max: 44
    apples_agent-4_mean: 1.89
    apples_agent-4_min: 0
    apples_agent-5_max: 141
    apples_agent-5_mean: 91.83
    apples_agent-5_min: 61
    cleaning_beam_agent-0_max: 532
    cleaning_beam_agent-0_mean: 373.38
    cleaning_beam_agent-0_min: 201
    cleaning_beam_agent-1_max: 487
    cleaning_beam_agent-1_mean: 267.17
    cleaning_beam_agent-1_min: 112
    cleaning_beam_agent-2_max: 628
    cleaning_beam_agent-2_mean: 438.09
    cleaning_beam_agent-2_min: 242
    cleaning_beam_agent-3_max: 41
    cleaning_beam_agent-3_mean: 19.45
    cleaning_beam_agent-3_min: 6
    cleaning_beam_agent-4_max: 539
    cleaning_beam_agent-4_mean: 456.46
    cleaning_beam_agent-4_min: 354
    cleaning_beam_agent-5_max: 108
    cleaning_beam_agent-5_mean: 35.83
    cleaning_beam_agent-5_min: 7
    fire_beam_agent-0_max: 1
    fire_beam_agent-0_mean: 0.01
    fire_beam_agent-0_min: 0
    fire_beam_agent-1_max: 0
    fire_beam_agent-1_mean: 0.0
    fire_beam_agent-1_min: 0
    fire_beam_agent-2_max: 0
    fire_beam_agent-2_mean: 0.0
    fire_beam_agent-2_min: 0
    fire_beam_agent-3_max: 0
    fire_beam_agent-3_mean: 0.0
    fire_beam_agent-3_min: 0
    fire_beam_agent-4_max: 0
    fire_beam_agent-4_mean: 0.0
    fire_beam_agent-4_min: 0
    fire_beam_agent-5_max: 0
    fire_beam_agent-5_mean: 0.0
    fire_beam_agent-5_min: 0
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-16_07-21-01
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1163.0000000000082
  episode_reward_mean: 996.5199999999875
  episode_reward_min: 534.0000000000109
  episodes_this_iter: 96
  episodes_total: 33888
  experiment_id: a89a262966564d0080a7029a5a6b06f2
  experiment_tag: '0'
  hostname: gpu043
  info:
    grad_time_ms: 11575.992
    learner:
      agent-0:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9406545162200928
        entropy_coeff: 0.0017600000137463212
        kl: 0.001344769261777401
        model: {}
        policy_loss: -0.003187218215316534
        total_loss: -0.0029838879127055407
        vf_explained_var: 0.09312023222446442
        vf_loss: 18.58879852294922
      agent-1:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.1515718698501587
        entropy_coeff: 0.0017600000137463212
        kl: 0.001126709976233542
        model: {}
        policy_loss: -0.003638959489762783
        total_loss: -0.0033722934313118458
        vf_explained_var: -0.1058240532875061
        vf_loss: 22.93430519104004
      agent-2:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 1.0292617082595825
        entropy_coeff: 0.0017600000137463212
        kl: 0.0008229375234805048
        model: {}
        policy_loss: -0.0027445137966424227
        total_loss: -0.002478914801031351
        vf_explained_var: -0.019781216979026794
        vf_loss: 20.77100372314453
      agent-3:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.4281328320503235
        entropy_coeff: 0.0017600000137463212
        kl: 0.0011802602093666792
        model: {}
        policy_loss: -0.002436097012832761
        total_loss: -0.0013017263263463974
        vf_explained_var: 0.07272614538669586
        vf_loss: 18.87882423400879
      agent-4:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.9427776336669922
        entropy_coeff: 0.0017600000137463212
        kl: 0.0014795203460380435
        model: {}
        policy_loss: -0.003713756799697876
        total_loss: -0.0033327832352370024
        vf_explained_var: 0.00037926435470581055
        vf_loss: 20.402606964111328
      agent-5:
        cur_kl_coeff: 0.0
        cur_lr: 1.2000000424450263e-05
        entropy: 0.6295021176338196
        entropy_coeff: 0.0017600000137463212
        kl: 0.001059238100424409
        model: {}
        policy_loss: -0.0029463921673595905
        total_loss: -0.002222084905952215
        vf_explained_var: 0.09784971177577972
        vf_loss: 18.32229995727539
    load_time_ms: 13338.737
    num_steps_sampled: 33888000
    num_steps_trained: 33888000
    sample_time_ms: 99508.008
    update_time_ms: 14.593
  iterations_since_restore: 293
  node_ip: 172.17.8.43
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 28.492134831460675
    ram_util_percent: 16.175280898876405
  pid: 30948
  policy_reward_max:
    agent-0: 193.8333333333333
    agent-1: 193.8333333333333
    agent-2: 193.8333333333333
    agent-3: 193.8333333333333
    agent-4: 193.8333333333333
    agent-5: 193.8333333333333
  policy_reward_mean:
    agent-0: 166.08666666666656
    agent-1: 166.08666666666656
    agent-2: 166.08666666666656
    agent-3: 166.08666666666656
    agent-4: 166.08666666666656
    agent-5: 166.08666666666656
  policy_reward_min:
    agent-0: 89.00000000000028
    agent-1: 89.00000000000028
    agent-2: 89.00000000000028
    agent-3: 89.00000000000028
    agent-4: 89.00000000000028
    agent-5: 89.00000000000028
  sampler_perf:
    mean_env_wait_ms: 27.214315121976465
    mean_inference_ms: 12.847469951231405
    mean_processing_ms: 57.34948430934452
  time_since_restore: 38138.06257772446
  time_this_iter_s: 124.82886052131653
  time_total_s: 47264.07439160347
  timestamp: 1637065261
  timesteps_since_restore: 28128000
  timesteps_this_iter: 96000
  timesteps_total: 33888000
  training_iteration: 353
  trial_id: '00000'
  == Status ==
Memory usage on this node: 12.7/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.5 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------+
| Trial name                           | status   | loc   |
|--------------------------------------+----------+-------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  |       |
+--------------------------------------+----------+-------+


[2m[36m(pid=14194)[0m 2021-11-17 16:55:04,597	INFO trainer.py:421 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=14194)[0m 2021-11-17 16:55:04,604	INFO trainer.py:580 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=14194)[0m 2021-11-17 16:56:56,645	INFO trainable.py:180 -- _setup took 112.047 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
[2m[36m(pid=14194)[0m 2021-11-17 16:56:56,645	INFO trainable.py:217 -- Getting current IP.
[2m[36m(pid=14194)[0m 2021-11-17 16:56:56,645	WARNING util.py:37 -- Install gputil for GPU system monitoring.
[2m[36m(pid=14194)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f6ea36e8588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 3.78125
    apples_agent-0_min: 0
    apples_agent-1_max: 21
    apples_agent-1_mean: 3.2708333333333335
    apples_agent-1_min: 0
    apples_agent-2_max: 24
    apples_agent-2_mean: 3.0
    apples_agent-2_min: 0
    apples_agent-3_max: 19
    apples_agent-3_mean: 4.239583333333333
    apples_agent-3_min: 0
    apples_agent-4_max: 22
    apples_agent-4_mean: 2.6979166666666665
    apples_agent-4_min: 0
    apples_agent-5_max: 24
    apples_agent-5_mean: 3.6354166666666665
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 131
    cleaning_beam_agent-0_mean: 108.48958333333333
    cleaning_beam_agent-0_min: 85
    cleaning_beam_agent-1_max: 136
    cleaning_beam_agent-1_mean: 112.38541666666667
    cleaning_beam_agent-1_min: 95
    cleaning_beam_agent-2_max: 134
    cleaning_beam_agent-2_mean: 113.30208333333333
    cleaning_beam_agent-2_min: 92
    cleaning_beam_agent-3_max: 134
    cleaning_beam_agent-3_mean: 109.08333333333333
    cleaning_beam_agent-3_min: 88
    cleaning_beam_agent-4_max: 145
    cleaning_beam_agent-4_mean: 111.28125
    cleaning_beam_agent-4_min: 81
    cleaning_beam_agent-5_max: 131
    cleaning_beam_agent-5_mean: 110.28125
    cleaning_beam_agent-5_min: 86
    fire_beam_agent-0_max: 133
    fire_beam_agent-0_mean: 111.02083333333333
    fire_beam_agent-0_min: 92
    fire_beam_agent-1_max: 141
    fire_beam_agent-1_mean: 116.32291666666667
    fire_beam_agent-1_min: 93
    fire_beam_agent-2_max: 125
    fire_beam_agent-2_mean: 106.28125
    fire_beam_agent-2_min: 86
    fire_beam_agent-3_max: 132
    fire_beam_agent-3_mean: 111.86458333333333
    fire_beam_agent-3_min: 92
    fire_beam_agent-4_max: 134
    fire_beam_agent-4_mean: 110.1875
    fire_beam_agent-4_min: 92
    fire_beam_agent-5_max: 139
    fire_beam_agent-5_mean: 113.8125
    fire_beam_agent-5_min: 93
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_17-00-12
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -3830.000000000093
  episode_reward_mean: -7015.968749999927
  episode_reward_min: -11119.999999999984
  episodes_this_iter: 96
  episodes_total: 96
  experiment_id: ae22031055194750b007235b78e0550b
  experiment_tag: '0'
  hostname: gpu016
  info:
    grad_time_ms: 23086.34
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.1866912841796875
        entropy_coeff: 0.0017600000137463212
        kl: 0.010627888143062592
        model: {}
        policy_loss: -0.0037222234532237053
        total_loss: 1.357877492904663
        vf_explained_var: -0.0005560219287872314
        vf_loss: 13633.2294921875
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.1903514862060547
        entropy_coeff: 0.0017600000137463212
        kl: 0.008752795867621899
        model: {}
        policy_loss: -0.0017764894291758537
        total_loss: 1.345481514930725
        vf_explained_var: -0.0007203519344329834
        vf_loss: 13493.6240234375
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.1890907287597656
        entropy_coeff: 0.0017600000137463212
        kl: 0.008060602471232414
        model: {}
        policy_loss: -0.0016028019599616528
        total_loss: 1.3442418575286865
        vf_explained_var: -0.0006574392318725586
        vf_loss: 13480.853515625
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.1902379989624023
        entropy_coeff: 0.0017600000137463212
        kl: 0.007562801241874695
        model: {}
        policy_loss: -0.0017217282438650727
        total_loss: 1.1835200786590576
        vf_explained_var: -0.0007627308368682861
        vf_loss: 11875.8408203125
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.1887640953063965
        entropy_coeff: 0.0017600000137463212
        kl: 0.008009325712919235
        model: {}
        policy_loss: -0.0016920475754886866
        total_loss: 1.1753082275390625
        vf_explained_var: -0.0008751749992370605
        vf_loss: 11792.5068359375
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.0012600000482052565
        entropy: 2.1893503665924072
        entropy_coeff: 0.0017600000137463212
        kl: 0.0072855642065405846
        model: {}
        policy_loss: -0.0019135992042720318
        total_loss: 1.1798540353775024
        vf_explained_var: -0.0009114742279052734
        vf_loss: 11841.638671875
    load_time_ms: 55602.761
    num_steps_sampled: 96000
    num_steps_trained: 96000
    sample_time_ms: 103986.278
    update_time_ms: 3365.373
  iterations_since_restore: 1
  node_ip: 172.17.8.16
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.495340501792114
    ram_util_percent: 13.06415770609319
  pid: 14194
  policy_reward_max:
    agent-0: -665.666666666667
    agent-1: -665.666666666667
    agent-2: -665.666666666667
    agent-3: -509.9999999999977
    agent-4: -509.9999999999977
    agent-5: -509.9999999999977
  policy_reward_mean:
    agent-0: -1202.940972222222
    agent-1: -1202.940972222222
    agent-2: -1202.940972222222
    agent-3: -1135.715277777778
    agent-4: -1135.715277777778
    agent-5: -1135.715277777778
  policy_reward_min:
    agent-0: -2081.66666666666
    agent-1: -2081.66666666666
    agent-2: -2081.66666666666
    agent-3: -2030.3333333333346
    agent-4: -2030.3333333333346
    agent-5: -2030.3333333333346
  sampler_perf:
    mean_env_wait_ms: 25.970526151247427
    mean_inference_ms: 14.185279161184582
    mean_processing_ms: 57.135995491083726
  time_since_restore: 189.68025374412537
  time_this_iter_s: 189.68025374412537
  time_total_s: 189.68025374412537
  timestamp: 1637186412
  timesteps_since_restore: 96000
  timesteps_this_iter: 96000
  timesteps_total: 96000
  training_iteration: 1
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 23.1/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.5 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+-------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |    ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+-------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.16:14194 |      1 |           189.68 | 96000 | -7015.97 |
+--------------------------------------+----------+-------------------+--------+------------------+-------+----------+


[2m[36m(pid=14194)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f6ea36e8588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 27
    apples_agent-0_mean: 2.79
    apples_agent-0_min: 0
    apples_agent-1_max: 36
    apples_agent-1_mean: 4.83
    apples_agent-1_min: 0
    apples_agent-2_max: 23
    apples_agent-2_mean: 4.29
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 3.92
    apples_agent-3_min: 0
    apples_agent-4_max: 23
    apples_agent-4_mean: 4.34
    apples_agent-4_min: 0
    apples_agent-5_max: 22
    apples_agent-5_mean: 4.47
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 119
    cleaning_beam_agent-0_mean: 97.22
    cleaning_beam_agent-0_min: 77
    cleaning_beam_agent-1_max: 133
    cleaning_beam_agent-1_mean: 113.83
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 139
    cleaning_beam_agent-2_mean: 118.78
    cleaning_beam_agent-2_min: 92
    cleaning_beam_agent-3_max: 162
    cleaning_beam_agent-3_mean: 125.48
    cleaning_beam_agent-3_min: 100
    cleaning_beam_agent-4_max: 149
    cleaning_beam_agent-4_mean: 128.87
    cleaning_beam_agent-4_min: 96
    cleaning_beam_agent-5_max: 144
    cleaning_beam_agent-5_mean: 117.29
    cleaning_beam_agent-5_min: 95
    fire_beam_agent-0_max: 127
    fire_beam_agent-0_mean: 100.46
    fire_beam_agent-0_min: 79
    fire_beam_agent-1_max: 128
    fire_beam_agent-1_mean: 102.35
    fire_beam_agent-1_min: 82
    fire_beam_agent-2_max: 115
    fire_beam_agent-2_mean: 81.28
    fire_beam_agent-2_min: 56
    fire_beam_agent-3_max: 120
    fire_beam_agent-3_mean: 91.26
    fire_beam_agent-3_min: 65
    fire_beam_agent-4_max: 150
    fire_beam_agent-4_mean: 123.18
    fire_beam_agent-4_min: 92
    fire_beam_agent-5_max: 133
    fire_beam_agent-5_mean: 111.06
    fire_beam_agent-5_min: 85
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_17-03-06
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -3547.0000000000377
  episode_reward_mean: -6147.449999999956
  episode_reward_min: -9125.999999999975
  episodes_this_iter: 96
  episodes_total: 192
  experiment_id: ae22031055194750b007235b78e0550b
  experiment_tag: '0'
  hostname: gpu016
  info:
    grad_time_ms: 17548.895
    learner:
      agent-0:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.179593563079834
        entropy_coeff: 0.0017600000137463212
        kl: 0.0033971054945141077
        model: {}
        policy_loss: -0.0008901801193132997
        total_loss: 0.8717119693756104
        vf_explained_var: -0.0008379817008972168
        vf_loss: 8757.587890625
      agent-1:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.185238838195801
        entropy_coeff: 0.0017600000137463212
        kl: 0.004953812342137098
        model: {}
        policy_loss: -0.0016199545934796333
        total_loss: 0.8587506413459778
        vf_explained_var: -0.0008296668529510498
        vf_loss: 8632.2587890625
      agent-2:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.176588535308838
        entropy_coeff: 0.0017600000137463212
        kl: 0.003799037542194128
        model: {}
        policy_loss: -0.0009168818360194564
        total_loss: 0.8609315752983093
        vf_explained_var: -0.0008998513221740723
        vf_loss: 8649.1943359375
      agent-3:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.1774377822875977
        entropy_coeff: 0.0017600000137463212
        kl: 0.0038291597738862038
        model: {}
        policy_loss: -0.0008458620868623257
        total_loss: 0.7399321794509888
        vf_explained_var: -0.0009180605411529541
        vf_loss: 7438.4453125
      agent-4:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.191349983215332
        entropy_coeff: 0.0017600000137463212
        kl: 0.004982532933354378
        model: {}
        policy_loss: -0.0014377147890627384
        total_loss: 0.7349834442138672
        vf_explained_var: -0.0009692609310150146
        vf_loss: 7392.81494140625
      agent-5:
        cur_kl_coeff: 0.20000000298023224
        cur_lr: 0.001254009548574686
        entropy: 2.1860055923461914
        entropy_coeff: 0.0017600000137463212
        kl: 0.004597374238073826
        model: {}
        policy_loss: -0.0011880998499691486
        total_loss: 0.7449946403503418
        vf_explained_var: -0.0008712708950042725
        vf_loss: 7491.1064453125
    load_time_ms: 50142.174
    num_steps_sampled: 192000
    num_steps_trained: 192000
    sample_time_ms: 109437.29
    update_time_ms: 1699.406
  iterations_since_restore: 2
  node_ip: 172.17.8.16
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.681451612903224
    ram_util_percent: 14.140725806451615
  pid: 14194
  policy_reward_max:
    agent-0: -362.6666666666658
    agent-1: -362.6666666666658
    agent-2: -362.6666666666658
    agent-3: -459.33333333333275
    agent-4: -459.33333333333275
    agent-5: -459.33333333333275
  policy_reward_mean:
    agent-0: -1056.0133333333338
    agent-1: -1056.0133333333338
    agent-2: -1056.0133333333338
    agent-3: -993.1366666666675
    agent-4: -993.1366666666675
    agent-5: -993.1366666666675
  policy_reward_min:
    agent-0: -1903.9999999999968
    agent-1: -1903.9999999999968
    agent-2: -1903.9999999999968
    agent-3: -1556.9999999999982
    agent-4: -1556.9999999999982
    agent-5: -1556.9999999999982
  sampler_perf:
    mean_env_wait_ms: 25.977115483833987
    mean_inference_ms: 13.611686920832478
    mean_processing_ms: 56.95384024710522
  time_since_restore: 361.47910380363464
  time_this_iter_s: 171.79885005950928
  time_total_s: 361.47910380363464
  timestamp: 1637186586
  timesteps_since_restore: 192000
  timesteps_this_iter: 96000
  timesteps_total: 192000
  training_iteration: 2
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.8/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.5 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.16:14194 |      2 |          361.479 | 192000 | -6147.45 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


[2m[36m(pid=14194)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f6ea36e8588> -> 96 episodes
Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 28
    apples_agent-0_mean: 4.89
    apples_agent-0_min: 0
    apples_agent-1_max: 16
    apples_agent-1_mean: 4.15
    apples_agent-1_min: 0
    apples_agent-2_max: 22
    apples_agent-2_mean: 4.34
    apples_agent-2_min: 0
    apples_agent-3_max: 24
    apples_agent-3_mean: 3.57
    apples_agent-3_min: 0
    apples_agent-4_max: 40
    apples_agent-4_mean: 4.42
    apples_agent-4_min: 0
    apples_agent-5_max: 30
    apples_agent-5_mean: 3.69
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 121
    cleaning_beam_agent-0_mean: 97.49
    cleaning_beam_agent-0_min: 76
    cleaning_beam_agent-1_max: 143
    cleaning_beam_agent-1_mean: 117.18
    cleaning_beam_agent-1_min: 93
    cleaning_beam_agent-2_max: 133
    cleaning_beam_agent-2_mean: 108.81
    cleaning_beam_agent-2_min: 92
    cleaning_beam_agent-3_max: 150
    cleaning_beam_agent-3_mean: 130.29
    cleaning_beam_agent-3_min: 104
    cleaning_beam_agent-4_max: 154
    cleaning_beam_agent-4_mean: 123.64
    cleaning_beam_agent-4_min: 102
    cleaning_beam_agent-5_max: 141
    cleaning_beam_agent-5_mean: 119.81
    cleaning_beam_agent-5_min: 100
    fire_beam_agent-0_max: 125
    fire_beam_agent-0_mean: 96.26
    fire_beam_agent-0_min: 70
    fire_beam_agent-1_max: 113
    fire_beam_agent-1_mean: 86.49
    fire_beam_agent-1_min: 68
    fire_beam_agent-2_max: 94
    fire_beam_agent-2_mean: 68.02
    fire_beam_agent-2_min: 51
    fire_beam_agent-3_max: 107
    fire_beam_agent-3_mean: 84.56
    fire_beam_agent-3_min: 59
    fire_beam_agent-4_max: 143
    fire_beam_agent-4_mean: 115.44
    fire_beam_agent-4_min: 79
    fire_beam_agent-5_max: 129
    fire_beam_agent-5_mean: 95.57
    fire_beam_agent-5_min: 72
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_17-05-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -2910.0000000000086
  episode_reward_mean: -5430.019999999979
  episode_reward_min: -8209.999999999884
  episodes_this_iter: 96
  episodes_total: 288
  experiment_id: ae22031055194750b007235b78e0550b
  experiment_tag: '0'
  hostname: gpu016
  info:
    grad_time_ms: 15619.545
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012480191653594375
        entropy: 2.166707992553711
        entropy_coeff: 0.0017600000137463212
        kl: 0.01199321262538433
        model: {}
        policy_loss: -0.0020861057564616203
        total_loss: 0.4939529299736023
        vf_explained_var: -0.001551300287246704
        vf_loss: 4986.53076171875
      agent-1:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012480191653594375
        entropy: 2.180159568786621
        entropy_coeff: 0.0017600000137463212
        kl: 0.0033375646453350782
        model: {}
        policy_loss: -0.0009587911190465093
        total_loss: 0.4909195899963379
        vf_explained_var: -0.0016317367553710938
        vf_loss: 4953.81689453125
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012480191653594375
        entropy: 2.165282726287842
        entropy_coeff: 0.0017600000137463212
        kl: 0.005314050242304802
        model: {}
        policy_loss: -0.0012874118983745575
        total_loss: 0.4913182258605957
        vf_explained_var: -0.0016039609909057617
        vf_loss: 4958.8515625
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012480191653594375
        entropy: 2.1624505519866943
        entropy_coeff: 0.0017600000137463212
        kl: 0.007840772159397602
        model: {}
        policy_loss: -0.001395420404151082
        total_loss: 0.5593752861022949
        vf_explained_var: -0.0017693638801574707
        vf_loss: 5637.92578125
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012480191653594375
        entropy: 2.182311773300171
        entropy_coeff: 0.0017600000137463212
        kl: 0.00776305329054594
        model: {}
        policy_loss: -0.0005973787046968937
        total_loss: 0.5596048831939697
        vf_explained_var: -0.0015926659107208252
        vf_loss: 5632.66796875
      agent-5:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012480191653594375
        entropy: 2.1861836910247803
        entropy_coeff: 0.0017600000137463212
        kl: 0.00399397686123848
        model: {}
        policy_loss: -0.0012630987912416458
        total_loss: 0.5625007152557373
        vf_explained_var: -0.0015028715133666992
        vf_loss: 5672.12109375
    load_time_ms: 44581.704
    num_steps_sampled: 288000
    num_steps_trained: 288000
    sample_time_ms: 110875.889
    update_time_ms: 1157.523
  iterations_since_restore: 3
  node_ip: 172.17.8.16
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 23.779735682819382
    ram_util_percent: 13.876211453744492
  pid: 14194
  policy_reward_max:
    agent-0: -378.0
    agent-1: -378.0
    agent-2: -378.0
    agent-3: -578.6666666666671
    agent-4: -578.6666666666671
    agent-5: -578.6666666666671
  policy_reward_mean:
    agent-0: -884.0666666666674
    agent-1: -884.0666666666674
    agent-2: -884.0666666666674
    agent-3: -925.9400000000012
    agent-4: -925.9400000000012
    agent-5: -925.9400000000012
  policy_reward_min:
    agent-0: -1555.666666666666
    agent-1: -1555.666666666666
    agent-2: -1555.666666666666
    agent-3: -2031.6666666666617
    agent-4: -2031.6666666666617
    agent-5: -2031.6666666666617
  sampler_perf:
    mean_env_wait_ms: 25.81172247040934
    mean_inference_ms: 13.350003063056937
    mean_processing_ms: 56.84914495904417
  time_since_restore: 520.6568729877472
  time_this_iter_s: 159.17776918411255
  time_total_s: 520.6568729877472
  timestamp: 1637186745
  timesteps_since_restore: 288000
  timesteps_this_iter: 96000
  timesteps_total: 288000
  training_iteration: 3
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.5 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.16:14194 |      3 |          520.657 | 288000 | -5430.02 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=14194)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f6ea36e8588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 31
    apples_agent-0_mean: 4.74
    apples_agent-0_min: 0
    apples_agent-1_max: 17
    apples_agent-1_mean: 3.79
    apples_agent-1_min: 0
    apples_agent-2_max: 32
    apples_agent-2_mean: 4.04
    apples_agent-2_min: 0
    apples_agent-3_max: 25
    apples_agent-3_mean: 4.28
    apples_agent-3_min: 0
    apples_agent-4_max: 18
    apples_agent-4_mean: 4.88
    apples_agent-4_min: 0
    apples_agent-5_max: 16
    apples_agent-5_mean: 3.84
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 111
    cleaning_beam_agent-0_mean: 93.08
    cleaning_beam_agent-0_min: 71
    cleaning_beam_agent-1_max: 150
    cleaning_beam_agent-1_mean: 124.89
    cleaning_beam_agent-1_min: 100
    cleaning_beam_agent-2_max: 141
    cleaning_beam_agent-2_mean: 111.7
    cleaning_beam_agent-2_min: 81
    cleaning_beam_agent-3_max: 167
    cleaning_beam_agent-3_mean: 132.96
    cleaning_beam_agent-3_min: 104
    cleaning_beam_agent-4_max: 145
    cleaning_beam_agent-4_mean: 123.94
    cleaning_beam_agent-4_min: 96
    cleaning_beam_agent-5_max: 163
    cleaning_beam_agent-5_mean: 126.38
    cleaning_beam_agent-5_min: 104
    fire_beam_agent-0_max: 113
    fire_beam_agent-0_mean: 68.53
    fire_beam_agent-0_min: 48
    fire_beam_agent-1_max: 101
    fire_beam_agent-1_mean: 74.43
    fire_beam_agent-1_min: 49
    fire_beam_agent-2_max: 75
    fire_beam_agent-2_mean: 47.97
    fire_beam_agent-2_min: 27
    fire_beam_agent-3_max: 89
    fire_beam_agent-3_mean: 49.79
    fire_beam_agent-3_min: 26
    fire_beam_agent-4_max: 139
    fire_beam_agent-4_mean: 108.31
    fire_beam_agent-4_min: 87
    fire_beam_agent-5_max: 111
    fire_beam_agent-5_mean: 80.08
    fire_beam_agent-5_min: 61
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_17-08-37
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -2491.9999999999886
  episode_reward_mean: -4269.35000000002
  episode_reward_min: -7827.999999999971
  episodes_this_iter: 96
  episodes_total: 384
  experiment_id: ae22031055194750b007235b78e0550b
  experiment_tag: '0'
  hostname: gpu016
  info:
    grad_time_ms: 14648.248
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012420287821441889
        entropy: 2.1650679111480713
        entropy_coeff: 0.0017600000137463212
        kl: 0.0060532125644385815
        model: {}
        policy_loss: -0.0009632944129407406
        total_loss: 0.24848642945289612
        vf_explained_var: -0.002107292413711548
        vf_loss: 2526.549072265625
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012420287821441889
        entropy: 2.1508140563964844
        entropy_coeff: 0.0017600000137463212
        kl: 0.01346150878816843
        model: {}
        policy_loss: -0.0007004556246101856
        total_loss: 0.24859395623207092
        vf_explained_var: -0.0022961199283599854
        vf_loss: 2524.06787109375
      agent-2:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012420287821441889
        entropy: 2.1519765853881836
        entropy_coeff: 0.0017600000137463212
        kl: 0.004605322144925594
        model: {}
        policy_loss: -0.0010683344444260001
        total_loss: 0.24825245141983032
        vf_explained_var: -0.0021995604038238525
        vf_loss: 2526.4775390625
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012420287821441889
        entropy: 2.137699604034424
        entropy_coeff: 0.0017600000137463212
        kl: 0.007033948786556721
        model: {}
        policy_loss: -0.001842136261984706
        total_loss: 0.24189235270023346
        vf_explained_var: -0.0019072890281677246
        vf_loss: 2467.9345703125
      agent-4:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012420287821441889
        entropy: 2.1816325187683105
        entropy_coeff: 0.0017600000137463212
        kl: 0.004966414533555508
        model: {}
        policy_loss: -0.0015555857680737972
        total_loss: 0.24100053310394287
        vf_explained_var: -0.0020066499710083008
        vf_loss: 2458.99169921875
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012420287821441889
        entropy: 2.163874387741089
        entropy_coeff: 0.0017600000137463212
        kl: 0.015460462309420109
        model: {}
        policy_loss: -0.0015595769509673119
        total_loss: 0.2418135702610016
        vf_explained_var: -0.001976698637008667
        vf_loss: 2464.08544921875
    load_time_ms: 45524.591
    num_steps_sampled: 384000
    num_steps_trained: 384000
    sample_time_ms: 111057.478
    update_time_ms: 883.023
  iterations_since_restore: 4
  node_ip: 172.17.8.16
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 22.78530612244898
    ram_util_percent: 13.891020408163264
  pid: 14194
  policy_reward_max:
    agent-0: -287.66666666666623
    agent-1: -287.66666666666623
    agent-2: -287.66666666666623
    agent-3: -415.99999999999994
    agent-4: -415.99999999999994
    agent-5: -415.99999999999994
  policy_reward_mean:
    agent-0: -715.9666666666673
    agent-1: -715.9666666666673
    agent-2: -715.9666666666673
    agent-3: -707.1500000000008
    agent-4: -707.1500000000008
    agent-5: -707.1500000000008
  policy_reward_min:
    agent-0: -1555.666666666666
    agent-1: -1555.666666666666
    agent-2: -1555.666666666666
    agent-3: -1168.0000000000007
    agent-4: -1168.0000000000007
    agent-5: -1168.0000000000007
  sampler_perf:
    mean_env_wait_ms: 25.663496364825082
    mean_inference_ms: 13.238467935306552
    mean_processing_ms: 56.85797816462877
  time_since_restore: 692.485025882721
  time_this_iter_s: 171.82815289497375
  time_total_s: 692.485025882721
  timestamp: 1637186917
  timesteps_since_restore: 384000
  timesteps_this_iter: 96000
  timesteps_total: 384000
  training_iteration: 4
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.6/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.5 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.16:14194 |      4 |          692.485 | 384000 | -4269.35 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
[2m[36m(pid=14194)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f6ea36e8588> -> 96 episodes
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 17
    apples_agent-0_mean: 3.78
    apples_agent-0_min: 0
    apples_agent-1_max: 31
    apples_agent-1_mean: 3.49
    apples_agent-1_min: 0
    apples_agent-2_max: 19
    apples_agent-2_mean: 3.84
    apples_agent-2_min: 0
    apples_agent-3_max: 26
    apples_agent-3_mean: 4.08
    apples_agent-3_min: 0
    apples_agent-4_max: 19
    apples_agent-4_mean: 3.03
    apples_agent-4_min: 0
    apples_agent-5_max: 19
    apples_agent-5_mean: 3.77
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 130
    cleaning_beam_agent-0_mean: 98.19
    cleaning_beam_agent-0_min: 78
    cleaning_beam_agent-1_max: 147
    cleaning_beam_agent-1_mean: 87.06
    cleaning_beam_agent-1_min: 64
    cleaning_beam_agent-2_max: 122
    cleaning_beam_agent-2_mean: 93.88
    cleaning_beam_agent-2_min: 68
    cleaning_beam_agent-3_max: 154
    cleaning_beam_agent-3_mean: 123.04
    cleaning_beam_agent-3_min: 94
    cleaning_beam_agent-4_max: 157
    cleaning_beam_agent-4_mean: 125.32
    cleaning_beam_agent-4_min: 99
    cleaning_beam_agent-5_max: 140
    cleaning_beam_agent-5_mean: 111.96
    cleaning_beam_agent-5_min: 90
    fire_beam_agent-0_max: 82
    fire_beam_agent-0_mean: 61.39
    fire_beam_agent-0_min: 40
    fire_beam_agent-1_max: 89
    fire_beam_agent-1_mean: 54.17
    fire_beam_agent-1_min: 36
    fire_beam_agent-2_max: 63
    fire_beam_agent-2_mean: 41.49
    fire_beam_agent-2_min: 18
    fire_beam_agent-3_max: 52
    fire_beam_agent-3_mean: 36.6
    fire_beam_agent-3_min: 21
    fire_beam_agent-4_max: 119
    fire_beam_agent-4_mean: 91.89
    fire_beam_agent-4_min: 73
    fire_beam_agent-5_max: 94
    fire_beam_agent-5_mean: 64.01
    fire_beam_agent-5_min: 47
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_17-11-10
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -1907.99999999998
  episode_reward_mean: -3624.050000000017
  episode_reward_min: -6721.999999999947
  episodes_this_iter: 96
  episodes_total: 480
  experiment_id: ae22031055194750b007235b78e0550b
  experiment_tag: '0'
  hostname: gpu016
  info:
    grad_time_ms: 14043.051
    learner:
      agent-0:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012360383989289403
        entropy: 2.1543776988983154
        entropy_coeff: 0.0017600000137463212
        kl: 0.003515241900458932
        model: {}
        policy_loss: -0.0006016488187015057
        total_loss: 0.15229369699954987
        vf_explained_var: -0.002420276403427124
        vf_loss: 1563.355224609375
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012360383989289403
        entropy: 2.0932092666625977
        entropy_coeff: 0.0017600000137463212
        kl: 0.010528774932026863
        model: {}
        policy_loss: -0.001452714204788208
        total_loss: 0.15315969288349152
        vf_explained_var: -0.002349942922592163
        vf_loss: 1577.7001953125
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012360383989289403
        entropy: 2.1153228282928467
        entropy_coeff: 0.0017600000137463212
        kl: 0.01848216913640499
        model: {}
        policy_loss: -0.0010595506755635142
        total_loss: 0.15313751995563507
        vf_explained_var: -0.00272369384765625
        vf_loss: 1569.9595947265625
      agent-3:
        cur_kl_coeff: 0.10000000149011612
        cur_lr: 0.0012360383989289403
        entropy: 2.122756004333496
        entropy_coeff: 0.0017600000137463212
        kl: 0.004058234393596649
        model: {}
        policy_loss: -0.001267509302124381
        total_loss: 0.16991490125656128
        vf_explained_var: -0.0020990967750549316
        vf_loss: 1745.126220703125
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012360383989289403
        entropy: 2.1609132289886475
        entropy_coeff: 0.0017600000137463212
        kl: 0.007936988025903702
        model: {}
        policy_loss: -0.001623824704438448
        total_loss: 0.16942761838436127
        vf_explained_var: -0.0020989179611206055
        vf_loss: 1744.578125
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012360383989289403
        entropy: 2.137371063232422
        entropy_coeff: 0.0017600000137463212
        kl: 0.008747713640332222
        model: {}
        policy_loss: -0.0009459368884563446
        total_loss: 0.169758141040802
        vf_explained_var: -0.002059429883956909
        vf_loss: 1740.28466796875
    load_time_ms: 42589.372
    num_steps_sampled: 480000
    num_steps_trained: 480000
    sample_time_ms: 110968.138
    update_time_ms: 713.851
  iterations_since_restore: 5
  node_ip: 172.17.8.16
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.24311926605505
    ram_util_percent: 13.697247706422019
  pid: 14194
  policy_reward_max:
    agent-0: -322.99999999999955
    agent-1: -322.99999999999955
    agent-2: -322.99999999999955
    agent-3: -231.66666666666728
    agent-4: -231.66666666666728
    agent-5: -231.66666666666728
  policy_reward_mean:
    agent-0: -609.1033333333334
    agent-1: -609.1033333333334
    agent-2: -609.1033333333334
    agent-3: -598.9133333333334
    agent-4: -598.9133333333334
    agent-5: -598.9133333333334
  policy_reward_min:
    agent-0: -1001.6666666666666
    agent-1: -1001.6666666666666
    agent-2: -1001.6666666666666
    agent-3: -1382.6666666666656
    agent-4: -1382.6666666666656
    agent-5: -1382.6666666666656
  sampler_perf:
    mean_env_wait_ms: 25.44643631296767
    mean_inference_ms: 13.165186931519207
    mean_processing_ms: 56.861498043871514
  time_since_restore: 845.7414102554321
  time_this_iter_s: 153.25638437271118
  time_total_s: 845.7414102554321
  timestamp: 1637187070
  timesteps_since_restore: 480000
  timesteps_this_iter: 96000
  timesteps_total: 480000
  training_iteration: 5
  trial_id: '00000'
  
== Status ==
Memory usage on this node: 22.4/172.9 GiB
Using FIFO scheduling algorithm.
Resources requested: 6/32 CPUs, 1.0/1 GPUs, 0.0/148.97 GiB heap, 0.0/33.5 GiB objects
Result logdir: /h/dtradke/ray_results/cleanup_baseline_PPO_2teams_6agents_custom_metrics
Number of trials: 1 (1 RUNNING)
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+
| Trial name                           | status   | loc               |   iter |   total time (s) |     ts |   reward |
|--------------------------------------+----------+-------------------+--------+------------------+--------+----------|
| BaselinePPOTrainer_cleanup_env_00000 | RUNNING  | 172.17.8.16:14194 |      5 |          845.741 | 480000 | -3624.05 |
+--------------------------------------+----------+-------------------+--------+------------------+--------+----------+


Result for BaselinePPOTrainer_cleanup_env_00000:
  callback_ok: true
  custom_metrics:
    apples_agent-0_max: 16
    apples_agent-0_mean: 3.94
    apples_agent-0_min: 0
    apples_agent-1_max: 37
    apples_agent-1_mean: 5.01
    apples_agent-1_min: 0
    apples_agent-2_max: 51
    apples_agent-2_mean: 4.06
    apples_agent-2_min: 0
    apples_agent-3_max: 23
    apples_agent-3_mean: 3.82
    apples_agent-3_min: 0
    apples_agent-4_max: 26
    apples_agent-4_mean: 4.53
    apples_agent-4_min: 0
    apples_agent-5_max: 20
    apples_agent-5_mean: 3.98
    apples_agent-5_min: 0
    cleaning_beam_agent-0_max: 126
    cleaning_beam_agent-0_mean: 103.25
    cleaning_beam_agent-0_min: 83
    cleaning_beam_agent-1_max: 106
    cleaning_beam_agent-1_mean: 73.34
    cleaning_beam_agent-1_min: 55
    cleaning_beam_agent-2_max: 113
    cleaning_beam_agent-2_mean: 92.55
    cleaning_beam_agent-2_min: 67
    cleaning_beam_agent-3_max: 186
    cleaning_beam_agent-3_mean: 160.64
    cleaning_beam_agent-3_min: 111
    cleaning_beam_agent-4_max: 156
    cleaning_beam_agent-4_mean: 127.86
    cleaning_beam_agent-4_min: 102
    cleaning_beam_agent-5_max: 173
    cleaning_beam_agent-5_mean: 141.4
    cleaning_beam_agent-5_min: 90
    fire_beam_agent-0_max: 80
    fire_beam_agent-0_mean: 58.0
    fire_beam_agent-0_min: 33
    fire_beam_agent-1_max: 65
    fire_beam_agent-1_mean: 40.55
    fire_beam_agent-1_min: 25
    fire_beam_agent-2_max: 51
    fire_beam_agent-2_mean: 31.01
    fire_beam_agent-2_min: 19
    fire_beam_agent-3_max: 50
    fire_beam_agent-3_mean: 29.15
    fire_beam_agent-3_min: 18
    fire_beam_agent-4_max: 97
    fire_beam_agent-4_mean: 67.25
    fire_beam_agent-4_min: 49
    fire_beam_agent-5_max: 73
    fire_beam_agent-5_mean: 48.45
    fire_beam_agent-5_min: 31
    num_batches_max: 6
    num_batches_mean: 6.0
    num_batches_min: 6
  date: 2021-11-17_17-13-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -1460.9999999999934
  episode_reward_mean: -2705.7900000000054
  episode_reward_min: -4710.000000000005
  episodes_this_iter: 96
  episodes_total: 576
  experiment_id: ae22031055194750b007235b78e0550b
  experiment_tag: '0'
  hostname: gpu016
  info:
    grad_time_ms: 13649.82
    learner:
      agent-0:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012300480157136917
        entropy: 2.149803400039673
        entropy_coeff: 0.0017600000137463212
        kl: 0.004560674540698528
        model: {}
        policy_loss: -0.0010640795808285475
        total_loss: 0.07323993742465973
        vf_explained_var: -0.0030930936336517334
        vf_loss: 778.5963745117188
      agent-1:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012300480157136917
        entropy: 2.060687303543091
        entropy_coeff: 0.0017600000137463212
        kl: 0.009531781077384949
        model: {}
        policy_loss: -0.001221235143020749
        total_loss: 0.07391911745071411
        vf_explained_var: -0.0029394924640655518
        vf_loss: 782.9058227539062
      agent-2:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012300480157136917
        entropy: 2.1190919876098633
        entropy_coeff: 0.0017600000137463212
        kl: 0.011707085184752941
        model: {}
        policy_loss: -0.001490608206950128
        total_loss: 0.07346097379922867
        vf_explained_var: -0.003351837396621704
        vf_loss: 780.9583129882812
      agent-3:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012300480157136917
        entropy: 2.0557382106781006
        entropy_coeff: 0.0017600000137463212
        kl: 0.011266972869634628
        model: {}
        policy_loss: -0.001855290844105184
        total_loss: 0.07009586691856384
        vf_explained_var: -0.0026679933071136475
        vf_loss: 750.0591430664062
      agent-4:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012300480157136917
        entropy: 2.1473476886749268
        entropy_coeff: 0.0017600000137463212
        kl: 0.003649833844974637
        model: {}
        policy_loss: -0.0012782672420144081
        total_loss: 0.06963285803794861
        vf_explained_var: -0.0030787289142608643
        vf_loss: 745.07958984375
      agent-5:
        cur_kl_coeff: 0.05000000074505806
        cur_lr: 0.0012300480157136917
        entropy: 2.1286840438842773
        entropy_coeff: 0.0017600000137463212
        kl: 0.009060217998921871
        model: {}
        policy_loss: -0.0008130474016070366
        total_loss: 0.07018495351076126
        vf_explained_var: -0.0029434263706207275
        vf_loss: 742.9146728515625
    load_time_ms: 40109.5
    num_steps_sampled: 576000
    num_steps_trained: 576000
    sample_time_ms: 110385.298
    update_time_ms: 602.691
  iterations_since_restore: 6
  node_ip: 172.17.8.16
  num_healthy_workers: 6
  off_policy_estimator: {}
  optimizer_steps_this_iter: 1
  perf:
    cpu_util_percent: 24.803827751196174
    ram_util_percent: 13.56842105263158
  pid: 14194
  policy_reward_max:
    agent-0: -201.66666666666686
    agent-1: -201.66666666666686
    agent-2: -201.66666666666686
    agent-3: -191.66666666666697
    agent-4: -191.66666666666697
    agent-5: -191.66666666666697
  policy_reward_mean:
    agent-0: -449.46999999999986
    agent-1: -449.46999999999986
    agent-2: -449.46999999999986
    agent-3: -452.45999999999987
    agent-4: -452.45999999999987
    agent-5: -452.45999999999987
  policy_reward_min:
    agent-0: -849.0000000000011
    agent-1: -849.0000000000011
    agent-2: -849.0000000000011
    agent-3: -818.0000000000014
    agent-4: -818.0000000000014
    agent-5: -818.0000000000014
  sampler_perf:
    mean_env_wait_ms: 25.291272259413127
    mean_inference_ms: 13.109134134446487
    mean_processing_ms: 56.82084911886181
  time_since_restore: 992.8130469322205
  time_this_iter_s: 147.07163667678833
  time_total_s: 992.8130469322205
  timestamp: 1637187218
  timesteps_since_restore: 576000
  timesteps_this_iter: 96000
  timesteps_total: 576000
  training_iteration: 6
  trial_id: '00000'
  
[2m[36m(pid=14194)[0m trainer.train() result: <ray.rllib.agents.trainer_template.BaselinePPOTrainer object at 0x7f6ea36e8588> -> 96 episodes
