 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-15 12:17:38,682	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.66 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-15 12:17:38,979	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 15709347840 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-15 12:24:09,559	WARNING util.py:137 -- The `fetch_result` operation took 1.4823517799377441 seconds to complete, which may be a performance bottleneck.
2021-11-15 12:24:12,139	WARNING util.py:137 -- The `process_trial` operation took 4.1751673221588135 seconds to complete, which may be a performance bottleneck.
2021-11-15 12:24:12,670	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.526339054107666 seconds to complete, which may be a performance bottleneck.
2021-11-15 12:45:36,925	WARNING worker.py:1090 -- The node with node id fbd6beb3b5205d67ce90b275a344208d9af323f0 has been marked dead because the detector has missed too many heartbeats from it.
2021-11-15 12:49:40,440	WARNING util.py:137 -- The `fetch_result` operation took 1.6247761249542236 seconds to complete, which may be a performance bottleneck.
2021-11-15 12:49:40,531	WARNING util.py:137 -- The `process_trial` operation took 1.7161006927490234 seconds to complete, which may be a performance bottleneck.
E1115 12:49:42.517872 28248 task_manager.cc:306] Task failed: IOError: 2: HandleServiceClosed: Type=ACTOR_TASK, Language=PYTHON, function_descriptor={type=PythonFunctionDescriptor, module_name=ray.rllib.agents.trainer_template, class_name=BaselinePPOTrainer, function_name=train, function_hash=}, task_id=55c3b2b635949d8145b95b1c0100, job_id=0100, num_args=0, num_returns=2, actor_task_spec={actor_id=45b95b1c0100, actor_caller_id=ffffffffffffffffffffffff0100, actor_counter=7}
2021-11-15 12:49:40,598	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #2...
2021-11-15 12:50:45,354	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #3...
2021-11-15 12:50:45,855	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #4...
2021-11-15 12:50:46,357	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #5...
2021-11-15 12:50:46,858	WARNING ray_trial_executor.py:497 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2021-11-15 12:50:46,859	WARNING util.py:137 -- The `on_step_begin` operation took 66.30545282363892 seconds to complete, which may be a performance bottleneck.
2021-11-15 12:50:46,870	WARNING ray_trial_executor.py:415 -- Over the last 60 seconds, the Tune event loop has been backlogged processing new results. Consider increasing your period of result reporting to improve performance.
2021-11-15 12:50:47,938	WARNING util.py:137 -- The `fetch_result` operation took 1.0678110122680664 seconds to complete, which may be a performance bottleneck.
2021-11-15 12:50:47,938	ERROR trial_runner.py:519 -- Trial BaselinePPOTrainer_cleanup_env_00000: Error processing event.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 467, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 431, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/worker.py", line 1517, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
E1115 12:50:48.471717 28248 task_manager.cc:306] Task failed: IOError: 14: failed to connect to all addresses: Type=ACTOR_TASK, Language=PYTHON, function_descriptor={type=PythonFunctionDescriptor, module_name=ray.rllib.agents.trainer_template, class_name=BaselinePPOTrainer, function_name=stop, function_hash=}, task_id=9fc77bf30b43899745b95b1c0100, job_id=0100, num_args=0, num_returns=2, actor_task_spec={actor_id=45b95b1c0100, actor_caller_id=ffffffffffffffffffffffff0100, actor_counter=8}
2021-11-15 12:50:48,718	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #2...
2021-11-15 12:50:49,220	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #3...
2021-11-15 12:50:49,721	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #4...
2021-11-15 12:50:50,223	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #5...
2021-11-15 12:50:50,724	WARNING ray_trial_executor.py:497 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2021-11-15 12:50:50,725	WARNING util.py:137 -- The `process_trial` operation took 3.855165719985962 seconds to complete, which may be a performance bottleneck.
2021-11-15 12:51:10,624	WARNING util.py:137 -- The `experiment_checkpoint` operation took 19.89886212348938 seconds to complete, which may be a performance bottleneck.
2021-11-15 12:51:10,624	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/experiment_state-2021-11-15_12-17-39.json'
2021-11-15 12:51:10,671	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #2...
2021-11-15 12:51:11,172	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #3...
2021-11-15 12:51:11,674	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #4...
2021-11-15 12:51:12,175	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #5...
2021-11-15 12:51:12,677	WARNING ray_trial_executor.py:497 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2021-11-15 12:51:12,677	WARNING util.py:137 -- The `on_step_begin` operation took 2.007101535797119 seconds to complete, which may be a performance bottleneck.
E1115 12:51:19.142113 28097 raylet_client.cc:90] IOError: [RayletClient] Connection closed unexpectedly. [RayletClient] Failed to disconnect from raylet.
Traceback (most recent call last):
  File "train.py", line 432, in <module>
    run(parsed_args, experiment)
  File "train.py", line 425, in run
    reuse_actors=args.tune_hparams,
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/tune.py", line 325, in run
    runner.step()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 341, in step
    self.trial_executor.on_no_available_trials(self)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_executor.py", line 175, in on_no_available_trials
    trial.config)))
ray.tune.error.TuneError: Insufficient cluster resources to launch trial: trial requested 6 CPUs, 1.0 GPUs but the cluster has only 0 CPUs, 0 GPUs, 0.0 GiB heap, 0.0 GiB objects. Pass `queue_trials=True` in ray.tune.run() or on the command line to queue trials until the cluster scales up or resources become available. 

You can adjust the resource requests of RLlib agents by setting `num_workers`, `num_gpus`, and other configs. See the DEFAULT_CONFIG defined by each agent for more info.

The config of this agent is: {'num_workers': 6, 'num_envs_per_worker': 16, 'rollout_fragment_length': 1000, 'sample_batch_size': -1, 'batch_mode': 'truncate_episodes', 'num_gpus': 1.0, 'train_batch_size': 96000, 'model': {'conv_filters': [[6, [3, 3], 1]], 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [32, 32], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'baseline_lstm', 'custom_action_dist': None, 'custom_options': {'cell_size': 128, 'num_other_agents': 5}, 'custom_preprocessor': None}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'soft_horizon': False, 'no_done_at_end': False, 'env_config': {'func_create': <function get_env_creator.<locals>.env_creator at 0x7f713a1579d8>, 'env_name': 'cleanup_env'}, 'env': 'cleanup_env', 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 0.0001, 'monitor': False, 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.examples.custom_metrics_and_callbacks.MyCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'use_pytorch': False, 'eager': False, 'eager_tracing': False, 'no_eager_on_workers': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'use_exec_api': False, 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 0, 'memory': 0, 'object_store_memory': 0, 'memory_per_worker': 0, 'object_store_memory_per_worker': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'agent-0': (None, Dict(curr_obs:Box(15, 15, 3), other_agent_actions:Box(5,), prev_visible_agents:Box(5,), visible_agents:Box(5,)), Discrete(9), {'custom_model': 'baseline_lstm'}), 'agent-1': (None, Dict(curr_obs:Box(15, 15, 3), other_agent_actions:Box(5,), prev_visible_agents:Box(5,), visible_agents:Box(5,)), Discrete(9), {'custom_model': 'baseline_lstm'}), 'agent-2': (None, Dict(curr_obs:Box(15, 15, 3), other_agent_actions:Box(5,), prev_visible_agents:Box(5,), visible_agents:Box(5,)), Discrete(9), {'custom_model': 'baseline_lstm'}), 'agent-3': (None, Dict(curr_obs:Box(15, 15, 3), other_agent_actions:Box(5,), prev_visible_agents:Box(5,), visible_agents:Box(5,)), Discrete(9), {'custom_model': 'baseline_lstm'}), 'agent-4': (None, Dict(curr_obs:Box(15, 15, 3), other_agent_actions:Box(5,), prev_visible_agents:Box(5,), visible_agents:Box(5,)), Discrete(9), {'custom_model': 'baseline_lstm'}), 'agent-5': (None, Dict(curr_obs:Box(15, 15, 3), other_agent_actions:Box(5,), prev_visible_agents:Box(5,), visible_agents:Box(5,)), Discrete(9), {'custom_model': 'baseline_lstm'})}, 'policy_mapping_fn': <function build_experiment_config_dict.<locals>.policy_mapping_fn at 0x7f713a157bf8>, 'policies_to_train': None}, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 24000.0, 'shuffle_sequences': True, 'num_sgd_iter': 10, 'lr_schedule': [(0, 0.00126), (20000000, 1.2e-05)], 'vf_share_layers': True, 'vf_loss_coeff': 0.0001, 'entropy_coeff': 0.00176, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': 40, 'kl_target': 0.01, 'simple_optimizer': False, '_fake_gpus': False}
