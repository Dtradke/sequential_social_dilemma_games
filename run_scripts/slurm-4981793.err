 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-15 12:18:39,854	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 52.04 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-15 12:18:40,149	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 15709335552 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-15 12:24:51,456	WARNING util.py:137 -- The `fetch_result` operation took 1.0405347347259521 seconds to complete, which may be a performance bottleneck.
2021-11-15 12:24:53,377	WARNING util.py:137 -- The `process_trial` operation took 3.0597567558288574 seconds to complete, which may be a performance bottleneck.
2021-11-15 12:24:54,038	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.655815601348877 seconds to complete, which may be a performance bottleneck.
2021-11-15 12:45:32,676	WARNING worker.py:1090 -- The node with node id 1b6409b35eaaf5154808edcbcc61ec6a0263f142 has been marked dead because the detector has missed too many heartbeats from it.
2021-11-15 12:49:25,931	WARNING util.py:137 -- The `process_trial` operation took 0.5031085014343262 seconds to complete, which may be a performance bottleneck.
E1115 12:49:27.644018 28983 task_manager.cc:306] Task failed: IOError: 2: HandleServiceClosed: Type=ACTOR_TASK, Language=PYTHON, function_descriptor={type=PythonFunctionDescriptor, module_name=ray.rllib.agents.trainer_template, class_name=BaselinePPOTrainer, function_name=train, function_hash=}, task_id=55c3b2b635949d8145b95b1c0100, job_id=0100, num_args=0, num_returns=2, actor_task_spec={actor_id=45b95b1c0100, actor_caller_id=ffffffffffffffffffffffff0100, actor_counter=7}
2021-11-15 12:49:26,182	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #2...
2021-11-15 12:50:20,003	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #3...
2021-11-15 12:50:20,504	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #4...
2021-11-15 12:50:21,006	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #5...
2021-11-15 12:50:21,507	WARNING ray_trial_executor.py:497 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2021-11-15 12:50:21,508	WARNING util.py:137 -- The `on_step_begin` operation took 55.546348571777344 seconds to complete, which may be a performance bottleneck.
2021-11-15 12:50:26,046	WARNING util.py:137 -- The `fetch_result` operation took 4.537297010421753 seconds to complete, which may be a performance bottleneck.
2021-11-15 12:50:26,046	ERROR trial_runner.py:519 -- Trial BaselinePPOTrainer_cleanup_env_00000: Error processing event.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 467, in _process_trial
    result = self.trial_executor.fetch_result(trial)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 431, in fetch_result
    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/worker.py", line 1517, in get
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
E1115 12:50:28.998256 28983 task_manager.cc:306] Task failed: IOError: 14: failed to connect to all addresses: Type=ACTOR_TASK, Language=PYTHON, function_descriptor={type=PythonFunctionDescriptor, module_name=ray.rllib.agents.trainer_template, class_name=BaselinePPOTrainer, function_name=stop, function_hash=}, task_id=9fc77bf30b43899745b95b1c0100, job_id=0100, num_args=0, num_returns=2, actor_task_spec={actor_id=45b95b1c0100, actor_caller_id=ffffffffffffffffffffffff0100, actor_counter=8}
2021-11-15 12:50:31,946	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #2...
2021-11-15 12:50:32,448	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #3...
2021-11-15 12:50:32,949	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #4...
2021-11-15 12:50:33,451	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #5...
2021-11-15 12:50:33,952	WARNING ray_trial_executor.py:497 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2021-11-15 12:50:33,953	WARNING util.py:137 -- The `process_trial` operation took 12.444161891937256 seconds to complete, which may be a performance bottleneck.
2021-11-15 12:51:02,980	WARNING util.py:137 -- The `experiment_checkpoint` operation took 29.027575492858887 seconds to complete, which may be a performance bottleneck.
2021-11-15 12:51:03,125	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #2...
2021-11-15 12:51:03,627	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #3...
2021-11-15 12:51:04,128	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #4...
2021-11-15 12:51:04,630	WARNING ray_trial_executor.py:480 -- Cluster resources not detected or are 0. Attempt #5...
2021-11-15 12:51:05,131	WARNING ray_trial_executor.py:497 -- Cluster resources cannot be detected or are 0. You can resume this experiment by passing in `resume=True` to `run`.
2021-11-15 12:51:05,131	WARNING util.py:137 -- The `on_step_begin` operation took 2.0068392753601074 seconds to complete, which may be a performance bottleneck.
E1115 12:51:08.722595 28828 raylet_client.cc:90] IOError: [RayletClient] Connection closed unexpectedly. [RayletClient] Failed to disconnect from raylet.
Traceback (most recent call last):
  File "train.py", line 432, in <module>
    run(parsed_args, experiment)
  File "train.py", line 425, in run
    reuse_actors=args.tune_hparams,
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/tune.py", line 325, in run
    runner.step()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 341, in step
    self.trial_executor.on_no_available_trials(self)
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_executor.py", line 175, in on_no_available_trials
    trial.config)))
ray.tune.error.TuneError: Insufficient cluster resources to launch trial: trial requested 6 CPUs, 1.0 GPUs but the cluster has only 0 CPUs, 0 GPUs, 0.0 GiB heap, 0.0 GiB objects. Pass `queue_trials=True` in ray.tune.run() or on the command line to queue trials until the cluster scales up or resources become available. 

You can adjust the resource requests of RLlib agents by setting `num_workers`, `num_gpus`, and other configs. See the DEFAULT_CONFIG defined by each agent for more info.

The config of this agent is: {'num_workers': 6, 'num_envs_per_worker': 16, 'rollout_fragment_length': 1000, 'sample_batch_size': -1, 'batch_mode': 'truncate_episodes', 'num_gpus': 1.0, 'train_batch_size': 96000, 'model': {'conv_filters': [[6, [3, 3], 1]], 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [32, 32], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': 'baseline_lstm', 'custom_action_dist': None, 'custom_options': {'cell_size': 128, 'num_other_agents': 5}, 'custom_preprocessor': None}, 'optimizer': {}, 'gamma': 0.99, 'horizon': 1000, 'soft_horizon': False, 'no_done_at_end': False, 'env_config': {'func_create': <function get_env_creator.<locals>.env_creator at 0x7f683812ea60>, 'env_name': 'cleanup_env'}, 'env': 'cleanup_env', 'normalize_actions': False, 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 0.0001, 'monitor': False, 'log_level': 'WARN', 'callbacks': <class 'ray.rllib.examples.custom_metrics_and_callbacks.MyCallbacks'>, 'ignore_worker_failures': False, 'log_sys_usage': True, 'use_pytorch': False, 'eager': False, 'eager_tracing': False, 'no_eager_on_workers': False, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'use_exec_api': False, 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 0, 'timesteps_per_iteration': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0.0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 0, 'memory': 0, 'object_store_memory': 0, 'memory_per_worker': 0, 'object_store_memory_per_worker': 0, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'agent-0': (None, Dict(curr_obs:Box(15, 15, 3), other_agent_actions:Box(5,), prev_visible_agents:Box(5,), visible_agents:Box(5,)), Discrete(9), {'custom_model': 'baseline_lstm'}), 'agent-1': (None, Dict(curr_obs:Box(15, 15, 3), other_agent_actions:Box(5,), prev_visible_agents:Box(5,), visible_agents:Box(5,)), Discrete(9), {'custom_model': 'baseline_lstm'}), 'agent-2': (None, Dict(curr_obs:Box(15, 15, 3), other_agent_actions:Box(5,), prev_visible_agents:Box(5,), visible_agents:Box(5,)), Discrete(9), {'custom_model': 'baseline_lstm'}), 'agent-3': (None, Dict(curr_obs:Box(15, 15, 3), other_agent_actions:Box(5,), prev_visible_agents:Box(5,), visible_agents:Box(5,)), Discrete(9), {'custom_model': 'baseline_lstm'}), 'agent-4': (None, Dict(curr_obs:Box(15, 15, 3), other_agent_actions:Box(5,), prev_visible_agents:Box(5,), visible_agents:Box(5,)), Discrete(9), {'custom_model': 'baseline_lstm'}), 'agent-5': (None, Dict(curr_obs:Box(15, 15, 3), other_agent_actions:Box(5,), prev_visible_agents:Box(5,), visible_agents:Box(5,)), Discrete(9), {'custom_model': 'baseline_lstm'})}, 'policy_mapping_fn': <function build_experiment_config_dict.<locals>.policy_mapping_fn at 0x7f683812ec80>, 'policies_to_train': None}, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 24000.0, 'shuffle_sequences': True, 'num_sgd_iter': 10, 'lr_schedule': [(0, 0.00126), (20000000, 1.2e-05)], 'vf_share_layers': True, 'vf_loss_coeff': 0.0001, 'entropy_coeff': 0.00176, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': 40, 'kl_target': 0.01, 'simple_optimizer': False, '_fake_gpus': False}
