 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 17:39:28,864	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 51.95 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 17:39:29,149	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 0 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 17:39:29,860	WARNING logger.py:328 -- Could not instantiate TBXLogger: [Errno 28] No space left on device.
slurmstepd: error: *** JOB 5006755 ON gpu148 CANCELLED AT 2021-11-17T18:44:57 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 18:47:23,523	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 53.91 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 18:47:23,813	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21168746496 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 18:47:24,515	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 18:47:24,670	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 18:47:24,670	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 18:47:24,807	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
slurmstepd: error: *** JOB 5006755 ON gpu051 CANCELLED AT 2021-11-17T19:51:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-17 19:54:07,270	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 54.17 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-17 19:54:07,535	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 18505134080 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-17 19:54:08,264	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-17 19:54:08,466	INFO trial_runner.py:169 -- Resuming trial.
2021-11-17 19:54:08,466	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-17 19:54:08,658	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-17 20:43:08,367	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.1305060386657715 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 5006755 ON gpu026 CANCELLED AT 2021-11-18T01:11:01 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 01:14:05,652	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 54.56 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 01:14:05,920	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21472919552 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 01:14:07,070	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 01:14:07,741	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 01:14:07,741	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 01:14:08,021	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-18 02:12:40,037	WARNING util.py:137 -- The `experiment_checkpoint` operation took 1.0998079776763916 seconds to complete, which may be a performance bottleneck.
slurmstepd: error: *** JOB 5006755 ON gpu006 CANCELLED AT 2021-11-18T02:17:00 DUE TO PREEMPTION ***
 Successfully loaded module "tensorflow2-gpu-cuda10.1-conda-python3.6"
2021-11-18 02:20:05,478	INFO resource_spec.py:212 -- Starting Ray with 148.97 GiB memory available for workers and up to 54.01 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).
2021-11-18 02:20:05,743	WARNING services.py:1494 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 21471653888 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.
2021-11-18 02:20:06,892	WARNING trial_runner.py:300 -- Attempting to resume experiment from /h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics. This feature is experimental, and may not work with all search algorithms. This will ignore any new changes to the specification.
2021-11-18 02:20:07,565	INFO trial_runner.py:169 -- Resuming trial.
2021-11-18 02:20:07,565	INFO trial_runner.py:246 -- TrialRunner resumed, ignoring new add_experiment.
2021-11-18 02:20:07,873	WARNING ray_trial_executor.py:674 -- Trial BaselinePPOTrainer_cleanup_env_00000: Reading checkpoint into memory.
2021-11-18 02:28:10,693	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/experiment_state-2021-11-18_02-20-07.json'
2021-11-18 05:47:13,113	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/experiment_state-2021-11-18_02-20-07.json'
2021-11-18 06:33:26,645	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5540914535522461 seconds to complete, which may be a performance bottleneck.
2021-11-18 07:19:30,295	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5396084785461426 seconds to complete, which may be a performance bottleneck.
2021-11-18 07:42:27,484	ERROR trial_runner.py:349 -- Trial Runner checkpointing failed.
Traceback (most recent call last):
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 347, in step
    self.checkpoint()
  File "/ssd003/home/dtradke/sequential_social_dilemma_games/venv/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 278, in checkpoint
    os.rename(tmp_file_name, self.checkpoint_file)
FileNotFoundError: [Errno 2] No such file or directory: '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/.tmp_checkpoint' -> '/h/dtradke/ray_results/cleanup_baseline_PPO_6teams_6agents_custom_metrics/experiment_state-2021-11-18_02-20-07.json'
2021-11-18 07:58:39,006	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5274674892425537 seconds to complete, which may be a performance bottleneck.
2021-11-18 08:39:37,109	WARNING util.py:137 -- The `experiment_checkpoint` operation took 0.5448567867279053 seconds to complete, which may be a performance bottleneck.
2021-11-18 08:58:20,530	WARNING util.py:137 -- The `process_trial_save` operation took 0.5040721893310547 seconds to complete, which may be a performance bottleneck.
2021-11-18 08:58:20,531	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
2021-11-18 09:21:36,875	WARNING util.py:137 -- The `process_trial_save` operation took 0.7027394771575928 seconds to complete, which may be a performance bottleneck.
2021-11-18 09:21:36,876	WARNING trial_runner.py:445 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.
slurmstepd: error: *** JOB 5006755 ON gpu003 CANCELLED AT 2021-11-18T09:36:52 ***
